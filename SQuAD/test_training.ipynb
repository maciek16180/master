{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from HRED_v2 import HRED\n",
    "from squad_load import get_glove_train_embs, get_squad_train_voc, load_squad_train\n",
    "\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "squad_path = '/pio/data/data/squad/'\n",
    "glove_path = '/pio/data/data/glove_vec/6B/'\n",
    "\n",
    "data = load_squad_train(squad_path)\n",
    "i_to_w, w_to_i, voc_size = get_squad_train_voc(squad_path)\n",
    "glove_embs = get_glove_train_embs(squad_path, glove_path)\n",
    "\n",
    "def get_w(idx):\n",
    "    return i_to_w[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87599, 86789)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def trim_data(data, trim=100):\n",
    "    return [d for d in data if max(map(len, d[1])) <= trim]\n",
    "\n",
    "data_trimmed = trim_data(data)\n",
    "len(data), len(data_trimmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model...\n",
      "Compiling theano functions...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "hred_net = HRED(voc_size=voc_size,\n",
    "                emb_size=300,\n",
    "                lv1_rec_size=300,\n",
    "                lv2_rec_size=300,\n",
    "                out_emb_size=300,\n",
    "                emb_init=glove_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.01092721,  0.00748011,  0.00762079,  0.01578316,  0.01117014,\n",
       "         0.01161463,  0.02582562,  0.01773244,  0.00615017], dtype=float32),\n",
       " array([ 0.00799526,  0.00572318,  0.00518265,  0.00791706,  0.01088359,\n",
       "         0.02770218,  0.02637224,  0.00865242,  0.0114931 ,  0.02562324,\n",
       "         0.02522738,  0.01880244,  0.01147158,  0.0155955 ,  0.01875451,\n",
       "         0.00431506], dtype=float32),\n",
       " array([ 0.00283376,  0.00202473,  0.00628567,  0.00842987,  0.00693043,\n",
       "         0.00607813,  0.00879546,  0.01265835,  0.01486384,  0.00704044,\n",
       "         0.0058486 ,  0.00434419,  0.00848916,  0.02093938,  0.02297711,\n",
       "         0.01494346,  0.01414375,  0.00701558,  0.01150942,  0.01826271,\n",
       "         0.00650786,  0.0057557 ,  0.0118857 ,  0.0068161 ,  0.02206977,\n",
       "         0.02071751,  0.01085666,  0.03032649,  0.00578328,  0.00204773], dtype=float32),\n",
       " array([ 0.00315659,  0.00204183,  0.00317063,  0.0037629 ,  0.00659333,\n",
       "         0.00453729,  0.00695357,  0.0258143 ,  0.01929855,  0.01103147,\n",
       "         0.02221514,  0.01778433,  0.00412622], dtype=float32),\n",
       " array([ 0.00226612,  0.00422287,  0.0032391 ,  0.01068364,  0.0036723 ,\n",
       "         0.00446924,  0.02401366,  0.00596373,  0.00642114,  0.03123824,\n",
       "         0.00954309,  0.00911377,  0.01174448,  0.01051481,  0.01069116,\n",
       "         0.00216034], dtype=float32),\n",
       " array([ 0.00144189,  0.00128125,  0.00277097,  0.01201042,  0.00711844,\n",
       "         0.00513171,  0.02022562,  0.00559598,  0.01813137,  0.00537653,\n",
       "         0.01040654,  0.00329347,  0.00401872,  0.00986178,  0.01497364,\n",
       "         0.02897604,  0.00702998,  0.00292034,  0.01184475,  0.02359263,\n",
       "         0.02497219,  0.00352096,  0.01526059,  0.00149232], dtype=float32),\n",
       " array([ 0.00183194,  0.00184392,  0.00310573,  0.00397697,  0.00301169,\n",
       "         0.00255903,  0.0057642 ,  0.00387531,  0.00579834,  0.00263868,\n",
       "         0.0044444 ,  0.00589541,  0.00669848,  0.0026088 ,  0.00567614,\n",
       "         0.0039336 ,  0.01838839,  0.01332028,  0.00780502,  0.00371904,\n",
       "         0.01152982,  0.00919361,  0.003619  ,  0.00227233,  0.001439  ,\n",
       "         0.00297902,  0.00438986,  0.00273083,  0.00388463,  0.00565312,\n",
       "         0.00870922,  0.00615351,  0.01061668,  0.00158571], dtype=float32)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = hred_net.get_output(data[:12], 3)\n",
    "ans[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05232447"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(chain(*ans[8]))[46]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'september'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_w(list(chain(*data[8][1][1:]))[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[46,\n",
       " 23,\n",
       " 45,\n",
       " 74,\n",
       " 7,\n",
       " 19,\n",
       " 49,\n",
       " 8,\n",
       " 6,\n",
       " 126,\n",
       " 10,\n",
       " 24,\n",
       " 28,\n",
       " 80,\n",
       " 9,\n",
       " 29,\n",
       " 54,\n",
       " 43,\n",
       " 31,\n",
       " 37,\n",
       " 4,\n",
       " 42,\n",
       " 123,\n",
       " 82,\n",
       " 11,\n",
       " 30,\n",
       " 111,\n",
       " 94,\n",
       " 16,\n",
       " 35,\n",
       " 81,\n",
       " 15,\n",
       " 50,\n",
       " 148,\n",
       " 62,\n",
       " 78,\n",
       " 53,\n",
       " 91,\n",
       " 204,\n",
       " 33,\n",
       " 13,\n",
       " 12,\n",
       " 105,\n",
       " 21,\n",
       " 20,\n",
       " 155,\n",
       " 152,\n",
       " 122,\n",
       " 36,\n",
       " 158,\n",
       " 125,\n",
       " 203,\n",
       " 217,\n",
       " 5,\n",
       " 22,\n",
       " 84,\n",
       " 14,\n",
       " 87,\n",
       " 27,\n",
       " 34,\n",
       " 3,\n",
       " 85,\n",
       " 107,\n",
       " 222,\n",
       " 186,\n",
       " 79,\n",
       " 25,\n",
       " 110,\n",
       " 128,\n",
       " 56,\n",
       " 175,\n",
       " 60,\n",
       " 127,\n",
       " 131,\n",
       " 52,\n",
       " 117,\n",
       " 63,\n",
       " 149,\n",
       " 112,\n",
       " 47,\n",
       " 26,\n",
       " 66,\n",
       " 55,\n",
       " 2,\n",
       " 32,\n",
       " 86,\n",
       " 197,\n",
       " 171,\n",
       " 151,\n",
       " 61,\n",
       " 108,\n",
       " 83,\n",
       " 137,\n",
       " 119,\n",
       " 124,\n",
       " 48,\n",
       " 67,\n",
       " 166,\n",
       " 201,\n",
       " 134,\n",
       " 239,\n",
       " 1,\n",
       " 172,\n",
       " 44,\n",
       " 236,\n",
       " 90,\n",
       " 233,\n",
       " 106,\n",
       " 77,\n",
       " 240,\n",
       " 104,\n",
       " 176,\n",
       " 71,\n",
       " 141,\n",
       " 75,\n",
       " 0,\n",
       " 227,\n",
       " 142,\n",
       " 101,\n",
       " 109,\n",
       " 198,\n",
       " 147,\n",
       " 202,\n",
       " 154,\n",
       " 73,\n",
       " 207,\n",
       " 18,\n",
       " 235,\n",
       " 226,\n",
       " 93,\n",
       " 65,\n",
       " 59,\n",
       " 150,\n",
       " 120,\n",
       " 194,\n",
       " 113,\n",
       " 195,\n",
       " 205,\n",
       " 121,\n",
       " 118,\n",
       " 153,\n",
       " 57,\n",
       " 232,\n",
       " 39,\n",
       " 17,\n",
       " 216,\n",
       " 132,\n",
       " 41,\n",
       " 140,\n",
       " 247,\n",
       " 116,\n",
       " 178,\n",
       " 64,\n",
       " 97,\n",
       " 179,\n",
       " 100,\n",
       " 58,\n",
       " 224,\n",
       " 76,\n",
       " 170,\n",
       " 70,\n",
       " 69,\n",
       " 51,\n",
       " 243,\n",
       " 103,\n",
       " 133,\n",
       " 169,\n",
       " 136,\n",
       " 218,\n",
       " 237,\n",
       " 238,\n",
       " 143,\n",
       " 72,\n",
       " 174,\n",
       " 167,\n",
       " 102,\n",
       " 225,\n",
       " 177,\n",
       " 200,\n",
       " 89,\n",
       " 181,\n",
       " 114,\n",
       " 146,\n",
       " 206,\n",
       " 144,\n",
       " 221,\n",
       " 173,\n",
       " 38,\n",
       " 139,\n",
       " 92,\n",
       " 99,\n",
       " 190,\n",
       " 242,\n",
       " 135,\n",
       " 193,\n",
       " 165,\n",
       " 191,\n",
       " 162,\n",
       " 163,\n",
       " 40,\n",
       " 168,\n",
       " 145,\n",
       " 130,\n",
       " 196,\n",
       " 183,\n",
       " 96,\n",
       " 199,\n",
       " 187,\n",
       " 234,\n",
       " 115,\n",
       " 180,\n",
       " 241,\n",
       " 95,\n",
       " 212,\n",
       " 244,\n",
       " 245,\n",
       " 156,\n",
       " 138,\n",
       " 246,\n",
       " 229,\n",
       " 210,\n",
       " 159,\n",
       " 189,\n",
       " 220,\n",
       " 188,\n",
       " 161,\n",
       " 88,\n",
       " 219,\n",
       " 223,\n",
       " 98,\n",
       " 160,\n",
       " 68,\n",
       " 215,\n",
       " 209,\n",
       " 228,\n",
       " 192,\n",
       " 129,\n",
       " 164,\n",
       " 213,\n",
       " 185,\n",
       " 184,\n",
       " 214,\n",
       " 231,\n",
       " 157,\n",
       " 230,\n",
       " 182,\n",
       " 208,\n",
       " 248,\n",
       " 211]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(reversed(np.array(list(chain(*ans[8]))).argsort()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Zwykły trening nie robił nic: wartości zwracane przez sieć dla zbioru uczącego nie miały żadnego sensu.\n",
    "# Największe prawdopodobieństwa były prawie niezależne od pytania, miało ono mały wpływ na to, co się dzieje.\n",
    "# Dla kosztu weighted_bin_ce mocno skrzywionego w stronę sytuacji t=1 wyszło podobnie.\n",
    "#\n",
    "# Następny krok: użyć zanurzeń GloVe, dodać cechy z https://arxiv.org/abs/1703.04816\n",
    "#\n",
    "# UPDATE:\n",
    "# Obie cechy podobieństwa kontekstu do pytania dodane.\n",
    "# W glove.6B jest około 70% słów ze zbioru uczącego SQuAD.\n",
    "# W tej chwili te wektory biorę z glove.6B.300d, a pozostałym daję losowy init, \n",
    "# po czym wszystkie zanurzenia są dalej uczone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 20 batches in 7.96s\ttraining loss:\t0.813393\n",
      "Done 40 batches in 16.89s\ttraining loss:\t0.618499\n",
      "Done 60 batches in 25.51s\ttraining loss:\t0.537964\n",
      "Done 80 batches in 33.72s\ttraining loss:\t0.476587\n",
      "Done 100 batches in 41.79s\ttraining loss:\t0.436442\n",
      "Done 120 batches in 48.56s\ttraining loss:\t0.419979\n",
      "Done 140 batches in 57.10s\ttraining loss:\t0.409618\n",
      "Done 160 batches in 65.07s\ttraining loss:\t0.408088\n",
      "Done 180 batches in 74.05s\ttraining loss:\t0.397444\n",
      "Done 200 batches in 81.68s\ttraining loss:\t0.401948\n",
      "Done 220 batches in 90.27s\ttraining loss:\t0.401396\n",
      "Done 240 batches in 98.95s\ttraining loss:\t0.410639\n",
      "Done 260 batches in 107.43s\ttraining loss:\t0.418680\n",
      "Done 280 batches in 115.18s\ttraining loss:\t0.427639\n",
      "Done 300 batches in 123.33s\ttraining loss:\t0.426674\n",
      "Done 320 batches in 131.00s\ttraining loss:\t0.422292\n",
      "Done 340 batches in 139.84s\ttraining loss:\t0.424456\n",
      "Done 360 batches in 148.91s\ttraining loss:\t0.415105\n",
      "Done 380 batches in 156.67s\ttraining loss:\t0.416838\n",
      "Done 400 batches in 163.90s\ttraining loss:\t0.418870\n",
      "Done 420 batches in 171.42s\ttraining loss:\t0.419026\n",
      "Done 440 batches in 180.23s\ttraining loss:\t0.417331\n",
      "Done 460 batches in 189.66s\ttraining loss:\t0.419791\n",
      "Done 480 batches in 198.85s\ttraining loss:\t0.420404\n",
      "Done 500 batches in 206.97s\ttraining loss:\t0.416177\n",
      "Done 520 batches in 214.49s\ttraining loss:\t0.415342\n",
      "Done 540 batches in 222.22s\ttraining loss:\t0.416872\n",
      "Done 560 batches in 229.79s\ttraining loss:\t0.416093\n",
      "Done 580 batches in 237.12s\ttraining loss:\t0.408871\n",
      "Done 600 batches in 244.83s\ttraining loss:\t0.408386\n",
      "Done 620 batches in 252.93s\ttraining loss:\t0.406717\n",
      "Done 640 batches in 260.90s\ttraining loss:\t0.407531\n",
      "Done 660 batches in 269.79s\ttraining loss:\t0.405836\n",
      "Done 680 batches in 277.40s\ttraining loss:\t0.407107\n",
      "Done 700 batches in 284.92s\ttraining loss:\t0.408122\n",
      "Done 720 batches in 292.42s\ttraining loss:\t0.411832\n",
      "Done 740 batches in 300.17s\ttraining loss:\t0.412702\n",
      "Done 760 batches in 308.85s\ttraining loss:\t0.413090\n",
      "Done 780 batches in 317.01s\ttraining loss:\t0.412683\n",
      "Done 800 batches in 325.79s\ttraining loss:\t0.411818\n",
      "Done 820 batches in 334.14s\ttraining loss:\t0.410459\n",
      "Done 840 batches in 343.48s\ttraining loss:\t0.407757\n",
      "Done 860 batches in 351.91s\ttraining loss:\t0.407190\n",
      "Done 880 batches in 358.82s\ttraining loss:\t0.406966\n",
      "Done 900 batches in 366.59s\ttraining loss:\t0.405027\n",
      "Done 920 batches in 375.56s\ttraining loss:\t0.404316\n",
      "Done 940 batches in 383.60s\ttraining loss:\t0.402061\n",
      "Done 960 batches in 392.46s\ttraining loss:\t0.399332\n",
      "Done 980 batches in 400.47s\ttraining loss:\t0.404043\n",
      "Done 1000 batches in 407.74s\ttraining loss:\t0.409821\n",
      "Done 1020 batches in 415.21s\ttraining loss:\t0.416582\n",
      "Done 1040 batches in 422.79s\ttraining loss:\t0.422596\n",
      "Done 1060 batches in 431.41s\ttraining loss:\t0.423714\n",
      "Done 1080 batches in 439.96s\ttraining loss:\t0.424034\n",
      "Done 1100 batches in 448.43s\ttraining loss:\t0.427439\n",
      "Done 1120 batches in 457.71s\ttraining loss:\t0.429270\n",
      "Done 1140 batches in 466.58s\ttraining loss:\t0.433387\n",
      "Done 1160 batches in 475.75s\ttraining loss:\t0.436254\n",
      "Done 1180 batches in 482.82s\ttraining loss:\t0.441358\n",
      "Done 1200 batches in 491.42s\ttraining loss:\t0.438379\n",
      "Done 1220 batches in 499.96s\ttraining loss:\t0.435319\n",
      "Done 1240 batches in 508.77s\ttraining loss:\t0.434273\n",
      "Done 1260 batches in 517.57s\ttraining loss:\t0.433936\n",
      "Done 1280 batches in 526.22s\ttraining loss:\t0.436965\n",
      "Done 1300 batches in 533.97s\ttraining loss:\t0.438821\n",
      "Done 1320 batches in 541.44s\ttraining loss:\t0.439510\n",
      "Done 1340 batches in 549.06s\ttraining loss:\t0.438974\n",
      "Done 1360 batches in 558.08s\ttraining loss:\t0.437469\n",
      "Done 1380 batches in 566.40s\ttraining loss:\t0.437325\n",
      "Done 1400 batches in 574.89s\ttraining loss:\t0.435750\n",
      "Done 1420 batches in 582.96s\ttraining loss:\t0.433850\n",
      "Done 1440 batches in 590.76s\ttraining loss:\t0.434015\n",
      "Done 1460 batches in 598.12s\ttraining loss:\t0.434544\n",
      "Done 1480 batches in 604.34s\ttraining loss:\t0.434400\n",
      "Done 1500 batches in 611.19s\ttraining loss:\t0.434535\n",
      "Done 1520 batches in 619.96s\ttraining loss:\t0.432634\n",
      "Done 1540 batches in 628.15s\ttraining loss:\t0.431123\n",
      "Done 1560 batches in 636.14s\ttraining loss:\t0.430235\n",
      "Done 1580 batches in 644.25s\ttraining loss:\t0.429545\n",
      "Done 1600 batches in 652.91s\ttraining loss:\t0.429710\n",
      "Done 1620 batches in 659.96s\ttraining loss:\t0.429660\n",
      "Done 1640 batches in 667.16s\ttraining loss:\t0.428866\n",
      "Done 1660 batches in 675.28s\ttraining loss:\t0.425995\n",
      "Done 1680 batches in 683.37s\ttraining loss:\t0.424745\n",
      "Done 1700 batches in 692.16s\ttraining loss:\t0.422782\n",
      "Done 1720 batches in 700.82s\ttraining loss:\t0.423887\n",
      "Done 1740 batches in 709.13s\ttraining loss:\t0.430753\n",
      "Done 1760 batches in 719.00s\ttraining loss:\t0.434818\n",
      "Done 1780 batches in 726.48s\ttraining loss:\t0.441409\n",
      "Done 1800 batches in 734.22s\ttraining loss:\t0.441740\n",
      "Done 1820 batches in 742.53s\ttraining loss:\t0.441645\n",
      "Done 1840 batches in 752.19s\ttraining loss:\t0.441036\n",
      "Done 1860 batches in 760.79s\ttraining loss:\t0.440892\n",
      "Done 1880 batches in 768.95s\ttraining loss:\t0.439639\n",
      "Done 1900 batches in 776.61s\ttraining loss:\t0.439933\n",
      "Done 1920 batches in 784.30s\ttraining loss:\t0.439975\n",
      "Done 1940 batches in 790.83s\ttraining loss:\t0.440051\n",
      "Done 1960 batches in 799.31s\ttraining loss:\t0.439937\n",
      "Done 1980 batches in 807.31s\ttraining loss:\t0.439576\n",
      "Done 2000 batches in 815.80s\ttraining loss:\t0.439899\n",
      "Done 2020 batches in 823.95s\ttraining loss:\t0.441725\n",
      "Done 2040 batches in 832.07s\ttraining loss:\t0.442261\n",
      "Done 2060 batches in 841.26s\ttraining loss:\t0.443953\n",
      "Done 2080 batches in 849.59s\ttraining loss:\t0.445274\n",
      "Done 2100 batches in 857.71s\ttraining loss:\t0.445195\n",
      "Done 2120 batches in 865.53s\ttraining loss:\t0.444672\n",
      "Done 2140 batches in 873.78s\ttraining loss:\t0.443986\n",
      "Done 2160 batches in 882.04s\ttraining loss:\t0.444449\n",
      "Done 2180 batches in 889.96s\ttraining loss:\t0.445268\n",
      "Done 2200 batches in 898.35s\ttraining loss:\t0.445640\n",
      "Done 2220 batches in 905.98s\ttraining loss:\t0.446017\n",
      "Done 2240 batches in 912.56s\ttraining loss:\t0.446291\n",
      "Done 2260 batches in 920.56s\ttraining loss:\t0.444988\n",
      "Done 2280 batches in 929.12s\ttraining loss:\t0.443758\n",
      "Done 2300 batches in 937.52s\ttraining loss:\t0.442648\n",
      "Done 2320 batches in 945.58s\ttraining loss:\t0.442092\n",
      "Done 2340 batches in 953.16s\ttraining loss:\t0.441642\n",
      "Done 2360 batches in 960.39s\ttraining loss:\t0.441188\n",
      "Done 2380 batches in 968.62s\ttraining loss:\t0.440855\n",
      "Done 2400 batches in 976.90s\ttraining loss:\t0.440366\n",
      "Done 2420 batches in 985.54s\ttraining loss:\t0.440122\n",
      "Done 2440 batches in 994.25s\ttraining loss:\t0.439948\n",
      "Done 2460 batches in 1002.95s\ttraining loss:\t0.439623\n",
      "Done 2480 batches in 1011.30s\ttraining loss:\t0.438941\n",
      "Done 2500 batches in 1018.73s\ttraining loss:\t0.438774\n",
      "Done 2520 batches in 1027.22s\ttraining loss:\t0.438608\n",
      "Done 2540 batches in 1034.78s\ttraining loss:\t0.438557\n",
      "Done 2560 batches in 1042.20s\ttraining loss:\t0.438424\n",
      "Done 2580 batches in 1051.12s\ttraining loss:\t0.438551\n",
      "Done 2600 batches in 1058.51s\ttraining loss:\t0.438934\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-64ada69a1fb5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhred_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_one_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_trimmed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/pio/scratch/1/i258346/masters_thesis/SQuAD/HRED_v2.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[1;34m(self, train_data, batch_size, log_interval)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m             \u001b[0mnum_batch_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m             \u001b[0mtrain_err\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbin_feat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manswers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext_mask\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnum_batch_words\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m             \u001b[0mtrain_batches\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             \u001b[0mnum_training_words\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnum_batch_words\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/i258346/.local/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    882\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 884\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/i258346/.local/lib/python2.7/site-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n)\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNoParams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    859\u001b[0m             \u001b[1;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 860\u001b[1;33m             \u001b[1;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    861\u001b[0m                 \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    862\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hred_net.train_one_epoch(data_trimmed, 5, log_interval=20) # weighted cost, random init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 20 batches in 2.25s\ttraining loss:\t0.228349\n",
      "Done 40 batches in 5.03s\ttraining loss:\t0.154742\n",
      "Done 60 batches in 7.65s\ttraining loss:\t0.126147\n",
      "Done 80 batches in 10.05s\ttraining loss:\t0.106807\n",
      "Done 100 batches in 12.52s\ttraining loss:\t0.095184\n",
      "Done 120 batches in 14.64s\ttraining loss:\t0.090502\n",
      "Done 140 batches in 17.25s\ttraining loss:\t0.087078\n",
      "Done 160 batches in 19.51s\ttraining loss:\t0.085557\n",
      "Done 180 batches in 22.31s\ttraining loss:\t0.082334\n",
      "Done 200 batches in 24.40s\ttraining loss:\t0.082256\n",
      "Done 220 batches in 26.96s\ttraining loss:\t0.081215\n",
      "Done 240 batches in 29.60s\ttraining loss:\t0.082376\n",
      "Done 260 batches in 32.51s\ttraining loss:\t0.083784\n",
      "Done 280 batches in 34.97s\ttraining loss:\t0.084956\n",
      "Done 300 batches in 37.36s\ttraining loss:\t0.084330\n",
      "Done 320 batches in 39.52s\ttraining loss:\t0.083391\n",
      "Done 340 batches in 42.24s\ttraining loss:\t0.083563\n",
      "Done 360 batches in 44.99s\ttraining loss:\t0.081207\n",
      "Done 380 batches in 47.14s\ttraining loss:\t0.081512\n",
      "Done 400 batches in 49.09s\ttraining loss:\t0.081817\n",
      "Done 420 batches in 51.60s\ttraining loss:\t0.081576\n",
      "Done 440 batches in 54.40s\ttraining loss:\t0.081043\n",
      "Done 460 batches in 57.43s\ttraining loss:\t0.081339\n",
      "Done 480 batches in 60.30s\ttraining loss:\t0.081267\n",
      "Done 500 batches in 62.65s\ttraining loss:\t0.080241\n",
      "Done 520 batches in 64.71s\ttraining loss:\t0.079872\n",
      "Done 540 batches in 66.90s\ttraining loss:\t0.080126\n",
      "Done 560 batches in 69.28s\ttraining loss:\t0.079805\n",
      "Done 580 batches in 71.59s\ttraining loss:\t0.078206\n",
      "Done 600 batches in 73.70s\ttraining loss:\t0.078006\n",
      "Done 620 batches in 76.02s\ttraining loss:\t0.077540\n",
      "Done 640 batches in 78.29s\ttraining loss:\t0.077661\n",
      "Done 660 batches in 81.00s\ttraining loss:\t0.077267\n",
      "Done 680 batches in 83.10s\ttraining loss:\t0.077439\n",
      "Done 700 batches in 85.16s\ttraining loss:\t0.077608\n",
      "Done 720 batches in 87.61s\ttraining loss:\t0.078403\n",
      "Done 740 batches in 90.07s\ttraining loss:\t0.078511\n",
      "Done 760 batches in 92.63s\ttraining loss:\t0.078503\n",
      "Done 780 batches in 94.92s\ttraining loss:\t0.078355\n",
      "Done 800 batches in 97.58s\ttraining loss:\t0.078138\n",
      "Done 820 batches in 100.06s\ttraining loss:\t0.077781\n",
      "Done 840 batches in 103.01s\ttraining loss:\t0.077201\n",
      "Done 860 batches in 105.70s\ttraining loss:\t0.077054\n",
      "Done 880 batches in 107.92s\ttraining loss:\t0.076950\n",
      "Done 900 batches in 110.14s\ttraining loss:\t0.076493\n",
      "Done 920 batches in 112.94s\ttraining loss:\t0.076241\n",
      "Done 940 batches in 115.26s\ttraining loss:\t0.075677\n",
      "Done 960 batches in 117.99s\ttraining loss:\t0.075053\n",
      "Done 980 batches in 120.26s\ttraining loss:\t0.076049\n",
      "Done 1000 batches in 122.23s\ttraining loss:\t0.077339\n",
      "Done 1020 batches in 124.55s\ttraining loss:\t0.078867\n",
      "Done 1040 batches in 127.04s\ttraining loss:\t0.080173\n",
      "Done 1060 batches in 129.60s\ttraining loss:\t0.080350\n",
      "Done 1080 batches in 132.17s\ttraining loss:\t0.080329\n",
      "Done 1100 batches in 134.68s\ttraining loss:\t0.081008\n",
      "Done 1120 batches in 137.63s\ttraining loss:\t0.081333\n",
      "Done 1140 batches in 140.35s\ttraining loss:\t0.082265\n",
      "Done 1160 batches in 143.38s\ttraining loss:\t0.082898\n",
      "Done 1180 batches in 145.65s\ttraining loss:\t0.083972\n",
      "Done 1200 batches in 148.24s\ttraining loss:\t0.083225\n",
      "Done 1220 batches in 150.71s\ttraining loss:\t0.082501\n",
      "Done 1240 batches in 153.33s\ttraining loss:\t0.082238\n",
      "Done 1260 batches in 155.95s\ttraining loss:\t0.082117\n",
      "Done 1280 batches in 158.54s\ttraining loss:\t0.082770\n",
      "Done 1300 batches in 160.68s\ttraining loss:\t0.083157\n",
      "Done 1320 batches in 163.10s\ttraining loss:\t0.083262\n",
      "Done 1340 batches in 165.49s\ttraining loss:\t0.083107\n",
      "Done 1360 batches in 168.28s\ttraining loss:\t0.082768\n",
      "Done 1380 batches in 170.73s\ttraining loss:\t0.082702\n",
      "Done 1400 batches in 173.25s\ttraining loss:\t0.082357\n",
      "Done 1420 batches in 175.58s\ttraining loss:\t0.081953\n",
      "Done 1440 batches in 177.77s\ttraining loss:\t0.082008\n",
      "Done 1460 batches in 179.76s\ttraining loss:\t0.082140\n",
      "Done 1480 batches in 181.61s\ttraining loss:\t0.082130\n",
      "Done 1500 batches in 183.58s\ttraining loss:\t0.082174\n",
      "Done 1520 batches in 186.24s\ttraining loss:\t0.081775\n",
      "Done 1540 batches in 188.61s\ttraining loss:\t0.081439\n",
      "Done 1560 batches in 190.90s\ttraining loss:\t0.081231\n",
      "Done 1580 batches in 193.30s\ttraining loss:\t0.081040\n",
      "Done 1600 batches in 195.93s\ttraining loss:\t0.081032\n",
      "Done 1620 batches in 197.83s\ttraining loss:\t0.081020\n",
      "Done 1640 batches in 200.18s\ttraining loss:\t0.080836\n",
      "Done 1660 batches in 202.61s\ttraining loss:\t0.080271\n",
      "Done 1680 batches in 204.94s\ttraining loss:\t0.080008\n",
      "Done 1700 batches in 207.61s\ttraining loss:\t0.079575\n",
      "Done 1720 batches in 210.23s\ttraining loss:\t0.079748\n",
      "Done 1740 batches in 212.68s\ttraining loss:\t0.081393\n",
      "Done 1760 batches in 215.40s\ttraining loss:\t0.082344\n",
      "Done 1780 batches in 217.83s\ttraining loss:\t0.084007\n",
      "Done 1800 batches in 220.25s\ttraining loss:\t0.083921\n",
      "Done 1820 batches in 222.67s\ttraining loss:\t0.083847\n",
      "Done 1840 batches in 225.76s\ttraining loss:\t0.083691\n",
      "Done 1860 batches in 228.37s\ttraining loss:\t0.083606\n",
      "Done 1880 batches in 230.73s\ttraining loss:\t0.083308\n",
      "Done 1900 batches in 232.90s\ttraining loss:\t0.083360\n",
      "Done 1920 batches in 235.19s\ttraining loss:\t0.083349\n",
      "Done 1940 batches in 237.19s\ttraining loss:\t0.083379\n",
      "Done 1960 batches in 239.77s\ttraining loss:\t0.083347\n",
      "Done 1980 batches in 242.10s\ttraining loss:\t0.083264\n",
      "Done 2000 batches in 244.65s\ttraining loss:\t0.083313\n",
      "Done 2020 batches in 246.98s\ttraining loss:\t0.083742\n",
      "Done 2040 batches in 249.30s\ttraining loss:\t0.083832\n",
      "Done 2060 batches in 252.19s\ttraining loss:\t0.084214\n",
      "Done 2080 batches in 255.02s\ttraining loss:\t0.084517\n",
      "Done 2100 batches in 257.63s\ttraining loss:\t0.084477\n",
      "Done 2120 batches in 259.75s\ttraining loss:\t0.084350\n",
      "Done 2140 batches in 262.12s\ttraining loss:\t0.084199\n",
      "Done 2160 batches in 264.48s\ttraining loss:\t0.084294\n",
      "Done 2180 batches in 266.70s\ttraining loss:\t0.084472\n",
      "Done 2200 batches in 269.17s\ttraining loss:\t0.084553\n",
      "Done 2220 batches in 271.34s\ttraining loss:\t0.084627\n",
      "Done 2240 batches in 273.79s\ttraining loss:\t0.084675\n",
      "Done 2260 batches in 276.32s\ttraining loss:\t0.084386\n",
      "Done 2280 batches in 278.86s\ttraining loss:\t0.084116\n",
      "Done 2300 batches in 281.38s\ttraining loss:\t0.083881\n",
      "Done 2320 batches in 283.70s\ttraining loss:\t0.083758\n",
      "Done 2340 batches in 285.80s\ttraining loss:\t0.083667\n",
      "Done 2360 batches in 287.73s\ttraining loss:\t0.083566\n",
      "Done 2380 batches in 290.19s\ttraining loss:\t0.083491\n",
      "Done 2400 batches in 293.14s\ttraining loss:\t0.083378\n",
      "Done 2420 batches in 295.73s\ttraining loss:\t0.083295\n",
      "Done 2440 batches in 298.31s\ttraining loss:\t0.083252\n",
      "Done 2460 batches in 300.87s\ttraining loss:\t0.083187\n",
      "Done 2480 batches in 303.39s\ttraining loss:\t0.083052\n",
      "Done 2500 batches in 305.45s\ttraining loss:\t0.083019\n",
      "Done 2520 batches in 307.97s\ttraining loss:\t0.082983\n",
      "Done 2540 batches in 310.39s\ttraining loss:\t0.082987\n",
      "Done 2560 batches in 312.73s\ttraining loss:\t0.082951\n",
      "Done 2580 batches in 315.43s\ttraining loss:\t0.082987\n",
      "Done 2600 batches in 317.41s\ttraining loss:\t0.083058\n",
      "Done 2620 batches in 319.94s\ttraining loss:\t0.083174\n",
      "Done 2640 batches in 322.48s\ttraining loss:\t0.083268\n",
      "Done 2660 batches in 324.62s\ttraining loss:\t0.083458\n",
      "Done 2680 batches in 326.73s\ttraining loss:\t0.083409\n",
      "Done 2700 batches in 329.01s\ttraining loss:\t0.083184\n",
      "Done 2720 batches in 331.53s\ttraining loss:\t0.082915\n",
      "Done 2740 batches in 334.17s\ttraining loss:\t0.082666\n",
      "Done 2760 batches in 336.83s\ttraining loss:\t0.082375\n",
      "Done 2780 batches in 339.10s\ttraining loss:\t0.082695\n",
      "Done 2800 batches in 341.49s\ttraining loss:\t0.082578\n",
      "Done 2820 batches in 343.95s\ttraining loss:\t0.082545\n",
      "Done 2840 batches in 346.30s\ttraining loss:\t0.082490\n",
      "Done 2860 batches in 348.47s\ttraining loss:\t0.082513\n",
      "Done 2880 batches in 350.55s\ttraining loss:\t0.082629\n",
      "Done 2900 batches in 352.74s\ttraining loss:\t0.082627\n",
      "Done 2920 batches in 355.22s\ttraining loss:\t0.082777\n",
      "Done 2940 batches in 357.38s\ttraining loss:\t0.082989\n",
      "Done 2960 batches in 360.03s\ttraining loss:\t0.082944\n",
      "Done 2980 batches in 362.65s\ttraining loss:\t0.082840\n",
      "Done 3000 batches in 365.10s\ttraining loss:\t0.082781\n",
      "Done 3020 batches in 367.54s\ttraining loss:\t0.082870\n",
      "Done 3040 batches in 369.62s\ttraining loss:\t0.083219\n",
      "Done 3060 batches in 372.31s\ttraining loss:\t0.083346\n",
      "Done 3080 batches in 374.34s\ttraining loss:\t0.083250\n",
      "Done 3100 batches in 376.43s\ttraining loss:\t0.083245\n",
      "Done 3120 batches in 378.61s\ttraining loss:\t0.083298\n",
      "Done 3140 batches in 380.43s\ttraining loss:\t0.083373\n",
      "Done 3160 batches in 382.61s\ttraining loss:\t0.083474\n",
      "Done 3180 batches in 385.21s\ttraining loss:\t0.083542\n",
      "Done 3200 batches in 387.36s\ttraining loss:\t0.083450\n",
      "Done 3220 batches in 389.75s\ttraining loss:\t0.083319\n",
      "Done 3240 batches in 392.17s\ttraining loss:\t0.083259\n",
      "Done 3260 batches in 394.37s\ttraining loss:\t0.083201\n",
      "Done 3280 batches in 397.29s\ttraining loss:\t0.083164\n",
      "Done 3300 batches in 399.99s\ttraining loss:\t0.083116\n",
      "Done 3320 batches in 402.28s\ttraining loss:\t0.083230\n",
      "Done 3340 batches in 404.82s\ttraining loss:\t0.083286\n",
      "Done 3360 batches in 407.30s\ttraining loss:\t0.083341\n",
      "Done 3380 batches in 409.63s\ttraining loss:\t0.083381\n",
      "Done 3400 batches in 411.81s\ttraining loss:\t0.083656\n",
      "Done 3420 batches in 414.28s\ttraining loss:\t0.083676\n",
      "Done 3440 batches in 416.79s\ttraining loss:\t0.083656\n",
      "Done 3460 batches in 419.58s\ttraining loss:\t0.083481\n",
      "Done 3480 batches in 421.96s\ttraining loss:\t0.083374\n",
      "Done 3500 batches in 424.48s\ttraining loss:\t0.083406\n",
      "Done 3520 batches in 426.69s\ttraining loss:\t0.083517\n",
      "Done 3540 batches in 428.92s\ttraining loss:\t0.083598\n",
      "Done 3560 batches in 431.32s\ttraining loss:\t0.083693\n",
      "Done 3580 batches in 433.80s\ttraining loss:\t0.083768\n",
      "Done 3600 batches in 436.10s\ttraining loss:\t0.083722\n",
      "Done 3620 batches in 438.77s\ttraining loss:\t0.083722\n",
      "Done 3640 batches in 441.50s\ttraining loss:\t0.083669\n",
      "Done 3660 batches in 444.50s\ttraining loss:\t0.083569\n",
      "Done 3680 batches in 447.27s\ttraining loss:\t0.083521\n",
      "Done 3700 batches in 449.57s\ttraining loss:\t0.083480\n",
      "Done 3720 batches in 452.27s\ttraining loss:\t0.083580\n",
      "Done 3740 batches in 454.56s\ttraining loss:\t0.083695\n",
      "Done 3760 batches in 456.78s\ttraining loss:\t0.083617\n",
      "Done 3780 batches in 459.11s\ttraining loss:\t0.083555\n",
      "Done 3800 batches in 461.14s\ttraining loss:\t0.083450\n",
      "Done 3820 batches in 463.86s\ttraining loss:\t0.083416\n",
      "Done 3840 batches in 465.90s\ttraining loss:\t0.083386\n",
      "Done 3860 batches in 468.20s\ttraining loss:\t0.083561\n",
      "Done 3880 batches in 470.61s\ttraining loss:\t0.083721\n",
      "Done 3900 batches in 472.78s\ttraining loss:\t0.083661\n",
      "Done 3920 batches in 475.16s\ttraining loss:\t0.083431\n",
      "Done 3940 batches in 477.57s\ttraining loss:\t0.083281\n",
      "Done 3960 batches in 479.76s\ttraining loss:\t0.083151\n",
      "Done 3980 batches in 481.76s\ttraining loss:\t0.083135\n",
      "Done 4000 batches in 483.84s\ttraining loss:\t0.083099\n",
      "Done 4020 batches in 486.14s\ttraining loss:\t0.083242\n",
      "Done 4040 batches in 488.55s\ttraining loss:\t0.083450\n",
      "Done 4060 batches in 491.19s\ttraining loss:\t0.083640\n",
      "Done 4080 batches in 493.71s\ttraining loss:\t0.083431\n",
      "Done 4100 batches in 496.13s\ttraining loss:\t0.083501\n",
      "Done 4120 batches in 498.46s\ttraining loss:\t0.083661\n",
      "Done 4140 batches in 500.75s\ttraining loss:\t0.083733\n",
      "Done 4160 batches in 503.19s\ttraining loss:\t0.083856\n",
      "Done 4180 batches in 505.25s\ttraining loss:\t0.084116\n",
      "Done 4200 batches in 507.40s\ttraining loss:\t0.084245\n",
      "Done 4220 batches in 509.21s\ttraining loss:\t0.084350\n",
      "Done 4240 batches in 510.99s\ttraining loss:\t0.084466\n",
      "Done 4260 batches in 512.63s\ttraining loss:\t0.084566\n",
      "Done 4280 batches in 514.65s\ttraining loss:\t0.084610\n",
      "Done 4300 batches in 516.76s\ttraining loss:\t0.084608\n",
      "Done 4320 batches in 519.22s\ttraining loss:\t0.084525\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-64ada69a1fb5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhred_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_one_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_trimmed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/pio/scratch/1/i258346/masters_thesis/SQuAD/HRED_v2.pyc\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[1;34m(self, train_data, batch_size, log_interval)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m             \u001b[0mnum_batch_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m             \u001b[0mtrain_err\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbin_feat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manswers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext_mask\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnum_batch_words\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m             \u001b[0mtrain_batches\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             \u001b[0mnum_training_words\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnum_batch_words\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/i258346/.local/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    882\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 884\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/i258346/.local/lib/python2.7/site-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n)\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNoParams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    859\u001b[0m             \u001b[1;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 860\u001b[1;33m             \u001b[1;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    861\u001b[0m                 \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    862\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hred_net.train_one_epoch(data_trimmed, 5, log_interval=20) # normal binary cross-entropy with random init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 20 batches in 2.62s\ttraining loss:\t0.196862\n",
      "Done 40 batches in 5.85s\ttraining loss:\t0.147509\n",
      "Done 60 batches in 8.90s\ttraining loss:\t0.121683\n",
      "Done 80 batches in 11.70s\ttraining loss:\t0.103534\n",
      "Done 100 batches in 14.59s\ttraining loss:\t0.092342\n",
      "Done 120 batches in 17.06s\ttraining loss:\t0.088309\n",
      "Done 140 batches in 20.10s\ttraining loss:\t0.084997\n",
      "Done 160 batches in 22.75s\ttraining loss:\t0.083670\n",
      "Done 180 batches in 26.01s\ttraining loss:\t0.080552\n",
      "Done 200 batches in 28.45s\ttraining loss:\t0.080706\n",
      "Done 220 batches in 31.44s\ttraining loss:\t0.080079\n",
      "Done 240 batches in 34.52s\ttraining loss:\t0.081357\n",
      "Done 260 batches in 37.91s\ttraining loss:\t0.084765\n",
      "Done 280 batches in 40.77s\ttraining loss:\t0.086710\n",
      "Done 300 batches in 43.56s\ttraining loss:\t0.086035\n",
      "Done 320 batches in 46.09s\ttraining loss:\t0.084813\n",
      "Done 340 batches in 49.25s\ttraining loss:\t0.084861\n",
      "Done 360 batches in 52.45s\ttraining loss:\t0.082334\n",
      "Done 380 batches in 54.97s\ttraining loss:\t0.082462\n",
      "Done 400 batches in 57.25s\ttraining loss:\t0.082701\n",
      "Done 420 batches in 60.18s\ttraining loss:\t0.082397\n",
      "Done 440 batches in 63.44s\ttraining loss:\t0.081846\n",
      "Done 460 batches in 66.96s\ttraining loss:\t0.082129\n",
      "Done 480 batches in 70.30s\ttraining loss:\t0.082273\n",
      "Done 500 batches in 73.04s\ttraining loss:\t0.081243\n",
      "Done 520 batches in 75.46s\ttraining loss:\t0.080875\n",
      "Done 540 batches in 78.02s\ttraining loss:\t0.081103\n",
      "Done 560 batches in 80.80s\ttraining loss:\t0.080769\n",
      "Done 580 batches in 83.50s\ttraining loss:\t0.079111\n",
      "Done 600 batches in 85.97s\ttraining loss:\t0.078852\n",
      "Done 620 batches in 88.68s\ttraining loss:\t0.078330\n",
      "Done 640 batches in 91.34s\ttraining loss:\t0.078394\n",
      "Done 660 batches in 94.50s\ttraining loss:\t0.078017\n",
      "Done 680 batches in 96.95s\ttraining loss:\t0.078227\n",
      "Done 700 batches in 99.37s\ttraining loss:\t0.078423\n",
      "Done 720 batches in 102.21s\ttraining loss:\t0.079233\n",
      "Done 740 batches in 105.09s\ttraining loss:\t0.079343\n",
      "Done 760 batches in 108.07s\ttraining loss:\t0.079324\n",
      "Done 780 batches in 110.75s\ttraining loss:\t0.079150\n",
      "Done 800 batches in 113.85s\ttraining loss:\t0.078905\n",
      "Done 820 batches in 116.74s\ttraining loss:\t0.078498\n",
      "Done 840 batches in 120.17s\ttraining loss:\t0.077881\n",
      "Done 860 batches in 123.31s\ttraining loss:\t0.077663\n",
      "Done 880 batches in 125.89s\ttraining loss:\t0.077541\n",
      "Done 900 batches in 128.50s\ttraining loss:\t0.077031\n",
      "Done 920 batches in 131.75s\ttraining loss:\t0.076775\n",
      "Done 940 batches in 134.46s\ttraining loss:\t0.076161\n",
      "Done 960 batches in 137.64s\ttraining loss:\t0.075530\n",
      "Done 980 batches in 140.30s\ttraining loss:\t0.076537\n",
      "Done 1000 batches in 142.61s\ttraining loss:\t0.077883\n",
      "Done 1020 batches in 145.33s\ttraining loss:\t0.079426\n",
      "Done 1040 batches in 148.23s\ttraining loss:\t0.080742\n",
      "Done 1060 batches in 151.21s\ttraining loss:\t0.080930\n",
      "Done 1080 batches in 154.21s\ttraining loss:\t0.080906\n",
      "Done 1100 batches in 157.14s\ttraining loss:\t0.081590\n",
      "Done 1120 batches in 160.58s\ttraining loss:\t0.081935\n",
      "Done 1140 batches in 163.74s\ttraining loss:\t0.082886\n",
      "Done 1160 batches in 167.27s\ttraining loss:\t0.083532\n",
      "Done 1180 batches in 169.93s\ttraining loss:\t0.084622\n",
      "Done 1200 batches in 172.95s\ttraining loss:\t0.083861\n",
      "Done 1220 batches in 175.84s\ttraining loss:\t0.083128\n",
      "Done 1240 batches in 178.88s\ttraining loss:\t0.082831\n",
      "Done 1260 batches in 181.94s\ttraining loss:\t0.082677\n",
      "Done 1280 batches in 184.95s\ttraining loss:\t0.083334\n",
      "Done 1300 batches in 187.46s\ttraining loss:\t0.083724\n",
      "Done 1320 batches in 190.28s\ttraining loss:\t0.083830\n",
      "Done 1340 batches in 193.08s\ttraining loss:\t0.083680\n",
      "Done 1360 batches in 196.33s\ttraining loss:\t0.083339\n",
      "Done 1380 batches in 199.18s\ttraining loss:\t0.083280\n",
      "Done 1400 batches in 202.12s\ttraining loss:\t0.082912\n",
      "Done 1420 batches in 204.86s\ttraining loss:\t0.082489\n",
      "Done 1440 batches in 207.41s\ttraining loss:\t0.082531\n",
      "Done 1460 batches in 209.76s\ttraining loss:\t0.082660\n",
      "Done 1480 batches in 211.93s\ttraining loss:\t0.082636\n",
      "Done 1500 batches in 214.25s\ttraining loss:\t0.082669\n",
      "Done 1520 batches in 217.34s\ttraining loss:\t0.082256\n",
      "Done 1540 batches in 220.12s\ttraining loss:\t0.081899\n",
      "Done 1560 batches in 222.79s\ttraining loss:\t0.081685\n",
      "Done 1580 batches in 225.60s\ttraining loss:\t0.081490\n",
      "Done 1600 batches in 228.66s\ttraining loss:\t0.081471\n",
      "Done 1620 batches in 230.88s\ttraining loss:\t0.081456\n",
      "Done 1640 batches in 233.63s\ttraining loss:\t0.081263\n",
      "Done 1660 batches in 236.47s\ttraining loss:\t0.080676\n",
      "Done 1680 batches in 239.20s\ttraining loss:\t0.080405\n",
      "Done 1700 batches in 242.31s\ttraining loss:\t0.079968\n",
      "Done 1720 batches in 245.37s\ttraining loss:\t0.080156\n",
      "Done 1740 batches in 248.23s\ttraining loss:\t0.081800\n",
      "Done 1760 batches in 251.39s\ttraining loss:\t0.082739\n",
      "Done 1780 batches in 254.24s\ttraining loss:\t0.084411\n",
      "Done 1800 batches in 257.05s\ttraining loss:\t0.084317\n",
      "Done 1820 batches in 259.87s\ttraining loss:\t0.084252\n",
      "Done 1840 batches in 263.46s\ttraining loss:\t0.084094\n",
      "Done 1860 batches in 266.50s\ttraining loss:\t0.084018\n",
      "Done 1880 batches in 269.26s\ttraining loss:\t0.083715\n",
      "Done 1900 batches in 271.79s\ttraining loss:\t0.083762\n",
      "Done 1920 batches in 274.48s\ttraining loss:\t0.083748\n",
      "Done 1940 batches in 276.82s\ttraining loss:\t0.083773\n",
      "Done 1960 batches in 279.84s\ttraining loss:\t0.083729\n",
      "Done 1980 batches in 282.55s\ttraining loss:\t0.083646\n",
      "Done 2000 batches in 285.53s\ttraining loss:\t0.083694\n",
      "Done 2020 batches in 288.26s\ttraining loss:\t0.084137\n",
      "Done 2040 batches in 290.97s\ttraining loss:\t0.084224\n",
      "Done 2060 batches in 294.33s\ttraining loss:\t0.084601\n",
      "Done 2080 batches in 297.63s\ttraining loss:\t0.084911\n",
      "Done 2100 batches in 300.67s\ttraining loss:\t0.084878\n",
      "Done 2120 batches in 303.15s\ttraining loss:\t0.084750\n",
      "Done 2140 batches in 305.92s\ttraining loss:\t0.084584\n",
      "Done 2160 batches in 308.68s\ttraining loss:\t0.084675\n",
      "Done 2180 batches in 311.28s\ttraining loss:\t0.084850\n",
      "Done 2200 batches in 314.16s\ttraining loss:\t0.084943\n",
      "Done 2220 batches in 316.69s\ttraining loss:\t0.085015\n",
      "Done 2240 batches in 319.57s\ttraining loss:\t0.085063\n",
      "Done 2260 batches in 322.52s\ttraining loss:\t0.084769\n",
      "Done 2280 batches in 325.48s\ttraining loss:\t0.084497\n",
      "Done 2300 batches in 328.42s\ttraining loss:\t0.084253\n",
      "Done 2320 batches in 331.13s\ttraining loss:\t0.084126\n",
      "Done 2340 batches in 333.59s\ttraining loss:\t0.084014\n",
      "Done 2360 batches in 335.86s\ttraining loss:\t0.083911\n",
      "Done 2380 batches in 338.73s\ttraining loss:\t0.083821\n",
      "Done 2400 batches in 342.15s\ttraining loss:\t0.083717\n",
      "Done 2420 batches in 345.18s\ttraining loss:\t0.083618\n",
      "Done 2440 batches in 348.18s\ttraining loss:\t0.083569\n",
      "Done 2460 batches in 351.17s\ttraining loss:\t0.083502\n",
      "Done 2480 batches in 354.11s\ttraining loss:\t0.083361\n",
      "Done 2500 batches in 356.53s\ttraining loss:\t0.083326\n",
      "Done 2520 batches in 359.46s\ttraining loss:\t0.083280\n",
      "Done 2540 batches in 362.29s\ttraining loss:\t0.083274\n",
      "Done 2560 batches in 365.02s\ttraining loss:\t0.083232\n",
      "Done 2580 batches in 368.17s\ttraining loss:\t0.083269\n",
      "Done 2600 batches in 370.49s\ttraining loss:\t0.083339\n",
      "Done 2620 batches in 373.44s\ttraining loss:\t0.083462\n",
      "Done 2640 batches in 376.40s\ttraining loss:\t0.083559\n",
      "Done 2660 batches in 378.91s\ttraining loss:\t0.083746\n",
      "Done 2680 batches in 381.38s\ttraining loss:\t0.083699\n",
      "Done 2700 batches in 384.05s\ttraining loss:\t0.083460\n",
      "Done 2720 batches in 386.98s\ttraining loss:\t0.083189\n",
      "Done 2740 batches in 390.07s\ttraining loss:\t0.082939\n",
      "Done 2760 batches in 393.23s\ttraining loss:\t0.082647\n",
      "Done 2780 batches in 395.93s\ttraining loss:\t0.082986\n",
      "Done 2800 batches in 398.78s\ttraining loss:\t0.082874\n",
      "Done 2820 batches in 401.71s\ttraining loss:\t0.082835\n",
      "Done 2840 batches in 404.51s\ttraining loss:\t0.082777\n",
      "Done 2860 batches in 407.09s\ttraining loss:\t0.082801\n",
      "Done 2880 batches in 409.58s\ttraining loss:\t0.082915\n",
      "Done 2900 batches in 412.20s\ttraining loss:\t0.082917\n",
      "Done 2920 batches in 415.15s\ttraining loss:\t0.083063\n",
      "Done 2940 batches in 417.72s\ttraining loss:\t0.083289\n",
      "Done 2960 batches in 420.86s\ttraining loss:\t0.083248\n",
      "Done 2980 batches in 423.98s\ttraining loss:\t0.083144\n",
      "Done 3000 batches in 426.90s\ttraining loss:\t0.083089\n",
      "Done 3020 batches in 429.80s\ttraining loss:\t0.083186\n",
      "Done 3040 batches in 432.28s\ttraining loss:\t0.083535\n",
      "Done 3060 batches in 435.45s\ttraining loss:\t0.083666\n",
      "Done 3080 batches in 437.85s\ttraining loss:\t0.083580\n",
      "Done 3100 batches in 440.32s\ttraining loss:\t0.083573\n",
      "Done 3120 batches in 442.88s\ttraining loss:\t0.083628\n",
      "Done 3140 batches in 445.03s\ttraining loss:\t0.083701\n",
      "Done 3160 batches in 447.58s\ttraining loss:\t0.083798\n",
      "Done 3180 batches in 450.62s\ttraining loss:\t0.083865\n",
      "Done 3200 batches in 453.16s\ttraining loss:\t0.083776\n",
      "Done 3220 batches in 455.98s\ttraining loss:\t0.083640\n",
      "Done 3240 batches in 458.84s\ttraining loss:\t0.083576\n",
      "Done 3260 batches in 461.44s\ttraining loss:\t0.083516\n",
      "Done 3280 batches in 464.87s\ttraining loss:\t0.083480\n",
      "Done 3300 batches in 468.04s\ttraining loss:\t0.083433\n",
      "Done 3320 batches in 470.74s\ttraining loss:\t0.083545\n",
      "Done 3340 batches in 473.74s\ttraining loss:\t0.083602\n",
      "Done 3360 batches in 476.67s\ttraining loss:\t0.083663\n",
      "Done 3380 batches in 479.42s\ttraining loss:\t0.083706\n",
      "Done 3400 batches in 481.99s\ttraining loss:\t0.083987\n",
      "Done 3420 batches in 484.90s\ttraining loss:\t0.084003\n",
      "Done 3440 batches in 487.86s\ttraining loss:\t0.083978\n",
      "Done 3460 batches in 491.14s\ttraining loss:\t0.083804\n",
      "Done 3480 batches in 493.95s\ttraining loss:\t0.083692\n",
      "Done 3500 batches in 496.91s\ttraining loss:\t0.083725\n",
      "Done 3520 batches in 499.53s\ttraining loss:\t0.083839\n",
      "Done 3540 batches in 502.16s\ttraining loss:\t0.083921\n",
      "Done 3560 batches in 504.99s\ttraining loss:\t0.084013\n",
      "Done 3580 batches in 507.92s\ttraining loss:\t0.084085\n",
      "Done 3600 batches in 510.64s\ttraining loss:\t0.084042\n",
      "Done 3620 batches in 513.77s\ttraining loss:\t0.084040\n",
      "Done 3640 batches in 516.98s\ttraining loss:\t0.083989\n",
      "Done 3660 batches in 520.50s\ttraining loss:\t0.083887\n",
      "Done 3680 batches in 523.76s\ttraining loss:\t0.083837\n",
      "Done 3700 batches in 526.48s\ttraining loss:\t0.083798\n",
      "Done 3720 batches in 529.65s\ttraining loss:\t0.083894\n",
      "Done 3740 batches in 532.35s\ttraining loss:\t0.084012\n",
      "Done 3760 batches in 534.97s\ttraining loss:\t0.083934\n",
      "Done 3780 batches in 537.72s\ttraining loss:\t0.083869\n",
      "Done 3800 batches in 540.12s\ttraining loss:\t0.083759\n",
      "Done 3820 batches in 543.31s\ttraining loss:\t0.083728\n",
      "Done 3840 batches in 545.74s\ttraining loss:\t0.083696\n",
      "Done 3860 batches in 548.45s\ttraining loss:\t0.083872\n",
      "Done 3880 batches in 551.30s\ttraining loss:\t0.084031\n",
      "Done 3900 batches in 553.86s\ttraining loss:\t0.083978\n",
      "Done 3920 batches in 556.67s\ttraining loss:\t0.083744\n",
      "Done 3940 batches in 559.51s\ttraining loss:\t0.083595\n",
      "Done 3960 batches in 562.09s\ttraining loss:\t0.083461\n",
      "Done 3980 batches in 564.47s\ttraining loss:\t0.083441\n",
      "Done 4000 batches in 566.94s\ttraining loss:\t0.083405\n",
      "Done 4020 batches in 569.64s\ttraining loss:\t0.083554\n",
      "Done 4040 batches in 572.48s\ttraining loss:\t0.083768\n",
      "Done 4060 batches in 575.60s\ttraining loss:\t0.083949\n",
      "Done 4080 batches in 578.57s\ttraining loss:\t0.083740\n",
      "Done 4100 batches in 581.48s\ttraining loss:\t0.083813\n",
      "Done 4120 batches in 584.28s\ttraining loss:\t0.083982\n",
      "Done 4140 batches in 587.03s\ttraining loss:\t0.084056\n",
      "Done 4160 batches in 589.96s\ttraining loss:\t0.084178\n",
      "Done 4180 batches in 592.45s\ttraining loss:\t0.084443\n",
      "Done 4200 batches in 595.04s\ttraining loss:\t0.084573\n",
      "Done 4220 batches in 597.22s\ttraining loss:\t0.084680\n",
      "Done 4240 batches in 599.36s\ttraining loss:\t0.084795\n",
      "Done 4260 batches in 601.33s\ttraining loss:\t0.084894\n",
      "Done 4280 batches in 603.73s\ttraining loss:\t0.084939\n",
      "Done 4300 batches in 606.25s\ttraining loss:\t0.084938\n",
      "Done 4320 batches in 609.17s\ttraining loss:\t0.084851\n",
      "Done 4340 batches in 611.97s\ttraining loss:\t0.084825\n",
      "Done 4360 batches in 614.90s\ttraining loss:\t0.084929\n",
      "Done 4380 batches in 617.37s\ttraining loss:\t0.085072\n",
      "Done 4400 batches in 620.38s\ttraining loss:\t0.085019\n",
      "Done 4420 batches in 623.37s\ttraining loss:\t0.084866\n",
      "Done 4440 batches in 626.32s\ttraining loss:\t0.084960\n",
      "Done 4460 batches in 629.33s\ttraining loss:\t0.084972\n",
      "Done 4480 batches in 632.20s\ttraining loss:\t0.085078\n",
      "Done 4500 batches in 635.14s\ttraining loss:\t0.085204\n",
      "Done 4520 batches in 637.74s\ttraining loss:\t0.085034\n",
      "Done 4540 batches in 640.46s\ttraining loss:\t0.084899\n",
      "Done 4560 batches in 642.99s\ttraining loss:\t0.084857\n",
      "Done 4580 batches in 646.33s\ttraining loss:\t0.084757\n",
      "Done 4600 batches in 649.04s\ttraining loss:\t0.084676\n",
      "Done 4620 batches in 651.39s\ttraining loss:\t0.084596\n",
      "Done 4640 batches in 654.44s\ttraining loss:\t0.084236\n",
      "Done 4660 batches in 657.30s\ttraining loss:\t0.083896\n",
      "Done 4680 batches in 659.98s\ttraining loss:\t0.083716\n",
      "Done 4700 batches in 662.72s\ttraining loss:\t0.083503\n",
      "Done 4720 batches in 665.75s\ttraining loss:\t0.083428\n",
      "Done 4740 batches in 668.45s\ttraining loss:\t0.083422\n",
      "Done 4760 batches in 671.02s\ttraining loss:\t0.083488\n",
      "Done 4780 batches in 673.50s\ttraining loss:\t0.083671\n",
      "Done 4800 batches in 676.24s\ttraining loss:\t0.083924\n",
      "Done 4820 batches in 679.56s\ttraining loss:\t0.084185\n",
      "Done 4840 batches in 682.30s\ttraining loss:\t0.084200\n",
      "Done 4860 batches in 684.88s\ttraining loss:\t0.084214\n",
      "Done 4880 batches in 687.65s\ttraining loss:\t0.084177\n",
      "Done 4900 batches in 690.56s\ttraining loss:\t0.084198\n",
      "Done 4920 batches in 692.77s\ttraining loss:\t0.084231\n",
      "Done 4940 batches in 696.20s\ttraining loss:\t0.084290\n",
      "Done 4960 batches in 698.82s\ttraining loss:\t0.084358\n",
      "Done 4980 batches in 701.24s\ttraining loss:\t0.084580\n",
      "Done 5000 batches in 703.72s\ttraining loss:\t0.084627\n",
      "Done 5020 batches in 706.69s\ttraining loss:\t0.084611\n",
      "Done 5040 batches in 709.69s\ttraining loss:\t0.084775\n",
      "Done 5060 batches in 712.99s\ttraining loss:\t0.084813\n",
      "Done 5080 batches in 715.55s\ttraining loss:\t0.084787\n",
      "Done 5100 batches in 718.35s\ttraining loss:\t0.084842\n",
      "Done 5120 batches in 721.85s\ttraining loss:\t0.084846\n",
      "Done 5140 batches in 724.92s\ttraining loss:\t0.084879\n",
      "Done 5160 batches in 727.99s\ttraining loss:\t0.084747\n",
      "Done 5180 batches in 731.09s\ttraining loss:\t0.084526\n",
      "Done 5200 batches in 733.91s\ttraining loss:\t0.084344\n",
      "Done 5220 batches in 736.69s\ttraining loss:\t0.084212\n",
      "Done 5240 batches in 739.17s\ttraining loss:\t0.084127\n",
      "Done 5260 batches in 741.85s\ttraining loss:\t0.084178\n",
      "Done 5280 batches in 744.86s\ttraining loss:\t0.084076\n",
      "Done 5300 batches in 747.76s\ttraining loss:\t0.083948\n",
      "Done 5320 batches in 750.37s\ttraining loss:\t0.083883\n",
      "Done 5340 batches in 753.79s\ttraining loss:\t0.083800\n",
      "Done 5360 batches in 756.22s\ttraining loss:\t0.084023\n",
      "Done 5380 batches in 759.06s\ttraining loss:\t0.084029\n",
      "Done 5400 batches in 762.21s\ttraining loss:\t0.084133\n",
      "Done 5420 batches in 765.67s\ttraining loss:\t0.083995\n",
      "Done 5440 batches in 768.49s\ttraining loss:\t0.084009\n",
      "Done 5460 batches in 771.64s\ttraining loss:\t0.083862\n",
      "Done 5480 batches in 774.67s\ttraining loss:\t0.083743\n",
      "Done 5500 batches in 778.31s\ttraining loss:\t0.083635\n",
      "Done 5520 batches in 781.37s\ttraining loss:\t0.083629\n",
      "Done 5540 batches in 784.93s\ttraining loss:\t0.083664\n",
      "Done 5560 batches in 787.91s\ttraining loss:\t0.083824\n",
      "Done 5580 batches in 790.96s\ttraining loss:\t0.083886\n",
      "Done 5600 batches in 793.66s\ttraining loss:\t0.083787\n",
      "Done 5620 batches in 796.40s\ttraining loss:\t0.083597\n",
      "Done 5640 batches in 799.38s\ttraining loss:\t0.083456\n",
      "Done 5660 batches in 801.88s\ttraining loss:\t0.083372\n",
      "Done 5680 batches in 804.47s\ttraining loss:\t0.083260\n",
      "Done 5700 batches in 807.13s\ttraining loss:\t0.083152\n",
      "Done 5720 batches in 810.24s\ttraining loss:\t0.083030\n",
      "Done 5740 batches in 813.18s\ttraining loss:\t0.083099\n",
      "Done 5760 batches in 816.39s\ttraining loss:\t0.083142\n",
      "Done 5780 batches in 819.16s\ttraining loss:\t0.083071\n",
      "Done 5800 batches in 821.97s\ttraining loss:\t0.083010\n",
      "Done 5820 batches in 824.91s\ttraining loss:\t0.082983\n",
      "Done 5840 batches in 828.12s\ttraining loss:\t0.082960\n",
      "Done 5860 batches in 831.52s\ttraining loss:\t0.082925\n",
      "Done 5880 batches in 834.45s\ttraining loss:\t0.082960\n",
      "Done 5900 batches in 837.57s\ttraining loss:\t0.082938\n",
      "Done 5920 batches in 840.91s\ttraining loss:\t0.082963\n",
      "Done 5940 batches in 844.52s\ttraining loss:\t0.082822\n",
      "Done 5960 batches in 847.41s\ttraining loss:\t0.082752\n",
      "Done 5980 batches in 850.11s\ttraining loss:\t0.082856\n",
      "Done 6000 batches in 852.83s\ttraining loss:\t0.082965\n",
      "Done 6020 batches in 855.90s\ttraining loss:\t0.083274\n",
      "Done 6040 batches in 858.63s\ttraining loss:\t0.083492\n",
      "Done 6060 batches in 861.74s\ttraining loss:\t0.083459\n",
      "Done 6080 batches in 864.86s\ttraining loss:\t0.083441\n",
      "Done 6100 batches in 867.78s\ttraining loss:\t0.083295\n",
      "Done 6120 batches in 870.71s\ttraining loss:\t0.083182\n",
      "Done 6140 batches in 873.59s\ttraining loss:\t0.083231\n",
      "Done 6160 batches in 876.41s\ttraining loss:\t0.083161\n",
      "Done 6180 batches in 879.32s\ttraining loss:\t0.083068\n",
      "Done 6200 batches in 882.73s\ttraining loss:\t0.082987\n",
      "Done 6220 batches in 886.08s\ttraining loss:\t0.082917\n",
      "Done 6240 batches in 889.32s\ttraining loss:\t0.082920\n",
      "Done 6260 batches in 892.18s\ttraining loss:\t0.082861\n",
      "Done 6280 batches in 895.46s\ttraining loss:\t0.082800\n",
      "Done 6300 batches in 898.60s\ttraining loss:\t0.082713\n",
      "Done 6320 batches in 901.73s\ttraining loss:\t0.082632\n",
      "Done 6340 batches in 904.72s\ttraining loss:\t0.082542\n",
      "Done 6360 batches in 907.63s\ttraining loss:\t0.082533\n",
      "Done 6380 batches in 910.61s\ttraining loss:\t0.082450\n",
      "Done 6400 batches in 913.55s\ttraining loss:\t0.082509\n",
      "Done 6420 batches in 916.60s\ttraining loss:\t0.082564\n",
      "Done 6440 batches in 919.48s\ttraining loss:\t0.082584\n",
      "Done 6460 batches in 922.17s\ttraining loss:\t0.082542\n",
      "Done 6480 batches in 925.14s\ttraining loss:\t0.082464\n",
      "Done 6500 batches in 928.49s\ttraining loss:\t0.082370\n",
      "Done 6520 batches in 931.71s\ttraining loss:\t0.082297\n",
      "Done 6540 batches in 935.00s\ttraining loss:\t0.082247\n",
      "Done 6560 batches in 938.04s\ttraining loss:\t0.082182\n",
      "Done 6580 batches in 941.19s\ttraining loss:\t0.082196\n",
      "Done 6600 batches in 944.06s\ttraining loss:\t0.082174\n",
      "Done 6620 batches in 946.96s\ttraining loss:\t0.082135\n",
      "Done 6640 batches in 949.74s\ttraining loss:\t0.082153\n",
      "Done 6660 batches in 952.58s\ttraining loss:\t0.082187\n",
      "Done 6680 batches in 955.83s\ttraining loss:\t0.082113\n",
      "Done 6700 batches in 958.85s\ttraining loss:\t0.081989\n",
      "Done 6720 batches in 961.91s\ttraining loss:\t0.082114\n",
      "Done 6740 batches in 964.80s\ttraining loss:\t0.082047\n",
      "Done 6760 batches in 967.83s\ttraining loss:\t0.081985\n",
      "Done 6780 batches in 970.87s\ttraining loss:\t0.082022\n",
      "Done 6800 batches in 973.77s\ttraining loss:\t0.081973\n",
      "Done 6820 batches in 976.63s\ttraining loss:\t0.081916\n",
      "Done 6840 batches in 979.47s\ttraining loss:\t0.081872\n",
      "Done 6860 batches in 982.25s\ttraining loss:\t0.082051\n",
      "Done 6880 batches in 985.07s\ttraining loss:\t0.082434\n",
      "Done 6900 batches in 988.26s\ttraining loss:\t0.082826\n",
      "Done 6920 batches in 991.43s\ttraining loss:\t0.083006\n",
      "Done 6940 batches in 994.43s\ttraining loss:\t0.082881\n",
      "Done 6960 batches in 997.51s\ttraining loss:\t0.082787\n",
      "Done 6980 batches in 1000.44s\ttraining loss:\t0.082685\n",
      "Done 7000 batches in 1003.61s\ttraining loss:\t0.082629\n",
      "Done 7020 batches in 1006.98s\ttraining loss:\t0.082551\n",
      "Done 7040 batches in 1010.04s\ttraining loss:\t0.082492\n",
      "Done 7060 batches in 1013.72s\ttraining loss:\t0.082585\n",
      "Done 7080 batches in 1016.93s\ttraining loss:\t0.082642\n",
      "Done 7100 batches in 1020.14s\ttraining loss:\t0.082570\n",
      "Done 7120 batches in 1023.06s\ttraining loss:\t0.082497\n",
      "Done 7140 batches in 1026.64s\ttraining loss:\t0.082354\n",
      "Done 7160 batches in 1030.19s\ttraining loss:\t0.082333\n",
      "Done 7180 batches in 1033.22s\ttraining loss:\t0.082586\n",
      "Done 7200 batches in 1036.19s\ttraining loss:\t0.082941\n",
      "Done 7220 batches in 1038.90s\ttraining loss:\t0.083118\n",
      "Done 7240 batches in 1042.07s\ttraining loss:\t0.083180\n",
      "Done 7260 batches in 1045.04s\ttraining loss:\t0.083117\n",
      "Done 7280 batches in 1048.33s\ttraining loss:\t0.083221\n",
      "Done 7300 batches in 1051.15s\ttraining loss:\t0.083294\n",
      "Done 7320 batches in 1054.13s\ttraining loss:\t0.083248\n",
      "Done 7340 batches in 1056.90s\ttraining loss:\t0.083137\n",
      "Done 7360 batches in 1060.13s\ttraining loss:\t0.083036\n",
      "Done 7380 batches in 1063.07s\ttraining loss:\t0.082950\n",
      "Done 7400 batches in 1065.91s\ttraining loss:\t0.082942\n",
      "Done 7420 batches in 1068.93s\ttraining loss:\t0.082957\n",
      "Done 7440 batches in 1071.34s\ttraining loss:\t0.082910\n",
      "Done 7460 batches in 1073.94s\ttraining loss:\t0.082960\n",
      "Done 7480 batches in 1076.81s\ttraining loss:\t0.082968\n",
      "Done 7500 batches in 1079.54s\ttraining loss:\t0.082948\n",
      "Done 7520 batches in 1082.07s\ttraining loss:\t0.082958\n",
      "Done 7540 batches in 1085.09s\ttraining loss:\t0.083026\n",
      "Done 7560 batches in 1087.63s\ttraining loss:\t0.083112\n",
      "Done 7580 batches in 1090.78s\ttraining loss:\t0.083073\n",
      "Done 7600 batches in 1093.93s\ttraining loss:\t0.082981\n",
      "Done 7620 batches in 1096.82s\ttraining loss:\t0.082920\n",
      "Done 7640 batches in 1099.82s\ttraining loss:\t0.082813\n",
      "Done 7660 batches in 1102.33s\ttraining loss:\t0.082701\n",
      "Done 7680 batches in 1104.93s\ttraining loss:\t0.082552\n",
      "Done 7700 batches in 1108.18s\ttraining loss:\t0.082393\n",
      "Done 7720 batches in 1111.05s\ttraining loss:\t0.082268\n",
      "Done 7740 batches in 1113.60s\ttraining loss:\t0.082195\n",
      "Done 7760 batches in 1116.43s\ttraining loss:\t0.082038\n",
      "Done 7780 batches in 1118.99s\ttraining loss:\t0.081941\n",
      "Done 7800 batches in 1122.08s\ttraining loss:\t0.081868\n",
      "Done 7820 batches in 1125.52s\ttraining loss:\t0.082204\n",
      "Done 7840 batches in 1128.73s\ttraining loss:\t0.082381\n",
      "Done 7860 batches in 1132.09s\ttraining loss:\t0.082361\n",
      "Done 7880 batches in 1135.33s\ttraining loss:\t0.082349\n",
      "Done 7900 batches in 1138.59s\ttraining loss:\t0.082312\n",
      "Done 7920 batches in 1141.56s\ttraining loss:\t0.082261\n",
      "Done 7940 batches in 1144.41s\ttraining loss:\t0.082252\n",
      "Done 7960 batches in 1147.16s\ttraining loss:\t0.082764\n",
      "Done 7980 batches in 1150.13s\ttraining loss:\t0.082894\n",
      "Done 8000 batches in 1152.88s\ttraining loss:\t0.082855\n",
      "Done 8020 batches in 1155.87s\ttraining loss:\t0.082765\n",
      "Done 8040 batches in 1159.22s\ttraining loss:\t0.082668\n",
      "Done 8060 batches in 1162.04s\ttraining loss:\t0.082611\n",
      "Done 8080 batches in 1164.54s\ttraining loss:\t0.082574\n",
      "Done 8100 batches in 1167.72s\ttraining loss:\t0.082530\n",
      "Done 8120 batches in 1170.55s\ttraining loss:\t0.082593\n",
      "Done 8140 batches in 1173.03s\ttraining loss:\t0.082813\n",
      "Done 8160 batches in 1175.39s\ttraining loss:\t0.082779\n",
      "Done 8180 batches in 1178.69s\ttraining loss:\t0.082650\n",
      "Done 8200 batches in 1182.01s\ttraining loss:\t0.082590\n",
      "Done 8220 batches in 1185.60s\ttraining loss:\t0.082539\n",
      "Done 8240 batches in 1188.80s\ttraining loss:\t0.082542\n",
      "Done 8260 batches in 1191.74s\ttraining loss:\t0.082536\n",
      "Done 8280 batches in 1194.92s\ttraining loss:\t0.082558\n",
      "Done 8300 batches in 1197.88s\ttraining loss:\t0.082632\n",
      "Done 8320 batches in 1201.18s\ttraining loss:\t0.082755\n",
      "Done 8340 batches in 1204.59s\ttraining loss:\t0.082855\n",
      "Done 8360 batches in 1208.03s\ttraining loss:\t0.082879\n",
      "Done 8380 batches in 1211.46s\ttraining loss:\t0.082821\n",
      "Done 8400 batches in 1214.83s\ttraining loss:\t0.082840\n",
      "Done 8420 batches in 1218.14s\ttraining loss:\t0.082825\n",
      "Done 8440 batches in 1221.38s\ttraining loss:\t0.082742\n",
      "Done 8460 batches in 1224.58s\ttraining loss:\t0.082736\n",
      "Done 8480 batches in 1227.33s\ttraining loss:\t0.082705\n",
      "Done 8500 batches in 1230.24s\ttraining loss:\t0.082752\n",
      "Done 8520 batches in 1233.12s\ttraining loss:\t0.082854\n",
      "Done 8540 batches in 1236.78s\ttraining loss:\t0.083130\n",
      "Done 8560 batches in 1240.33s\ttraining loss:\t0.083248\n",
      "Done 8580 batches in 1243.87s\ttraining loss:\t0.083372\n",
      "Done 8600 batches in 1246.97s\ttraining loss:\t0.083472\n",
      "Done 8620 batches in 1250.04s\ttraining loss:\t0.083495\n",
      "Done 8640 batches in 1253.54s\ttraining loss:\t0.083460\n",
      "Done 8660 batches in 1256.87s\ttraining loss:\t0.083462\n",
      "Done 8680 batches in 1260.05s\ttraining loss:\t0.083446\n",
      "Done 8700 batches in 1262.66s\ttraining loss:\t0.083442\n",
      "Done 8720 batches in 1265.35s\ttraining loss:\t0.083394\n",
      "Done 8740 batches in 1268.26s\ttraining loss:\t0.083386\n",
      "Done 8760 batches in 1271.64s\ttraining loss:\t0.083334\n",
      "Done 8780 batches in 1274.91s\ttraining loss:\t0.083340\n",
      "Done 8800 batches in 1278.19s\ttraining loss:\t0.083321\n",
      "Done 8820 batches in 1281.27s\ttraining loss:\t0.083300\n",
      "Done 8840 batches in 1283.85s\ttraining loss:\t0.083260\n",
      "Done 8860 batches in 1286.85s\ttraining loss:\t0.083256\n",
      "Done 8880 batches in 1290.27s\ttraining loss:\t0.083291\n",
      "Done 8900 batches in 1293.37s\ttraining loss:\t0.083269\n",
      "Done 8920 batches in 1296.37s\ttraining loss:\t0.083237\n",
      "Done 8940 batches in 1299.38s\ttraining loss:\t0.083477\n",
      "Done 8960 batches in 1302.38s\ttraining loss:\t0.083802\n",
      "Done 8980 batches in 1305.79s\ttraining loss:\t0.083715\n",
      "Done 9000 batches in 1308.93s\ttraining loss:\t0.083643\n",
      "Done 9020 batches in 1312.45s\ttraining loss:\t0.083633\n",
      "Done 9040 batches in 1315.69s\ttraining loss:\t0.083639\n",
      "Done 9060 batches in 1318.94s\ttraining loss:\t0.083720\n",
      "Done 9080 batches in 1321.74s\ttraining loss:\t0.083722\n",
      "Done 9100 batches in 1324.87s\ttraining loss:\t0.083768\n",
      "Done 9120 batches in 1328.26s\ttraining loss:\t0.083730\n",
      "Done 9140 batches in 1331.38s\ttraining loss:\t0.083651\n",
      "Done 9160 batches in 1334.37s\ttraining loss:\t0.083545\n",
      "Done 9180 batches in 1337.46s\ttraining loss:\t0.083498\n",
      "Done 9200 batches in 1340.39s\ttraining loss:\t0.083500\n",
      "Done 9220 batches in 1343.22s\ttraining loss:\t0.083493\n",
      "Done 9240 batches in 1345.88s\ttraining loss:\t0.083437\n",
      "Done 9260 batches in 1348.66s\ttraining loss:\t0.083358\n",
      "Done 9280 batches in 1351.55s\ttraining loss:\t0.083276\n",
      "Done 9300 batches in 1354.83s\ttraining loss:\t0.083173\n",
      "Done 9320 batches in 1357.38s\ttraining loss:\t0.083213\n",
      "Done 9340 batches in 1360.10s\ttraining loss:\t0.083214\n",
      "Done 9360 batches in 1364.01s\ttraining loss:\t0.083218\n",
      "Done 9380 batches in 1366.85s\ttraining loss:\t0.083263\n",
      "Done 9400 batches in 1370.35s\ttraining loss:\t0.083433\n",
      "Done 9420 batches in 1373.02s\ttraining loss:\t0.083653\n",
      "Done 9440 batches in 1376.11s\ttraining loss:\t0.083935\n",
      "Done 9460 batches in 1379.49s\ttraining loss:\t0.084240\n",
      "Done 9480 batches in 1382.93s\ttraining loss:\t0.084413\n",
      "Done 9500 batches in 1386.31s\ttraining loss:\t0.084360\n",
      "Done 9520 batches in 1389.39s\ttraining loss:\t0.084280\n",
      "Done 9540 batches in 1392.32s\ttraining loss:\t0.084241\n",
      "Done 9560 batches in 1395.85s\ttraining loss:\t0.084275\n",
      "Done 9580 batches in 1399.25s\ttraining loss:\t0.084366\n",
      "Done 9600 batches in 1402.42s\ttraining loss:\t0.084426\n",
      "Done 9620 batches in 1405.17s\ttraining loss:\t0.084377\n",
      "Done 9640 batches in 1407.84s\ttraining loss:\t0.084359\n",
      "Done 9660 batches in 1410.96s\ttraining loss:\t0.084392\n",
      "Done 9680 batches in 1413.75s\ttraining loss:\t0.084692\n",
      "Done 9700 batches in 1417.08s\ttraining loss:\t0.085049\n",
      "Done 9720 batches in 1420.52s\ttraining loss:\t0.085108\n",
      "Done 9740 batches in 1423.92s\ttraining loss:\t0.085139\n",
      "Done 9760 batches in 1426.62s\ttraining loss:\t0.085137\n",
      "Done 9780 batches in 1429.34s\ttraining loss:\t0.085129\n",
      "Done 9800 batches in 1432.67s\ttraining loss:\t0.085066\n",
      "Done 9820 batches in 1435.80s\ttraining loss:\t0.085090\n",
      "Done 9840 batches in 1439.04s\ttraining loss:\t0.085091\n",
      "Done 9860 batches in 1442.78s\ttraining loss:\t0.085104\n",
      "Done 9880 batches in 1445.76s\ttraining loss:\t0.085102\n",
      "Done 9900 batches in 1448.53s\ttraining loss:\t0.085053\n",
      "Done 9920 batches in 1451.54s\ttraining loss:\t0.085088\n",
      "Done 9940 batches in 1454.99s\ttraining loss:\t0.085043\n",
      "Done 9960 batches in 1458.56s\ttraining loss:\t0.084959\n",
      "Done 9980 batches in 1461.39s\ttraining loss:\t0.084894\n",
      "Done 10000 batches in 1464.20s\ttraining loss:\t0.084833\n",
      "Done 10020 batches in 1467.39s\ttraining loss:\t0.084778\n",
      "Done 10040 batches in 1470.42s\ttraining loss:\t0.084694\n",
      "Done 10060 batches in 1473.26s\ttraining loss:\t0.084642\n",
      "Done 10080 batches in 1476.39s\ttraining loss:\t0.084574\n",
      "Done 10100 batches in 1479.33s\ttraining loss:\t0.084652\n",
      "Done 10120 batches in 1482.38s\ttraining loss:\t0.084640\n",
      "Done 10140 batches in 1485.57s\ttraining loss:\t0.084635\n",
      "Done 10160 batches in 1488.48s\ttraining loss:\t0.084623\n",
      "Done 10180 batches in 1491.07s\ttraining loss:\t0.084627\n",
      "Done 10200 batches in 1493.87s\ttraining loss:\t0.084721\n",
      "Done 10220 batches in 1496.94s\ttraining loss:\t0.084807\n",
      "Done 10240 batches in 1499.94s\ttraining loss:\t0.084855\n",
      "Done 10260 batches in 1502.82s\ttraining loss:\t0.084847\n",
      "Done 10280 batches in 1505.96s\ttraining loss:\t0.084766\n",
      "Done 10300 batches in 1509.23s\ttraining loss:\t0.084738\n",
      "Done 10320 batches in 1512.04s\ttraining loss:\t0.084703\n",
      "Done 10340 batches in 1514.79s\ttraining loss:\t0.084676\n",
      "Done 10360 batches in 1517.58s\ttraining loss:\t0.084633\n",
      "Done 10380 batches in 1520.37s\ttraining loss:\t0.084680\n",
      "Done 10400 batches in 1523.65s\ttraining loss:\t0.084639\n",
      "Done 10420 batches in 1527.62s\ttraining loss:\t0.084595\n",
      "Done 10440 batches in 1531.60s\ttraining loss:\t0.084549\n",
      "Done 10460 batches in 1534.71s\ttraining loss:\t0.084562\n",
      "Done 10480 batches in 1537.61s\ttraining loss:\t0.084678\n",
      "Done 10500 batches in 1540.28s\ttraining loss:\t0.084734\n",
      "Done 10520 batches in 1543.06s\ttraining loss:\t0.084730\n",
      "Done 10540 batches in 1546.37s\ttraining loss:\t0.084706\n",
      "Done 10560 batches in 1548.98s\ttraining loss:\t0.085080\n",
      "Done 10580 batches in 1552.04s\ttraining loss:\t0.085193\n",
      "Done 10600 batches in 1555.28s\ttraining loss:\t0.085324\n",
      "Done 10620 batches in 1558.53s\ttraining loss:\t0.085368\n",
      "Done 10640 batches in 1561.91s\ttraining loss:\t0.085380\n",
      "Done 10660 batches in 1564.67s\ttraining loss:\t0.085402\n",
      "Done 10680 batches in 1567.68s\ttraining loss:\t0.085436\n",
      "Done 10700 batches in 1570.72s\ttraining loss:\t0.085427\n",
      "Done 10720 batches in 1573.80s\ttraining loss:\t0.085421\n",
      "Done 10740 batches in 1576.68s\ttraining loss:\t0.085458\n",
      "Done 10760 batches in 1579.36s\ttraining loss:\t0.085450\n",
      "Done 10780 batches in 1582.24s\ttraining loss:\t0.085451\n",
      "Done 10800 batches in 1585.17s\ttraining loss:\t0.085493\n",
      "Done 10820 batches in 1587.50s\ttraining loss:\t0.085548\n",
      "Done 10840 batches in 1590.76s\ttraining loss:\t0.085705\n",
      "Done 10860 batches in 1593.42s\ttraining loss:\t0.085731\n",
      "Done 10880 batches in 1596.03s\ttraining loss:\t0.085700\n",
      "Done 10900 batches in 1599.10s\ttraining loss:\t0.085635\n",
      "Done 10920 batches in 1601.65s\ttraining loss:\t0.085632\n",
      "Done 10940 batches in 1605.12s\ttraining loss:\t0.085701\n",
      "Done 10960 batches in 1608.03s\ttraining loss:\t0.086006\n",
      "Done 10980 batches in 1611.36s\ttraining loss:\t0.086138\n",
      "Done 11000 batches in 1614.44s\ttraining loss:\t0.086157\n",
      "Done 11020 batches in 1617.16s\ttraining loss:\t0.086151\n",
      "Done 11040 batches in 1620.02s\ttraining loss:\t0.086219\n",
      "Done 11060 batches in 1623.00s\ttraining loss:\t0.086256\n",
      "Done 11080 batches in 1625.68s\ttraining loss:\t0.086209\n",
      "Done 11100 batches in 1628.83s\ttraining loss:\t0.086156\n",
      "Done 11120 batches in 1631.94s\ttraining loss:\t0.086098\n",
      "Done 11140 batches in 1634.49s\ttraining loss:\t0.086074\n",
      "Done 11160 batches in 1638.16s\ttraining loss:\t0.086146\n",
      "Done 11180 batches in 1641.69s\ttraining loss:\t0.086364\n",
      "Done 11200 batches in 1644.85s\ttraining loss:\t0.086382\n",
      "Done 11220 batches in 1647.81s\ttraining loss:\t0.086345\n",
      "Done 11240 batches in 1650.62s\ttraining loss:\t0.086317\n",
      "Done 11260 batches in 1654.42s\ttraining loss:\t0.086370\n",
      "Done 11280 batches in 1657.29s\ttraining loss:\t0.086387\n",
      "Done 11300 batches in 1660.34s\ttraining loss:\t0.086320\n",
      "Done 11320 batches in 1663.56s\ttraining loss:\t0.086279\n",
      "Done 11340 batches in 1666.76s\ttraining loss:\t0.086349\n",
      "Done 11360 batches in 1669.93s\ttraining loss:\t0.086545\n",
      "Done 11380 batches in 1673.71s\ttraining loss:\t0.086543\n",
      "Done 11400 batches in 1676.89s\ttraining loss:\t0.086529\n",
      "Done 11420 batches in 1680.41s\ttraining loss:\t0.086538\n",
      "Done 11440 batches in 1683.00s\ttraining loss:\t0.086604\n",
      "Done 11460 batches in 1686.68s\ttraining loss:\t0.086623\n",
      "Done 11480 batches in 1690.24s\ttraining loss:\t0.086567\n",
      "Done 11500 batches in 1693.33s\ttraining loss:\t0.086592\n",
      "Done 11520 batches in 1696.84s\ttraining loss:\t0.086629\n",
      "Done 11540 batches in 1700.12s\ttraining loss:\t0.086592\n",
      "Done 11560 batches in 1703.13s\ttraining loss:\t0.086567\n",
      "Done 11580 batches in 1706.36s\ttraining loss:\t0.086588\n",
      "Done 11600 batches in 1709.65s\ttraining loss:\t0.086594\n",
      "Done 11620 batches in 1712.93s\ttraining loss:\t0.086672\n",
      "Done 11640 batches in 1715.73s\ttraining loss:\t0.086750\n",
      "Done 11660 batches in 1718.97s\ttraining loss:\t0.086914\n",
      "Done 11680 batches in 1722.60s\ttraining loss:\t0.086937\n",
      "Done 11700 batches in 1726.19s\ttraining loss:\t0.086878\n",
      "Done 11720 batches in 1729.22s\ttraining loss:\t0.086859\n",
      "Done 11740 batches in 1732.24s\ttraining loss:\t0.086939\n",
      "Done 11760 batches in 1735.52s\ttraining loss:\t0.087036\n",
      "Done 11780 batches in 1738.48s\ttraining loss:\t0.087095\n",
      "Done 11800 batches in 1741.53s\ttraining loss:\t0.087146\n",
      "Done 11820 batches in 1744.66s\ttraining loss:\t0.087163\n",
      "Done 11840 batches in 1747.42s\ttraining loss:\t0.087136\n",
      "Done 11860 batches in 1750.88s\ttraining loss:\t0.087115\n",
      "Done 11880 batches in 1754.07s\ttraining loss:\t0.087111\n",
      "Done 11900 batches in 1757.26s\ttraining loss:\t0.087101\n",
      "Done 11920 batches in 1760.44s\ttraining loss:\t0.087076\n",
      "Done 11940 batches in 1763.78s\ttraining loss:\t0.087062\n",
      "Done 11960 batches in 1766.66s\ttraining loss:\t0.087026\n",
      "Done 11980 batches in 1770.04s\ttraining loss:\t0.086997\n",
      "Done 12000 batches in 1772.95s\ttraining loss:\t0.086915\n",
      "Done 12020 batches in 1776.15s\ttraining loss:\t0.086897\n",
      "Done 12040 batches in 1779.18s\ttraining loss:\t0.086878\n",
      "Done 12060 batches in 1782.10s\ttraining loss:\t0.086851\n",
      "Done 12080 batches in 1785.32s\ttraining loss:\t0.086816\n",
      "Done 12100 batches in 1787.99s\ttraining loss:\t0.086769\n",
      "Done 12120 batches in 1790.87s\ttraining loss:\t0.086692\n",
      "Done 12140 batches in 1793.53s\ttraining loss:\t0.086630\n",
      "Done 12160 batches in 1796.33s\ttraining loss:\t0.086603\n",
      "Done 12180 batches in 1799.29s\ttraining loss:\t0.086556\n",
      "Done 12200 batches in 1801.74s\ttraining loss:\t0.086538\n",
      "Done 12220 batches in 1804.21s\ttraining loss:\t0.086524\n",
      "Done 12240 batches in 1807.43s\ttraining loss:\t0.086506\n",
      "Done 12260 batches in 1810.46s\ttraining loss:\t0.086511\n",
      "Done 12280 batches in 1813.81s\ttraining loss:\t0.086602\n",
      "Done 12300 batches in 1816.62s\ttraining loss:\t0.086688\n",
      "Done 12320 batches in 1819.68s\ttraining loss:\t0.086632\n",
      "Done 12340 batches in 1822.57s\ttraining loss:\t0.086583\n",
      "Done 12360 batches in 1825.92s\ttraining loss:\t0.086534\n",
      "Done 12380 batches in 1829.79s\ttraining loss:\t0.086492\n",
      "Done 12400 batches in 1833.48s\ttraining loss:\t0.086561\n",
      "Done 12420 batches in 1837.00s\ttraining loss:\t0.086588\n",
      "Done 12440 batches in 1840.24s\ttraining loss:\t0.086585\n",
      "Done 12460 batches in 1842.93s\ttraining loss:\t0.086676\n",
      "Done 12480 batches in 1845.85s\ttraining loss:\t0.086773\n",
      "Done 12500 batches in 1849.35s\ttraining loss:\t0.086796\n",
      "Done 12520 batches in 1852.28s\ttraining loss:\t0.086830\n",
      "Done 12540 batches in 1855.58s\ttraining loss:\t0.086806\n",
      "Done 12560 batches in 1858.42s\ttraining loss:\t0.086833\n",
      "Done 12580 batches in 1861.22s\ttraining loss:\t0.086874\n",
      "Done 12600 batches in 1864.58s\ttraining loss:\t0.086872\n",
      "Done 12620 batches in 1868.16s\ttraining loss:\t0.086881\n",
      "Done 12640 batches in 1871.45s\ttraining loss:\t0.086878\n",
      "Done 12660 batches in 1874.62s\ttraining loss:\t0.086863\n",
      "Done 12680 batches in 1877.48s\ttraining loss:\t0.086878\n",
      "Done 12700 batches in 1880.94s\ttraining loss:\t0.086880\n",
      "Done 12720 batches in 1883.50s\ttraining loss:\t0.086955\n",
      "Done 12740 batches in 1886.52s\ttraining loss:\t0.086976\n",
      "Done 12760 batches in 1890.24s\ttraining loss:\t0.086935\n",
      "Done 12780 batches in 1893.53s\ttraining loss:\t0.086930\n",
      "Done 12800 batches in 1896.07s\ttraining loss:\t0.086984\n",
      "Done 12820 batches in 1899.13s\ttraining loss:\t0.086944\n",
      "Done 12840 batches in 1901.73s\ttraining loss:\t0.086872\n",
      "Done 12860 batches in 1904.09s\ttraining loss:\t0.086797\n",
      "Done 12880 batches in 1906.85s\ttraining loss:\t0.086715\n",
      "Done 12900 batches in 1910.22s\ttraining loss:\t0.086696\n",
      "Done 12920 batches in 1913.60s\ttraining loss:\t0.086639\n",
      "Done 12940 batches in 1917.32s\ttraining loss:\t0.086610\n",
      "Done 12960 batches in 1920.63s\ttraining loss:\t0.086592\n",
      "Done 12980 batches in 1924.13s\ttraining loss:\t0.086561\n",
      "Done 13000 batches in 1927.53s\ttraining loss:\t0.086513\n",
      "Done 13020 batches in 1930.63s\ttraining loss:\t0.086479\n",
      "Done 13040 batches in 1934.25s\ttraining loss:\t0.086401\n",
      "Done 13060 batches in 1937.20s\ttraining loss:\t0.086355\n",
      "Done 13080 batches in 1940.37s\ttraining loss:\t0.086443\n",
      "Done 13100 batches in 1943.56s\ttraining loss:\t0.086472\n",
      "Done 13120 batches in 1946.54s\ttraining loss:\t0.086480\n",
      "Done 13140 batches in 1949.47s\ttraining loss:\t0.086489\n",
      "Done 13160 batches in 1952.55s\ttraining loss:\t0.086492\n",
      "Done 13180 batches in 1955.28s\ttraining loss:\t0.086539\n",
      "Done 13200 batches in 1958.48s\ttraining loss:\t0.086539\n",
      "Done 13220 batches in 1961.32s\ttraining loss:\t0.086519\n",
      "Done 13240 batches in 1964.58s\ttraining loss:\t0.086526\n",
      "Done 13260 batches in 1967.15s\ttraining loss:\t0.086562\n",
      "Done 13280 batches in 1969.91s\ttraining loss:\t0.086590\n",
      "Done 13300 batches in 1972.98s\ttraining loss:\t0.086588\n",
      "Done 13320 batches in 1975.97s\ttraining loss:\t0.086608\n",
      "Done 13340 batches in 1979.89s\ttraining loss:\t0.086635\n",
      "Done 13360 batches in 1983.44s\ttraining loss:\t0.086620\n",
      "Done 13380 batches in 1986.58s\ttraining loss:\t0.086622\n",
      "Done 13400 batches in 1989.61s\ttraining loss:\t0.086608\n",
      "Done 13420 batches in 1992.64s\ttraining loss:\t0.086584\n",
      "Done 13440 batches in 1995.73s\ttraining loss:\t0.086603\n",
      "Done 13460 batches in 1999.15s\ttraining loss:\t0.086641\n",
      "Done 13480 batches in 2002.51s\ttraining loss:\t0.086614\n",
      "Done 13500 batches in 2005.39s\ttraining loss:\t0.086633\n",
      "Done 13520 batches in 2008.72s\ttraining loss:\t0.086643\n",
      "Done 13540 batches in 2012.07s\ttraining loss:\t0.086688\n",
      "Done 13560 batches in 2015.19s\ttraining loss:\t0.086756\n",
      "Done 13580 batches in 2018.66s\ttraining loss:\t0.086691\n",
      "Done 13600 batches in 2021.79s\ttraining loss:\t0.086642\n",
      "Done 13620 batches in 2025.43s\ttraining loss:\t0.086657\n",
      "Done 13640 batches in 2028.28s\ttraining loss:\t0.086666\n",
      "Done 13660 batches in 2031.54s\ttraining loss:\t0.086701\n",
      "Done 13680 batches in 2034.78s\ttraining loss:\t0.086736\n",
      "Done 13700 batches in 2037.30s\ttraining loss:\t0.086726\n",
      "Done 13720 batches in 2040.22s\ttraining loss:\t0.086734\n",
      "Done 13740 batches in 2043.61s\ttraining loss:\t0.086754\n",
      "Done 13760 batches in 2046.73s\ttraining loss:\t0.086733\n",
      "Done 13780 batches in 2050.07s\ttraining loss:\t0.086686\n",
      "Done 13800 batches in 2053.13s\ttraining loss:\t0.086738\n",
      "Done 13820 batches in 2056.31s\ttraining loss:\t0.086694\n",
      "Done 13840 batches in 2059.36s\ttraining loss:\t0.086654\n",
      "Done 13860 batches in 2062.45s\ttraining loss:\t0.086651\n",
      "Done 13880 batches in 2066.09s\ttraining loss:\t0.086657\n",
      "Done 13900 batches in 2069.65s\ttraining loss:\t0.086661\n",
      "Done 13920 batches in 2072.63s\ttraining loss:\t0.086817\n",
      "Done 13940 batches in 2075.15s\ttraining loss:\t0.087026\n",
      "Done 13960 batches in 2078.00s\ttraining loss:\t0.087218\n",
      "Done 13980 batches in 2081.21s\ttraining loss:\t0.087249\n",
      "Done 14000 batches in 2084.72s\ttraining loss:\t0.087270\n",
      "Done 14020 batches in 2087.39s\ttraining loss:\t0.087457\n",
      "Done 14040 batches in 2090.09s\ttraining loss:\t0.087541\n",
      "Done 14060 batches in 2093.17s\ttraining loss:\t0.087506\n",
      "Done 14080 batches in 2096.24s\ttraining loss:\t0.087404\n",
      "Done 14100 batches in 2099.39s\ttraining loss:\t0.087358\n",
      "Done 14120 batches in 2102.65s\ttraining loss:\t0.087349\n",
      "Done 14140 batches in 2105.86s\ttraining loss:\t0.087368\n",
      "Done 14160 batches in 2108.56s\ttraining loss:\t0.087355\n",
      "Done 14180 batches in 2111.66s\ttraining loss:\t0.087357\n",
      "Done 14200 batches in 2114.85s\ttraining loss:\t0.087397\n",
      "Done 14220 batches in 2117.65s\ttraining loss:\t0.087570\n",
      "Done 14240 batches in 2120.50s\ttraining loss:\t0.087694\n",
      "Done 14260 batches in 2123.23s\ttraining loss:\t0.087727\n",
      "Done 14280 batches in 2125.94s\ttraining loss:\t0.087722\n",
      "Done 14300 batches in 2129.18s\ttraining loss:\t0.087702\n",
      "Done 14320 batches in 2132.57s\ttraining loss:\t0.087923\n",
      "Done 14340 batches in 2136.15s\ttraining loss:\t0.088073\n",
      "Done 14360 batches in 2138.74s\ttraining loss:\t0.088051\n",
      "Done 14380 batches in 2141.68s\ttraining loss:\t0.088048\n",
      "Done 14400 batches in 2144.34s\ttraining loss:\t0.088021\n",
      "Done 14420 batches in 2147.48s\ttraining loss:\t0.088014\n",
      "Done 14440 batches in 2150.35s\ttraining loss:\t0.087984\n",
      "Done 14460 batches in 2153.41s\ttraining loss:\t0.087939\n",
      "Done 14480 batches in 2156.25s\ttraining loss:\t0.087912\n",
      "Done 14500 batches in 2158.87s\ttraining loss:\t0.087949\n",
      "Done 14520 batches in 2161.46s\ttraining loss:\t0.088075\n",
      "Done 14540 batches in 2165.03s\ttraining loss:\t0.088171\n",
      "Done 14560 batches in 2167.67s\ttraining loss:\t0.088207\n",
      "Done 14580 batches in 2170.68s\ttraining loss:\t0.088147\n",
      "Done 14600 batches in 2173.65s\ttraining loss:\t0.088105\n",
      "Done 14620 batches in 2176.28s\ttraining loss:\t0.088072\n",
      "Done 14640 batches in 2179.84s\ttraining loss:\t0.088257\n",
      "Done 14660 batches in 2183.11s\ttraining loss:\t0.088443\n",
      "Done 14680 batches in 2186.12s\ttraining loss:\t0.088447\n",
      "Done 14700 batches in 2189.42s\ttraining loss:\t0.088377\n",
      "Done 14720 batches in 2192.56s\ttraining loss:\t0.088428\n",
      "Done 14740 batches in 2195.55s\ttraining loss:\t0.088413\n",
      "Done 14760 batches in 2198.20s\ttraining loss:\t0.088389\n",
      "Done 14780 batches in 2201.18s\ttraining loss:\t0.088436\n",
      "Done 14800 batches in 2203.97s\ttraining loss:\t0.088479\n",
      "Done 14820 batches in 2207.25s\ttraining loss:\t0.088539\n",
      "Done 14840 batches in 2210.08s\ttraining loss:\t0.088593\n",
      "Done 14860 batches in 2212.89s\ttraining loss:\t0.088657\n",
      "Done 14880 batches in 2215.70s\ttraining loss:\t0.088726\n",
      "Done 14900 batches in 2219.25s\ttraining loss:\t0.088777\n",
      "Done 14920 batches in 2222.32s\ttraining loss:\t0.088756\n",
      "Done 14940 batches in 2225.10s\ttraining loss:\t0.088715\n",
      "Done 14960 batches in 2228.08s\ttraining loss:\t0.088669\n",
      "Done 14980 batches in 2230.82s\ttraining loss:\t0.088644\n",
      "Done 15000 batches in 2234.13s\ttraining loss:\t0.088635\n",
      "Done 15020 batches in 2237.74s\ttraining loss:\t0.088595\n",
      "Done 15040 batches in 2241.17s\ttraining loss:\t0.088603\n",
      "Done 15060 batches in 2244.50s\ttraining loss:\t0.088571\n",
      "Done 15080 batches in 2247.39s\ttraining loss:\t0.088630\n",
      "Done 15100 batches in 2250.65s\ttraining loss:\t0.088659\n",
      "Done 15120 batches in 2253.82s\ttraining loss:\t0.088637\n",
      "Done 15140 batches in 2256.97s\ttraining loss:\t0.088611\n",
      "Done 15160 batches in 2260.06s\ttraining loss:\t0.088609\n",
      "Done 15180 batches in 2262.80s\ttraining loss:\t0.088588\n",
      "Done 15200 batches in 2265.72s\ttraining loss:\t0.088587\n",
      "Done 15220 batches in 2268.45s\ttraining loss:\t0.088603\n",
      "Done 15240 batches in 2271.40s\ttraining loss:\t0.088608\n",
      "Done 15260 batches in 2274.23s\ttraining loss:\t0.088597\n",
      "Done 15280 batches in 2276.77s\ttraining loss:\t0.088610\n",
      "Done 15300 batches in 2279.48s\ttraining loss:\t0.088628\n",
      "Done 15320 batches in 2282.20s\ttraining loss:\t0.088661\n",
      "Done 15340 batches in 2284.61s\ttraining loss:\t0.088647\n",
      "Done 15360 batches in 2287.58s\ttraining loss:\t0.088608\n",
      "Done 15380 batches in 2290.73s\ttraining loss:\t0.088736\n",
      "Done 15400 batches in 2293.78s\ttraining loss:\t0.088858\n",
      "Done 15420 batches in 2297.20s\ttraining loss:\t0.088989\n",
      "Done 15440 batches in 2300.45s\ttraining loss:\t0.089067\n",
      "Done 15460 batches in 2303.09s\ttraining loss:\t0.089064\n",
      "Done 15480 batches in 2305.33s\ttraining loss:\t0.089038\n",
      "Done 15500 batches in 2307.78s\ttraining loss:\t0.089025\n",
      "Done 15520 batches in 2310.78s\ttraining loss:\t0.089016\n",
      "Done 15540 batches in 2313.88s\ttraining loss:\t0.088978\n",
      "Done 15560 batches in 2316.80s\ttraining loss:\t0.088913\n",
      "Done 15580 batches in 2319.73s\ttraining loss:\t0.088848\n",
      "Done 15600 batches in 2322.79s\ttraining loss:\t0.088868\n",
      "Done 15620 batches in 2325.65s\ttraining loss:\t0.088814\n",
      "Done 15640 batches in 2329.17s\ttraining loss:\t0.088778\n",
      "Done 15660 batches in 2332.00s\ttraining loss:\t0.088745\n",
      "Done 15680 batches in 2335.07s\ttraining loss:\t0.088732\n",
      "Done 15700 batches in 2338.05s\ttraining loss:\t0.088738\n",
      "Done 15720 batches in 2341.16s\ttraining loss:\t0.088750\n",
      "Done 15740 batches in 2344.26s\ttraining loss:\t0.088744\n",
      "Done 15760 batches in 2347.02s\ttraining loss:\t0.088755\n",
      "Done 15780 batches in 2349.85s\ttraining loss:\t0.088738\n",
      "Done 15800 batches in 2352.76s\ttraining loss:\t0.088729\n",
      "Done 15820 batches in 2355.73s\ttraining loss:\t0.088700\n",
      "Done 15840 batches in 2358.61s\ttraining loss:\t0.088665\n",
      "Done 15860 batches in 2361.40s\ttraining loss:\t0.088701\n",
      "Done 15880 batches in 2364.20s\ttraining loss:\t0.088742\n",
      "Done 15900 batches in 2367.76s\ttraining loss:\t0.088834\n",
      "Done 15920 batches in 2370.97s\ttraining loss:\t0.088869\n",
      "Done 15940 batches in 2373.55s\ttraining loss:\t0.088884\n",
      "Done 15960 batches in 2376.98s\ttraining loss:\t0.088889\n",
      "Done 15980 batches in 2380.13s\ttraining loss:\t0.088904\n",
      "Done 16000 batches in 2383.15s\ttraining loss:\t0.088864\n",
      "Done 16020 batches in 2386.39s\ttraining loss:\t0.088825\n",
      "Done 16040 batches in 2389.64s\ttraining loss:\t0.088861\n",
      "Done 16060 batches in 2392.92s\ttraining loss:\t0.088839\n",
      "Done 16080 batches in 2396.15s\ttraining loss:\t0.088823\n",
      "Done 16100 batches in 2399.05s\ttraining loss:\t0.088799\n",
      "Done 16120 batches in 2402.02s\ttraining loss:\t0.088768\n",
      "Done 16140 batches in 2405.77s\ttraining loss:\t0.088923\n",
      "Done 16160 batches in 2408.95s\ttraining loss:\t0.088945\n",
      "Done 16180 batches in 2411.72s\ttraining loss:\t0.088910\n",
      "Done 16200 batches in 2415.03s\ttraining loss:\t0.088925\n",
      "Done 16220 batches in 2418.26s\ttraining loss:\t0.088963\n",
      "Done 16240 batches in 2421.85s\ttraining loss:\t0.088990\n",
      "Done 16260 batches in 2425.42s\ttraining loss:\t0.088998\n",
      "Done 16280 batches in 2428.20s\ttraining loss:\t0.088990\n",
      "Done 16300 batches in 2431.08s\ttraining loss:\t0.088979\n",
      "Done 16320 batches in 2433.97s\ttraining loss:\t0.088951\n",
      "Done 16340 batches in 2437.01s\ttraining loss:\t0.088936\n",
      "Done 16360 batches in 2440.33s\ttraining loss:\t0.088889\n",
      "Done 16380 batches in 2443.72s\ttraining loss:\t0.088830\n",
      "Done 16400 batches in 2446.94s\ttraining loss:\t0.088782\n",
      "Done 16420 batches in 2449.63s\ttraining loss:\t0.088786\n",
      "Done 16440 batches in 2452.52s\ttraining loss:\t0.088773\n",
      "Done 16460 batches in 2455.54s\ttraining loss:\t0.088762\n",
      "Done 16480 batches in 2458.50s\ttraining loss:\t0.088785\n",
      "Done 16500 batches in 2461.45s\ttraining loss:\t0.088813\n",
      "Done 16520 batches in 2464.43s\ttraining loss:\t0.088802\n",
      "Done 16540 batches in 2467.02s\ttraining loss:\t0.088793\n",
      "Done 16560 batches in 2469.32s\ttraining loss:\t0.088778\n",
      "Done 16580 batches in 2472.82s\ttraining loss:\t0.088745\n",
      "Done 16600 batches in 2476.08s\ttraining loss:\t0.088703\n",
      "Done 16620 batches in 2479.14s\ttraining loss:\t0.088654\n",
      "Done 16640 batches in 2482.04s\ttraining loss:\t0.088582\n",
      "Done 16660 batches in 2485.15s\ttraining loss:\t0.088534\n",
      "Done 16680 batches in 2488.05s\ttraining loss:\t0.088491\n",
      "Done 16700 batches in 2490.99s\ttraining loss:\t0.088477\n",
      "Done 16720 batches in 2494.84s\ttraining loss:\t0.088473\n",
      "Done 16740 batches in 2498.63s\ttraining loss:\t0.088493\n",
      "Done 16760 batches in 2501.77s\ttraining loss:\t0.088491\n",
      "Done 16780 batches in 2504.37s\ttraining loss:\t0.088508\n",
      "Done 16800 batches in 2506.89s\ttraining loss:\t0.088514\n",
      "Done 16820 batches in 2510.29s\ttraining loss:\t0.088515\n",
      "Done 16840 batches in 2513.35s\ttraining loss:\t0.088513\n",
      "Done 16860 batches in 2516.67s\ttraining loss:\t0.088510\n",
      "Done 16880 batches in 2519.81s\ttraining loss:\t0.088509\n",
      "Done 16900 batches in 2522.90s\ttraining loss:\t0.088581\n",
      "Done 16920 batches in 2525.88s\ttraining loss:\t0.088571\n",
      "Done 16940 batches in 2529.04s\ttraining loss:\t0.088551\n",
      "Done 16960 batches in 2532.27s\ttraining loss:\t0.088557\n",
      "Done 16980 batches in 2535.22s\ttraining loss:\t0.088527\n",
      "Done 17000 batches in 2538.74s\ttraining loss:\t0.088495\n",
      "Done 17020 batches in 2541.63s\ttraining loss:\t0.088479\n",
      "Done 17040 batches in 2545.02s\ttraining loss:\t0.088452\n",
      "Done 17060 batches in 2548.13s\ttraining loss:\t0.088467\n",
      "Done 17080 batches in 2551.35s\ttraining loss:\t0.088471\n",
      "Done 17100 batches in 2554.56s\ttraining loss:\t0.088448\n",
      "Done 17120 batches in 2557.94s\ttraining loss:\t0.088397\n",
      "Done 17140 batches in 2560.62s\ttraining loss:\t0.088377\n",
      "Done 17160 batches in 2563.66s\ttraining loss:\t0.088374\n",
      "Done 17180 batches in 2567.17s\ttraining loss:\t0.088421\n",
      "Done 17200 batches in 2569.85s\ttraining loss:\t0.088406\n",
      "Done 17220 batches in 2572.69s\ttraining loss:\t0.088424\n",
      "Done 17240 batches in 2575.49s\ttraining loss:\t0.088436\n",
      "Done 17260 batches in 2578.22s\ttraining loss:\t0.088445\n",
      "Done 17280 batches in 2580.91s\ttraining loss:\t0.088432\n",
      "Done 17300 batches in 2583.54s\ttraining loss:\t0.088433\n",
      "Done 17320 batches in 2586.07s\ttraining loss:\t0.088382\n",
      "Done 17340 batches in 2588.72s\ttraining loss:\t0.088340\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.088305816713791049"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hred_net.train_one_epoch(data_trimmed, 5, log_interval=20) # normal binary cross-entropy with glove init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hred_net.save_params()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: GeForce GTX 780 (CNMeM is enabled with initial size: 30.0% of memory, cuDNN 5105)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "import numpy as np\n",
    "from squad_load import get_glove_train_embs, get_squad_train_voc, load_squad_train\n",
    "from itertools import chain\n",
    "\n",
    "%aimport QANet\n",
    "%aimport HRED_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples total: 87599\n",
      "Working examples: 86474\n"
     ]
    }
   ],
   "source": [
    "squad_path = '/pio/data/data/squad/'\n",
    "glove_path = '/pio/data/data/glove_vec/6B/'\n",
    "\n",
    "data = load_squad_train(squad_path)\n",
    "i_to_w, w_to_i, voc_size = get_squad_train_voc(squad_path)\n",
    "glove_embs = get_glove_train_embs(squad_path, glove_path)\n",
    "\n",
    "def get_w(idx):\n",
    "    return i_to_w[idx]\n",
    "\n",
    "def filter_broken_answers(data):\n",
    "    return [d for d in data if d[0]]\n",
    "\n",
    "print 'Examples total:', len(data)\n",
    "\n",
    "data = filter_broken_answers(data)\n",
    "\n",
    "print 'Working examples:', len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87599, 86789)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def trim_data(data, trim=100):\n",
    "    return [d for d in data if max(map(len, d[1])) <= trim]\n",
    "\n",
    "data_trimmed = trim_data(data)\n",
    "len(data), len(data_trimmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QANet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model...\n",
      "Compiling theano functions...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "qa_net = QANet.QANet(voc_size=voc_size,\n",
    "                     emb_size=300,\n",
    "                     rec_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 20 batches in 7.42s\ttraining loss:\t10.478611\n",
      "Done 40 batches in 14.69s\ttraining loss:\t10.027485\n",
      "Done 60 batches in 22.49s\ttraining loss:\t9.594314\n",
      "Done 80 batches in 29.89s\ttraining loss:\t9.252035\n",
      "Done 100 batches in 37.41s\ttraining loss:\t8.876036\n",
      "Done 120 batches in 44.87s\ttraining loss:\t8.926693\n",
      "Done 140 batches in 51.85s\ttraining loss:\t8.656413\n",
      "Done 160 batches in 58.01s\ttraining loss:\t8.347217\n",
      "Done 180 batches in 64.93s\ttraining loss:\t8.118872\n",
      "Done 200 batches in 71.03s\ttraining loss:\t7.919667\n",
      "Done 220 batches in 78.75s\ttraining loss:\t7.795685\n",
      "Done 240 batches in 85.78s\ttraining loss:\t7.673677\n",
      "Done 260 batches in 92.36s\ttraining loss:\t7.615648\n",
      "Done 280 batches in 99.02s\ttraining loss:\t7.540717\n",
      "Done 300 batches in 105.88s\ttraining loss:\t7.519067\n",
      "Done 320 batches in 111.58s\ttraining loss:\t7.349733\n",
      "Done 340 batches in 118.06s\ttraining loss:\t7.233174\n",
      "Done 360 batches in 126.22s\ttraining loss:\t7.179774\n",
      "Done 380 batches in 131.95s\ttraining loss:\t7.092907\n",
      "Done 400 batches in 136.90s\ttraining loss:\t6.981471\n",
      "Done 420 batches in 142.39s\ttraining loss:\t6.888877\n",
      "Done 440 batches in 148.86s\ttraining loss:\t6.813112\n",
      "Done 460 batches in 155.25s\ttraining loss:\t6.744000\n",
      "Done 480 batches in 162.46s\ttraining loss:\t6.700695\n",
      "Done 500 batches in 169.89s\ttraining loss:\t6.632010\n",
      "Done 520 batches in 175.70s\ttraining loss:\t6.586960\n",
      "Done 540 batches in 180.98s\ttraining loss:\t6.511653\n",
      "Done 560 batches in 186.88s\ttraining loss:\t6.455828\n",
      "Done 580 batches in 194.58s\ttraining loss:\t6.402196\n",
      "Done 600 batches in 200.35s\ttraining loss:\t6.357399\n",
      "Done 620 batches in 206.58s\ttraining loss:\t6.312548\n",
      "Done 640 batches in 212.79s\ttraining loss:\t6.278920\n",
      "Done 660 batches in 219.10s\ttraining loss:\t6.249654\n",
      "Done 680 batches in 224.68s\ttraining loss:\t6.202593\n",
      "Done 700 batches in 229.71s\ttraining loss:\t6.159956\n",
      "Done 720 batches in 234.98s\ttraining loss:\t6.130556\n",
      "Done 740 batches in 240.96s\ttraining loss:\t6.090091\n",
      "Done 760 batches in 247.16s\ttraining loss:\t6.047019\n",
      "Done 780 batches in 252.60s\ttraining loss:\t5.997145\n",
      "Done 800 batches in 258.06s\ttraining loss:\t5.955173\n",
      "Done 820 batches in 264.01s\ttraining loss:\t5.916089\n",
      "Done 840 batches in 270.53s\ttraining loss:\t5.872704\n",
      "Done 860 batches in 276.92s\ttraining loss:\t5.831538\n",
      "Done 880 batches in 282.95s\ttraining loss:\t5.794558\n",
      "Done 900 batches in 288.96s\ttraining loss:\t5.757450\n",
      "Done 920 batches in 294.89s\ttraining loss:\t5.726492\n",
      "Done 940 batches in 302.59s\ttraining loss:\t5.713602\n",
      "Done 960 batches in 309.91s\ttraining loss:\t5.687121\n",
      "Done 980 batches in 316.20s\ttraining loss:\t5.705335\n",
      "Done 1000 batches in 322.71s\ttraining loss:\t5.698780\n",
      "Done 1020 batches in 328.74s\ttraining loss:\t5.695532\n",
      "Done 1040 batches in 334.53s\ttraining loss:\t5.689040\n",
      "Done 1060 batches in 340.30s\ttraining loss:\t5.675752\n",
      "Done 1080 batches in 346.00s\ttraining loss:\t5.654539\n",
      "Done 1100 batches in 351.88s\ttraining loss:\t5.643246\n",
      "Done 1120 batches in 358.12s\ttraining loss:\t5.621617\n",
      "Done 1140 batches in 364.19s\ttraining loss:\t5.604476\n",
      "Done 1160 batches in 370.26s\ttraining loss:\t5.582107\n",
      "Done 1180 batches in 376.97s\ttraining loss:\t5.571496\n",
      "Done 1200 batches in 384.78s\ttraining loss:\t5.559333\n",
      "Done 1220 batches in 391.94s\ttraining loss:\t5.546231\n",
      "Done 1240 batches in 399.13s\ttraining loss:\t5.525426\n",
      "Done 1260 batches in 405.97s\ttraining loss:\t5.510896\n",
      "Done 1280 batches in 411.31s\ttraining loss:\t5.499303\n",
      "Done 1300 batches in 416.61s\ttraining loss:\t5.485819\n",
      "Done 1320 batches in 422.44s\ttraining loss:\t5.461077\n",
      "Done 1340 batches in 428.44s\ttraining loss:\t5.426835\n",
      "Done 1360 batches in 434.40s\ttraining loss:\t5.400033\n",
      "Done 1380 batches in 440.29s\ttraining loss:\t5.373163\n",
      "Done 1400 batches in 446.82s\ttraining loss:\t5.356131\n",
      "Done 1420 batches in 452.14s\ttraining loss:\t5.326238\n",
      "Done 1440 batches in 456.33s\ttraining loss:\t5.294720\n",
      "Done 1460 batches in 460.93s\ttraining loss:\t5.263149\n",
      "Done 1480 batches in 464.67s\ttraining loss:\t5.231778\n",
      "Done 1500 batches in 470.52s\ttraining loss:\t5.200919\n",
      "Done 1520 batches in 477.09s\ttraining loss:\t5.183161\n",
      "Done 1540 batches in 482.76s\ttraining loss:\t5.169418\n",
      "Done 1560 batches in 489.04s\ttraining loss:\t5.155224\n",
      "Done 1580 batches in 494.69s\ttraining loss:\t5.142246\n",
      "Done 1600 batches in 499.88s\ttraining loss:\t5.126823\n",
      "Done 1620 batches in 505.83s\ttraining loss:\t5.119417\n",
      "Done 1640 batches in 513.56s\ttraining loss:\t5.103091\n",
      "Done 1660 batches in 520.16s\ttraining loss:\t5.087929\n",
      "Done 1680 batches in 528.02s\ttraining loss:\t5.074596\n",
      "Done 1700 batches in 534.29s\ttraining loss:\t5.057415\n",
      "Done 1720 batches in 540.83s\ttraining loss:\t5.075432\n",
      "Done 1740 batches in 546.79s\ttraining loss:\t5.072319\n",
      "Done 1760 batches in 553.33s\ttraining loss:\t5.081602\n",
      "Done 1780 batches in 560.46s\ttraining loss:\t5.068404\n",
      "Done 1800 batches in 566.27s\ttraining loss:\t5.052806\n",
      "Done 1820 batches in 572.52s\ttraining loss:\t5.038939\n",
      "Done 1840 batches in 579.21s\ttraining loss:\t5.025692\n",
      "Done 1860 batches in 584.83s\ttraining loss:\t5.014708\n",
      "Done 1880 batches in 590.21s\ttraining loss:\t5.002577\n",
      "Done 1900 batches in 595.44s\ttraining loss:\t4.992350\n",
      "Done 1920 batches in 600.12s\ttraining loss:\t4.977703\n",
      "Done 1940 batches in 605.59s\ttraining loss:\t4.967338\n",
      "Done 1960 batches in 611.07s\ttraining loss:\t4.949747\n",
      "Done 1980 batches in 616.76s\ttraining loss:\t4.937816\n",
      "Done 2000 batches in 622.29s\ttraining loss:\t4.936263\n",
      "Done 2020 batches in 628.13s\ttraining loss:\t4.930846\n",
      "Done 2040 batches in 633.83s\ttraining loss:\t4.925184\n",
      "Done 2060 batches in 640.58s\ttraining loss:\t4.924871\n",
      "Done 2080 batches in 646.46s\ttraining loss:\t4.919613\n",
      "Done 2100 batches in 652.26s\ttraining loss:\t4.907884\n",
      "Done 2120 batches in 658.51s\ttraining loss:\t4.900140\n",
      "Done 2140 batches in 663.75s\ttraining loss:\t4.896724\n",
      "Done 2160 batches in 668.61s\ttraining loss:\t4.889897\n",
      "Done 2180 batches in 673.83s\ttraining loss:\t4.881670\n",
      "Done 2200 batches in 679.23s\ttraining loss:\t4.871057\n",
      "Done 2220 batches in 684.61s\ttraining loss:\t4.864455\n",
      "Done 2240 batches in 691.55s\ttraining loss:\t4.854322\n",
      "Done 2260 batches in 698.60s\ttraining loss:\t4.840000\n",
      "Done 2280 batches in 704.62s\ttraining loss:\t4.826772\n",
      "Done 2300 batches in 710.30s\ttraining loss:\t4.813769\n",
      "Done 2320 batches in 715.75s\ttraining loss:\t4.803552\n",
      "Done 2340 batches in 721.53s\ttraining loss:\t4.794238\n",
      "Done 2360 batches in 728.43s\ttraining loss:\t4.786250\n",
      "Done 2380 batches in 734.83s\ttraining loss:\t4.781668\n",
      "Done 2400 batches in 742.27s\ttraining loss:\t4.768242\n",
      "Done 2420 batches in 748.19s\ttraining loss:\t4.759768\n",
      "Done 2440 batches in 754.14s\ttraining loss:\t4.748216\n",
      "Done 2460 batches in 760.70s\ttraining loss:\t4.743147\n",
      "Done 2480 batches in 765.99s\ttraining loss:\t4.729740\n",
      "Done 2500 batches in 771.22s\ttraining loss:\t4.716191\n",
      "Done 2520 batches in 776.80s\ttraining loss:\t4.704321\n",
      "Done 2540 batches in 782.49s\ttraining loss:\t4.690401\n",
      "Done 2560 batches in 787.81s\ttraining loss:\t4.679982\n",
      "Done 2580 batches in 793.00s\ttraining loss:\t4.676953\n",
      "Done 2600 batches in 797.71s\ttraining loss:\t4.668097\n",
      "Done 2620 batches in 802.48s\ttraining loss:\t4.660043\n",
      "Done 2640 batches in 807.37s\ttraining loss:\t4.650462\n",
      "Done 2660 batches in 813.18s\ttraining loss:\t4.637808\n",
      "Done 2680 batches in 819.58s\ttraining loss:\t4.627169\n",
      "Done 2700 batches in 826.68s\ttraining loss:\t4.619266\n",
      "Done 2720 batches in 832.43s\ttraining loss:\t4.610789\n",
      "Done 2740 batches in 838.69s\ttraining loss:\t4.604965\n",
      "Done 2760 batches in 844.23s\ttraining loss:\t4.606377\n",
      "Done 2780 batches in 849.91s\ttraining loss:\t4.599037\n",
      "Done 2800 batches in 855.11s\ttraining loss:\t4.586053\n",
      "Done 2820 batches in 860.88s\ttraining loss:\t4.574987\n",
      "Done 2840 batches in 865.59s\ttraining loss:\t4.562461\n",
      "Done 2860 batches in 869.67s\ttraining loss:\t4.550939\n",
      "Done 2880 batches in 875.68s\ttraining loss:\t4.544527\n",
      "Done 2900 batches in 881.05s\ttraining loss:\t4.539647\n",
      "Done 2920 batches in 887.03s\ttraining loss:\t4.532991\n",
      "Done 2940 batches in 894.09s\ttraining loss:\t4.522645\n",
      "Done 2960 batches in 900.79s\ttraining loss:\t4.512822\n",
      "Done 2980 batches in 906.22s\ttraining loss:\t4.507427\n",
      "Done 3000 batches in 911.27s\ttraining loss:\t4.501788\n",
      "Done 3020 batches in 915.90s\ttraining loss:\t4.500545\n",
      "Done 3040 batches in 921.54s\ttraining loss:\t4.494154\n",
      "Done 3060 batches in 926.85s\ttraining loss:\t4.486923\n",
      "Done 3080 batches in 930.79s\ttraining loss:\t4.476936\n",
      "Done 3100 batches in 935.12s\ttraining loss:\t4.465692\n",
      "Done 3120 batches in 939.02s\ttraining loss:\t4.456563\n",
      "Done 3140 batches in 943.46s\ttraining loss:\t4.453458\n",
      "Done 3160 batches in 948.17s\ttraining loss:\t4.445949\n",
      "Done 3180 batches in 953.12s\ttraining loss:\t4.437191\n",
      "Done 3200 batches in 959.57s\ttraining loss:\t4.427976\n",
      "Done 3220 batches in 964.95s\ttraining loss:\t4.419669\n",
      "Done 3240 batches in 970.54s\ttraining loss:\t4.411965\n",
      "Done 3260 batches in 975.72s\ttraining loss:\t4.405086\n",
      "Done 3280 batches in 980.73s\ttraining loss:\t4.399898\n",
      "Done 3300 batches in 985.90s\ttraining loss:\t4.390854\n",
      "Done 3320 batches in 991.91s\ttraining loss:\t4.387978\n",
      "Done 3340 batches in 998.46s\ttraining loss:\t4.384394\n",
      "Done 3360 batches in 1003.39s\ttraining loss:\t4.382831\n",
      "Done 3380 batches in 1008.22s\ttraining loss:\t4.380270\n",
      "Done 3400 batches in 1014.12s\ttraining loss:\t4.372104\n",
      "Done 3420 batches in 1019.70s\ttraining loss:\t4.367283\n",
      "Done 3440 batches in 1026.17s\ttraining loss:\t4.361580\n",
      "Done 3460 batches in 1031.42s\ttraining loss:\t4.357860\n",
      "Done 3480 batches in 1036.65s\ttraining loss:\t4.356419\n",
      "Done 3500 batches in 1041.46s\ttraining loss:\t4.355420\n",
      "Done 3520 batches in 1046.37s\ttraining loss:\t4.351402\n",
      "Done 3540 batches in 1051.47s\ttraining loss:\t4.348822\n",
      "Done 3560 batches in 1056.65s\ttraining loss:\t4.343552\n",
      "Done 3580 batches in 1062.30s\ttraining loss:\t4.337399\n",
      "Done 3600 batches in 1068.47s\ttraining loss:\t4.333900\n",
      "Done 3620 batches in 1074.37s\ttraining loss:\t4.328105\n",
      "Done 3640 batches in 1080.69s\ttraining loss:\t4.322047\n",
      "Done 3660 batches in 1086.15s\ttraining loss:\t4.316083\n",
      "Done 3680 batches in 1092.14s\ttraining loss:\t4.314680\n",
      "Done 3700 batches in 1097.79s\ttraining loss:\t4.308978\n",
      "Done 3720 batches in 1104.11s\ttraining loss:\t4.299540\n",
      "Done 3740 batches in 1108.79s\ttraining loss:\t4.290244\n",
      "Done 3760 batches in 1114.02s\ttraining loss:\t4.284021\n",
      "Done 3780 batches in 1119.97s\ttraining loss:\t4.278221\n",
      "Done 3800 batches in 1125.82s\ttraining loss:\t4.270164\n",
      "Done 3820 batches in 1130.96s\ttraining loss:\t4.269827\n",
      "Done 3840 batches in 1135.65s\ttraining loss:\t4.268010\n",
      "Done 3860 batches in 1141.32s\ttraining loss:\t4.266363\n",
      "Done 3880 batches in 1148.19s\ttraining loss:\t4.260972\n",
      "Done 3900 batches in 1154.72s\ttraining loss:\t4.255460\n",
      "Done 3920 batches in 1160.34s\ttraining loss:\t4.248079\n",
      "Done 3940 batches in 1165.69s\ttraining loss:\t4.244255\n",
      "Done 3960 batches in 1171.00s\ttraining loss:\t4.240320\n",
      "Done 3980 batches in 1176.06s\ttraining loss:\t4.234872\n",
      "Done 4000 batches in 1181.42s\ttraining loss:\t4.237355\n",
      "Done 4020 batches in 1186.83s\ttraining loss:\t4.238844\n",
      "Done 4040 batches in 1194.13s\ttraining loss:\t4.235445\n",
      "Done 4060 batches in 1200.65s\ttraining loss:\t4.230146\n",
      "Done 4080 batches in 1206.27s\ttraining loss:\t4.224855\n",
      "Done 4100 batches in 1211.92s\ttraining loss:\t4.217016\n",
      "Done 4120 batches in 1218.05s\ttraining loss:\t4.207752\n",
      "Done 4140 batches in 1222.84s\ttraining loss:\t4.204382\n",
      "Done 4160 batches in 1228.72s\ttraining loss:\t4.200798\n",
      "Done 4180 batches in 1233.80s\ttraining loss:\t4.195580\n",
      "Done 4200 batches in 1238.04s\ttraining loss:\t4.188191\n",
      "Done 4220 batches in 1241.81s\ttraining loss:\t4.180055\n",
      "Done 4240 batches in 1246.63s\ttraining loss:\t4.175846\n",
      "Done 4260 batches in 1251.07s\ttraining loss:\t4.169816\n",
      "Done 4280 batches in 1257.45s\ttraining loss:\t4.167085\n",
      "Done 4300 batches in 1264.32s\ttraining loss:\t4.162926\n",
      "Done 4320 batches in 1270.56s\ttraining loss:\t4.157849\n",
      "Done 4340 batches in 1276.22s\ttraining loss:\t4.152726\n",
      "Done 4360 batches in 1283.15s\ttraining loss:\t4.149502\n",
      "Done 4380 batches in 1289.05s\ttraining loss:\t4.138822\n",
      "Done 4400 batches in 1294.57s\ttraining loss:\t4.134171\n",
      "Done 4420 batches in 1300.23s\ttraining loss:\t4.129781\n",
      "Done 4440 batches in 1305.29s\ttraining loss:\t4.124763\n",
      "Done 4460 batches in 1310.83s\ttraining loss:\t4.121086\n",
      "Done 4480 batches in 1316.21s\ttraining loss:\t4.119841\n",
      "Done 4500 batches in 1321.62s\ttraining loss:\t4.115849\n",
      "Done 4520 batches in 1326.14s\ttraining loss:\t4.110481\n",
      "Done 4540 batches in 1332.60s\ttraining loss:\t4.105596\n",
      "Done 4560 batches in 1337.98s\ttraining loss:\t4.099771\n",
      "Done 4580 batches in 1343.99s\ttraining loss:\t4.093460\n",
      "Done 4600 batches in 1352.20s\ttraining loss:\t4.087010\n",
      "Done 4620 batches in 1360.27s\ttraining loss:\t4.083262\n",
      "Done 4640 batches in 1366.25s\ttraining loss:\t4.077699\n",
      "Done 4660 batches in 1373.32s\ttraining loss:\t4.073170\n",
      "Done 4680 batches in 1378.60s\ttraining loss:\t4.068850\n",
      "Done 4700 batches in 1384.08s\ttraining loss:\t4.064217\n",
      "Done 4720 batches in 1389.76s\ttraining loss:\t4.064304\n",
      "Done 4740 batches in 1394.71s\ttraining loss:\t4.067410\n",
      "Done 4760 batches in 1399.60s\ttraining loss:\t4.068040\n",
      "Done 4780 batches in 1406.52s\ttraining loss:\t4.067943\n",
      "Done 4800 batches in 1412.00s\ttraining loss:\t4.064490\n",
      "Done 4820 batches in 1417.07s\ttraining loss:\t4.059342\n",
      "Done 4840 batches in 1423.00s\ttraining loss:\t4.054487\n",
      "Done 4860 batches in 1428.22s\ttraining loss:\t4.049691\n",
      "Done 4880 batches in 1433.24s\ttraining loss:\t4.044957\n",
      "Done 4900 batches in 1438.81s\ttraining loss:\t4.043158\n",
      "Done 4920 batches in 1444.66s\ttraining loss:\t4.042390\n",
      "Done 4940 batches in 1449.95s\ttraining loss:\t4.040562\n",
      "Done 4960 batches in 1456.91s\ttraining loss:\t4.035501\n",
      "Done 4980 batches in 1462.21s\ttraining loss:\t4.028601\n",
      "Done 5000 batches in 1467.99s\ttraining loss:\t4.030273\n",
      "Done 5020 batches in 1473.99s\ttraining loss:\t4.026064\n",
      "Done 5040 batches in 1479.54s\ttraining loss:\t4.021822\n",
      "Done 5060 batches in 1484.51s\ttraining loss:\t4.019687\n",
      "Done 5080 batches in 1490.91s\ttraining loss:\t4.017281\n",
      "Done 5100 batches in 1497.39s\ttraining loss:\t4.015096\n",
      "Done 5120 batches in 1504.89s\ttraining loss:\t4.012062\n",
      "Done 5140 batches in 1513.00s\ttraining loss:\t4.006022\n",
      "Done 5160 batches in 1519.97s\ttraining loss:\t4.000657\n",
      "Done 5180 batches in 1526.60s\ttraining loss:\t3.996476\n",
      "Done 5200 batches in 1532.92s\ttraining loss:\t3.993208\n",
      "Done 5220 batches in 1539.34s\ttraining loss:\t3.991186\n",
      "Done 5240 batches in 1546.00s\ttraining loss:\t3.986548\n",
      "Done 5260 batches in 1552.85s\ttraining loss:\t3.984011\n",
      "Done 5280 batches in 1559.00s\ttraining loss:\t3.981794\n",
      "Done 5300 batches in 1566.83s\ttraining loss:\t3.980716\n",
      "Done 5320 batches in 1573.49s\ttraining loss:\t3.978504\n",
      "Done 5340 batches in 1580.10s\ttraining loss:\t3.976899\n",
      "Done 5360 batches in 1586.09s\ttraining loss:\t3.974127\n",
      "Done 5380 batches in 1592.57s\ttraining loss:\t3.971341\n",
      "Done 5400 batches in 1598.41s\ttraining loss:\t3.967717\n",
      "Done 5420 batches in 1605.34s\ttraining loss:\t3.964201\n",
      "Done 5440 batches in 1611.64s\ttraining loss:\t3.962904\n",
      "Done 5460 batches in 1619.34s\ttraining loss:\t3.959209\n",
      "Done 5480 batches in 1625.92s\ttraining loss:\t3.954999\n",
      "Done 5500 batches in 1631.89s\ttraining loss:\t3.950635\n",
      "Done 5520 batches in 1638.17s\ttraining loss:\t3.952399\n",
      "Done 5540 batches in 1644.77s\ttraining loss:\t3.948792\n",
      "Done 5560 batches in 1651.65s\ttraining loss:\t3.945101\n",
      "Done 5580 batches in 1658.48s\ttraining loss:\t3.940907\n",
      "Done 5600 batches in 1665.13s\ttraining loss:\t3.938411\n",
      "Done 5620 batches in 1670.68s\ttraining loss:\t3.934674\n",
      "Done 5640 batches in 1676.85s\ttraining loss:\t3.931792\n",
      "Done 5660 batches in 1682.86s\ttraining loss:\t3.926799\n",
      "Done 5680 batches in 1689.19s\ttraining loss:\t3.923961\n",
      "Done 5700 batches in 1695.68s\ttraining loss:\t3.923001\n",
      "Done 5720 batches in 1701.90s\ttraining loss:\t3.920119\n",
      "Done 5740 batches in 1708.66s\ttraining loss:\t3.914804\n",
      "Done 5760 batches in 1715.23s\ttraining loss:\t3.912998\n",
      "Done 5780 batches in 1720.97s\ttraining loss:\t3.912376\n",
      "Done 5800 batches in 1727.14s\ttraining loss:\t3.912140\n",
      "Done 5820 batches in 1734.54s\ttraining loss:\t3.913896\n",
      "Done 5840 batches in 1741.79s\ttraining loss:\t3.914443\n",
      "Done 5860 batches in 1748.81s\ttraining loss:\t3.914874\n",
      "Done 5880 batches in 1756.50s\ttraining loss:\t3.912957\n",
      "Done 5900 batches in 1765.13s\ttraining loss:\t3.910701\n",
      "Done 5920 batches in 1771.28s\ttraining loss:\t3.908117\n",
      "Done 5940 batches in 1777.34s\ttraining loss:\t3.906928\n",
      "Done 5960 batches in 1783.69s\ttraining loss:\t3.905773\n",
      "Done 5980 batches in 1790.78s\ttraining loss:\t3.906302\n",
      "Done 6000 batches in 1796.93s\ttraining loss:\t3.906896\n",
      "Done 6020 batches in 1803.91s\ttraining loss:\t3.909094\n",
      "Done 6040 batches in 1810.65s\ttraining loss:\t3.908263\n",
      "Done 6060 batches in 1819.94s\ttraining loss:\t3.906859\n",
      "Done 6080 batches in 1827.93s\ttraining loss:\t3.908029\n",
      "Done 6100 batches in 1834.55s\ttraining loss:\t3.906435\n",
      "Done 6120 batches in 1841.38s\ttraining loss:\t3.903883\n",
      "Done 6140 batches in 1847.63s\ttraining loss:\t3.901609\n",
      "Done 6160 batches in 1853.84s\ttraining loss:\t3.899439\n",
      "Done 6180 batches in 1860.58s\ttraining loss:\t3.897392\n",
      "Done 6200 batches in 1867.15s\ttraining loss:\t3.893753\n",
      "Done 6220 batches in 1874.06s\ttraining loss:\t3.889349\n",
      "Done 6240 batches in 1881.24s\ttraining loss:\t3.884606\n",
      "Done 6260 batches in 1889.12s\ttraining loss:\t3.878698\n",
      "Done 6280 batches in 1896.89s\ttraining loss:\t3.872717\n",
      "Done 6300 batches in 1904.41s\ttraining loss:\t3.870690\n",
      "Done 6320 batches in 1911.20s\ttraining loss:\t3.869303\n",
      "Done 6340 batches in 1919.15s\ttraining loss:\t3.867074\n",
      "Done 6360 batches in 1925.32s\ttraining loss:\t3.865490\n",
      "Done 6380 batches in 1931.55s\ttraining loss:\t3.863709\n",
      "Done 6400 batches in 1937.98s\ttraining loss:\t3.860114\n",
      "Done 6420 batches in 1943.40s\ttraining loss:\t3.857669\n",
      "Done 6440 batches in 1950.26s\ttraining loss:\t3.855861\n",
      "Done 6460 batches in 1957.75s\ttraining loss:\t3.854337\n",
      "Done 6480 batches in 1964.33s\ttraining loss:\t3.851677\n",
      "Done 6500 batches in 1970.64s\ttraining loss:\t3.847867\n",
      "Done 6520 batches in 1976.99s\ttraining loss:\t3.843482\n",
      "Done 6540 batches in 1983.64s\ttraining loss:\t3.841775\n",
      "Done 6560 batches in 1990.86s\ttraining loss:\t3.839447\n",
      "Done 6580 batches in 1997.40s\ttraining loss:\t3.837001\n",
      "Done 6600 batches in 2003.39s\ttraining loss:\t3.835650\n",
      "Done 6620 batches in 2009.39s\ttraining loss:\t3.832913\n",
      "Done 6640 batches in 2016.32s\ttraining loss:\t3.829378\n",
      "Done 6660 batches in 2022.48s\ttraining loss:\t3.826277\n",
      "Done 6680 batches in 2029.01s\ttraining loss:\t3.823782\n",
      "Done 6700 batches in 2036.06s\ttraining loss:\t3.823333\n",
      "Done 6720 batches in 2043.10s\ttraining loss:\t3.821099\n",
      "Done 6740 batches in 2050.27s\ttraining loss:\t3.819917\n",
      "Done 6760 batches in 2056.30s\ttraining loss:\t3.818356\n",
      "Done 6780 batches in 2062.63s\ttraining loss:\t3.815611\n",
      "Done 6800 batches in 2069.87s\ttraining loss:\t3.814867\n",
      "Done 6820 batches in 2075.76s\ttraining loss:\t3.822131\n",
      "Done 6840 batches in 2082.41s\ttraining loss:\t3.827397\n",
      "Done 6860 batches in 2089.10s\ttraining loss:\t3.830258\n",
      "Done 6880 batches in 2095.83s\ttraining loss:\t3.830115\n",
      "Done 6900 batches in 2103.04s\ttraining loss:\t3.826342\n",
      "Done 6920 batches in 2109.53s\ttraining loss:\t3.824267\n",
      "Done 6940 batches in 2115.76s\ttraining loss:\t3.822002\n",
      "Done 6960 batches in 2122.50s\ttraining loss:\t3.819287\n",
      "Done 6980 batches in 2130.28s\ttraining loss:\t3.817981\n",
      "Done 7000 batches in 2137.51s\ttraining loss:\t3.816139\n",
      "Done 7020 batches in 2144.45s\ttraining loss:\t3.814664\n",
      "Done 7040 batches in 2152.16s\ttraining loss:\t3.818105\n",
      "Done 7060 batches in 2159.08s\ttraining loss:\t3.820789\n",
      "Done 7080 batches in 2166.23s\ttraining loss:\t3.819717\n",
      "Done 7100 batches in 2172.58s\ttraining loss:\t3.818093\n",
      "Done 7120 batches in 2179.13s\ttraining loss:\t3.821796\n",
      "Done 7140 batches in 2186.38s\ttraining loss:\t3.827828\n",
      "Done 7160 batches in 2192.51s\ttraining loss:\t3.832018\n",
      "Done 7180 batches in 2199.22s\ttraining loss:\t3.833260\n",
      "Done 7200 batches in 2206.12s\ttraining loss:\t3.832743\n",
      "Done 7220 batches in 2213.01s\ttraining loss:\t3.832204\n",
      "Done 7240 batches in 2218.68s\ttraining loss:\t3.831109\n",
      "Done 7260 batches in 2224.50s\ttraining loss:\t3.830597\n",
      "Done 7280 batches in 2231.80s\ttraining loss:\t3.828579\n",
      "Done 7300 batches in 2238.04s\ttraining loss:\t3.826537\n",
      "Done 7320 batches in 2245.36s\ttraining loss:\t3.824803\n",
      "Done 7340 batches in 2251.89s\ttraining loss:\t3.821973\n",
      "Done 7360 batches in 2258.62s\ttraining loss:\t3.822075\n",
      "Done 7380 batches in 2265.77s\ttraining loss:\t3.820815\n",
      "Done 7400 batches in 2272.77s\ttraining loss:\t3.820018\n",
      "Done 7420 batches in 2279.89s\ttraining loss:\t3.819350\n",
      "Done 7440 batches in 2286.87s\ttraining loss:\t3.818935\n",
      "Done 7460 batches in 2292.42s\ttraining loss:\t3.816968\n",
      "Done 7480 batches in 2297.86s\ttraining loss:\t3.816064\n",
      "Done 7500 batches in 2303.70s\ttraining loss:\t3.815847\n",
      "Done 7520 batches in 2310.48s\ttraining loss:\t3.814019\n",
      "Done 7540 batches in 2317.06s\ttraining loss:\t3.808907\n",
      "Done 7560 batches in 2324.21s\ttraining loss:\t3.803035\n",
      "Done 7580 batches in 2330.92s\ttraining loss:\t3.799204\n",
      "Done 7600 batches in 2337.75s\ttraining loss:\t3.796942\n",
      "Done 7620 batches in 2345.53s\ttraining loss:\t3.793314\n",
      "Done 7640 batches in 2352.82s\ttraining loss:\t3.790816\n",
      "Done 7660 batches in 2359.52s\ttraining loss:\t3.787273\n",
      "Done 7680 batches in 2366.44s\ttraining loss:\t3.783930\n",
      "Done 7700 batches in 2374.29s\ttraining loss:\t3.781300\n",
      "Done 7720 batches in 2381.91s\ttraining loss:\t3.777611\n",
      "Done 7740 batches in 2388.49s\ttraining loss:\t3.774938\n",
      "Done 7760 batches in 2394.49s\ttraining loss:\t3.778198\n",
      "Done 7780 batches in 2401.12s\ttraining loss:\t3.780445\n",
      "Done 7800 batches in 2408.35s\ttraining loss:\t3.777995\n",
      "Done 7820 batches in 2415.82s\ttraining loss:\t3.777858\n",
      "Done 7840 batches in 2421.88s\ttraining loss:\t3.774890\n",
      "Done 7860 batches in 2428.35s\ttraining loss:\t3.769916\n",
      "Done 7880 batches in 2434.64s\ttraining loss:\t3.765486\n",
      "Done 7900 batches in 2441.77s\ttraining loss:\t3.770531\n",
      "Done 7920 batches in 2448.03s\ttraining loss:\t3.773015\n",
      "Done 7940 batches in 2454.64s\ttraining loss:\t3.772723\n",
      "Done 7960 batches in 2460.69s\ttraining loss:\t3.771383\n",
      "Done 7980 batches in 2467.06s\ttraining loss:\t3.769974\n",
      "Done 8000 batches in 2473.31s\ttraining loss:\t3.768934\n",
      "Done 8020 batches in 2480.22s\ttraining loss:\t3.766084\n",
      "Done 8040 batches in 2486.50s\ttraining loss:\t3.763791\n",
      "Done 8060 batches in 2493.45s\ttraining loss:\t3.763897\n",
      "Done 8080 batches in 2499.24s\ttraining loss:\t3.766334\n",
      "Done 8100 batches in 2505.09s\ttraining loss:\t3.766282\n",
      "Done 8120 batches in 2512.70s\ttraining loss:\t3.763381\n",
      "Done 8140 batches in 2520.68s\ttraining loss:\t3.758470\n",
      "Done 8160 batches in 2527.17s\ttraining loss:\t3.755703\n",
      "Done 8180 batches in 2534.15s\ttraining loss:\t3.755825\n",
      "Done 8200 batches in 2540.21s\ttraining loss:\t3.755780\n",
      "Done 8220 batches in 2546.83s\ttraining loss:\t3.756159\n",
      "Done 8240 batches in 2553.88s\ttraining loss:\t3.758618\n",
      "Done 8260 batches in 2560.24s\ttraining loss:\t3.757527\n",
      "Done 8280 batches in 2566.07s\ttraining loss:\t3.757585\n",
      "Done 8300 batches in 2571.93s\ttraining loss:\t3.756795\n",
      "Done 8320 batches in 2578.15s\ttraining loss:\t3.754481\n",
      "Done 8340 batches in 2584.27s\ttraining loss:\t3.752342\n",
      "Done 8360 batches in 2589.96s\ttraining loss:\t3.750275\n",
      "Done 8380 batches in 2596.45s\ttraining loss:\t3.747320\n",
      "Done 8400 batches in 2602.80s\ttraining loss:\t3.745242\n",
      "Done 8420 batches in 2609.19s\ttraining loss:\t3.744210\n",
      "Done 8440 batches in 2614.84s\ttraining loss:\t3.744033\n",
      "Done 8460 batches in 2621.03s\ttraining loss:\t3.743057\n",
      "Done 8480 batches in 2628.08s\ttraining loss:\t3.749285\n",
      "Done 8500 batches in 2634.95s\ttraining loss:\t3.750433\n",
      "Done 8520 batches in 2641.19s\ttraining loss:\t3.751308\n",
      "Done 8540 batches in 2647.55s\ttraining loss:\t3.752248\n",
      "Done 8560 batches in 2653.27s\ttraining loss:\t3.753622\n",
      "Done 8580 batches in 2660.05s\ttraining loss:\t3.752761\n",
      "Done 8600 batches in 2666.34s\ttraining loss:\t3.751202\n",
      "Done 8620 batches in 2672.76s\ttraining loss:\t3.751603\n",
      "Done 8640 batches in 2678.21s\ttraining loss:\t3.750075\n",
      "Done 8660 batches in 2684.06s\ttraining loss:\t3.748629\n",
      "Done 8680 batches in 2690.30s\ttraining loss:\t3.748111\n",
      "Done 8700 batches in 2697.05s\ttraining loss:\t3.747552\n",
      "Done 8720 batches in 2703.01s\ttraining loss:\t3.746531\n",
      "Done 8740 batches in 2709.75s\ttraining loss:\t3.744953\n",
      "Done 8760 batches in 2715.60s\ttraining loss:\t3.743256\n",
      "Done 8780 batches in 2721.71s\ttraining loss:\t3.742912\n",
      "Done 8800 batches in 2729.72s\ttraining loss:\t3.742420\n",
      "Done 8820 batches in 2736.65s\ttraining loss:\t3.741490\n",
      "Done 8840 batches in 2743.80s\ttraining loss:\t3.740520\n",
      "Done 8860 batches in 2750.73s\ttraining loss:\t3.739125\n",
      "Done 8880 batches in 2757.28s\ttraining loss:\t3.741402\n",
      "Done 8900 batches in 2763.35s\ttraining loss:\t3.744726\n",
      "Done 8920 batches in 2770.80s\ttraining loss:\t3.743641\n",
      "Done 8940 batches in 2777.99s\ttraining loss:\t3.742762\n",
      "Done 8960 batches in 2784.44s\ttraining loss:\t3.741287\n",
      "Done 8980 batches in 2792.01s\ttraining loss:\t3.740054\n",
      "Done 9000 batches in 2799.26s\ttraining loss:\t3.739906\n",
      "Done 9020 batches in 2807.15s\ttraining loss:\t3.741569\n",
      "Done 9040 batches in 2813.77s\ttraining loss:\t3.743160\n",
      "Done 9060 batches in 2819.73s\ttraining loss:\t3.743568\n",
      "Done 9080 batches in 2827.31s\ttraining loss:\t3.741603\n",
      "Done 9100 batches in 2833.59s\ttraining loss:\t3.739761\n",
      "Done 9120 batches in 2840.51s\ttraining loss:\t3.738347\n",
      "Done 9140 batches in 2847.16s\ttraining loss:\t3.738766\n",
      "Done 9160 batches in 2854.00s\ttraining loss:\t3.742604\n",
      "Done 9180 batches in 2860.40s\ttraining loss:\t3.741011\n",
      "Done 9200 batches in 2868.31s\ttraining loss:\t3.738775\n",
      "Done 9220 batches in 2875.91s\ttraining loss:\t3.737578\n",
      "Done 9240 batches in 2883.82s\ttraining loss:\t3.736058\n",
      "Done 9260 batches in 2889.77s\ttraining loss:\t3.734828\n",
      "Done 9280 batches in 2895.61s\ttraining loss:\t3.733706\n",
      "Done 9300 batches in 2901.64s\ttraining loss:\t3.732808\n",
      "Done 9320 batches in 2907.56s\ttraining loss:\t3.732083\n",
      "Done 9340 batches in 2914.01s\ttraining loss:\t3.733152\n",
      "Done 9360 batches in 2919.99s\ttraining loss:\t3.736448\n",
      "Done 9380 batches in 2925.57s\ttraining loss:\t3.740003\n",
      "Done 9400 batches in 2931.83s\ttraining loss:\t3.744631\n",
      "Done 9420 batches in 2938.43s\ttraining loss:\t3.747119\n",
      "Done 9440 batches in 2945.33s\ttraining loss:\t3.745471\n",
      "Done 9460 batches in 2952.98s\ttraining loss:\t3.746599\n",
      "Done 9480 batches in 2959.26s\ttraining loss:\t3.745553\n",
      "Done 9500 batches in 2964.73s\ttraining loss:\t3.745092\n",
      "Done 9520 batches in 2971.71s\ttraining loss:\t3.745448\n",
      "Done 9540 batches in 2977.92s\ttraining loss:\t3.745161\n",
      "Done 9560 batches in 2984.79s\ttraining loss:\t3.743417\n",
      "Done 9580 batches in 2991.01s\ttraining loss:\t3.741397\n",
      "Done 9600 batches in 2997.48s\ttraining loss:\t3.741979\n",
      "Done 9620 batches in 3004.14s\ttraining loss:\t3.744734\n",
      "Done 9640 batches in 3011.43s\ttraining loss:\t3.746124\n",
      "Done 9660 batches in 3018.26s\ttraining loss:\t3.747919\n",
      "Done 9680 batches in 3025.74s\ttraining loss:\t3.746686\n",
      "Done 9700 batches in 3031.75s\ttraining loss:\t3.744657\n",
      "Done 9720 batches in 3037.32s\ttraining loss:\t3.741929\n",
      "Done 9740 batches in 3044.11s\ttraining loss:\t3.739558\n",
      "Done 9760 batches in 3050.31s\ttraining loss:\t3.739222\n",
      "Done 9780 batches in 3056.67s\ttraining loss:\t3.738610\n",
      "Done 9800 batches in 3063.93s\ttraining loss:\t3.739202\n",
      "Done 9820 batches in 3071.22s\ttraining loss:\t3.737575\n",
      "Done 9840 batches in 3079.02s\ttraining loss:\t3.736285\n",
      "Done 9860 batches in 3085.30s\ttraining loss:\t3.734763\n",
      "Done 9880 batches in 3091.20s\ttraining loss:\t3.733560\n",
      "Done 9900 batches in 3099.03s\ttraining loss:\t3.732501\n",
      "Done 9920 batches in 3105.88s\ttraining loss:\t3.731163\n",
      "Done 9940 batches in 3112.08s\ttraining loss:\t3.730193\n",
      "Done 9960 batches in 3118.35s\ttraining loss:\t3.728711\n",
      "Done 9980 batches in 3125.13s\ttraining loss:\t3.727564\n",
      "Done 10000 batches in 3132.35s\ttraining loss:\t3.727362\n",
      "Done 10020 batches in 3140.16s\ttraining loss:\t3.726620\n",
      "Done 10040 batches in 3146.17s\ttraining loss:\t3.728448\n",
      "Done 10060 batches in 3152.55s\ttraining loss:\t3.727112\n",
      "Done 10080 batches in 3159.36s\ttraining loss:\t3.725285\n",
      "Done 10100 batches in 3165.61s\ttraining loss:\t3.723483\n",
      "Done 10120 batches in 3171.42s\ttraining loss:\t3.721434\n",
      "Done 10140 batches in 3177.56s\ttraining loss:\t3.721040\n",
      "Done 10160 batches in 3183.40s\ttraining loss:\t3.721894\n",
      "Done 10180 batches in 3189.75s\ttraining loss:\t3.722321\n",
      "Done 10200 batches in 3195.79s\ttraining loss:\t3.721754\n",
      "Done 10220 batches in 3202.77s\ttraining loss:\t3.720539\n",
      "Done 10240 batches in 3208.90s\ttraining loss:\t3.719378\n",
      "Done 10260 batches in 3215.71s\ttraining loss:\t3.717814\n",
      "Done 10280 batches in 3221.84s\ttraining loss:\t3.716290\n",
      "Done 10300 batches in 3228.98s\ttraining loss:\t3.715446\n",
      "Done 10320 batches in 3235.64s\ttraining loss:\t3.716042\n",
      "Done 10340 batches in 3242.19s\ttraining loss:\t3.715267\n",
      "Done 10360 batches in 3249.71s\ttraining loss:\t3.714554\n",
      "Done 10380 batches in 3256.95s\ttraining loss:\t3.713341\n",
      "Done 10400 batches in 3263.37s\ttraining loss:\t3.712917\n",
      "Done 10420 batches in 3270.04s\ttraining loss:\t3.714076\n",
      "Done 10440 batches in 3276.41s\ttraining loss:\t3.714320\n",
      "Done 10460 batches in 3282.45s\ttraining loss:\t3.712926\n",
      "Done 10480 batches in 3288.04s\ttraining loss:\t3.711279\n",
      "Done 10500 batches in 3294.56s\ttraining loss:\t3.714364\n",
      "Done 10520 batches in 3300.91s\ttraining loss:\t3.714258\n",
      "Done 10540 batches in 3307.21s\ttraining loss:\t3.715591\n",
      "Done 10560 batches in 3313.98s\ttraining loss:\t3.718095\n",
      "Done 10580 batches in 3321.34s\ttraining loss:\t3.719517\n",
      "Done 10600 batches in 3327.75s\ttraining loss:\t3.719472\n",
      "Done 10620 batches in 3333.98s\ttraining loss:\t3.720208\n",
      "Done 10640 batches in 3340.52s\ttraining loss:\t3.719605\n",
      "Done 10660 batches in 3347.32s\ttraining loss:\t3.718530\n",
      "Done 10680 batches in 3353.66s\ttraining loss:\t3.718104\n",
      "Done 10700 batches in 3359.91s\ttraining loss:\t3.715616\n",
      "Done 10720 batches in 3367.07s\ttraining loss:\t3.716246\n",
      "Done 10740 batches in 3374.63s\ttraining loss:\t3.716128\n",
      "Done 10760 batches in 3380.59s\ttraining loss:\t3.715726\n",
      "Done 10780 batches in 3386.47s\ttraining loss:\t3.715658\n",
      "Done 10800 batches in 3393.02s\ttraining loss:\t3.716235\n",
      "Done 10820 batches in 3400.25s\ttraining loss:\t3.716259\n",
      "Done 10840 batches in 3406.33s\ttraining loss:\t3.714891\n",
      "Done 10860 batches in 3413.54s\ttraining loss:\t3.713894\n",
      "Done 10880 batches in 3421.52s\ttraining loss:\t3.713712\n",
      "Done 10900 batches in 3427.72s\ttraining loss:\t3.717263\n",
      "Done 10920 batches in 3435.30s\ttraining loss:\t3.720784\n",
      "Done 10940 batches in 3442.16s\ttraining loss:\t3.719824\n",
      "Done 10960 batches in 3448.32s\ttraining loss:\t3.718318\n",
      "Done 10980 batches in 3454.97s\ttraining loss:\t3.719419\n",
      "Done 11000 batches in 3460.94s\ttraining loss:\t3.720143\n",
      "Done 11020 batches in 3467.22s\ttraining loss:\t3.719358\n",
      "Done 11040 batches in 3473.83s\ttraining loss:\t3.717866\n",
      "Done 11060 batches in 3480.47s\ttraining loss:\t3.715946\n",
      "Done 11080 batches in 3487.21s\ttraining loss:\t3.713783\n",
      "Done 11100 batches in 3494.48s\ttraining loss:\t3.713460\n",
      "Done 11120 batches in 3501.98s\ttraining loss:\t3.714778\n",
      "Done 11140 batches in 3509.28s\ttraining loss:\t3.715838\n",
      "Done 11160 batches in 3516.32s\ttraining loss:\t3.716217\n",
      "Done 11180 batches in 3522.23s\ttraining loss:\t3.716156\n",
      "Done 11200 batches in 3528.47s\ttraining loss:\t3.716157\n",
      "Done 11220 batches in 3534.63s\ttraining loss:\t3.718068\n",
      "Done 11240 batches in 3541.33s\ttraining loss:\t3.717199\n",
      "Done 11260 batches in 3548.20s\ttraining loss:\t3.715237\n",
      "Done 11280 batches in 3555.09s\ttraining loss:\t3.715872\n",
      "Done 11300 batches in 3561.58s\ttraining loss:\t3.717037\n",
      "Done 11320 batches in 3567.97s\ttraining loss:\t3.719182\n",
      "Done 11340 batches in 3575.32s\ttraining loss:\t3.719873\n",
      "Done 11360 batches in 3581.67s\ttraining loss:\t3.721015\n",
      "Done 11380 batches in 3589.06s\ttraining loss:\t3.723953\n",
      "Done 11400 batches in 3595.12s\ttraining loss:\t3.725298\n",
      "Done 11420 batches in 3601.51s\ttraining loss:\t3.723327\n",
      "Done 11440 batches in 3607.57s\ttraining loss:\t3.721990\n",
      "Done 11460 batches in 3614.85s\ttraining loss:\t3.722227\n",
      "Done 11480 batches in 3622.18s\ttraining loss:\t3.722188\n",
      "Done 11500 batches in 3628.58s\ttraining loss:\t3.721048\n",
      "Done 11520 batches in 3634.98s\ttraining loss:\t3.720838\n",
      "Done 11540 batches in 3641.09s\ttraining loss:\t3.720423\n",
      "Done 11560 batches in 3648.09s\ttraining loss:\t3.719679\n",
      "Done 11580 batches in 3654.48s\ttraining loss:\t3.719377\n",
      "Done 11600 batches in 3660.54s\ttraining loss:\t3.719830\n",
      "Done 11620 batches in 3667.32s\ttraining loss:\t3.721002\n",
      "Done 11640 batches in 3674.56s\ttraining loss:\t3.719196\n",
      "Done 11660 batches in 3681.57s\ttraining loss:\t3.717723\n",
      "Done 11680 batches in 3687.14s\ttraining loss:\t3.716072\n",
      "Done 11700 batches in 3692.68s\ttraining loss:\t3.714390\n",
      "Done 11720 batches in 3698.54s\ttraining loss:\t3.712901\n",
      "Done 11740 batches in 3704.34s\ttraining loss:\t3.712095\n",
      "Done 11760 batches in 3710.29s\ttraining loss:\t3.711841\n",
      "Done 11780 batches in 3716.45s\ttraining loss:\t3.709962\n",
      "Done 11800 batches in 3722.79s\ttraining loss:\t3.708866\n",
      "Done 11820 batches in 3728.38s\ttraining loss:\t3.707301\n",
      "Done 11840 batches in 3735.74s\ttraining loss:\t3.705982\n",
      "Done 11860 batches in 3744.02s\ttraining loss:\t3.705095\n",
      "Done 11880 batches in 3750.14s\ttraining loss:\t3.704586\n",
      "Done 11900 batches in 3755.86s\ttraining loss:\t3.701925\n",
      "Done 11920 batches in 3762.14s\ttraining loss:\t3.700245\n",
      "Done 11940 batches in 3769.31s\ttraining loss:\t3.698021\n",
      "Done 11960 batches in 3776.17s\ttraining loss:\t3.695030\n",
      "Done 11980 batches in 3782.19s\ttraining loss:\t3.694041\n",
      "Done 12000 batches in 3788.18s\ttraining loss:\t3.693318\n",
      "Done 12020 batches in 3794.17s\ttraining loss:\t3.692454\n",
      "Done 12040 batches in 3800.11s\ttraining loss:\t3.690721\n",
      "Done 12060 batches in 3807.68s\ttraining loss:\t3.689699\n",
      "Done 12080 batches in 3815.76s\ttraining loss:\t3.688025\n",
      "Done 12100 batches in 3822.18s\ttraining loss:\t3.687467\n",
      "Done 12120 batches in 3828.48s\ttraining loss:\t3.687139\n",
      "Done 12140 batches in 3834.86s\ttraining loss:\t3.686976\n",
      "Done 12160 batches in 3840.98s\ttraining loss:\t3.686896\n",
      "Done 12180 batches in 3846.99s\ttraining loss:\t3.686402\n",
      "Done 12200 batches in 3852.64s\ttraining loss:\t3.684014\n",
      "Done 12220 batches in 3859.32s\ttraining loss:\t3.682706\n",
      "Done 12240 batches in 3865.24s\ttraining loss:\t3.683299\n",
      "Done 12260 batches in 3871.97s\ttraining loss:\t3.682604\n",
      "Done 12280 batches in 3878.01s\ttraining loss:\t3.680359\n",
      "Done 12300 batches in 3885.51s\ttraining loss:\t3.680324\n",
      "Done 12320 batches in 3892.71s\ttraining loss:\t3.680645\n",
      "Done 12340 batches in 3899.41s\ttraining loss:\t3.681121\n",
      "Done 12360 batches in 3906.39s\ttraining loss:\t3.681252\n",
      "Done 12380 batches in 3912.99s\ttraining loss:\t3.680604\n",
      "Done 12400 batches in 3918.60s\ttraining loss:\t3.680596\n",
      "Done 12420 batches in 3923.85s\ttraining loss:\t3.680370\n",
      "Done 12440 batches in 3930.81s\ttraining loss:\t3.681150\n",
      "Done 12460 batches in 3937.56s\ttraining loss:\t3.681509\n",
      "Done 12480 batches in 3944.59s\ttraining loss:\t3.680868\n",
      "Done 12500 batches in 3951.86s\ttraining loss:\t3.680366\n",
      "Done 12520 batches in 3957.93s\ttraining loss:\t3.680619\n",
      "Done 12540 batches in 3964.61s\ttraining loss:\t3.680644\n",
      "Done 12560 batches in 3970.94s\ttraining loss:\t3.679560\n",
      "Done 12580 batches in 3976.93s\ttraining loss:\t3.678126\n",
      "Done 12600 batches in 3982.79s\ttraining loss:\t3.676856\n",
      "Done 12620 batches in 3988.54s\ttraining loss:\t3.674967\n",
      "Done 12640 batches in 3995.66s\ttraining loss:\t3.674304\n",
      "Done 12660 batches in 4001.74s\ttraining loss:\t3.674286\n",
      "Done 12680 batches in 4007.28s\ttraining loss:\t3.673241\n",
      "Done 12700 batches in 4013.94s\ttraining loss:\t3.673325\n",
      "Done 12720 batches in 4021.30s\ttraining loss:\t3.673622\n",
      "Done 12740 batches in 4027.39s\ttraining loss:\t3.673342\n",
      "Done 12760 batches in 4035.33s\ttraining loss:\t3.672784\n",
      "Done 12780 batches in 4041.62s\ttraining loss:\t3.670587\n",
      "Done 12800 batches in 4048.45s\ttraining loss:\t3.668483\n",
      "Done 12820 batches in 4055.36s\ttraining loss:\t3.666697\n",
      "Done 12840 batches in 4061.78s\ttraining loss:\t3.664999\n",
      "Done 12860 batches in 4069.50s\ttraining loss:\t3.663664\n",
      "Done 12880 batches in 4075.77s\ttraining loss:\t3.662614\n",
      "Done 12900 batches in 4081.84s\ttraining loss:\t3.660845\n",
      "Done 12920 batches in 4088.56s\ttraining loss:\t3.658310\n",
      "Done 12940 batches in 4095.08s\ttraining loss:\t3.656535\n",
      "Done 12960 batches in 4102.06s\ttraining loss:\t3.654381\n",
      "Done 12980 batches in 4108.59s\ttraining loss:\t3.652096\n",
      "Done 13000 batches in 4114.69s\ttraining loss:\t3.650235\n",
      "Done 13020 batches in 4121.81s\ttraining loss:\t3.650121\n",
      "Done 13040 batches in 4128.25s\ttraining loss:\t3.651001\n",
      "Done 13060 batches in 4134.33s\ttraining loss:\t3.649026\n",
      "Done 13080 batches in 4141.29s\ttraining loss:\t3.648885\n",
      "Done 13100 batches in 4148.24s\ttraining loss:\t3.648840\n",
      "Done 13120 batches in 4154.92s\ttraining loss:\t3.649951\n",
      "Done 13140 batches in 4161.40s\ttraining loss:\t3.650788\n",
      "Done 13160 batches in 4168.18s\ttraining loss:\t3.650791\n",
      "Done 13180 batches in 4174.45s\ttraining loss:\t3.648552\n",
      "Done 13200 batches in 4181.50s\ttraining loss:\t3.647505\n",
      "Done 13220 batches in 4187.23s\ttraining loss:\t3.645897\n",
      "Done 13240 batches in 4193.28s\ttraining loss:\t3.645493\n",
      "Done 13260 batches in 4199.19s\ttraining loss:\t3.644650\n",
      "Done 13280 batches in 4205.62s\ttraining loss:\t3.645459\n",
      "Done 13300 batches in 4212.23s\ttraining loss:\t3.644687\n",
      "Done 13320 batches in 4219.01s\ttraining loss:\t3.643364\n",
      "Done 13340 batches in 4224.90s\ttraining loss:\t3.642351\n",
      "Done 13360 batches in 4231.73s\ttraining loss:\t3.640944\n",
      "Done 13380 batches in 4238.02s\ttraining loss:\t3.640335\n",
      "Done 13400 batches in 4244.31s\ttraining loss:\t3.640220\n",
      "Done 13420 batches in 4250.67s\ttraining loss:\t3.639665\n",
      "Done 13440 batches in 4257.31s\ttraining loss:\t3.639188\n",
      "Done 13460 batches in 4263.66s\ttraining loss:\t3.639790\n",
      "Done 13480 batches in 4270.14s\ttraining loss:\t3.640123\n",
      "Done 13500 batches in 4276.67s\ttraining loss:\t3.641081\n",
      "Done 13520 batches in 4284.26s\ttraining loss:\t3.641972\n",
      "Done 13540 batches in 4292.25s\ttraining loss:\t3.642029\n",
      "Done 13560 batches in 4298.93s\ttraining loss:\t3.641078\n",
      "Done 13580 batches in 4304.91s\ttraining loss:\t3.640343\n",
      "Done 13600 batches in 4311.57s\ttraining loss:\t3.640834\n",
      "Done 13620 batches in 4318.32s\ttraining loss:\t3.640659\n",
      "Done 13640 batches in 4325.67s\ttraining loss:\t3.640324\n",
      "Done 13660 batches in 4332.51s\ttraining loss:\t3.638925\n",
      "Done 13680 batches in 4338.54s\ttraining loss:\t3.639423\n",
      "Done 13700 batches in 4345.94s\ttraining loss:\t3.639252\n",
      "Done 13720 batches in 4352.67s\ttraining loss:\t3.638602\n",
      "Done 13740 batches in 4358.91s\ttraining loss:\t3.638708\n",
      "Done 13760 batches in 4365.01s\ttraining loss:\t3.637653\n",
      "Done 13780 batches in 4371.48s\ttraining loss:\t3.636223\n",
      "Done 13800 batches in 4377.73s\ttraining loss:\t3.635318\n",
      "Done 13820 batches in 4383.65s\ttraining loss:\t3.635021\n",
      "Done 13840 batches in 4390.67s\ttraining loss:\t3.634739\n",
      "Done 13860 batches in 4397.62s\ttraining loss:\t3.638695\n",
      "Done 13880 batches in 4404.73s\ttraining loss:\t3.641909\n",
      "Done 13900 batches in 4410.70s\ttraining loss:\t3.644538\n",
      "Done 13920 batches in 4417.19s\ttraining loss:\t3.645748\n",
      "Done 13940 batches in 4424.00s\ttraining loss:\t3.646309\n",
      "Done 13960 batches in 4430.67s\ttraining loss:\t3.648454\n",
      "Done 13980 batches in 4436.10s\ttraining loss:\t3.648854\n",
      "Done 14000 batches in 4442.09s\ttraining loss:\t3.648318\n",
      "Done 14020 batches in 4450.12s\ttraining loss:\t3.646561\n",
      "Done 14040 batches in 4457.14s\ttraining loss:\t3.645080\n",
      "Done 14060 batches in 4463.44s\ttraining loss:\t3.643567\n",
      "Done 14080 batches in 4469.73s\ttraining loss:\t3.642211\n",
      "Done 14100 batches in 4475.88s\ttraining loss:\t3.641461\n",
      "Done 14120 batches in 4481.96s\ttraining loss:\t3.641034\n",
      "Done 14140 batches in 4488.13s\ttraining loss:\t3.640348\n",
      "Done 14160 batches in 4494.46s\ttraining loss:\t3.640489\n",
      "Done 14180 batches in 4500.96s\ttraining loss:\t3.640523\n",
      "Done 14200 batches in 4506.70s\ttraining loss:\t3.640248\n",
      "Done 14220 batches in 4513.19s\ttraining loss:\t3.639283\n",
      "Done 14240 batches in 4520.74s\ttraining loss:\t3.640271\n",
      "Done 14260 batches in 4527.05s\ttraining loss:\t3.643712\n",
      "Done 14280 batches in 4532.61s\ttraining loss:\t3.644522\n",
      "Done 14300 batches in 4538.31s\ttraining loss:\t3.642769\n",
      "Done 14320 batches in 4544.32s\ttraining loss:\t3.641332\n",
      "Done 14340 batches in 4550.38s\ttraining loss:\t3.640148\n",
      "Done 14360 batches in 4556.97s\ttraining loss:\t3.639783\n",
      "Done 14380 batches in 4563.69s\ttraining loss:\t3.639125\n",
      "Done 14400 batches in 4570.20s\ttraining loss:\t3.637565\n",
      "Done 14420 batches in 4577.06s\ttraining loss:\t3.636241\n",
      "Done 14440 batches in 4583.83s\ttraining loss:\t3.635874\n",
      "Done 14460 batches in 4590.59s\ttraining loss:\t3.637052\n",
      "Done 14480 batches in 4596.70s\ttraining loss:\t3.637302\n",
      "Done 14500 batches in 4603.41s\ttraining loss:\t3.637194\n",
      "Done 14520 batches in 4610.22s\ttraining loss:\t3.636794\n",
      "Done 14540 batches in 4616.45s\ttraining loss:\t3.636356\n",
      "Done 14560 batches in 4622.33s\ttraining loss:\t3.635838\n",
      "Done 14580 batches in 4629.20s\ttraining loss:\t3.639932\n",
      "Done 14600 batches in 4635.35s\ttraining loss:\t3.642992\n",
      "Done 14620 batches in 4642.61s\ttraining loss:\t3.643303\n",
      "Done 14640 batches in 4650.14s\ttraining loss:\t3.643402\n",
      "Done 14660 batches in 4656.39s\ttraining loss:\t3.643243\n",
      "Done 14680 batches in 4662.41s\ttraining loss:\t3.642194\n",
      "Done 14700 batches in 4668.62s\ttraining loss:\t3.641634\n",
      "Done 14720 batches in 4674.42s\ttraining loss:\t3.641449\n",
      "Done 14740 batches in 4680.27s\ttraining loss:\t3.640593\n",
      "Done 14760 batches in 4686.06s\ttraining loss:\t3.640405\n",
      "Done 14780 batches in 4692.37s\ttraining loss:\t3.640004\n",
      "Done 14800 batches in 4699.00s\ttraining loss:\t3.639251\n",
      "Done 14820 batches in 4705.27s\ttraining loss:\t3.639314\n",
      "Done 14840 batches in 4711.90s\ttraining loss:\t3.639114\n",
      "Done 14860 batches in 4718.13s\ttraining loss:\t3.638982\n",
      "Done 14880 batches in 4724.63s\ttraining loss:\t3.638464\n",
      "Done 14900 batches in 4730.88s\ttraining loss:\t3.638156\n",
      "Done 14920 batches in 4737.46s\ttraining loss:\t3.637828\n",
      "Done 14940 batches in 4744.98s\ttraining loss:\t3.636918\n",
      "Done 14960 batches in 4752.58s\ttraining loss:\t3.636465\n",
      "Done 14980 batches in 4758.82s\ttraining loss:\t3.635541\n",
      "Done 15000 batches in 4765.74s\ttraining loss:\t3.634139\n",
      "Done 15020 batches in 4771.88s\ttraining loss:\t3.633422\n",
      "Done 15040 batches in 4777.86s\ttraining loss:\t3.632019\n",
      "Done 15060 batches in 4784.37s\ttraining loss:\t3.631725\n",
      "Done 15080 batches in 4790.97s\ttraining loss:\t3.631227\n",
      "Done 15100 batches in 4797.21s\ttraining loss:\t3.631392\n",
      "Done 15120 batches in 4803.11s\ttraining loss:\t3.631007\n",
      "Done 15140 batches in 4809.58s\ttraining loss:\t3.630184\n",
      "Done 15160 batches in 4815.95s\ttraining loss:\t3.629431\n",
      "Done 15180 batches in 4821.88s\ttraining loss:\t3.628569\n",
      "Done 15200 batches in 4827.28s\ttraining loss:\t3.626824\n",
      "Done 15220 batches in 4833.62s\ttraining loss:\t3.626107\n",
      "Done 15240 batches in 4840.17s\ttraining loss:\t3.626351\n",
      "Done 15260 batches in 4846.45s\ttraining loss:\t3.626782\n",
      "Done 15280 batches in 4852.29s\ttraining loss:\t3.626347\n",
      "Done 15300 batches in 4858.60s\ttraining loss:\t3.626970\n",
      "Done 15320 batches in 4864.66s\ttraining loss:\t3.628289\n",
      "Done 15340 batches in 4871.11s\ttraining loss:\t3.629435\n",
      "Done 15360 batches in 4877.11s\ttraining loss:\t3.630089\n",
      "Done 15380 batches in 4884.50s\ttraining loss:\t3.630640\n",
      "Done 15400 batches in 4890.36s\ttraining loss:\t3.630340\n",
      "Done 15420 batches in 4897.11s\ttraining loss:\t3.630035\n",
      "Done 15440 batches in 4903.32s\ttraining loss:\t3.630074\n",
      "Done 15460 batches in 4909.53s\ttraining loss:\t3.629204\n",
      "Done 15480 batches in 4917.27s\ttraining loss:\t3.628456\n",
      "Done 15500 batches in 4925.18s\ttraining loss:\t3.626341\n",
      "Done 15520 batches in 4931.84s\ttraining loss:\t3.624561\n",
      "Done 15540 batches in 4938.15s\ttraining loss:\t3.624026\n",
      "Done 15560 batches in 4944.84s\ttraining loss:\t3.622960\n",
      "Done 15580 batches in 4951.03s\ttraining loss:\t3.622211\n",
      "Done 15600 batches in 4958.10s\ttraining loss:\t3.621576\n",
      "Done 15620 batches in 4964.29s\ttraining loss:\t3.621221\n",
      "Done 15640 batches in 4971.05s\ttraining loss:\t3.620433\n",
      "Done 15660 batches in 4977.37s\ttraining loss:\t3.619717\n",
      "Done 15680 batches in 4983.14s\ttraining loss:\t3.618931\n",
      "Done 15700 batches in 4989.09s\ttraining loss:\t3.617601\n",
      "Done 15720 batches in 4996.42s\ttraining loss:\t3.617426\n",
      "Done 15740 batches in 5003.70s\ttraining loss:\t3.617202\n",
      "Done 15760 batches in 5010.35s\ttraining loss:\t3.615698\n",
      "Done 15780 batches in 5017.07s\ttraining loss:\t3.614382\n",
      "Done 15800 batches in 5022.62s\ttraining loss:\t3.613659\n",
      "Done 15820 batches in 5028.35s\ttraining loss:\t3.612866\n",
      "Done 15840 batches in 5035.14s\ttraining loss:\t3.613087\n",
      "Done 15860 batches in 5041.53s\ttraining loss:\t3.612225\n",
      "Done 15880 batches in 5046.92s\ttraining loss:\t3.611803\n",
      "Done 15900 batches in 5052.39s\ttraining loss:\t3.610983\n",
      "Done 15920 batches in 5059.48s\ttraining loss:\t3.609097\n",
      "Done 15940 batches in 5067.32s\ttraining loss:\t3.607969\n",
      "Done 15960 batches in 5073.55s\ttraining loss:\t3.607929\n",
      "Done 15980 batches in 5080.02s\ttraining loss:\t3.607497\n",
      "Done 16000 batches in 5086.31s\ttraining loss:\t3.606272\n",
      "Done 16020 batches in 5092.51s\ttraining loss:\t3.605583\n",
      "Done 16040 batches in 5098.36s\ttraining loss:\t3.605212\n",
      "Done 16060 batches in 5104.62s\ttraining loss:\t3.605580\n",
      "Done 16080 batches in 5112.81s\ttraining loss:\t3.609751\n",
      "Done 16100 batches in 5119.22s\ttraining loss:\t3.608063\n",
      "Done 16120 batches in 5125.46s\ttraining loss:\t3.607282\n",
      "Done 16140 batches in 5131.81s\ttraining loss:\t3.606788\n",
      "Done 16160 batches in 5137.84s\ttraining loss:\t3.607058\n",
      "Done 16180 batches in 5144.30s\ttraining loss:\t3.606364\n",
      "Done 16200 batches in 5151.37s\ttraining loss:\t3.605874\n",
      "Done 16220 batches in 5157.28s\ttraining loss:\t3.605520\n",
      "Done 16240 batches in 5163.50s\ttraining loss:\t3.604400\n",
      "Done 16260 batches in 5171.61s\ttraining loss:\t3.602908\n",
      "Done 16280 batches in 5179.00s\ttraining loss:\t3.601397\n",
      "Done 16300 batches in 5187.27s\ttraining loss:\t3.600988\n",
      "Done 16320 batches in 5193.87s\ttraining loss:\t3.600822\n",
      "Done 16340 batches in 5201.71s\ttraining loss:\t3.600397\n",
      "Done 16360 batches in 5207.67s\ttraining loss:\t3.599746\n",
      "Done 16380 batches in 5214.14s\ttraining loss:\t3.598598\n",
      "Done 16400 batches in 5220.28s\ttraining loss:\t3.597396\n",
      "Done 16420 batches in 5225.92s\ttraining loss:\t3.596526\n",
      "Done 16440 batches in 5231.99s\ttraining loss:\t3.595812\n",
      "Done 16460 batches in 5238.11s\ttraining loss:\t3.595384\n",
      "Done 16480 batches in 5244.23s\ttraining loss:\t3.594600\n",
      "Done 16500 batches in 5250.49s\ttraining loss:\t3.593766\n",
      "Done 16520 batches in 5257.82s\ttraining loss:\t3.593644\n",
      "Done 16540 batches in 5264.24s\ttraining loss:\t3.593522\n",
      "Done 16560 batches in 5271.04s\ttraining loss:\t3.593768\n",
      "Done 16580 batches in 5278.53s\ttraining loss:\t3.593025\n",
      "Done 16600 batches in 5285.68s\ttraining loss:\t3.592505\n",
      "Done 16620 batches in 5292.13s\ttraining loss:\t3.591499\n",
      "Done 16640 batches in 5298.20s\ttraining loss:\t3.590527\n",
      "Done 16660 batches in 5304.91s\ttraining loss:\t3.589219\n",
      "Done 16680 batches in 5311.35s\ttraining loss:\t3.588331\n",
      "Done 16700 batches in 5318.33s\ttraining loss:\t3.587074\n",
      "Done 16720 batches in 5323.75s\ttraining loss:\t3.586292\n",
      "Done 16740 batches in 5329.74s\ttraining loss:\t3.585220\n",
      "Done 16760 batches in 5336.61s\ttraining loss:\t3.584082\n",
      "Done 16780 batches in 5343.01s\ttraining loss:\t3.583366\n",
      "Done 16800 batches in 5348.60s\ttraining loss:\t3.581713\n",
      "Done 16820 batches in 5354.96s\ttraining loss:\t3.581995\n",
      "Done 16840 batches in 5361.39s\ttraining loss:\t3.582533\n",
      "Done 16860 batches in 5368.43s\ttraining loss:\t3.581293\n",
      "Done 16880 batches in 5375.81s\ttraining loss:\t3.580405\n",
      "Done 16900 batches in 5382.46s\ttraining loss:\t3.579272\n",
      "Done 16920 batches in 5388.84s\ttraining loss:\t3.578635\n",
      "Done 16940 batches in 5395.23s\ttraining loss:\t3.577759\n",
      "Done 16960 batches in 5400.64s\ttraining loss:\t3.576470\n",
      "Done 16980 batches in 5406.83s\ttraining loss:\t3.575504\n",
      "Done 17000 batches in 5412.88s\ttraining loss:\t3.575428\n",
      "Done 17020 batches in 5418.69s\ttraining loss:\t3.575096\n",
      "Done 17040 batches in 5425.44s\ttraining loss:\t3.574404\n",
      "Done 17060 batches in 5432.62s\ttraining loss:\t3.574922\n",
      "Done 17080 batches in 5439.50s\ttraining loss:\t3.574680\n",
      "Done 17100 batches in 5445.60s\ttraining loss:\t3.573482\n",
      "Done 17120 batches in 5451.36s\ttraining loss:\t3.573950\n",
      "Done 17140 batches in 5458.09s\ttraining loss:\t3.573636\n",
      "Done 17160 batches in 5464.82s\ttraining loss:\t3.573082\n",
      "Done 17180 batches in 5471.06s\ttraining loss:\t3.571577\n",
      "Done 17200 batches in 5476.96s\ttraining loss:\t3.570194\n",
      "Done 17220 batches in 5483.59s\ttraining loss:\t3.568970\n",
      "Done 17240 batches in 5489.10s\ttraining loss:\t3.567689\n",
      "Done 17260 batches in 5495.44s\ttraining loss:\t3.566647\n",
      "Done 17280 batches in 5501.41s\ttraining loss:\t3.565554\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.564743398702102"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_net.train_one_epoch(data, 5, log_interval=20) # glove init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HRED v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model...\n",
      "Compiling theano functions...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "hred_net = HRED(voc_size=voc_size,\n",
    "                emb_size=300,\n",
    "                lv1_rec_size=300,\n",
    "                lv2_rec_size=300,\n",
    "                out_emb_size=300,\n",
    "                emb_init=glove_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.6484955 ,  0.39728042,  0.29447019,  0.21638568,  0.34087649,\n",
       "         0.42938796,  0.48248306,  0.45450044,  0.63264418], dtype=float32),\n",
       " array([ 0.43808964,  0.31143361,  0.32550314,  0.41264656,  0.40189919,\n",
       "         0.67630386,  0.55489218,  0.2793248 ,  0.29060629,  0.52473378,\n",
       "         0.40135068,  0.42928901,  0.24922398,  0.29269892,  0.26922536,\n",
       "         0.37188664], dtype=float32),\n",
       " array([ 0.48105857,  0.35441753,  0.33231139,  0.33165267,  0.25691301,\n",
       "         0.29037777,  0.38888532,  0.45136821,  0.26165053,  0.35437146,\n",
       "         0.36461809,  0.29422539,  0.29016727,  0.75232959,  0.50326639,\n",
       "         0.4879126 ,  0.19824371,  0.41686958,  0.44867724,  0.63904244,\n",
       "         0.59564072,  0.3762525 ,  0.55565   ,  0.20368919,  0.45814279,\n",
       "         0.13423204,  0.25729677,  0.28506696,  0.130806  ,  0.43678874], dtype=float32),\n",
       " array([ 0.36110905,  0.18258747,  0.213524  ,  0.28226432,  0.41397128,\n",
       "         0.32674885,  0.29788589,  0.61358768,  0.34168947,  0.22745501,\n",
       "         0.51893896,  0.23157899,  0.34638187], dtype=float32),\n",
       " array([ 0.4535464 ,  0.40573937,  0.2589761 ,  0.60236984,  0.27459544,\n",
       "         0.28887132,  0.41115206,  0.3765592 ,  0.29957584,  0.65952957,\n",
       "         0.65185702,  0.43861535,  0.41025203,  0.34527314,  0.50343114,\n",
       "         0.44476756], dtype=float32),\n",
       " array([ 0.25122422,  0.22101936,  0.24613954,  0.55296057,  0.48097202,\n",
       "         0.28318909,  0.36703086,  0.37572479,  0.13140583,  0.30395606,\n",
       "         0.22432627,  0.41698629,  0.31270713,  0.38566667,  0.34730825,\n",
       "         0.62415415,  0.41321987,  0.25133049,  0.53575897,  0.26159209,\n",
       "         0.43648791,  0.4528788 ,  0.681656  ,  0.56678104], dtype=float32),\n",
       " array([ 0.36841837,  0.21861434,  0.35582578,  0.34456691,  0.26033381,\n",
       "         0.26834723,  0.30452481,  0.29360229,  0.45077142,  0.36123395,\n",
       "         0.30142045,  0.47159186,  0.57626271,  0.47261915,  0.61174881,\n",
       "         0.5398609 ,  0.66426522,  0.60254252,  0.50124556,  0.29331216,\n",
       "         0.66241181,  0.56065774,  0.31302401,  0.34609857,  0.25346887,\n",
       "         0.28972301,  0.40852475,  0.43229055,  0.26449507,  0.49642214,\n",
       "         0.35869539,  0.39115593,  0.27490386,  0.39772668], dtype=float32)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = hred_net.get_output(data[:12], 3)\n",
    "ans[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05232447"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(chain(*ans[8]))[46]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'september'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_w(list(chain(*data[8][1][1:]))[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[46,\n",
       " 23,\n",
       " 45,\n",
       " 74,\n",
       " 7,\n",
       " 19,\n",
       " 49,\n",
       " 8,\n",
       " 6,\n",
       " 126,\n",
       " 10,\n",
       " 24,\n",
       " 28,\n",
       " 80,\n",
       " 9,\n",
       " 29,\n",
       " 54,\n",
       " 43,\n",
       " 31,\n",
       " 37,\n",
       " 4,\n",
       " 42,\n",
       " 123,\n",
       " 82,\n",
       " 11,\n",
       " 30,\n",
       " 111,\n",
       " 94,\n",
       " 16,\n",
       " 35,\n",
       " 81,\n",
       " 15,\n",
       " 50,\n",
       " 148,\n",
       " 62,\n",
       " 78,\n",
       " 53,\n",
       " 91,\n",
       " 204,\n",
       " 33,\n",
       " 13,\n",
       " 12,\n",
       " 105,\n",
       " 21,\n",
       " 20,\n",
       " 155,\n",
       " 152,\n",
       " 122,\n",
       " 36,\n",
       " 158,\n",
       " 125,\n",
       " 203,\n",
       " 217,\n",
       " 5,\n",
       " 22,\n",
       " 84,\n",
       " 14,\n",
       " 87,\n",
       " 27,\n",
       " 34,\n",
       " 3,\n",
       " 85,\n",
       " 107,\n",
       " 222,\n",
       " 186,\n",
       " 79,\n",
       " 25,\n",
       " 110,\n",
       " 128,\n",
       " 56,\n",
       " 175,\n",
       " 60,\n",
       " 127,\n",
       " 131,\n",
       " 52,\n",
       " 117,\n",
       " 63,\n",
       " 149,\n",
       " 112,\n",
       " 47,\n",
       " 26,\n",
       " 66,\n",
       " 55,\n",
       " 2,\n",
       " 32,\n",
       " 86,\n",
       " 197,\n",
       " 171,\n",
       " 151,\n",
       " 61,\n",
       " 108,\n",
       " 83,\n",
       " 137,\n",
       " 119,\n",
       " 124,\n",
       " 48,\n",
       " 67,\n",
       " 166,\n",
       " 201,\n",
       " 134,\n",
       " 239,\n",
       " 1,\n",
       " 172,\n",
       " 44,\n",
       " 236,\n",
       " 90,\n",
       " 233,\n",
       " 106,\n",
       " 77,\n",
       " 240,\n",
       " 104,\n",
       " 176,\n",
       " 71,\n",
       " 141,\n",
       " 75,\n",
       " 0,\n",
       " 227,\n",
       " 142,\n",
       " 101,\n",
       " 109,\n",
       " 198,\n",
       " 147,\n",
       " 202,\n",
       " 154,\n",
       " 73,\n",
       " 207,\n",
       " 18,\n",
       " 235,\n",
       " 226,\n",
       " 93,\n",
       " 65,\n",
       " 59,\n",
       " 150,\n",
       " 120,\n",
       " 194,\n",
       " 113,\n",
       " 195,\n",
       " 205,\n",
       " 121,\n",
       " 118,\n",
       " 153,\n",
       " 57,\n",
       " 232,\n",
       " 39,\n",
       " 17,\n",
       " 216,\n",
       " 132,\n",
       " 41,\n",
       " 140,\n",
       " 247,\n",
       " 116,\n",
       " 178,\n",
       " 64,\n",
       " 97,\n",
       " 179,\n",
       " 100,\n",
       " 58,\n",
       " 224,\n",
       " 76,\n",
       " 170,\n",
       " 70,\n",
       " 69,\n",
       " 51,\n",
       " 243,\n",
       " 103,\n",
       " 133,\n",
       " 169,\n",
       " 136,\n",
       " 218,\n",
       " 237,\n",
       " 238,\n",
       " 143,\n",
       " 72,\n",
       " 174,\n",
       " 167,\n",
       " 102,\n",
       " 225,\n",
       " 177,\n",
       " 200,\n",
       " 89,\n",
       " 181,\n",
       " 114,\n",
       " 146,\n",
       " 206,\n",
       " 144,\n",
       " 221,\n",
       " 173,\n",
       " 38,\n",
       " 139,\n",
       " 92,\n",
       " 99,\n",
       " 190,\n",
       " 242,\n",
       " 135,\n",
       " 193,\n",
       " 165,\n",
       " 191,\n",
       " 162,\n",
       " 163,\n",
       " 40,\n",
       " 168,\n",
       " 145,\n",
       " 130,\n",
       " 196,\n",
       " 183,\n",
       " 96,\n",
       " 199,\n",
       " 187,\n",
       " 234,\n",
       " 115,\n",
       " 180,\n",
       " 241,\n",
       " 95,\n",
       " 212,\n",
       " 244,\n",
       " 245,\n",
       " 156,\n",
       " 138,\n",
       " 246,\n",
       " 229,\n",
       " 210,\n",
       " 159,\n",
       " 189,\n",
       " 220,\n",
       " 188,\n",
       " 161,\n",
       " 88,\n",
       " 219,\n",
       " 223,\n",
       " 98,\n",
       " 160,\n",
       " 68,\n",
       " 215,\n",
       " 209,\n",
       " 228,\n",
       " 192,\n",
       " 129,\n",
       " 164,\n",
       " 213,\n",
       " 185,\n",
       " 184,\n",
       " 214,\n",
       " 231,\n",
       " 157,\n",
       " 230,\n",
       " 182,\n",
       " 208,\n",
       " 248,\n",
       " 211]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(reversed(np.array(list(chain(*ans[8]))).argsort()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Zwykły trening nie robił nic: wartości zwracane przez sieć dla zbioru uczącego nie miały żadnego sensu.\n",
    "# Największe prawdopodobieństwa były prawie niezależne od pytania, miało ono mały wpływ na to, co się dzieje.\n",
    "# Dla kosztu weighted_bin_ce mocno skrzywionego w stronę sytuacji t=1 wyszło podobnie.\n",
    "#\n",
    "# Następny krok: użyć zanurzeń GloVe, dodać cechy z https://arxiv.org/abs/1703.04816\n",
    "#\n",
    "# UPDATE:\n",
    "# Obie cechy podobieństwa kontekstu do pytania dodane.\n",
    "# W glove.6B jest około 70% słów ze zbioru uczącego SQuAD.\n",
    "# W tej chwili te wektory biorę z glove.6B.300d, a pozostałym daję losowy init, \n",
    "# po czym wszystkie zanurzenia są dalej uczone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 20 batches in 7.96s\ttraining loss:\t0.813393\n",
      "Done 40 batches in 16.89s\ttraining loss:\t0.618499\n",
      "Done 60 batches in 25.51s\ttraining loss:\t0.537964\n",
      "Done 80 batches in 33.72s\ttraining loss:\t0.476587\n",
      "Done 100 batches in 41.79s\ttraining loss:\t0.436442\n",
      "Done 120 batches in 48.56s\ttraining loss:\t0.419979\n",
      "Done 140 batches in 57.10s\ttraining loss:\t0.409618\n",
      "Done 160 batches in 65.07s\ttraining loss:\t0.408088\n",
      "Done 180 batches in 74.05s\ttraining loss:\t0.397444\n",
      "Done 200 batches in 81.68s\ttraining loss:\t0.401948\n",
      "Done 220 batches in 90.27s\ttraining loss:\t0.401396\n",
      "Done 240 batches in 98.95s\ttraining loss:\t0.410639\n",
      "Done 260 batches in 107.43s\ttraining loss:\t0.418680\n",
      "Done 280 batches in 115.18s\ttraining loss:\t0.427639\n",
      "Done 300 batches in 123.33s\ttraining loss:\t0.426674\n",
      "Done 320 batches in 131.00s\ttraining loss:\t0.422292\n",
      "Done 340 batches in 139.84s\ttraining loss:\t0.424456\n",
      "Done 360 batches in 148.91s\ttraining loss:\t0.415105\n",
      "Done 380 batches in 156.67s\ttraining loss:\t0.416838\n",
      "Done 400 batches in 163.90s\ttraining loss:\t0.418870\n",
      "Done 420 batches in 171.42s\ttraining loss:\t0.419026\n",
      "Done 440 batches in 180.23s\ttraining loss:\t0.417331\n",
      "Done 460 batches in 189.66s\ttraining loss:\t0.419791\n",
      "Done 480 batches in 198.85s\ttraining loss:\t0.420404\n",
      "Done 500 batches in 206.97s\ttraining loss:\t0.416177\n",
      "Done 520 batches in 214.49s\ttraining loss:\t0.415342\n",
      "Done 540 batches in 222.22s\ttraining loss:\t0.416872\n",
      "Done 560 batches in 229.79s\ttraining loss:\t0.416093\n",
      "Done 580 batches in 237.12s\ttraining loss:\t0.408871\n",
      "Done 600 batches in 244.83s\ttraining loss:\t0.408386\n",
      "Done 620 batches in 252.93s\ttraining loss:\t0.406717\n",
      "Done 640 batches in 260.90s\ttraining loss:\t0.407531\n",
      "Done 660 batches in 269.79s\ttraining loss:\t0.405836\n",
      "Done 680 batches in 277.40s\ttraining loss:\t0.407107\n",
      "Done 700 batches in 284.92s\ttraining loss:\t0.408122\n",
      "Done 720 batches in 292.42s\ttraining loss:\t0.411832\n",
      "Done 740 batches in 300.17s\ttraining loss:\t0.412702\n",
      "Done 760 batches in 308.85s\ttraining loss:\t0.413090\n",
      "Done 780 batches in 317.01s\ttraining loss:\t0.412683\n",
      "Done 800 batches in 325.79s\ttraining loss:\t0.411818\n",
      "Done 820 batches in 334.14s\ttraining loss:\t0.410459\n",
      "Done 840 batches in 343.48s\ttraining loss:\t0.407757\n",
      "Done 860 batches in 351.91s\ttraining loss:\t0.407190\n",
      "Done 880 batches in 358.82s\ttraining loss:\t0.406966\n",
      "Done 900 batches in 366.59s\ttraining loss:\t0.405027\n",
      "Done 920 batches in 375.56s\ttraining loss:\t0.404316\n",
      "Done 940 batches in 383.60s\ttraining loss:\t0.402061\n",
      "Done 960 batches in 392.46s\ttraining loss:\t0.399332\n",
      "Done 980 batches in 400.47s\ttraining loss:\t0.404043\n",
      "Done 1000 batches in 407.74s\ttraining loss:\t0.409821\n",
      "Done 1020 batches in 415.21s\ttraining loss:\t0.416582\n",
      "Done 1040 batches in 422.79s\ttraining loss:\t0.422596\n",
      "Done 1060 batches in 431.41s\ttraining loss:\t0.423714\n",
      "Done 1080 batches in 439.96s\ttraining loss:\t0.424034\n",
      "Done 1100 batches in 448.43s\ttraining loss:\t0.427439\n",
      "Done 1120 batches in 457.71s\ttraining loss:\t0.429270\n",
      "Done 1140 batches in 466.58s\ttraining loss:\t0.433387\n",
      "Done 1160 batches in 475.75s\ttraining loss:\t0.436254\n",
      "Done 1180 batches in 482.82s\ttraining loss:\t0.441358\n",
      "Done 1200 batches in 491.42s\ttraining loss:\t0.438379\n",
      "Done 1220 batches in 499.96s\ttraining loss:\t0.435319\n",
      "Done 1240 batches in 508.77s\ttraining loss:\t0.434273\n",
      "Done 1260 batches in 517.57s\ttraining loss:\t0.433936\n",
      "Done 1280 batches in 526.22s\ttraining loss:\t0.436965\n",
      "Done 1300 batches in 533.97s\ttraining loss:\t0.438821\n",
      "Done 1320 batches in 541.44s\ttraining loss:\t0.439510\n",
      "Done 1340 batches in 549.06s\ttraining loss:\t0.438974\n",
      "Done 1360 batches in 558.08s\ttraining loss:\t0.437469\n",
      "Done 1380 batches in 566.40s\ttraining loss:\t0.437325\n",
      "Done 1400 batches in 574.89s\ttraining loss:\t0.435750\n",
      "Done 1420 batches in 582.96s\ttraining loss:\t0.433850\n",
      "Done 1440 batches in 590.76s\ttraining loss:\t0.434015\n",
      "Done 1460 batches in 598.12s\ttraining loss:\t0.434544\n",
      "Done 1480 batches in 604.34s\ttraining loss:\t0.434400\n",
      "Done 1500 batches in 611.19s\ttraining loss:\t0.434535\n",
      "Done 1520 batches in 619.96s\ttraining loss:\t0.432634\n",
      "Done 1540 batches in 628.15s\ttraining loss:\t0.431123\n",
      "Done 1560 batches in 636.14s\ttraining loss:\t0.430235\n",
      "Done 1580 batches in 644.25s\ttraining loss:\t0.429545\n",
      "Done 1600 batches in 652.91s\ttraining loss:\t0.429710\n",
      "Done 1620 batches in 659.96s\ttraining loss:\t0.429660\n",
      "Done 1640 batches in 667.16s\ttraining loss:\t0.428866\n",
      "Done 1660 batches in 675.28s\ttraining loss:\t0.425995\n",
      "Done 1680 batches in 683.37s\ttraining loss:\t0.424745\n",
      "Done 1700 batches in 692.16s\ttraining loss:\t0.422782\n",
      "Done 1720 batches in 700.82s\ttraining loss:\t0.423887\n",
      "Done 1740 batches in 709.13s\ttraining loss:\t0.430753\n",
      "Done 1760 batches in 719.00s\ttraining loss:\t0.434818\n",
      "Done 1780 batches in 726.48s\ttraining loss:\t0.441409\n",
      "Done 1800 batches in 734.22s\ttraining loss:\t0.441740\n",
      "Done 1820 batches in 742.53s\ttraining loss:\t0.441645\n",
      "Done 1840 batches in 752.19s\ttraining loss:\t0.441036\n",
      "Done 1860 batches in 760.79s\ttraining loss:\t0.440892\n",
      "Done 1880 batches in 768.95s\ttraining loss:\t0.439639\n",
      "Done 1900 batches in 776.61s\ttraining loss:\t0.439933\n",
      "Done 1920 batches in 784.30s\ttraining loss:\t0.439975\n",
      "Done 1940 batches in 790.83s\ttraining loss:\t0.440051\n",
      "Done 1960 batches in 799.31s\ttraining loss:\t0.439937\n",
      "Done 1980 batches in 807.31s\ttraining loss:\t0.439576\n",
      "Done 2000 batches in 815.80s\ttraining loss:\t0.439899\n",
      "Done 2020 batches in 823.95s\ttraining loss:\t0.441725\n",
      "Done 2040 batches in 832.07s\ttraining loss:\t0.442261\n",
      "Done 2060 batches in 841.26s\ttraining loss:\t0.443953\n",
      "Done 2080 batches in 849.59s\ttraining loss:\t0.445274\n",
      "Done 2100 batches in 857.71s\ttraining loss:\t0.445195\n",
      "Done 2120 batches in 865.53s\ttraining loss:\t0.444672\n",
      "Done 2140 batches in 873.78s\ttraining loss:\t0.443986\n",
      "Done 2160 batches in 882.04s\ttraining loss:\t0.444449\n",
      "Done 2180 batches in 889.96s\ttraining loss:\t0.445268\n",
      "Done 2200 batches in 898.35s\ttraining loss:\t0.445640\n",
      "Done 2220 batches in 905.98s\ttraining loss:\t0.446017\n",
      "Done 2240 batches in 912.56s\ttraining loss:\t0.446291\n",
      "Done 2260 batches in 920.56s\ttraining loss:\t0.444988\n",
      "Done 2280 batches in 929.12s\ttraining loss:\t0.443758\n",
      "Done 2300 batches in 937.52s\ttraining loss:\t0.442648\n",
      "Done 2320 batches in 945.58s\ttraining loss:\t0.442092\n",
      "Done 2340 batches in 953.16s\ttraining loss:\t0.441642\n",
      "Done 2360 batches in 960.39s\ttraining loss:\t0.441188\n",
      "Done 2380 batches in 968.62s\ttraining loss:\t0.440855\n",
      "Done 2400 batches in 976.90s\ttraining loss:\t0.440366\n",
      "Done 2420 batches in 985.54s\ttraining loss:\t0.440122\n",
      "Done 2440 batches in 994.25s\ttraining loss:\t0.439948\n",
      "Done 2460 batches in 1002.95s\ttraining loss:\t0.439623\n",
      "Done 2480 batches in 1011.30s\ttraining loss:\t0.438941\n",
      "Done 2500 batches in 1018.73s\ttraining loss:\t0.438774\n",
      "Done 2520 batches in 1027.22s\ttraining loss:\t0.438608\n",
      "Done 2540 batches in 1034.78s\ttraining loss:\t0.438557\n",
      "Done 2560 batches in 1042.20s\ttraining loss:\t0.438424\n",
      "Done 2580 batches in 1051.12s\ttraining loss:\t0.438551\n",
      "Done 2600 batches in 1058.51s\ttraining loss:\t0.438934\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-64ada69a1fb5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhred_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_one_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_trimmed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/pio/scratch/1/i258346/masters_thesis/SQuAD/HRED_v2.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[1;34m(self, train_data, batch_size, log_interval)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m             \u001b[0mnum_batch_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m             \u001b[0mtrain_err\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbin_feat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manswers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext_mask\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnum_batch_words\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m             \u001b[0mtrain_batches\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             \u001b[0mnum_training_words\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnum_batch_words\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/i258346/.local/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    882\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 884\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/i258346/.local/lib/python2.7/site-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n)\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNoParams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    859\u001b[0m             \u001b[1;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 860\u001b[1;33m             \u001b[1;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    861\u001b[0m                 \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    862\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hred_net.train_one_epoch(data_trimmed, 5, log_interval=20) # weighted cost, random init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 20 batches in 2.25s\ttraining loss:\t0.228349\n",
      "Done 40 batches in 5.03s\ttraining loss:\t0.154742\n",
      "Done 60 batches in 7.65s\ttraining loss:\t0.126147\n",
      "Done 80 batches in 10.05s\ttraining loss:\t0.106807\n",
      "Done 100 batches in 12.52s\ttraining loss:\t0.095184\n",
      "Done 120 batches in 14.64s\ttraining loss:\t0.090502\n",
      "Done 140 batches in 17.25s\ttraining loss:\t0.087078\n",
      "Done 160 batches in 19.51s\ttraining loss:\t0.085557\n",
      "Done 180 batches in 22.31s\ttraining loss:\t0.082334\n",
      "Done 200 batches in 24.40s\ttraining loss:\t0.082256\n",
      "Done 220 batches in 26.96s\ttraining loss:\t0.081215\n",
      "Done 240 batches in 29.60s\ttraining loss:\t0.082376\n",
      "Done 260 batches in 32.51s\ttraining loss:\t0.083784\n",
      "Done 280 batches in 34.97s\ttraining loss:\t0.084956\n",
      "Done 300 batches in 37.36s\ttraining loss:\t0.084330\n",
      "Done 320 batches in 39.52s\ttraining loss:\t0.083391\n",
      "Done 340 batches in 42.24s\ttraining loss:\t0.083563\n",
      "Done 360 batches in 44.99s\ttraining loss:\t0.081207\n",
      "Done 380 batches in 47.14s\ttraining loss:\t0.081512\n",
      "Done 400 batches in 49.09s\ttraining loss:\t0.081817\n",
      "Done 420 batches in 51.60s\ttraining loss:\t0.081576\n",
      "Done 440 batches in 54.40s\ttraining loss:\t0.081043\n",
      "Done 460 batches in 57.43s\ttraining loss:\t0.081339\n",
      "Done 480 batches in 60.30s\ttraining loss:\t0.081267\n",
      "Done 500 batches in 62.65s\ttraining loss:\t0.080241\n",
      "Done 520 batches in 64.71s\ttraining loss:\t0.079872\n",
      "Done 540 batches in 66.90s\ttraining loss:\t0.080126\n",
      "Done 560 batches in 69.28s\ttraining loss:\t0.079805\n",
      "Done 580 batches in 71.59s\ttraining loss:\t0.078206\n",
      "Done 600 batches in 73.70s\ttraining loss:\t0.078006\n",
      "Done 620 batches in 76.02s\ttraining loss:\t0.077540\n",
      "Done 640 batches in 78.29s\ttraining loss:\t0.077661\n",
      "Done 660 batches in 81.00s\ttraining loss:\t0.077267\n",
      "Done 680 batches in 83.10s\ttraining loss:\t0.077439\n",
      "Done 700 batches in 85.16s\ttraining loss:\t0.077608\n",
      "Done 720 batches in 87.61s\ttraining loss:\t0.078403\n",
      "Done 740 batches in 90.07s\ttraining loss:\t0.078511\n",
      "Done 760 batches in 92.63s\ttraining loss:\t0.078503\n",
      "Done 780 batches in 94.92s\ttraining loss:\t0.078355\n",
      "Done 800 batches in 97.58s\ttraining loss:\t0.078138\n",
      "Done 820 batches in 100.06s\ttraining loss:\t0.077781\n",
      "Done 840 batches in 103.01s\ttraining loss:\t0.077201\n",
      "Done 860 batches in 105.70s\ttraining loss:\t0.077054\n",
      "Done 880 batches in 107.92s\ttraining loss:\t0.076950\n",
      "Done 900 batches in 110.14s\ttraining loss:\t0.076493\n",
      "Done 920 batches in 112.94s\ttraining loss:\t0.076241\n",
      "Done 940 batches in 115.26s\ttraining loss:\t0.075677\n",
      "Done 960 batches in 117.99s\ttraining loss:\t0.075053\n",
      "Done 980 batches in 120.26s\ttraining loss:\t0.076049\n",
      "Done 1000 batches in 122.23s\ttraining loss:\t0.077339\n",
      "Done 1020 batches in 124.55s\ttraining loss:\t0.078867\n",
      "Done 1040 batches in 127.04s\ttraining loss:\t0.080173\n",
      "Done 1060 batches in 129.60s\ttraining loss:\t0.080350\n",
      "Done 1080 batches in 132.17s\ttraining loss:\t0.080329\n",
      "Done 1100 batches in 134.68s\ttraining loss:\t0.081008\n",
      "Done 1120 batches in 137.63s\ttraining loss:\t0.081333\n",
      "Done 1140 batches in 140.35s\ttraining loss:\t0.082265\n",
      "Done 1160 batches in 143.38s\ttraining loss:\t0.082898\n",
      "Done 1180 batches in 145.65s\ttraining loss:\t0.083972\n",
      "Done 1200 batches in 148.24s\ttraining loss:\t0.083225\n",
      "Done 1220 batches in 150.71s\ttraining loss:\t0.082501\n",
      "Done 1240 batches in 153.33s\ttraining loss:\t0.082238\n",
      "Done 1260 batches in 155.95s\ttraining loss:\t0.082117\n",
      "Done 1280 batches in 158.54s\ttraining loss:\t0.082770\n",
      "Done 1300 batches in 160.68s\ttraining loss:\t0.083157\n",
      "Done 1320 batches in 163.10s\ttraining loss:\t0.083262\n",
      "Done 1340 batches in 165.49s\ttraining loss:\t0.083107\n",
      "Done 1360 batches in 168.28s\ttraining loss:\t0.082768\n",
      "Done 1380 batches in 170.73s\ttraining loss:\t0.082702\n",
      "Done 1400 batches in 173.25s\ttraining loss:\t0.082357\n",
      "Done 1420 batches in 175.58s\ttraining loss:\t0.081953\n",
      "Done 1440 batches in 177.77s\ttraining loss:\t0.082008\n",
      "Done 1460 batches in 179.76s\ttraining loss:\t0.082140\n",
      "Done 1480 batches in 181.61s\ttraining loss:\t0.082130\n",
      "Done 1500 batches in 183.58s\ttraining loss:\t0.082174\n",
      "Done 1520 batches in 186.24s\ttraining loss:\t0.081775\n",
      "Done 1540 batches in 188.61s\ttraining loss:\t0.081439\n",
      "Done 1560 batches in 190.90s\ttraining loss:\t0.081231\n",
      "Done 1580 batches in 193.30s\ttraining loss:\t0.081040\n",
      "Done 1600 batches in 195.93s\ttraining loss:\t0.081032\n",
      "Done 1620 batches in 197.83s\ttraining loss:\t0.081020\n",
      "Done 1640 batches in 200.18s\ttraining loss:\t0.080836\n",
      "Done 1660 batches in 202.61s\ttraining loss:\t0.080271\n",
      "Done 1680 batches in 204.94s\ttraining loss:\t0.080008\n",
      "Done 1700 batches in 207.61s\ttraining loss:\t0.079575\n",
      "Done 1720 batches in 210.23s\ttraining loss:\t0.079748\n",
      "Done 1740 batches in 212.68s\ttraining loss:\t0.081393\n",
      "Done 1760 batches in 215.40s\ttraining loss:\t0.082344\n",
      "Done 1780 batches in 217.83s\ttraining loss:\t0.084007\n",
      "Done 1800 batches in 220.25s\ttraining loss:\t0.083921\n",
      "Done 1820 batches in 222.67s\ttraining loss:\t0.083847\n",
      "Done 1840 batches in 225.76s\ttraining loss:\t0.083691\n",
      "Done 1860 batches in 228.37s\ttraining loss:\t0.083606\n",
      "Done 1880 batches in 230.73s\ttraining loss:\t0.083308\n",
      "Done 1900 batches in 232.90s\ttraining loss:\t0.083360\n",
      "Done 1920 batches in 235.19s\ttraining loss:\t0.083349\n",
      "Done 1940 batches in 237.19s\ttraining loss:\t0.083379\n",
      "Done 1960 batches in 239.77s\ttraining loss:\t0.083347\n",
      "Done 1980 batches in 242.10s\ttraining loss:\t0.083264\n",
      "Done 2000 batches in 244.65s\ttraining loss:\t0.083313\n",
      "Done 2020 batches in 246.98s\ttraining loss:\t0.083742\n",
      "Done 2040 batches in 249.30s\ttraining loss:\t0.083832\n",
      "Done 2060 batches in 252.19s\ttraining loss:\t0.084214\n",
      "Done 2080 batches in 255.02s\ttraining loss:\t0.084517\n",
      "Done 2100 batches in 257.63s\ttraining loss:\t0.084477\n",
      "Done 2120 batches in 259.75s\ttraining loss:\t0.084350\n",
      "Done 2140 batches in 262.12s\ttraining loss:\t0.084199\n",
      "Done 2160 batches in 264.48s\ttraining loss:\t0.084294\n",
      "Done 2180 batches in 266.70s\ttraining loss:\t0.084472\n",
      "Done 2200 batches in 269.17s\ttraining loss:\t0.084553\n",
      "Done 2220 batches in 271.34s\ttraining loss:\t0.084627\n",
      "Done 2240 batches in 273.79s\ttraining loss:\t0.084675\n",
      "Done 2260 batches in 276.32s\ttraining loss:\t0.084386\n",
      "Done 2280 batches in 278.86s\ttraining loss:\t0.084116\n",
      "Done 2300 batches in 281.38s\ttraining loss:\t0.083881\n",
      "Done 2320 batches in 283.70s\ttraining loss:\t0.083758\n",
      "Done 2340 batches in 285.80s\ttraining loss:\t0.083667\n",
      "Done 2360 batches in 287.73s\ttraining loss:\t0.083566\n",
      "Done 2380 batches in 290.19s\ttraining loss:\t0.083491\n",
      "Done 2400 batches in 293.14s\ttraining loss:\t0.083378\n",
      "Done 2420 batches in 295.73s\ttraining loss:\t0.083295\n",
      "Done 2440 batches in 298.31s\ttraining loss:\t0.083252\n",
      "Done 2460 batches in 300.87s\ttraining loss:\t0.083187\n",
      "Done 2480 batches in 303.39s\ttraining loss:\t0.083052\n",
      "Done 2500 batches in 305.45s\ttraining loss:\t0.083019\n",
      "Done 2520 batches in 307.97s\ttraining loss:\t0.082983\n",
      "Done 2540 batches in 310.39s\ttraining loss:\t0.082987\n",
      "Done 2560 batches in 312.73s\ttraining loss:\t0.082951\n",
      "Done 2580 batches in 315.43s\ttraining loss:\t0.082987\n",
      "Done 2600 batches in 317.41s\ttraining loss:\t0.083058\n",
      "Done 2620 batches in 319.94s\ttraining loss:\t0.083174\n",
      "Done 2640 batches in 322.48s\ttraining loss:\t0.083268\n",
      "Done 2660 batches in 324.62s\ttraining loss:\t0.083458\n",
      "Done 2680 batches in 326.73s\ttraining loss:\t0.083409\n",
      "Done 2700 batches in 329.01s\ttraining loss:\t0.083184\n",
      "Done 2720 batches in 331.53s\ttraining loss:\t0.082915\n",
      "Done 2740 batches in 334.17s\ttraining loss:\t0.082666\n",
      "Done 2760 batches in 336.83s\ttraining loss:\t0.082375\n",
      "Done 2780 batches in 339.10s\ttraining loss:\t0.082695\n",
      "Done 2800 batches in 341.49s\ttraining loss:\t0.082578\n",
      "Done 2820 batches in 343.95s\ttraining loss:\t0.082545\n",
      "Done 2840 batches in 346.30s\ttraining loss:\t0.082490\n",
      "Done 2860 batches in 348.47s\ttraining loss:\t0.082513\n",
      "Done 2880 batches in 350.55s\ttraining loss:\t0.082629\n",
      "Done 2900 batches in 352.74s\ttraining loss:\t0.082627\n",
      "Done 2920 batches in 355.22s\ttraining loss:\t0.082777\n",
      "Done 2940 batches in 357.38s\ttraining loss:\t0.082989\n",
      "Done 2960 batches in 360.03s\ttraining loss:\t0.082944\n",
      "Done 2980 batches in 362.65s\ttraining loss:\t0.082840\n",
      "Done 3000 batches in 365.10s\ttraining loss:\t0.082781\n",
      "Done 3020 batches in 367.54s\ttraining loss:\t0.082870\n",
      "Done 3040 batches in 369.62s\ttraining loss:\t0.083219\n",
      "Done 3060 batches in 372.31s\ttraining loss:\t0.083346\n",
      "Done 3080 batches in 374.34s\ttraining loss:\t0.083250\n",
      "Done 3100 batches in 376.43s\ttraining loss:\t0.083245\n",
      "Done 3120 batches in 378.61s\ttraining loss:\t0.083298\n",
      "Done 3140 batches in 380.43s\ttraining loss:\t0.083373\n",
      "Done 3160 batches in 382.61s\ttraining loss:\t0.083474\n",
      "Done 3180 batches in 385.21s\ttraining loss:\t0.083542\n",
      "Done 3200 batches in 387.36s\ttraining loss:\t0.083450\n",
      "Done 3220 batches in 389.75s\ttraining loss:\t0.083319\n",
      "Done 3240 batches in 392.17s\ttraining loss:\t0.083259\n",
      "Done 3260 batches in 394.37s\ttraining loss:\t0.083201\n",
      "Done 3280 batches in 397.29s\ttraining loss:\t0.083164\n",
      "Done 3300 batches in 399.99s\ttraining loss:\t0.083116\n",
      "Done 3320 batches in 402.28s\ttraining loss:\t0.083230\n",
      "Done 3340 batches in 404.82s\ttraining loss:\t0.083286\n",
      "Done 3360 batches in 407.30s\ttraining loss:\t0.083341\n",
      "Done 3380 batches in 409.63s\ttraining loss:\t0.083381\n",
      "Done 3400 batches in 411.81s\ttraining loss:\t0.083656\n",
      "Done 3420 batches in 414.28s\ttraining loss:\t0.083676\n",
      "Done 3440 batches in 416.79s\ttraining loss:\t0.083656\n",
      "Done 3460 batches in 419.58s\ttraining loss:\t0.083481\n",
      "Done 3480 batches in 421.96s\ttraining loss:\t0.083374\n",
      "Done 3500 batches in 424.48s\ttraining loss:\t0.083406\n",
      "Done 3520 batches in 426.69s\ttraining loss:\t0.083517\n",
      "Done 3540 batches in 428.92s\ttraining loss:\t0.083598\n",
      "Done 3560 batches in 431.32s\ttraining loss:\t0.083693\n",
      "Done 3580 batches in 433.80s\ttraining loss:\t0.083768\n",
      "Done 3600 batches in 436.10s\ttraining loss:\t0.083722\n",
      "Done 3620 batches in 438.77s\ttraining loss:\t0.083722\n",
      "Done 3640 batches in 441.50s\ttraining loss:\t0.083669\n",
      "Done 3660 batches in 444.50s\ttraining loss:\t0.083569\n",
      "Done 3680 batches in 447.27s\ttraining loss:\t0.083521\n",
      "Done 3700 batches in 449.57s\ttraining loss:\t0.083480\n",
      "Done 3720 batches in 452.27s\ttraining loss:\t0.083580\n",
      "Done 3740 batches in 454.56s\ttraining loss:\t0.083695\n",
      "Done 3760 batches in 456.78s\ttraining loss:\t0.083617\n",
      "Done 3780 batches in 459.11s\ttraining loss:\t0.083555\n",
      "Done 3800 batches in 461.14s\ttraining loss:\t0.083450\n",
      "Done 3820 batches in 463.86s\ttraining loss:\t0.083416\n",
      "Done 3840 batches in 465.90s\ttraining loss:\t0.083386\n",
      "Done 3860 batches in 468.20s\ttraining loss:\t0.083561\n",
      "Done 3880 batches in 470.61s\ttraining loss:\t0.083721\n",
      "Done 3900 batches in 472.78s\ttraining loss:\t0.083661\n",
      "Done 3920 batches in 475.16s\ttraining loss:\t0.083431\n",
      "Done 3940 batches in 477.57s\ttraining loss:\t0.083281\n",
      "Done 3960 batches in 479.76s\ttraining loss:\t0.083151\n",
      "Done 3980 batches in 481.76s\ttraining loss:\t0.083135\n",
      "Done 4000 batches in 483.84s\ttraining loss:\t0.083099\n",
      "Done 4020 batches in 486.14s\ttraining loss:\t0.083242\n",
      "Done 4040 batches in 488.55s\ttraining loss:\t0.083450\n",
      "Done 4060 batches in 491.19s\ttraining loss:\t0.083640\n",
      "Done 4080 batches in 493.71s\ttraining loss:\t0.083431\n",
      "Done 4100 batches in 496.13s\ttraining loss:\t0.083501\n",
      "Done 4120 batches in 498.46s\ttraining loss:\t0.083661\n",
      "Done 4140 batches in 500.75s\ttraining loss:\t0.083733\n",
      "Done 4160 batches in 503.19s\ttraining loss:\t0.083856\n",
      "Done 4180 batches in 505.25s\ttraining loss:\t0.084116\n",
      "Done 4200 batches in 507.40s\ttraining loss:\t0.084245\n",
      "Done 4220 batches in 509.21s\ttraining loss:\t0.084350\n",
      "Done 4240 batches in 510.99s\ttraining loss:\t0.084466\n",
      "Done 4260 batches in 512.63s\ttraining loss:\t0.084566\n",
      "Done 4280 batches in 514.65s\ttraining loss:\t0.084610\n",
      "Done 4300 batches in 516.76s\ttraining loss:\t0.084608\n",
      "Done 4320 batches in 519.22s\ttraining loss:\t0.084525\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-64ada69a1fb5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhred_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_one_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_trimmed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/pio/scratch/1/i258346/masters_thesis/SQuAD/HRED_v2.pyc\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[1;34m(self, train_data, batch_size, log_interval)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m             \u001b[0mnum_batch_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m             \u001b[0mtrain_err\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbin_feat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manswers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext_mask\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnum_batch_words\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m             \u001b[0mtrain_batches\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             \u001b[0mnum_training_words\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnum_batch_words\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/i258346/.local/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    882\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 884\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/i258346/.local/lib/python2.7/site-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n)\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNoParams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    859\u001b[0m             \u001b[1;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 860\u001b[1;33m             \u001b[1;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    861\u001b[0m                 \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    862\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hred_net.train_one_epoch(data_trimmed, 5, log_interval=20) # normal binary cross-entropy with random init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 20 batches in 2.66s\ttraining loss:\t0.247073\n",
      "Done 40 batches in 5.97s\ttraining loss:\t0.187469\n",
      "Done 60 batches in 9.10s\ttraining loss:\t0.148785\n",
      "Done 80 batches in 11.96s\ttraining loss:\t0.123427\n",
      "Done 100 batches in 14.91s\ttraining loss:\t0.108109\n",
      "Done 120 batches in 17.43s\ttraining loss:\t0.102956\n",
      "Done 140 batches in 20.55s\ttraining loss:\t0.097911\n",
      "Done 160 batches in 23.25s\ttraining loss:\t0.095246\n",
      "Done 180 batches in 26.59s\ttraining loss:\t0.090887\n",
      "Done 200 batches in 30.07s\ttraining loss:\t0.090136\n",
      "Done 220 batches in 33.13s\ttraining loss:\t0.088647\n",
      "Done 240 batches in 36.28s\ttraining loss:\t0.089258\n",
      "Done 260 batches in 39.74s\ttraining loss:\t0.091964\n",
      "Done 280 batches in 42.68s\ttraining loss:\t0.093008\n",
      "Done 300 batches in 45.52s\ttraining loss:\t0.091937\n",
      "Done 320 batches in 48.11s\ttraining loss:\t0.090498\n",
      "Done 340 batches in 51.34s\ttraining loss:\t0.090224\n",
      "Done 360 batches in 54.62s\ttraining loss:\t0.087337\n",
      "Done 380 batches in 57.19s\ttraining loss:\t0.087323\n",
      "Done 400 batches in 59.53s\ttraining loss:\t0.087407\n",
      "Done 420 batches in 62.52s\ttraining loss:\t0.086922\n",
      "Done 440 batches in 65.86s\ttraining loss:\t0.086167\n",
      "Done 460 batches in 69.47s\ttraining loss:\t0.086232\n",
      "Done 480 batches in 72.88s\ttraining loss:\t0.086210\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-acdb6acd611e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhred_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_one_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_trimmed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# normal binary cross-entropy with glove init\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/pio/scratch/1/i258346/masters_thesis/SQuAD/HRED_v2.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[1;34m(self, train_data, batch_size, log_interval)\u001b[0m\n\u001b[0;32m    132\u001b[0m             \u001b[0mnum_batch_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m             \u001b[0mtrain_err\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbin_feat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manswers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext_mask\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnum_batch_words\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m             \u001b[0mtrain_batches\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m             \u001b[0mnum_training_words\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnum_batch_words\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/i258346/.local/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    882\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 884\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/i258346/.local/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[0;32m    987\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[0;32m    988\u001b[0m                  allow_gc=allow_gc):\n\u001b[1;32m--> 989\u001b[1;33m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    990\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/i258346/.local/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36mp\u001b[1;34m(node, args, outs)\u001b[0m\n\u001b[0;32m    976\u001b[0m                                                 \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    977\u001b[0m                                                 \u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 978\u001b[1;33m                                                 self, node)\n\u001b[0m\u001b[0;32m    979\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hred_net.train_one_epoch(data_trimmed, 5, log_interval=20) # normal binary cross-entropy with glove init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hred_net.save_params()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: GeForce GTX 780 (CNMeM is enabled with initial size: 30.0% of memory, cuDNN 5105)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "import numpy as np\n",
    "import lasagne as L\n",
    "from squad_load import get_glove_train_embs, get_squad_train_voc, load_squad_train\n",
    "from itertools import chain\n",
    "\n",
    "%aimport QANet_wo_charemb_glove_unk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples total: 87599\n",
      "Working examples: 86474\n",
      "CPU times: user 9 s, sys: 190 ms, total: 9.19 s\n",
      "Wall time: 9.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "squad_path = '/pio/data/data/squad/'\n",
    "glove_path = '/pio/data/data/glove_vec/6B/'\n",
    "\n",
    "data = np.load(squad_path + 'train_with_glove_vocab.pkl')\n",
    "glove_embs = np.load(glove_path + 'glove.6B.300d.npy')\n",
    "voc_size = glove_embs.shape[0]\n",
    "\n",
    "# Some answers get broken in the process of tokenization, because some answer words are not properly split.\n",
    "def filter_broken_answers(data):\n",
    "    return [d for d in data if d[0]]\n",
    "\n",
    "print 'Examples total:', len(data)\n",
    "\n",
    "data = filter_broken_answers(data)\n",
    "\n",
    "print 'Working examples:', len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimmed examples: 86355\n"
     ]
    }
   ],
   "source": [
    "trim = 300\n",
    "data = [d[:2] + [d[2][:trim]] for d in data if max(d[0][0]) < trim]\n",
    "print 'Trimmed examples:', len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dev = np.load(squad_path + 'dev_with_glove_vocab.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QANet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model...\n",
      "Using custom update_fn.\n",
      "Compiling theano functions...\n",
      "Done\n",
      "CPU times: user 1min 17s, sys: 2.1 s, total: 1min 19s\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "update_fn = lambda l, p: L.updates.adam(l, p)\n",
    "\n",
    "qa_net = QANet_wo_charemb_glove_unk.QANet(voc_size=voc_size,\n",
    "                                          emb_size=300,\n",
    "                                          rec_size=300,\n",
    "                                          emb_init=glove_embs,\n",
    "                                          train_inds=[],\n",
    "                                          update_fn=update_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "qa_net.load_params('trained_models/glove_vocab/simplified_all_fixed3_1ep.npz', glove_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 5 batches in 4.76s\ttraining loss:\t9.284095\n",
      "Done 10 batches in 9.43s\ttraining loss:\t8.997582\n",
      "Done 15 batches in 13.89s\ttraining loss:\t8.464465\n",
      "Done 20 batches in 18.78s\ttraining loss:\t8.095969\n",
      "Done 25 batches in 23.56s\ttraining loss:\t7.775052\n",
      "Done 30 batches in 28.26s\ttraining loss:\t7.550472\n",
      "Done 35 batches in 32.42s\ttraining loss:\t7.391315\n",
      "Done 40 batches in 37.18s\ttraining loss:\t7.235471\n",
      "Done 45 batches in 41.60s\ttraining loss:\t7.132887\n",
      "Done 50 batches in 46.32s\ttraining loss:\t7.046891\n",
      "Done 55 batches in 50.74s\ttraining loss:\t6.958380\n",
      "Done 60 batches in 55.21s\ttraining loss:\t6.879590\n",
      "Done 65 batches in 60.05s\ttraining loss:\t6.823590\n",
      "Done 70 batches in 65.01s\ttraining loss:\t6.789325\n",
      "Done 75 batches in 68.90s\ttraining loss:\t6.755375\n",
      "Done 80 batches in 73.55s\ttraining loss:\t6.680267\n",
      "Done 85 batches in 78.50s\ttraining loss:\t6.642193\n",
      "Done 90 batches in 83.20s\ttraining loss:\t6.574215\n",
      "Done 95 batches in 88.16s\ttraining loss:\t6.534975\n",
      "Done 100 batches in 92.41s\ttraining loss:\t6.512547\n",
      "Done 105 batches in 97.40s\ttraining loss:\t6.473039\n",
      "Done 110 batches in 102.08s\ttraining loss:\t6.452169\n",
      "Done 115 batches in 106.54s\ttraining loss:\t6.417274\n",
      "Done 120 batches in 111.14s\ttraining loss:\t6.409570\n",
      "Done 125 batches in 115.43s\ttraining loss:\t6.390872\n",
      "Done 130 batches in 120.37s\ttraining loss:\t6.360432\n",
      "Done 135 batches in 124.68s\ttraining loss:\t6.349251\n",
      "Done 140 batches in 129.12s\ttraining loss:\t6.333109\n",
      "Done 145 batches in 133.51s\ttraining loss:\t6.312440\n",
      "Done 150 batches in 138.34s\ttraining loss:\t6.296668\n",
      "Done 155 batches in 142.72s\ttraining loss:\t6.274733\n",
      "Done 160 batches in 147.50s\ttraining loss:\t6.249804\n",
      "Done 165 batches in 152.22s\ttraining loss:\t6.238760\n",
      "Done 170 batches in 157.05s\ttraining loss:\t6.199100\n",
      "Done 175 batches in 161.44s\ttraining loss:\t6.180207\n",
      "Done 180 batches in 165.80s\ttraining loss:\t6.160228\n",
      "Done 185 batches in 170.39s\ttraining loss:\t6.135528\n",
      "Done 190 batches in 174.67s\ttraining loss:\t6.108728\n",
      "Done 195 batches in 179.44s\ttraining loss:\t6.092819\n",
      "Done 200 batches in 184.57s\ttraining loss:\t6.071500\n",
      "Done 205 batches in 188.98s\ttraining loss:\t6.048491\n",
      "Done 210 batches in 193.89s\ttraining loss:\t6.016317\n",
      "Done 215 batches in 198.74s\ttraining loss:\t6.011391\n",
      "Done 220 batches in 203.43s\ttraining loss:\t5.992020\n",
      "Done 225 batches in 207.70s\ttraining loss:\t5.974260\n",
      "Done 230 batches in 212.35s\ttraining loss:\t5.962942\n",
      "Done 235 batches in 217.22s\ttraining loss:\t5.949736\n",
      "Done 240 batches in 221.82s\ttraining loss:\t5.937152\n",
      "Done 245 batches in 226.55s\ttraining loss:\t5.923310\n",
      "Done 250 batches in 231.47s\ttraining loss:\t5.911100\n",
      "Done 255 batches in 236.36s\ttraining loss:\t5.893846\n",
      "Done 260 batches in 241.00s\ttraining loss:\t5.873737\n",
      "Done 265 batches in 245.42s\ttraining loss:\t5.860914\n",
      "Done 270 batches in 250.17s\ttraining loss:\t5.843983\n",
      "Done 275 batches in 254.53s\ttraining loss:\t5.826652\n",
      "Done 280 batches in 258.81s\ttraining loss:\t5.811023\n",
      "Done 285 batches in 263.42s\ttraining loss:\t5.794596\n",
      "Done 290 batches in 268.17s\ttraining loss:\t5.772505\n",
      "Done 295 batches in 273.02s\ttraining loss:\t5.764119\n",
      "Done 300 batches in 278.22s\ttraining loss:\t5.741761\n",
      "Done 305 batches in 283.27s\ttraining loss:\t5.722065\n",
      "Done 310 batches in 287.84s\ttraining loss:\t5.713720\n",
      "Done 315 batches in 292.79s\ttraining loss:\t5.708441\n",
      "Done 320 batches in 297.53s\ttraining loss:\t5.688842\n",
      "Done 325 batches in 302.35s\ttraining loss:\t5.684227\n",
      "Done 330 batches in 307.42s\ttraining loss:\t5.665823\n",
      "Done 335 batches in 311.98s\ttraining loss:\t5.656209\n",
      "Done 340 batches in 316.40s\ttraining loss:\t5.644887\n",
      "Done 345 batches in 320.93s\ttraining loss:\t5.636338\n",
      "Done 350 batches in 325.45s\ttraining loss:\t5.621981\n",
      "Done 355 batches in 330.42s\ttraining loss:\t5.610653\n",
      "Done 360 batches in 335.21s\ttraining loss:\t5.592315\n",
      "Done 365 batches in 339.91s\ttraining loss:\t5.581079\n",
      "Done 370 batches in 344.20s\ttraining loss:\t5.568268\n",
      "Done 375 batches in 349.04s\ttraining loss:\t5.550565\n",
      "Done 380 batches in 353.80s\ttraining loss:\t5.535974\n",
      "Done 385 batches in 358.12s\ttraining loss:\t5.522186\n",
      "Done 390 batches in 363.11s\ttraining loss:\t5.507569\n",
      "Done 395 batches in 367.46s\ttraining loss:\t5.492879\n",
      "Done 400 batches in 372.02s\ttraining loss:\t5.477595\n",
      "Done 405 batches in 376.78s\ttraining loss:\t5.464687\n",
      "Done 410 batches in 381.31s\ttraining loss:\t5.453486\n",
      "Done 415 batches in 385.97s\ttraining loss:\t5.439460\n",
      "Done 420 batches in 390.39s\ttraining loss:\t5.427611\n",
      "Done 425 batches in 395.03s\ttraining loss:\t5.414195\n",
      "Done 430 batches in 399.28s\ttraining loss:\t5.405657\n",
      "Done 435 batches in 404.17s\ttraining loss:\t5.391122\n",
      "Done 440 batches in 408.26s\ttraining loss:\t5.376257\n",
      "Done 445 batches in 413.14s\ttraining loss:\t5.365417\n",
      "Done 450 batches in 418.12s\ttraining loss:\t5.353204\n",
      "Done 455 batches in 423.03s\ttraining loss:\t5.345509\n",
      "Done 460 batches in 427.48s\ttraining loss:\t5.339386\n",
      "Done 465 batches in 431.93s\ttraining loss:\t5.326763\n",
      "Done 470 batches in 436.54s\ttraining loss:\t5.319201\n",
      "Done 475 batches in 441.36s\ttraining loss:\t5.309004\n",
      "Done 480 batches in 445.49s\ttraining loss:\t5.298563\n",
      "Done 485 batches in 450.33s\ttraining loss:\t5.292668\n",
      "Done 490 batches in 454.91s\ttraining loss:\t5.280903\n",
      "Done 495 batches in 459.50s\ttraining loss:\t5.267416\n",
      "Done 500 batches in 464.27s\ttraining loss:\t5.257998\n",
      "Done 505 batches in 468.88s\ttraining loss:\t5.240787\n",
      "Done 510 batches in 473.64s\ttraining loss:\t5.228863\n",
      "Done 515 batches in 478.04s\ttraining loss:\t5.219816\n",
      "Done 520 batches in 482.13s\ttraining loss:\t5.210184\n",
      "Done 525 batches in 486.35s\ttraining loss:\t5.198951\n",
      "Done 530 batches in 491.04s\ttraining loss:\t5.192778\n",
      "Done 535 batches in 495.63s\ttraining loss:\t5.185940\n",
      "Done 540 batches in 500.06s\ttraining loss:\t5.180287\n",
      "Done 545 batches in 504.80s\ttraining loss:\t5.173631\n",
      "Done 550 batches in 509.21s\ttraining loss:\t5.165789\n",
      "Done 555 batches in 513.67s\ttraining loss:\t5.156674\n",
      "Done 560 batches in 518.06s\ttraining loss:\t5.149382\n",
      "Done 565 batches in 523.07s\ttraining loss:\t5.140841\n",
      "Done 570 batches in 527.47s\ttraining loss:\t5.134372\n",
      "Done 575 batches in 531.70s\ttraining loss:\t5.125638\n",
      "Done 580 batches in 536.52s\ttraining loss:\t5.117680\n",
      "Done 585 batches in 541.29s\ttraining loss:\t5.112055\n",
      "Done 590 batches in 546.50s\ttraining loss:\t5.105783\n",
      "Done 595 batches in 550.63s\ttraining loss:\t5.100126\n",
      "Done 600 batches in 555.31s\ttraining loss:\t5.091459\n",
      "Done 605 batches in 559.86s\ttraining loss:\t5.084832\n",
      "Done 610 batches in 564.47s\ttraining loss:\t5.076435\n",
      "Done 615 batches in 569.11s\ttraining loss:\t5.071099\n",
      "Done 620 batches in 573.71s\ttraining loss:\t5.066990\n",
      "Done 625 batches in 578.13s\ttraining loss:\t5.060664\n",
      "Done 630 batches in 583.11s\ttraining loss:\t5.050812\n",
      "Done 635 batches in 587.47s\ttraining loss:\t5.046882\n",
      "Done 640 batches in 591.90s\ttraining loss:\t5.040681\n",
      "Done 645 batches in 596.91s\ttraining loss:\t5.033417\n",
      "Done 650 batches in 601.24s\ttraining loss:\t5.027457\n",
      "Done 655 batches in 606.31s\ttraining loss:\t5.021831\n",
      "Done 660 batches in 611.19s\ttraining loss:\t5.014980\n",
      "Done 665 batches in 615.67s\ttraining loss:\t5.008259\n",
      "Done 670 batches in 620.39s\ttraining loss:\t5.002556\n",
      "Done 675 batches in 625.22s\ttraining loss:\t4.994609\n",
      "Done 680 batches in 629.22s\ttraining loss:\t4.987855\n",
      "Done 685 batches in 633.95s\ttraining loss:\t4.979715\n",
      "Done 690 batches in 638.65s\ttraining loss:\t4.975446\n",
      "Done 695 batches in 643.31s\ttraining loss:\t4.966470\n",
      "Done 700 batches in 648.18s\ttraining loss:\t4.958055\n",
      "Done 705 batches in 652.94s\ttraining loss:\t4.949591\n",
      "Done 710 batches in 657.58s\ttraining loss:\t4.941618\n",
      "Done 715 batches in 662.30s\ttraining loss:\t4.933886\n",
      "Done 720 batches in 667.00s\ttraining loss:\t4.925881\n",
      "Done 725 batches in 671.80s\ttraining loss:\t4.921005\n",
      "Done 730 batches in 676.07s\ttraining loss:\t4.912287\n",
      "Done 735 batches in 680.38s\ttraining loss:\t4.906534\n",
      "Done 740 batches in 684.55s\ttraining loss:\t4.901060\n",
      "Done 745 batches in 689.26s\ttraining loss:\t4.895170\n",
      "Done 750 batches in 693.92s\ttraining loss:\t4.887349\n",
      "Done 755 batches in 698.24s\ttraining loss:\t4.884204\n",
      "Done 760 batches in 702.84s\ttraining loss:\t4.877970\n",
      "Done 765 batches in 707.22s\ttraining loss:\t4.868289\n",
      "Done 770 batches in 711.81s\ttraining loss:\t4.861020\n",
      "Done 775 batches in 716.39s\ttraining loss:\t4.852290\n",
      "Done 780 batches in 720.96s\ttraining loss:\t4.846092\n",
      "Done 785 batches in 725.90s\ttraining loss:\t4.836723\n",
      "Done 790 batches in 730.89s\ttraining loss:\t4.830805\n",
      "Done 795 batches in 735.73s\ttraining loss:\t4.824032\n",
      "Done 800 batches in 741.06s\ttraining loss:\t4.819070\n",
      "Done 805 batches in 745.78s\ttraining loss:\t4.810322\n",
      "Done 810 batches in 750.33s\ttraining loss:\t4.805173\n",
      "Done 815 batches in 755.05s\ttraining loss:\t4.799043\n",
      "Done 820 batches in 759.65s\ttraining loss:\t4.791083\n",
      "Done 825 batches in 764.52s\ttraining loss:\t4.784932\n",
      "Done 830 batches in 769.01s\ttraining loss:\t4.780393\n",
      "Done 835 batches in 773.88s\ttraining loss:\t4.774569\n",
      "Done 840 batches in 778.27s\ttraining loss:\t4.768716\n",
      "Done 845 batches in 782.95s\ttraining loss:\t4.765006\n",
      "Done 850 batches in 787.83s\ttraining loss:\t4.759158\n",
      "Done 855 batches in 792.57s\ttraining loss:\t4.755342\n",
      "Done 860 batches in 797.16s\ttraining loss:\t4.748918\n",
      "Done 865 batches in 801.61s\ttraining loss:\t4.743257\n",
      "Done 870 batches in 806.62s\ttraining loss:\t4.737561\n",
      "Done 875 batches in 811.51s\ttraining loss:\t4.731237\n",
      "Done 880 batches in 816.13s\ttraining loss:\t4.724670\n",
      "Done 885 batches in 820.72s\ttraining loss:\t4.721579\n",
      "Done 890 batches in 825.69s\ttraining loss:\t4.714515\n",
      "Done 895 batches in 830.57s\ttraining loss:\t4.710307\n",
      "Done 900 batches in 835.21s\ttraining loss:\t4.706841\n",
      "Done 905 batches in 839.68s\ttraining loss:\t4.703023\n",
      "Done 910 batches in 844.45s\ttraining loss:\t4.699298\n",
      "Done 915 batches in 849.36s\ttraining loss:\t4.691869\n",
      "Done 920 batches in 853.98s\ttraining loss:\t4.687182\n",
      "Done 925 batches in 858.77s\ttraining loss:\t4.682705\n",
      "Done 930 batches in 863.35s\ttraining loss:\t4.677164\n",
      "Done 935 batches in 867.85s\ttraining loss:\t4.671567\n",
      "Done 940 batches in 872.22s\ttraining loss:\t4.666131\n",
      "Done 945 batches in 877.37s\ttraining loss:\t4.660934\n",
      "Done 950 batches in 882.06s\ttraining loss:\t4.654837\n",
      "Done 955 batches in 886.21s\ttraining loss:\t4.652435\n",
      "Done 960 batches in 891.22s\ttraining loss:\t4.646475\n",
      "Done 965 batches in 895.80s\ttraining loss:\t4.642135\n",
      "Done 970 batches in 900.73s\ttraining loss:\t4.636838\n",
      "Done 975 batches in 905.57s\ttraining loss:\t4.630764\n",
      "Done 980 batches in 910.44s\ttraining loss:\t4.625028\n",
      "Done 985 batches in 914.76s\ttraining loss:\t4.619620\n",
      "Done 990 batches in 919.26s\ttraining loss:\t4.615785\n",
      "Done 995 batches in 923.45s\ttraining loss:\t4.611466\n",
      "Done 1000 batches in 927.96s\ttraining loss:\t4.608935\n",
      "Done 1005 batches in 932.77s\ttraining loss:\t4.604206\n",
      "Done 1010 batches in 937.20s\ttraining loss:\t4.599731\n",
      "Done 1015 batches in 941.95s\ttraining loss:\t4.595397\n",
      "Done 1020 batches in 946.71s\ttraining loss:\t4.591958\n",
      "Done 1025 batches in 950.88s\ttraining loss:\t4.589908\n",
      "Done 1030 batches in 955.35s\ttraining loss:\t4.585471\n",
      "Done 1035 batches in 959.69s\ttraining loss:\t4.581476\n",
      "Done 1040 batches in 964.21s\ttraining loss:\t4.576492\n",
      "Done 1045 batches in 968.82s\ttraining loss:\t4.571126\n",
      "Done 1050 batches in 973.79s\ttraining loss:\t4.565775\n",
      "Done 1055 batches in 978.61s\ttraining loss:\t4.563524\n",
      "Done 1060 batches in 983.17s\ttraining loss:\t4.559008\n",
      "Done 1065 batches in 987.57s\ttraining loss:\t4.556051\n",
      "Done 1070 batches in 992.46s\ttraining loss:\t4.554029\n",
      "Done 1075 batches in 997.49s\ttraining loss:\t4.550084\n",
      "Done 1080 batches in 1002.18s\ttraining loss:\t4.545204\n",
      "Done 1085 batches in 1006.64s\ttraining loss:\t4.542434\n",
      "Done 1090 batches in 1011.41s\ttraining loss:\t4.539607\n",
      "Done 1095 batches in 1016.47s\ttraining loss:\t4.535171\n",
      "Done 1100 batches in 1020.69s\ttraining loss:\t4.529242\n",
      "Done 1105 batches in 1025.75s\ttraining loss:\t4.525387\n",
      "Done 1110 batches in 1030.38s\ttraining loss:\t4.522519\n",
      "Done 1115 batches in 1034.73s\ttraining loss:\t4.519374\n",
      "Done 1120 batches in 1039.13s\ttraining loss:\t4.516468\n",
      "Done 1125 batches in 1043.57s\ttraining loss:\t4.512401\n",
      "Done 1130 batches in 1048.47s\ttraining loss:\t4.508241\n",
      "Done 1135 batches in 1053.07s\ttraining loss:\t4.502665\n",
      "Done 1140 batches in 1057.68s\ttraining loss:\t4.497193\n",
      "Done 1145 batches in 1062.34s\ttraining loss:\t4.494808\n",
      "Done 1150 batches in 1066.84s\ttraining loss:\t4.489904\n",
      "Done 1155 batches in 1071.67s\ttraining loss:\t4.484863\n",
      "Done 1160 batches in 1076.91s\ttraining loss:\t4.479355\n",
      "Done 1165 batches in 1081.64s\ttraining loss:\t4.475325\n",
      "Done 1170 batches in 1086.14s\ttraining loss:\t4.472228\n",
      "Done 1175 batches in 1090.44s\ttraining loss:\t4.466123\n",
      "Done 1180 batches in 1095.39s\ttraining loss:\t4.460723\n",
      "Done 1185 batches in 1099.83s\ttraining loss:\t4.454662\n",
      "Done 1190 batches in 1104.63s\ttraining loss:\t4.451439\n",
      "Done 1195 batches in 1109.30s\ttraining loss:\t4.448495\n",
      "Done 1200 batches in 1114.21s\ttraining loss:\t4.445647\n",
      "Done 1205 batches in 1118.75s\ttraining loss:\t4.439065\n",
      "Done 1210 batches in 1123.59s\ttraining loss:\t4.437782\n",
      "Done 1215 batches in 1128.16s\ttraining loss:\t4.433624\n",
      "Done 1220 batches in 1132.86s\ttraining loss:\t4.428859\n",
      "Done 1225 batches in 1137.75s\ttraining loss:\t4.424117\n",
      "Done 1230 batches in 1142.41s\ttraining loss:\t4.419641\n",
      "Done 1235 batches in 1146.73s\ttraining loss:\t4.415517\n",
      "Done 1240 batches in 1151.45s\ttraining loss:\t4.412341\n",
      "Done 1245 batches in 1156.25s\ttraining loss:\t4.409152\n",
      "Done 1250 batches in 1161.00s\ttraining loss:\t4.406349\n",
      "Done 1255 batches in 1165.83s\ttraining loss:\t4.401902\n",
      "Done 1260 batches in 1170.67s\ttraining loss:\t4.398958\n",
      "Done 1265 batches in 1175.61s\ttraining loss:\t4.394213\n",
      "Done 1270 batches in 1180.14s\ttraining loss:\t4.390292\n",
      "Done 1275 batches in 1184.71s\ttraining loss:\t4.386492\n",
      "Done 1280 batches in 1189.49s\ttraining loss:\t4.382677\n",
      "Done 1285 batches in 1194.57s\ttraining loss:\t4.380361\n",
      "Done 1290 batches in 1198.95s\ttraining loss:\t4.377811\n",
      "Done 1295 batches in 1203.51s\ttraining loss:\t4.373415\n",
      "Done 1300 batches in 1208.37s\ttraining loss:\t4.370061\n",
      "Done 1305 batches in 1213.04s\ttraining loss:\t4.368280\n",
      "Done 1310 batches in 1217.49s\ttraining loss:\t4.365802\n",
      "Done 1315 batches in 1222.48s\ttraining loss:\t4.363410\n",
      "Done 1320 batches in 1227.18s\ttraining loss:\t4.360116\n",
      "Done 1325 batches in 1231.67s\ttraining loss:\t4.356822\n",
      "Done 1330 batches in 1235.91s\ttraining loss:\t4.351807\n",
      "Done 1335 batches in 1240.08s\ttraining loss:\t4.349159\n",
      "Done 1340 batches in 1244.25s\ttraining loss:\t4.344697\n",
      "Done 1345 batches in 1249.06s\ttraining loss:\t4.341515\n",
      "Done 1350 batches in 1253.81s\ttraining loss:\t4.338120\n",
      "Done 1355 batches in 1257.77s\ttraining loss:\t4.334705\n",
      "Done 1360 batches in 1262.02s\ttraining loss:\t4.331218\n",
      "Done 1365 batches in 1266.42s\ttraining loss:\t4.327889\n",
      "Done 1370 batches in 1271.36s\ttraining loss:\t4.324080\n",
      "Done 1375 batches in 1276.04s\ttraining loss:\t4.320623\n",
      "Done 1380 batches in 1280.87s\ttraining loss:\t4.315844\n",
      "Done 1385 batches in 1285.11s\ttraining loss:\t4.311832\n",
      "Done 1390 batches in 1289.53s\ttraining loss:\t4.308072\n",
      "Done 1395 batches in 1293.98s\ttraining loss:\t4.304441\n",
      "Done 1400 batches in 1299.10s\ttraining loss:\t4.300429\n",
      "Done 1405 batches in 1304.09s\ttraining loss:\t4.296764\n",
      "Done 1410 batches in 1308.65s\ttraining loss:\t4.292762\n",
      "Done 1415 batches in 1313.11s\ttraining loss:\t4.288914\n",
      "Done 1420 batches in 1317.53s\ttraining loss:\t4.286985\n",
      "Done 1425 batches in 1322.28s\ttraining loss:\t4.284118\n",
      "Done 1430 batches in 1327.41s\ttraining loss:\t4.281891\n",
      "Done 1435 batches in 1332.31s\ttraining loss:\t4.277417\n",
      "Done 1440 batches in 1337.28s\ttraining loss:\t4.273305\n",
      "Done 1445 batches in 1341.69s\ttraining loss:\t4.269747\n",
      "Done 1450 batches in 1345.89s\ttraining loss:\t4.266413\n",
      "Done 1455 batches in 1351.19s\ttraining loss:\t4.263099\n",
      "Done 1460 batches in 1356.24s\ttraining loss:\t4.260391\n",
      "Done 1465 batches in 1360.73s\ttraining loss:\t4.257035\n",
      "Done 1470 batches in 1365.26s\ttraining loss:\t4.254856\n",
      "Done 1475 batches in 1370.07s\ttraining loss:\t4.252207\n",
      "Done 1480 batches in 1375.13s\ttraining loss:\t4.249894\n",
      "Done 1485 batches in 1379.87s\ttraining loss:\t4.245955\n",
      "Done 1490 batches in 1384.57s\ttraining loss:\t4.242884\n",
      "Done 1495 batches in 1389.47s\ttraining loss:\t4.240870\n",
      "Done 1500 batches in 1393.72s\ttraining loss:\t4.237737\n",
      "Done 1505 batches in 1398.72s\ttraining loss:\t4.234319\n",
      "Done 1510 batches in 1403.44s\ttraining loss:\t4.231500\n",
      "Done 1515 batches in 1408.36s\ttraining loss:\t4.229336\n",
      "Done 1520 batches in 1412.73s\ttraining loss:\t4.226256\n",
      "Done 1525 batches in 1416.99s\ttraining loss:\t4.223665\n",
      "Done 1530 batches in 1421.30s\ttraining loss:\t4.219641\n",
      "Done 1535 batches in 1425.64s\ttraining loss:\t4.216929\n",
      "Done 1540 batches in 1430.06s\ttraining loss:\t4.215118\n",
      "Done 1545 batches in 1434.39s\ttraining loss:\t4.210199\n",
      "Done 1550 batches in 1438.88s\ttraining loss:\t4.207689\n",
      "Done 1555 batches in 1443.41s\ttraining loss:\t4.205916\n",
      "Done 1560 batches in 1447.99s\ttraining loss:\t4.203850\n",
      "Done 1565 batches in 1452.58s\ttraining loss:\t4.200815\n",
      "Done 1570 batches in 1457.61s\ttraining loss:\t4.197987\n",
      "Done 1575 batches in 1462.26s\ttraining loss:\t4.194270\n",
      "Done 1580 batches in 1466.65s\ttraining loss:\t4.190875\n",
      "Done 1585 batches in 1471.27s\ttraining loss:\t4.186457\n",
      "Done 1590 batches in 1476.05s\ttraining loss:\t4.184960\n",
      "Done 1595 batches in 1480.37s\ttraining loss:\t4.181488\n",
      "Done 1600 batches in 1484.94s\ttraining loss:\t4.180427\n",
      "Done 1605 batches in 1489.37s\ttraining loss:\t4.178491\n",
      "Done 1610 batches in 1493.79s\ttraining loss:\t4.176884\n",
      "Done 1615 batches in 1498.60s\ttraining loss:\t4.174063\n",
      "Done 1620 batches in 1503.10s\ttraining loss:\t4.171892\n",
      "Done 1625 batches in 1508.23s\ttraining loss:\t4.170062\n",
      "Done 1630 batches in 1512.83s\ttraining loss:\t4.168115\n",
      "Done 1635 batches in 1517.40s\ttraining loss:\t4.165094\n",
      "Done 1640 batches in 1521.96s\ttraining loss:\t4.163330\n",
      "Done 1645 batches in 1527.03s\ttraining loss:\t4.160629\n",
      "Done 1650 batches in 1531.57s\ttraining loss:\t4.158541\n",
      "Done 1655 batches in 1536.39s\ttraining loss:\t4.155918\n",
      "Done 1660 batches in 1541.00s\ttraining loss:\t4.152551\n",
      "Done 1665 batches in 1545.41s\ttraining loss:\t4.149003\n",
      "Done 1670 batches in 1550.19s\ttraining loss:\t4.146375\n",
      "Done 1675 batches in 1554.88s\ttraining loss:\t4.144111\n",
      "Done 1680 batches in 1559.92s\ttraining loss:\t4.141391\n",
      "Done 1685 batches in 1563.72s\ttraining loss:\t4.139266\n",
      "Done 1690 batches in 1568.41s\ttraining loss:\t4.136624\n",
      "Done 1695 batches in 1572.95s\ttraining loss:\t4.134436\n",
      "Done 1700 batches in 1577.45s\ttraining loss:\t4.131362\n",
      "Done 1705 batches in 1582.30s\ttraining loss:\t4.130022\n",
      "Done 1710 batches in 1587.09s\ttraining loss:\t4.128228\n",
      "Done 1715 batches in 1591.65s\ttraining loss:\t4.125764\n",
      "Done 1720 batches in 1596.20s\ttraining loss:\t4.123249\n",
      "Done 1725 batches in 1601.49s\ttraining loss:\t4.121662\n",
      "Done 1730 batches in 1605.88s\ttraining loss:\t4.118177\n",
      "Done 1735 batches in 1610.43s\ttraining loss:\t4.116367\n",
      "Done 1740 batches in 1615.35s\ttraining loss:\t4.113571\n",
      "Done 1745 batches in 1620.11s\ttraining loss:\t4.108912\n",
      "Done 1750 batches in 1624.61s\ttraining loss:\t4.107590\n",
      "Done 1755 batches in 1629.58s\ttraining loss:\t4.104998\n",
      "Done 1760 batches in 1633.89s\ttraining loss:\t4.101149\n",
      "Done 1765 batches in 1638.10s\ttraining loss:\t4.098651\n",
      "Done 1770 batches in 1643.02s\ttraining loss:\t4.095640\n",
      "Done 1775 batches in 1647.81s\ttraining loss:\t4.093680\n",
      "Done 1780 batches in 1652.73s\ttraining loss:\t4.091290\n",
      "Done 1785 batches in 1657.27s\ttraining loss:\t4.088833\n",
      "Done 1790 batches in 1661.79s\ttraining loss:\t4.085363\n",
      "Done 1795 batches in 1666.19s\ttraining loss:\t4.083690\n",
      "Done 1800 batches in 1670.48s\ttraining loss:\t4.080516\n",
      "Done 1805 batches in 1675.38s\ttraining loss:\t4.079300\n",
      "Done 1810 batches in 1680.10s\ttraining loss:\t4.077285\n",
      "Done 1815 batches in 1684.37s\ttraining loss:\t4.074368\n",
      "Done 1820 batches in 1688.92s\ttraining loss:\t4.071597\n",
      "Done 1825 batches in 1693.06s\ttraining loss:\t4.067391\n",
      "Done 1830 batches in 1697.55s\ttraining loss:\t4.064425\n",
      "Done 1835 batches in 1702.26s\ttraining loss:\t4.062604\n",
      "Done 1840 batches in 1707.09s\ttraining loss:\t4.061432\n",
      "Done 1845 batches in 1711.59s\ttraining loss:\t4.059680\n",
      "Done 1850 batches in 1716.36s\ttraining loss:\t4.057945\n",
      "Done 1855 batches in 1721.03s\ttraining loss:\t4.054737\n",
      "Done 1860 batches in 1725.39s\ttraining loss:\t4.054071\n",
      "Done 1865 batches in 1730.18s\ttraining loss:\t4.051794\n",
      "Done 1870 batches in 1734.88s\ttraining loss:\t4.050539\n",
      "Done 1875 batches in 1739.38s\ttraining loss:\t4.047623\n",
      "Done 1880 batches in 1743.61s\ttraining loss:\t4.045243\n",
      "Done 1885 batches in 1748.03s\ttraining loss:\t4.042681\n",
      "Done 1890 batches in 1752.90s\ttraining loss:\t4.040093\n",
      "Done 1895 batches in 1757.50s\ttraining loss:\t4.037794\n",
      "Done 1900 batches in 1762.27s\ttraining loss:\t4.036261\n",
      "Done 1905 batches in 1766.76s\ttraining loss:\t4.035278\n",
      "Done 1910 batches in 1771.96s\ttraining loss:\t4.032580\n",
      "Done 1915 batches in 1777.12s\ttraining loss:\t4.030610\n",
      "Done 1920 batches in 1781.75s\ttraining loss:\t4.028376\n",
      "Done 1925 batches in 1786.63s\ttraining loss:\t4.025848\n",
      "Done 1930 batches in 1791.39s\ttraining loss:\t4.023517\n",
      "Done 1935 batches in 1796.04s\ttraining loss:\t4.020883\n",
      "Done 1940 batches in 1800.92s\ttraining loss:\t4.018774\n",
      "Done 1945 batches in 1805.06s\ttraining loss:\t4.015724\n",
      "Done 1950 batches in 1809.80s\ttraining loss:\t4.012629\n",
      "Done 1955 batches in 1814.50s\ttraining loss:\t4.011057\n",
      "Done 1960 batches in 1819.34s\ttraining loss:\t4.009058\n",
      "Done 1965 batches in 1823.47s\ttraining loss:\t4.007690\n",
      "Done 1970 batches in 1827.99s\ttraining loss:\t4.005897\n",
      "Done 1975 batches in 1832.75s\ttraining loss:\t4.004096\n",
      "Done 1980 batches in 1837.34s\ttraining loss:\t4.002661\n",
      "Done 1985 batches in 1842.18s\ttraining loss:\t3.999533\n",
      "Done 1990 batches in 1846.63s\ttraining loss:\t3.998696\n",
      "Done 1995 batches in 1851.74s\ttraining loss:\t3.997651\n",
      "Done 2000 batches in 1856.76s\ttraining loss:\t3.994657\n",
      "Done 2005 batches in 1861.32s\ttraining loss:\t3.992790\n",
      "Done 2010 batches in 1865.42s\ttraining loss:\t3.990941\n",
      "Done 2015 batches in 1869.94s\ttraining loss:\t3.988824\n",
      "Done 2020 batches in 1874.53s\ttraining loss:\t3.985903\n",
      "Done 2025 batches in 1878.70s\ttraining loss:\t3.983990\n",
      "Done 2030 batches in 1883.56s\ttraining loss:\t3.981291\n",
      "Done 2035 batches in 1888.29s\ttraining loss:\t3.979179\n",
      "Done 2040 batches in 1892.85s\ttraining loss:\t3.977468\n",
      "Done 2045 batches in 1897.26s\ttraining loss:\t3.975012\n",
      "Done 2050 batches in 1901.95s\ttraining loss:\t3.973595\n",
      "Done 2055 batches in 1906.85s\ttraining loss:\t3.971562\n",
      "Done 2060 batches in 1911.77s\ttraining loss:\t3.970532\n",
      "Done 2065 batches in 1916.27s\ttraining loss:\t3.968805\n",
      "Done 2070 batches in 1920.67s\ttraining loss:\t3.967618\n",
      "Done 2075 batches in 1924.97s\ttraining loss:\t3.965471\n",
      "Done 2080 batches in 1929.52s\ttraining loss:\t3.962482\n",
      "Done 2085 batches in 1934.42s\ttraining loss:\t3.961327\n",
      "Done 2090 batches in 1938.97s\ttraining loss:\t3.959930\n",
      "Done 2095 batches in 1943.93s\ttraining loss:\t3.958682\n",
      "Done 2100 batches in 1947.82s\ttraining loss:\t3.957247\n",
      "Done 2105 batches in 1952.56s\ttraining loss:\t3.954960\n",
      "Done 2110 batches in 1957.34s\ttraining loss:\t3.952981\n",
      "Done 2115 batches in 1962.35s\ttraining loss:\t3.951210\n",
      "Done 2120 batches in 1966.70s\ttraining loss:\t3.949638\n",
      "Done 2125 batches in 1971.10s\ttraining loss:\t3.947720\n",
      "Done 2130 batches in 1975.98s\ttraining loss:\t3.945890\n",
      "Done 2135 batches in 1980.40s\ttraining loss:\t3.943546\n",
      "Done 2140 batches in 1984.94s\ttraining loss:\t3.942138\n",
      "Done 2145 batches in 1989.50s\ttraining loss:\t3.939605\n",
      "Done 2150 batches in 1994.42s\ttraining loss:\t3.937720\n",
      "Done 2155 batches in 1999.22s\ttraining loss:\t3.936361\n",
      "Done 2160 batches in 2003.89s\ttraining loss:\t3.935372\n",
      "Done 2165 batches in 2008.46s\ttraining loss:\t3.932508\n",
      "Done 2170 batches in 2013.01s\ttraining loss:\t3.931296\n",
      "Done 2175 batches in 2018.06s\ttraining loss:\t3.929926\n",
      "Done 2180 batches in 2022.94s\ttraining loss:\t3.928346\n",
      "Done 2185 batches in 2027.51s\ttraining loss:\t3.925765\n",
      "Done 2190 batches in 2031.89s\ttraining loss:\t3.924179\n",
      "Done 2195 batches in 2036.73s\ttraining loss:\t3.922073\n",
      "Done 2200 batches in 2041.53s\ttraining loss:\t3.920249\n",
      "Done 2205 batches in 2046.13s\ttraining loss:\t3.919307\n",
      "Done 2210 batches in 2051.02s\ttraining loss:\t3.917303\n",
      "Done 2215 batches in 2056.01s\ttraining loss:\t3.915247\n",
      "Done 2220 batches in 2060.43s\ttraining loss:\t3.913282\n",
      "Done 2225 batches in 2065.08s\ttraining loss:\t3.912519\n",
      "Done 2230 batches in 2069.34s\ttraining loss:\t3.911640\n",
      "Done 2235 batches in 2073.62s\ttraining loss:\t3.909708\n",
      "Done 2240 batches in 2078.70s\ttraining loss:\t3.907140\n",
      "Done 2245 batches in 2083.67s\ttraining loss:\t3.906878\n",
      "Done 2250 batches in 2088.21s\ttraining loss:\t3.905100\n",
      "Done 2255 batches in 2093.15s\ttraining loss:\t3.903170\n",
      "Done 2260 batches in 2098.33s\ttraining loss:\t3.901825\n",
      "Done 2265 batches in 2102.61s\ttraining loss:\t3.900348\n",
      "Done 2270 batches in 2107.52s\ttraining loss:\t3.898353\n",
      "Done 2275 batches in 2112.22s\ttraining loss:\t3.897088\n",
      "Done 2280 batches in 2117.09s\ttraining loss:\t3.894947\n",
      "Done 2285 batches in 2121.84s\ttraining loss:\t3.892974\n",
      "Done 2290 batches in 2126.58s\ttraining loss:\t3.891184\n",
      "Done 2295 batches in 2131.66s\ttraining loss:\t3.889232\n",
      "Done 2300 batches in 2136.75s\ttraining loss:\t3.886978\n",
      "Done 2305 batches in 2141.03s\ttraining loss:\t3.885387\n",
      "Done 2310 batches in 2145.83s\ttraining loss:\t3.884495\n",
      "Done 2315 batches in 2151.78s\ttraining loss:\t3.883506\n",
      "Done 2320 batches in 2158.43s\ttraining loss:\t3.881777\n",
      "Done 2325 batches in 2164.69s\ttraining loss:\t3.880300\n",
      "Done 2330 batches in 2171.35s\ttraining loss:\t3.878508\n",
      "Done 2335 batches in 2177.11s\ttraining loss:\t3.876066\n",
      "Done 2340 batches in 2183.34s\ttraining loss:\t3.873940\n",
      "Done 2345 batches in 2190.11s\ttraining loss:\t3.872712\n",
      "Done 2350 batches in 2196.48s\ttraining loss:\t3.871392\n",
      "Done 2355 batches in 2202.98s\ttraining loss:\t3.870282\n",
      "Done 2360 batches in 2208.62s\ttraining loss:\t3.869070\n",
      "Done 2365 batches in 2214.87s\ttraining loss:\t3.867496\n",
      "Done 2370 batches in 2220.24s\ttraining loss:\t3.864850\n",
      "Done 2375 batches in 2224.84s\ttraining loss:\t3.863728\n",
      "Done 2380 batches in 2229.79s\ttraining loss:\t3.862656\n",
      "Done 2385 batches in 2234.23s\ttraining loss:\t3.861105\n",
      "Done 2390 batches in 2238.93s\ttraining loss:\t3.859239\n",
      "Done 2395 batches in 2243.81s\ttraining loss:\t3.856918\n",
      "Done 2400 batches in 2248.59s\ttraining loss:\t3.856422\n",
      "Done 2405 batches in 2253.68s\ttraining loss:\t3.854822\n",
      "Done 2410 batches in 2258.49s\ttraining loss:\t3.853252\n",
      "Done 2415 batches in 2263.41s\ttraining loss:\t3.850859\n",
      "Done 2420 batches in 2268.21s\ttraining loss:\t3.849598\n",
      "Done 2425 batches in 2272.66s\ttraining loss:\t3.849142\n",
      "Done 2430 batches in 2277.53s\ttraining loss:\t3.847996\n",
      "Done 2435 batches in 2282.62s\ttraining loss:\t3.846221\n",
      "Done 2440 batches in 2287.32s\ttraining loss:\t3.844039\n",
      "Done 2445 batches in 2291.82s\ttraining loss:\t3.842686\n",
      "Done 2450 batches in 2296.68s\ttraining loss:\t3.840784\n",
      "Done 2455 batches in 2301.13s\ttraining loss:\t3.839916\n",
      "Done 2460 batches in 2305.69s\ttraining loss:\t3.837856\n",
      "Done 2465 batches in 2310.21s\ttraining loss:\t3.835818\n",
      "Done 2470 batches in 2314.74s\ttraining loss:\t3.835821\n",
      "Done 2475 batches in 2319.24s\ttraining loss:\t3.834453\n",
      "Done 2480 batches in 2323.59s\ttraining loss:\t3.833348\n",
      "Done 2485 batches in 2328.30s\ttraining loss:\t3.831969\n",
      "Done 2490 batches in 2332.93s\ttraining loss:\t3.830012\n",
      "Done 2495 batches in 2337.53s\ttraining loss:\t3.828129\n",
      "Done 2500 batches in 2342.11s\ttraining loss:\t3.827432\n",
      "Done 2505 batches in 2347.05s\ttraining loss:\t3.824958\n",
      "Done 2510 batches in 2351.67s\ttraining loss:\t3.823028\n",
      "Done 2515 batches in 2356.39s\ttraining loss:\t3.822450\n",
      "Done 2520 batches in 2360.75s\ttraining loss:\t3.820932\n",
      "Done 2525 batches in 2365.40s\ttraining loss:\t3.819308\n",
      "Done 2530 batches in 2370.02s\ttraining loss:\t3.818078\n",
      "Done 2535 batches in 2374.61s\ttraining loss:\t3.816769\n",
      "Done 2540 batches in 2379.63s\ttraining loss:\t3.815475\n",
      "Done 2545 batches in 2384.66s\ttraining loss:\t3.814512\n",
      "Done 2550 batches in 2389.59s\ttraining loss:\t3.812735\n",
      "Done 2555 batches in 2394.40s\ttraining loss:\t3.811260\n",
      "Done 2560 batches in 2399.35s\ttraining loss:\t3.810773\n",
      "Done 2565 batches in 2404.05s\ttraining loss:\t3.808741\n",
      "Done 2570 batches in 2408.54s\ttraining loss:\t3.806019\n",
      "Done 2575 batches in 2413.22s\ttraining loss:\t3.804798\n",
      "Done 2580 batches in 2417.12s\ttraining loss:\t3.803358\n",
      "Done 2585 batches in 2421.92s\ttraining loss:\t3.802990\n",
      "Done 2590 batches in 2426.74s\ttraining loss:\t3.802069\n",
      "Done 2595 batches in 2431.47s\ttraining loss:\t3.801000\n",
      "Done 2600 batches in 2436.39s\ttraining loss:\t3.799753\n",
      "Done 2605 batches in 2440.68s\ttraining loss:\t3.799055\n",
      "Done 2610 batches in 2445.74s\ttraining loss:\t3.797606\n",
      "Done 2615 batches in 2450.31s\ttraining loss:\t3.795912\n",
      "Done 2620 batches in 2454.81s\ttraining loss:\t3.794404\n",
      "Done 2625 batches in 2459.66s\ttraining loss:\t3.792818\n",
      "Done 2630 batches in 2463.97s\ttraining loss:\t3.791716\n",
      "Done 2635 batches in 2468.53s\ttraining loss:\t3.790401\n",
      "Done 2640 batches in 2473.20s\ttraining loss:\t3.789039\n",
      "Done 2645 batches in 2477.83s\ttraining loss:\t3.788001\n",
      "Done 2650 batches in 2482.25s\ttraining loss:\t3.787183\n",
      "Done 2655 batches in 2486.69s\ttraining loss:\t3.785585\n",
      "Done 2660 batches in 2490.71s\ttraining loss:\t3.784487\n",
      "Done 2665 batches in 2495.28s\ttraining loss:\t3.783410\n",
      "Done 2670 batches in 2499.84s\ttraining loss:\t3.782273\n",
      "Done 2675 batches in 2504.35s\ttraining loss:\t3.781063\n",
      "Done 2680 batches in 2508.75s\ttraining loss:\t3.779414\n",
      "Done 2685 batches in 2513.37s\ttraining loss:\t3.777500\n",
      "Done 2690 batches in 2518.19s\ttraining loss:\t3.776827\n",
      "Done 2695 batches in 2523.32s\ttraining loss:\t3.775260\n",
      "Done 2700 batches in 2527.93s\ttraining loss:\t3.774283\n",
      "Done 2705 batches in 2532.57s\ttraining loss:\t3.773284\n",
      "Done 2710 batches in 2537.14s\ttraining loss:\t3.771876\n",
      "Done 2715 batches in 2542.17s\ttraining loss:\t3.770344\n",
      "Done 2720 batches in 2547.17s\ttraining loss:\t3.768641\n",
      "Done 2725 batches in 2551.97s\ttraining loss:\t3.766982\n",
      "Done 2730 batches in 2556.08s\ttraining loss:\t3.765975\n",
      "Done 2735 batches in 2560.67s\ttraining loss:\t3.764354\n",
      "Done 2740 batches in 2565.13s\ttraining loss:\t3.763254\n",
      "Done 2745 batches in 2569.90s\ttraining loss:\t3.761208\n",
      "Done 2750 batches in 2574.78s\ttraining loss:\t3.759361\n",
      "Done 2755 batches in 2578.63s\ttraining loss:\t3.758239\n",
      "Done 2760 batches in 2583.54s\ttraining loss:\t3.757178\n",
      "Done 2765 batches in 2588.24s\ttraining loss:\t3.755231\n",
      "Done 2770 batches in 2592.94s\ttraining loss:\t3.754821\n",
      "Done 2775 batches in 2597.55s\ttraining loss:\t3.753250\n",
      "Done 2780 batches in 2602.05s\ttraining loss:\t3.751666\n",
      "Done 2785 batches in 2606.41s\ttraining loss:\t3.750362\n",
      "Done 2790 batches in 2611.29s\ttraining loss:\t3.749256\n",
      "Done 2795 batches in 2616.07s\ttraining loss:\t3.748038\n",
      "Done 2800 batches in 2620.27s\ttraining loss:\t3.746492\n",
      "Done 2805 batches in 2625.11s\ttraining loss:\t3.745143\n",
      "Done 2810 batches in 2629.64s\ttraining loss:\t3.743852\n",
      "Done 2815 batches in 2634.33s\ttraining loss:\t3.742382\n",
      "Done 2820 batches in 2639.16s\ttraining loss:\t3.741745\n",
      "Done 2825 batches in 2643.82s\ttraining loss:\t3.739603\n",
      "Done 2830 batches in 2648.52s\ttraining loss:\t3.738687\n",
      "Done 2835 batches in 2653.08s\ttraining loss:\t3.737647\n",
      "Done 2840 batches in 2657.93s\ttraining loss:\t3.736411\n",
      "Done 2845 batches in 2662.30s\ttraining loss:\t3.734916\n",
      "Done 2850 batches in 2666.79s\ttraining loss:\t3.733187\n",
      "Done 2855 batches in 2671.37s\ttraining loss:\t3.732266\n",
      "Done 2860 batches in 2675.91s\ttraining loss:\t3.731869\n",
      "Done 2865 batches in 2680.63s\ttraining loss:\t3.729551\n",
      "Done 2870 batches in 2684.86s\ttraining loss:\t3.728126\n",
      "Done 2875 batches in 2689.44s\ttraining loss:\t3.727235\n",
      "Done 2880 batches in 2693.98s\ttraining loss:\t3.725845\n",
      "Done 2885 batches in 2698.57s\ttraining loss:\t3.724942\n",
      "Done 2890 batches in 2703.17s\ttraining loss:\t3.723605\n",
      "Done 2895 batches in 2707.86s\ttraining loss:\t3.722138\n",
      "Done 2900 batches in 2712.45s\ttraining loss:\t3.720627\n",
      "Done 2905 batches in 2716.74s\ttraining loss:\t3.719423\n",
      "Done 2910 batches in 2720.75s\ttraining loss:\t3.718135\n",
      "Done 2915 batches in 2725.09s\ttraining loss:\t3.715903\n",
      "Done 2920 batches in 2729.19s\ttraining loss:\t3.714438\n",
      "Done 2925 batches in 2734.01s\ttraining loss:\t3.713276\n",
      "Done 2930 batches in 2738.80s\ttraining loss:\t3.712277\n",
      "Done 2935 batches in 2743.06s\ttraining loss:\t3.711037\n",
      "Done 2940 batches in 2747.59s\ttraining loss:\t3.709475\n",
      "Done 2945 batches in 2752.22s\ttraining loss:\t3.708061\n",
      "Done 2950 batches in 2756.63s\ttraining loss:\t3.707315\n",
      "Done 2955 batches in 2761.38s\ttraining loss:\t3.706715\n",
      "Done 2960 batches in 2766.40s\ttraining loss:\t3.705699\n",
      "Done 2965 batches in 2770.88s\ttraining loss:\t3.704736\n",
      "Done 2970 batches in 2774.75s\ttraining loss:\t3.704179\n",
      "Done 2975 batches in 2779.28s\ttraining loss:\t3.703590\n",
      "Done 2980 batches in 2784.01s\ttraining loss:\t3.703129\n",
      "Done 2985 batches in 2788.79s\ttraining loss:\t3.702303\n",
      "Done 2990 batches in 2793.27s\ttraining loss:\t3.701550\n",
      "Done 2995 batches in 2797.72s\ttraining loss:\t3.700819\n",
      "Done 3000 batches in 2802.59s\ttraining loss:\t3.700066\n",
      "Done 3005 batches in 2807.15s\ttraining loss:\t3.698684\n",
      "Done 3010 batches in 2812.29s\ttraining loss:\t3.697478\n",
      "Done 3015 batches in 2817.33s\ttraining loss:\t3.695531\n",
      "Done 3020 batches in 2822.25s\ttraining loss:\t3.694259\n",
      "Done 3025 batches in 2826.60s\ttraining loss:\t3.692791\n",
      "Done 3030 batches in 2831.75s\ttraining loss:\t3.690801\n",
      "Done 3035 batches in 2836.77s\ttraining loss:\t3.689768\n",
      "Done 3040 batches in 2841.70s\ttraining loss:\t3.689187\n",
      "Done 3045 batches in 2846.53s\ttraining loss:\t3.688189\n",
      "Done 3050 batches in 2851.65s\ttraining loss:\t3.686872\n",
      "Done 3055 batches in 2856.18s\ttraining loss:\t3.686243\n",
      "Done 3060 batches in 2860.86s\ttraining loss:\t3.685051\n",
      "Done 3065 batches in 2865.50s\ttraining loss:\t3.683742\n",
      "Done 3070 batches in 2870.07s\ttraining loss:\t3.682324\n",
      "Done 3075 batches in 2874.45s\ttraining loss:\t3.681111\n",
      "Done 3080 batches in 2879.25s\ttraining loss:\t3.680010\n",
      "Done 3085 batches in 2884.22s\ttraining loss:\t3.678513\n",
      "Done 3090 batches in 2889.29s\ttraining loss:\t3.677832\n",
      "Done 3095 batches in 2893.67s\ttraining loss:\t3.676777\n",
      "Done 3100 batches in 2898.08s\ttraining loss:\t3.675919\n",
      "Done 3105 batches in 2902.90s\ttraining loss:\t3.674511\n",
      "Done 3110 batches in 2907.60s\ttraining loss:\t3.673839\n",
      "Done 3115 batches in 2912.15s\ttraining loss:\t3.672802\n",
      "Done 3120 batches in 2916.97s\ttraining loss:\t3.671279\n",
      "Done 3125 batches in 2921.63s\ttraining loss:\t3.669902\n",
      "Done 3130 batches in 2926.44s\ttraining loss:\t3.669319\n",
      "Done 3135 batches in 2930.72s\ttraining loss:\t3.668537\n",
      "Done 3140 batches in 2935.20s\ttraining loss:\t3.667141\n",
      "Done 3145 batches in 2939.87s\ttraining loss:\t3.666225\n",
      "Done 3150 batches in 2944.06s\ttraining loss:\t3.665271\n",
      "Done 3155 batches in 2949.07s\ttraining loss:\t3.664401\n",
      "Done 3160 batches in 2953.71s\ttraining loss:\t3.663856\n",
      "Done 3165 batches in 2957.97s\ttraining loss:\t3.662867\n",
      "Done 3170 batches in 2962.38s\ttraining loss:\t3.662105\n",
      "Done 3175 batches in 2967.09s\ttraining loss:\t3.660966\n",
      "Done 3180 batches in 2972.19s\ttraining loss:\t3.660594\n",
      "Done 3185 batches in 2976.71s\ttraining loss:\t3.660121\n",
      "Done 3190 batches in 2981.07s\ttraining loss:\t3.659052\n",
      "Done 3195 batches in 2986.00s\ttraining loss:\t3.658070\n",
      "Done 3200 batches in 2990.62s\ttraining loss:\t3.657551\n",
      "Done 3205 batches in 2995.01s\ttraining loss:\t3.656336\n",
      "Done 3210 batches in 2999.94s\ttraining loss:\t3.655004\n",
      "Done 3215 batches in 3004.32s\ttraining loss:\t3.653737\n",
      "Done 3220 batches in 3008.76s\ttraining loss:\t3.652842\n",
      "Done 3225 batches in 3013.35s\ttraining loss:\t3.651801\n",
      "Done 3230 batches in 3018.38s\ttraining loss:\t3.651170\n",
      "Done 3235 batches in 3023.50s\ttraining loss:\t3.650230\n",
      "Done 3240 batches in 3028.18s\ttraining loss:\t3.649303\n",
      "Done 3245 batches in 3032.62s\ttraining loss:\t3.648325\n",
      "Done 3250 batches in 3037.15s\ttraining loss:\t3.647769\n",
      "Done 3255 batches in 3041.57s\ttraining loss:\t3.646497\n",
      "Done 3260 batches in 3046.32s\ttraining loss:\t3.645404\n",
      "Done 3265 batches in 3050.58s\ttraining loss:\t3.644401\n",
      "Done 3270 batches in 3055.34s\ttraining loss:\t3.643973\n",
      "Done 3275 batches in 3060.39s\ttraining loss:\t3.642888\n",
      "Done 3280 batches in 3064.87s\ttraining loss:\t3.641955\n",
      "Done 3285 batches in 3069.23s\ttraining loss:\t3.641018\n",
      "Done 3290 batches in 3074.08s\ttraining loss:\t3.640483\n",
      "Done 3295 batches in 3079.08s\ttraining loss:\t3.639528\n",
      "Done 3300 batches in 3085.30s\ttraining loss:\t3.638067\n",
      "Done 3305 batches in 3091.83s\ttraining loss:\t3.636822\n",
      "Done 3310 batches in 3099.17s\ttraining loss:\t3.635518\n",
      "Done 3315 batches in 3104.84s\ttraining loss:\t3.634727\n",
      "Done 3320 batches in 3111.30s\ttraining loss:\t3.633577\n",
      "Done 3325 batches in 3118.04s\ttraining loss:\t3.631894\n",
      "Done 3330 batches in 3123.46s\ttraining loss:\t3.630754\n",
      "Done 3335 batches in 3128.99s\ttraining loss:\t3.629160\n",
      "Done 3340 batches in 3134.20s\ttraining loss:\t3.629057\n",
      "Done 3345 batches in 3138.74s\ttraining loss:\t3.627546\n",
      "Done 3350 batches in 3143.33s\ttraining loss:\t3.626536\n",
      "Done 3355 batches in 3147.66s\ttraining loss:\t3.625185\n",
      "Done 3360 batches in 3152.21s\ttraining loss:\t3.624598\n",
      "Done 3365 batches in 3156.42s\ttraining loss:\t3.623605\n",
      "Done 3370 batches in 3161.15s\ttraining loss:\t3.622598\n",
      "Done 3375 batches in 3165.97s\ttraining loss:\t3.622158\n",
      "Done 3380 batches in 3170.56s\ttraining loss:\t3.621550\n",
      "Done 3385 batches in 3175.26s\ttraining loss:\t3.620289\n",
      "Done 3390 batches in 3179.97s\ttraining loss:\t3.619668\n",
      "Done 3395 batches in 3184.60s\ttraining loss:\t3.618442\n",
      "Done 3400 batches in 3189.74s\ttraining loss:\t3.617317\n",
      "Done 3405 batches in 3194.48s\ttraining loss:\t3.616398\n",
      "Done 3410 batches in 3198.81s\ttraining loss:\t3.615352\n",
      "Done 3415 batches in 3203.89s\ttraining loss:\t3.614423\n",
      "Done 3420 batches in 3208.28s\ttraining loss:\t3.613352\n",
      "Done 3425 batches in 3213.25s\ttraining loss:\t3.612046\n",
      "Done 3430 batches in 3218.07s\ttraining loss:\t3.611218\n",
      "Done 3435 batches in 3222.51s\ttraining loss:\t3.610258\n",
      "Done 3440 batches in 3227.32s\ttraining loss:\t3.609069\n",
      "Done 3445 batches in 3232.34s\ttraining loss:\t3.608523\n",
      "Done 3450 batches in 3236.70s\ttraining loss:\t3.607518\n",
      "Done 3455 batches in 3240.93s\ttraining loss:\t3.606690\n",
      "Done 3460 batches in 3245.94s\ttraining loss:\t3.605884\n",
      "Done 3465 batches in 3250.46s\ttraining loss:\t3.605065\n",
      "Done 3470 batches in 3255.16s\ttraining loss:\t3.604149\n",
      "Done 3475 batches in 3259.61s\ttraining loss:\t3.602811\n",
      "Done 3480 batches in 3264.31s\ttraining loss:\t3.601750\n",
      "Done 3485 batches in 3268.97s\ttraining loss:\t3.600357\n",
      "Done 3490 batches in 3273.63s\ttraining loss:\t3.599335\n",
      "Done 3495 batches in 3278.43s\ttraining loss:\t3.598181\n",
      "Done 3500 batches in 3283.45s\ttraining loss:\t3.597343\n",
      "Done 3505 batches in 3288.15s\ttraining loss:\t3.596434\n",
      "Done 3510 batches in 3292.51s\ttraining loss:\t3.595397\n",
      "Done 3515 batches in 3297.05s\ttraining loss:\t3.594291\n",
      "Done 3520 batches in 3301.81s\ttraining loss:\t3.593188\n",
      "Done 3525 batches in 3306.81s\ttraining loss:\t3.592091\n",
      "Done 3530 batches in 3311.53s\ttraining loss:\t3.590853\n",
      "Done 3535 batches in 3316.21s\ttraining loss:\t3.590452\n",
      "Done 3540 batches in 3320.71s\ttraining loss:\t3.589666\n",
      "Done 3545 batches in 3325.68s\ttraining loss:\t3.588371\n",
      "Done 3550 batches in 3330.55s\ttraining loss:\t3.586637\n",
      "Done 3555 batches in 3335.13s\ttraining loss:\t3.585585\n",
      "Done 3560 batches in 3339.71s\ttraining loss:\t3.584355\n",
      "Done 3565 batches in 3344.19s\ttraining loss:\t3.583141\n",
      "Done 3570 batches in 3349.17s\ttraining loss:\t3.582081\n",
      "Done 3575 batches in 3353.57s\ttraining loss:\t3.580789\n",
      "Done 3580 batches in 3358.08s\ttraining loss:\t3.579687\n",
      "Done 3585 batches in 3362.35s\ttraining loss:\t3.578355\n",
      "Done 3590 batches in 3366.57s\ttraining loss:\t3.577417\n",
      "Done 3595 batches in 3371.63s\ttraining loss:\t3.575872\n",
      "Done 3600 batches in 3376.49s\ttraining loss:\t3.574480\n",
      "Done 3605 batches in 3381.02s\ttraining loss:\t3.573605\n",
      "Done 3610 batches in 3386.53s\ttraining loss:\t3.572740\n",
      "Done 3615 batches in 3392.75s\ttraining loss:\t3.571203\n",
      "Done 3620 batches in 3397.31s\ttraining loss:\t3.571028\n",
      "Done 3625 batches in 3401.72s\ttraining loss:\t3.569901\n",
      "Done 3630 batches in 3406.27s\ttraining loss:\t3.569350\n",
      "Done 3635 batches in 3410.37s\ttraining loss:\t3.568465\n",
      "Done 3640 batches in 3414.76s\ttraining loss:\t3.567568\n",
      "Done 3645 batches in 3419.49s\ttraining loss:\t3.566567\n",
      "Done 3650 batches in 3423.86s\ttraining loss:\t3.565377\n",
      "Done 3655 batches in 3428.57s\ttraining loss:\t3.564437\n",
      "Done 3660 batches in 3432.98s\ttraining loss:\t3.563743\n",
      "Done 3665 batches in 3438.01s\ttraining loss:\t3.563042\n",
      "Done 3670 batches in 3442.79s\ttraining loss:\t3.561576\n",
      "Done 3675 batches in 3447.54s\ttraining loss:\t3.560272\n",
      "Done 3680 batches in 3452.61s\ttraining loss:\t3.558524\n",
      "Done 3685 batches in 3456.90s\ttraining loss:\t3.558384\n",
      "Done 3690 batches in 3461.51s\ttraining loss:\t3.558179\n",
      "Done 3695 batches in 3466.55s\ttraining loss:\t3.556962\n",
      "Done 3700 batches in 3471.41s\ttraining loss:\t3.555955\n",
      "Done 3705 batches in 3476.30s\ttraining loss:\t3.554999\n",
      "Done 3710 batches in 3480.33s\ttraining loss:\t3.553672\n",
      "Done 3715 batches in 3484.57s\ttraining loss:\t3.552142\n",
      "Done 3720 batches in 3489.23s\ttraining loss:\t3.551279\n",
      "Done 3725 batches in 3494.24s\ttraining loss:\t3.551351\n",
      "Done 3730 batches in 3498.82s\ttraining loss:\t3.549939\n",
      "Done 3735 batches in 3505.19s\ttraining loss:\t3.549158\n",
      "Done 3740 batches in 3510.56s\ttraining loss:\t3.548410\n",
      "Done 3745 batches in 3516.50s\ttraining loss:\t3.546826\n",
      "Done 3750 batches in 3521.70s\ttraining loss:\t3.546078\n",
      "Done 3755 batches in 3526.11s\ttraining loss:\t3.545023\n",
      "Done 3760 batches in 3530.93s\ttraining loss:\t3.544755\n",
      "Done 3765 batches in 3535.66s\ttraining loss:\t3.543526\n",
      "Done 3770 batches in 3540.33s\ttraining loss:\t3.542373\n",
      "Done 3775 batches in 3545.21s\ttraining loss:\t3.541210\n",
      "Done 3780 batches in 3549.89s\ttraining loss:\t3.540898\n",
      "Done 3785 batches in 3554.42s\ttraining loss:\t3.540295\n",
      "Done 3790 batches in 3559.19s\ttraining loss:\t3.539208\n",
      "Done 3795 batches in 3563.75s\ttraining loss:\t3.538412\n",
      "Done 3800 batches in 3568.55s\ttraining loss:\t3.537325\n",
      "Done 3805 batches in 3573.21s\ttraining loss:\t3.536006\n",
      "Done 3810 batches in 3577.72s\ttraining loss:\t3.534463\n",
      "Done 3815 batches in 3582.04s\ttraining loss:\t3.534196\n",
      "Done 3820 batches in 3586.34s\ttraining loss:\t3.533418\n",
      "Done 3825 batches in 3590.97s\ttraining loss:\t3.532935\n",
      "Done 3830 batches in 3595.87s\ttraining loss:\t3.531419\n",
      "Done 3835 batches in 3600.96s\ttraining loss:\t3.531330\n",
      "Done 3840 batches in 3605.28s\ttraining loss:\t3.531369\n",
      "Done 3845 batches in 3609.67s\ttraining loss:\t3.530127\n",
      "Done 3850 batches in 3614.06s\ttraining loss:\t3.529079\n",
      "Done 3855 batches in 3618.53s\ttraining loss:\t3.528418\n",
      "Done 3860 batches in 3623.15s\ttraining loss:\t3.527249\n",
      "Done 3865 batches in 3627.50s\ttraining loss:\t3.526271\n",
      "Done 3870 batches in 3632.27s\ttraining loss:\t3.525521\n",
      "Done 3875 batches in 3636.84s\ttraining loss:\t3.524644\n",
      "Done 3880 batches in 3641.58s\ttraining loss:\t3.523749\n",
      "Done 3885 batches in 3646.59s\ttraining loss:\t3.523542\n",
      "Done 3890 batches in 3650.95s\ttraining loss:\t3.522253\n",
      "Done 3895 batches in 3655.36s\ttraining loss:\t3.521159\n",
      "Done 3900 batches in 3659.88s\ttraining loss:\t3.520271\n",
      "Done 3905 batches in 3664.52s\ttraining loss:\t3.519808\n",
      "Done 3910 batches in 3669.12s\ttraining loss:\t3.518273\n",
      "Done 3915 batches in 3673.28s\ttraining loss:\t3.517655\n",
      "Done 3920 batches in 3678.45s\ttraining loss:\t3.516933\n",
      "Done 3925 batches in 3684.74s\ttraining loss:\t3.516655\n",
      "Done 3930 batches in 3691.43s\ttraining loss:\t3.515689\n",
      "Done 3935 batches in 3698.51s\ttraining loss:\t3.514833\n",
      "Done 3940 batches in 3705.08s\ttraining loss:\t3.513982\n",
      "Done 3945 batches in 3711.83s\ttraining loss:\t3.512977\n",
      "Done 3950 batches in 3718.50s\ttraining loss:\t3.511841\n",
      "Done 3955 batches in 3725.31s\ttraining loss:\t3.511078\n",
      "Done 3960 batches in 3731.98s\ttraining loss:\t3.510059\n",
      "Done 3965 batches in 3738.49s\ttraining loss:\t3.509035\n",
      "Done 3970 batches in 3744.74s\ttraining loss:\t3.508252\n",
      "Done 3975 batches in 3750.95s\ttraining loss:\t3.507375\n",
      "Done 3980 batches in 3756.86s\ttraining loss:\t3.506320\n",
      "Done 3985 batches in 3763.83s\ttraining loss:\t3.504891\n",
      "Done 3990 batches in 3770.49s\ttraining loss:\t3.503925\n",
      "Done 3995 batches in 3776.74s\ttraining loss:\t3.502823\n",
      "Done 4000 batches in 3782.92s\ttraining loss:\t3.502061\n",
      "Done 4005 batches in 3790.45s\ttraining loss:\t3.501687\n",
      "Done 4010 batches in 3797.22s\ttraining loss:\t3.500531\n",
      "Done 4015 batches in 3803.37s\ttraining loss:\t3.500340\n",
      "Done 4020 batches in 3810.21s\ttraining loss:\t3.499153\n",
      "Done 4025 batches in 3816.35s\ttraining loss:\t3.498434\n",
      "Done 4030 batches in 3823.26s\ttraining loss:\t3.497422\n",
      "Done 4035 batches in 3829.72s\ttraining loss:\t3.496585\n",
      "Done 4040 batches in 3835.97s\ttraining loss:\t3.495703\n",
      "Done 4045 batches in 3842.27s\ttraining loss:\t3.494680\n",
      "Done 4050 batches in 3849.54s\ttraining loss:\t3.494028\n",
      "Done 4055 batches in 3855.04s\ttraining loss:\t3.492653\n",
      "Done 4060 batches in 3862.01s\ttraining loss:\t3.491548\n",
      "Done 4065 batches in 3867.95s\ttraining loss:\t3.490931\n",
      "Done 4070 batches in 3874.34s\ttraining loss:\t3.489940\n",
      "Done 4075 batches in 3880.28s\ttraining loss:\t3.488791\n",
      "Done 4080 batches in 3886.90s\ttraining loss:\t3.487595\n",
      "Done 4085 batches in 3893.57s\ttraining loss:\t3.486176\n",
      "Done 4090 batches in 3900.29s\ttraining loss:\t3.485419\n",
      "Done 4095 batches in 3907.42s\ttraining loss:\t3.484712\n",
      "Done 4100 batches in 3913.57s\ttraining loss:\t3.483687\n",
      "Done 4105 batches in 3920.40s\ttraining loss:\t3.482533\n",
      "Done 4110 batches in 3926.20s\ttraining loss:\t3.482116\n",
      "Done 4115 batches in 3932.38s\ttraining loss:\t3.481572\n",
      "Done 4120 batches in 3937.69s\ttraining loss:\t3.481020\n",
      "Done 4125 batches in 3942.12s\ttraining loss:\t3.480421\n",
      "Done 4130 batches in 3947.20s\ttraining loss:\t3.479729\n",
      "Done 4135 batches in 3951.86s\ttraining loss:\t3.479153\n",
      "Done 4140 batches in 3956.58s\ttraining loss:\t3.479029\n",
      "Done 4145 batches in 3960.90s\ttraining loss:\t3.478345\n",
      "Done 4150 batches in 3965.12s\ttraining loss:\t3.477620\n",
      "Done 4155 batches in 3969.33s\ttraining loss:\t3.476918\n",
      "Done 4160 batches in 3974.03s\ttraining loss:\t3.475830\n",
      "Done 4165 batches in 3978.77s\ttraining loss:\t3.474600\n",
      "Done 4170 batches in 3983.61s\ttraining loss:\t3.473719\n",
      "Done 4175 batches in 3988.26s\ttraining loss:\t3.473083\n",
      "Done 4180 batches in 3992.75s\ttraining loss:\t3.472739\n",
      "Done 4185 batches in 3997.50s\ttraining loss:\t3.470993\n",
      "Done 4190 batches in 4002.48s\ttraining loss:\t3.469867\n",
      "Done 4195 batches in 4007.24s\ttraining loss:\t3.469197\n",
      "Done 4200 batches in 4012.09s\ttraining loss:\t3.468476\n",
      "Done 4205 batches in 4016.71s\ttraining loss:\t3.467666\n",
      "Done 4210 batches in 4021.12s\ttraining loss:\t3.466801\n",
      "Done 4215 batches in 4026.14s\ttraining loss:\t3.465905\n",
      "Done 4220 batches in 4031.19s\ttraining loss:\t3.465390\n",
      "Done 4225 batches in 4035.91s\ttraining loss:\t3.464873\n",
      "Done 4230 batches in 4040.58s\ttraining loss:\t3.464214\n",
      "Done 4235 batches in 4044.93s\ttraining loss:\t3.463434\n",
      "Done 4240 batches in 4049.93s\ttraining loss:\t3.462510\n",
      "Done 4245 batches in 4054.75s\ttraining loss:\t3.461237\n",
      "Done 4250 batches in 4059.97s\ttraining loss:\t3.460644\n",
      "Done 4255 batches in 4064.81s\ttraining loss:\t3.459938\n",
      "Done 4260 batches in 4069.40s\ttraining loss:\t3.459603\n",
      "Done 4265 batches in 4073.67s\ttraining loss:\t3.458781\n",
      "Done 4270 batches in 4078.08s\ttraining loss:\t3.457939\n",
      "Done 4275 batches in 4082.92s\ttraining loss:\t3.457199\n",
      "Done 4280 batches in 4087.06s\ttraining loss:\t3.456140\n",
      "Done 4285 batches in 4091.51s\ttraining loss:\t3.455415\n",
      "Done 4290 batches in 4096.09s\ttraining loss:\t3.454771\n",
      "Done 4295 batches in 4100.28s\ttraining loss:\t3.453831\n",
      "Done 4300 batches in 4104.91s\ttraining loss:\t3.452780\n",
      "Done 4305 batches in 4109.80s\ttraining loss:\t3.451603\n",
      "Done 4310 batches in 4114.38s\ttraining loss:\t3.450423\n",
      "Done 4315 batches in 4119.03s\ttraining loss:\t3.449849\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.4495795286423867"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1ep train set na słowniku z glove\n",
    "\n",
    "# dane są przycięte do długości 300 (jeśli odpowiedź się nie mieści, to pytanie jest usuwane z danych)\n",
    "# przycięto około 1400 próbek, usunięto 119\n",
    "\n",
    "# zanurzenia z glove, unk to średnie słowo, nie trenujemy zanurzeń\n",
    "\n",
    "qa_net.train_one_epoch(data, 20, log_interval=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qa_net.save_params('trained_models/glove_unks/simplified_glove_unks_all_fixed2_1ep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 5 batches in 5.10s\ttraining loss:\t8.929138\n",
      "Done 10 batches in 9.82s\ttraining loss:\t8.647834\n",
      "Done 15 batches in 14.73s\ttraining loss:\t8.301548\n",
      "Done 20 batches in 19.32s\ttraining loss:\t8.065414\n",
      "Done 25 batches in 23.68s\ttraining loss:\t7.757741\n",
      "Done 30 batches in 28.15s\ttraining loss:\t7.532715\n",
      "Done 35 batches in 32.81s\ttraining loss:\t7.467324\n",
      "Done 40 batches in 37.12s\ttraining loss:\t7.296044\n",
      "Done 45 batches in 42.06s\ttraining loss:\t7.128645\n",
      "Done 50 batches in 46.62s\ttraining loss:\t7.039674\n",
      "Done 55 batches in 50.88s\ttraining loss:\t6.948180\n",
      "Done 60 batches in 55.94s\ttraining loss:\t6.881593\n",
      "Done 65 batches in 60.44s\ttraining loss:\t6.819214\n",
      "Done 70 batches in 65.20s\ttraining loss:\t6.788998\n",
      "Done 75 batches in 69.92s\ttraining loss:\t6.759360\n",
      "Done 80 batches in 74.74s\ttraining loss:\t6.708144\n",
      "Done 85 batches in 79.59s\ttraining loss:\t6.646897\n",
      "Done 90 batches in 84.35s\ttraining loss:\t6.621784\n",
      "Done 95 batches in 88.97s\ttraining loss:\t6.583456\n",
      "Done 100 batches in 93.74s\ttraining loss:\t6.558462\n",
      "Done 105 batches in 98.13s\ttraining loss:\t6.509333\n",
      "Done 110 batches in 102.66s\ttraining loss:\t6.447264\n",
      "Done 115 batches in 107.58s\ttraining loss:\t6.425469\n",
      "Done 120 batches in 112.39s\ttraining loss:\t6.396333\n",
      "Done 125 batches in 116.53s\ttraining loss:\t6.364375\n",
      "Done 130 batches in 121.41s\ttraining loss:\t6.328272\n",
      "Done 135 batches in 125.80s\ttraining loss:\t6.314107\n",
      "Done 140 batches in 130.26s\ttraining loss:\t6.282779\n",
      "Done 145 batches in 135.05s\ttraining loss:\t6.256335\n",
      "Done 150 batches in 139.74s\ttraining loss:\t6.238062\n",
      "Done 155 batches in 144.39s\ttraining loss:\t6.205370\n",
      "Done 160 batches in 149.20s\ttraining loss:\t6.190608\n",
      "Done 165 batches in 153.45s\ttraining loss:\t6.167520\n",
      "Done 170 batches in 158.16s\ttraining loss:\t6.146890\n",
      "Done 175 batches in 162.93s\ttraining loss:\t6.135056\n",
      "Done 180 batches in 167.72s\ttraining loss:\t6.106925\n",
      "Done 185 batches in 172.28s\ttraining loss:\t6.097112\n",
      "Done 190 batches in 176.80s\ttraining loss:\t6.075929\n",
      "Done 195 batches in 181.44s\ttraining loss:\t6.045616\n",
      "Done 200 batches in 186.44s\ttraining loss:\t6.015664\n",
      "Done 205 batches in 190.79s\ttraining loss:\t5.991802\n",
      "Done 210 batches in 195.61s\ttraining loss:\t5.961786\n",
      "Done 215 batches in 200.31s\ttraining loss:\t5.951291\n",
      "Done 220 batches in 205.65s\ttraining loss:\t5.929719\n",
      "Done 225 batches in 210.60s\ttraining loss:\t5.904634\n",
      "Done 230 batches in 215.30s\ttraining loss:\t5.879464\n",
      "Done 235 batches in 220.23s\ttraining loss:\t5.862832\n",
      "Done 240 batches in 225.16s\ttraining loss:\t5.847148\n",
      "Done 245 batches in 229.36s\ttraining loss:\t5.830102\n",
      "Done 250 batches in 233.75s\ttraining loss:\t5.814518\n",
      "Done 255 batches in 238.13s\ttraining loss:\t5.798353\n",
      "Done 260 batches in 242.85s\ttraining loss:\t5.778518\n",
      "Done 265 batches in 247.33s\ttraining loss:\t5.758732\n",
      "Done 270 batches in 251.85s\ttraining loss:\t5.740414\n",
      "Done 275 batches in 256.70s\ttraining loss:\t5.721760\n",
      "Done 280 batches in 261.15s\ttraining loss:\t5.709866\n",
      "Done 285 batches in 266.12s\ttraining loss:\t5.697576\n",
      "Done 290 batches in 270.75s\ttraining loss:\t5.680449\n",
      "Done 295 batches in 275.24s\ttraining loss:\t5.666458\n",
      "Done 300 batches in 279.18s\ttraining loss:\t5.646447\n",
      "Done 305 batches in 284.06s\ttraining loss:\t5.635891\n",
      "Done 310 batches in 288.80s\ttraining loss:\t5.623032\n",
      "Done 315 batches in 293.69s\ttraining loss:\t5.610134\n",
      "Done 320 batches in 298.11s\ttraining loss:\t5.603663\n",
      "Done 325 batches in 302.84s\ttraining loss:\t5.594819\n",
      "Done 330 batches in 308.01s\ttraining loss:\t5.580864\n",
      "Done 335 batches in 313.30s\ttraining loss:\t5.563640\n",
      "Done 340 batches in 317.79s\ttraining loss:\t5.556414\n",
      "Done 345 batches in 322.92s\ttraining loss:\t5.538998\n",
      "Done 350 batches in 327.95s\ttraining loss:\t5.525605\n",
      "Done 355 batches in 333.03s\ttraining loss:\t5.516600\n",
      "Done 360 batches in 337.49s\ttraining loss:\t5.504855\n",
      "Done 365 batches in 341.96s\ttraining loss:\t5.493972\n",
      "Done 370 batches in 346.93s\ttraining loss:\t5.481198\n",
      "Done 375 batches in 351.32s\ttraining loss:\t5.466583\n",
      "Done 380 batches in 356.19s\ttraining loss:\t5.456256\n",
      "Done 385 batches in 360.97s\ttraining loss:\t5.443904\n",
      "Done 390 batches in 366.04s\ttraining loss:\t5.430340\n",
      "Done 395 batches in 370.18s\ttraining loss:\t5.418297\n",
      "Done 400 batches in 374.37s\ttraining loss:\t5.407849\n",
      "Done 405 batches in 378.84s\ttraining loss:\t5.397911\n",
      "Done 410 batches in 383.23s\ttraining loss:\t5.387523\n",
      "Done 415 batches in 387.74s\ttraining loss:\t5.374789\n",
      "Done 420 batches in 392.37s\ttraining loss:\t5.367569\n",
      "Done 425 batches in 396.64s\ttraining loss:\t5.354836\n",
      "Done 430 batches in 401.16s\ttraining loss:\t5.340704\n",
      "Done 435 batches in 405.93s\ttraining loss:\t5.327846\n",
      "Done 440 batches in 410.52s\ttraining loss:\t5.321125\n",
      "Done 445 batches in 414.79s\ttraining loss:\t5.307707\n",
      "Done 450 batches in 419.52s\ttraining loss:\t5.293340\n",
      "Done 455 batches in 423.77s\ttraining loss:\t5.285637\n",
      "Done 460 batches in 428.30s\ttraining loss:\t5.273475\n",
      "Done 465 batches in 432.51s\ttraining loss:\t5.265893\n",
      "Done 470 batches in 437.25s\ttraining loss:\t5.252285\n",
      "Done 475 batches in 441.99s\ttraining loss:\t5.241627\n",
      "Done 480 batches in 446.58s\ttraining loss:\t5.229525\n",
      "Done 485 batches in 451.13s\ttraining loss:\t5.218821\n",
      "Done 490 batches in 456.14s\ttraining loss:\t5.207426\n",
      "Done 495 batches in 460.82s\ttraining loss:\t5.197896\n",
      "Done 500 batches in 465.29s\ttraining loss:\t5.188789\n",
      "Done 505 batches in 470.44s\ttraining loss:\t5.180447\n",
      "Done 510 batches in 475.03s\ttraining loss:\t5.169848\n",
      "Done 515 batches in 479.70s\ttraining loss:\t5.167332\n",
      "Done 520 batches in 484.14s\ttraining loss:\t5.156645\n",
      "Done 525 batches in 488.79s\ttraining loss:\t5.149876\n",
      "Done 530 batches in 493.27s\ttraining loss:\t5.140988\n",
      "Done 535 batches in 497.70s\ttraining loss:\t5.124530\n",
      "Done 540 batches in 502.40s\ttraining loss:\t5.114834\n",
      "Done 545 batches in 507.30s\ttraining loss:\t5.102662\n",
      "Done 550 batches in 511.71s\ttraining loss:\t5.098416\n",
      "Done 555 batches in 515.94s\ttraining loss:\t5.093097\n",
      "Done 560 batches in 520.43s\ttraining loss:\t5.090552\n",
      "Done 565 batches in 525.43s\ttraining loss:\t5.082896\n",
      "Done 570 batches in 529.80s\ttraining loss:\t5.075965\n",
      "Done 575 batches in 534.46s\ttraining loss:\t5.067342\n",
      "Done 580 batches in 539.09s\ttraining loss:\t5.058232\n",
      "Done 585 batches in 544.29s\ttraining loss:\t5.047117\n",
      "Done 590 batches in 549.23s\ttraining loss:\t5.042551\n",
      "Done 595 batches in 553.69s\ttraining loss:\t5.036321\n",
      "Done 600 batches in 558.34s\ttraining loss:\t5.029953\n",
      "Done 605 batches in 562.48s\ttraining loss:\t5.019602\n",
      "Done 610 batches in 566.80s\ttraining loss:\t5.012374\n",
      "Done 615 batches in 571.35s\ttraining loss:\t5.004480\n",
      "Done 620 batches in 576.38s\ttraining loss:\t4.995202\n",
      "Done 625 batches in 581.26s\ttraining loss:\t4.990080\n",
      "Done 630 batches in 585.82s\ttraining loss:\t4.982889\n",
      "Done 635 batches in 590.11s\ttraining loss:\t4.972696\n",
      "Done 640 batches in 594.47s\ttraining loss:\t4.963477\n",
      "Done 645 batches in 598.66s\ttraining loss:\t4.953635\n",
      "Done 650 batches in 603.42s\ttraining loss:\t4.945359\n",
      "Done 655 batches in 608.02s\ttraining loss:\t4.936204\n",
      "Done 660 batches in 612.74s\ttraining loss:\t4.929897\n",
      "Done 665 batches in 616.93s\ttraining loss:\t4.918351\n",
      "Done 670 batches in 621.41s\ttraining loss:\t4.913602\n",
      "Done 675 batches in 626.29s\ttraining loss:\t4.906517\n",
      "Done 680 batches in 631.10s\ttraining loss:\t4.898644\n",
      "Done 685 batches in 635.98s\ttraining loss:\t4.893716\n",
      "Done 690 batches in 640.89s\ttraining loss:\t4.890166\n",
      "Done 695 batches in 645.61s\ttraining loss:\t4.889637\n",
      "Done 700 batches in 650.43s\ttraining loss:\t4.884291\n",
      "Done 705 batches in 654.85s\ttraining loss:\t4.876046\n",
      "Done 710 batches in 659.88s\ttraining loss:\t4.866531\n",
      "Done 715 batches in 664.42s\ttraining loss:\t4.857917\n",
      "Done 720 batches in 668.74s\ttraining loss:\t4.850683\n",
      "Done 725 batches in 673.13s\ttraining loss:\t4.845067\n",
      "Done 730 batches in 678.20s\ttraining loss:\t4.839695\n",
      "Done 735 batches in 682.99s\ttraining loss:\t4.832538\n",
      "Done 740 batches in 687.74s\ttraining loss:\t4.828230\n",
      "Done 745 batches in 692.74s\ttraining loss:\t4.819228\n",
      "Done 750 batches in 697.68s\ttraining loss:\t4.816158\n",
      "Done 755 batches in 702.37s\ttraining loss:\t4.811931\n",
      "Done 760 batches in 707.09s\ttraining loss:\t4.806310\n",
      "Done 765 batches in 711.50s\ttraining loss:\t4.799104\n",
      "Done 770 batches in 715.87s\ttraining loss:\t4.793952\n",
      "Done 775 batches in 720.52s\ttraining loss:\t4.789366\n",
      "Done 780 batches in 724.90s\ttraining loss:\t4.780603\n",
      "Done 785 batches in 729.68s\ttraining loss:\t4.774186\n",
      "Done 790 batches in 734.28s\ttraining loss:\t4.766016\n",
      "Done 795 batches in 738.61s\ttraining loss:\t4.762229\n",
      "Done 800 batches in 743.32s\ttraining loss:\t4.755297\n",
      "Done 805 batches in 747.32s\ttraining loss:\t4.750632\n",
      "Done 810 batches in 751.68s\ttraining loss:\t4.743231\n",
      "Done 815 batches in 756.26s\ttraining loss:\t4.738627\n",
      "Done 820 batches in 761.15s\ttraining loss:\t4.735995\n",
      "Done 825 batches in 765.98s\ttraining loss:\t4.730688\n",
      "Done 830 batches in 770.58s\ttraining loss:\t4.724809\n",
      "Done 835 batches in 775.43s\ttraining loss:\t4.718472\n",
      "Done 840 batches in 780.33s\ttraining loss:\t4.713679\n",
      "Done 845 batches in 784.73s\ttraining loss:\t4.710771\n",
      "Done 850 batches in 788.97s\ttraining loss:\t4.704042\n",
      "Done 855 batches in 793.79s\ttraining loss:\t4.697812\n",
      "Done 860 batches in 798.46s\ttraining loss:\t4.693662\n",
      "Done 865 batches in 803.44s\ttraining loss:\t4.687482\n",
      "Done 870 batches in 808.13s\ttraining loss:\t4.680767\n",
      "Done 875 batches in 812.59s\ttraining loss:\t4.676138\n",
      "Done 880 batches in 817.15s\ttraining loss:\t4.670123\n",
      "Done 885 batches in 821.93s\ttraining loss:\t4.665356\n",
      "Done 890 batches in 826.57s\ttraining loss:\t4.662455\n",
      "Done 895 batches in 831.54s\ttraining loss:\t4.656068\n",
      "Done 900 batches in 836.20s\ttraining loss:\t4.653276\n",
      "Done 905 batches in 840.77s\ttraining loss:\t4.650680\n",
      "Done 910 batches in 845.51s\ttraining loss:\t4.644352\n",
      "Done 915 batches in 850.07s\ttraining loss:\t4.641033\n",
      "Done 920 batches in 854.71s\ttraining loss:\t4.638419\n",
      "Done 925 batches in 859.48s\ttraining loss:\t4.632566\n",
      "Done 930 batches in 864.09s\ttraining loss:\t4.625421\n",
      "Done 935 batches in 868.51s\ttraining loss:\t4.622218\n",
      "Done 940 batches in 873.23s\ttraining loss:\t4.618796\n",
      "Done 945 batches in 877.78s\ttraining loss:\t4.613791\n",
      "Done 950 batches in 882.29s\ttraining loss:\t4.610362\n",
      "Done 955 batches in 886.80s\ttraining loss:\t4.604761\n",
      "Done 960 batches in 891.36s\ttraining loss:\t4.598856\n",
      "Done 965 batches in 896.11s\ttraining loss:\t4.593264\n",
      "Done 970 batches in 901.41s\ttraining loss:\t4.587273\n",
      "Done 975 batches in 906.39s\ttraining loss:\t4.584555\n",
      "Done 980 batches in 910.65s\ttraining loss:\t4.578161\n",
      "Done 985 batches in 915.58s\ttraining loss:\t4.572119\n",
      "Done 990 batches in 920.03s\ttraining loss:\t4.570687\n",
      "Done 995 batches in 924.46s\ttraining loss:\t4.567795\n",
      "Done 1000 batches in 928.85s\ttraining loss:\t4.563045\n",
      "Done 1005 batches in 933.74s\ttraining loss:\t4.559405\n",
      "Done 1010 batches in 937.90s\ttraining loss:\t4.551032\n",
      "Done 1015 batches in 942.63s\ttraining loss:\t4.546020\n",
      "Done 1020 batches in 947.53s\ttraining loss:\t4.542843\n",
      "Done 1025 batches in 952.53s\ttraining loss:\t4.538626\n",
      "Done 1030 batches in 956.97s\ttraining loss:\t4.533196\n",
      "Done 1035 batches in 961.49s\ttraining loss:\t4.528786\n",
      "Done 1040 batches in 966.14s\ttraining loss:\t4.524403\n",
      "Done 1045 batches in 970.71s\ttraining loss:\t4.520639\n",
      "Done 1050 batches in 975.23s\ttraining loss:\t4.517999\n",
      "Done 1055 batches in 980.11s\ttraining loss:\t4.511817\n",
      "Done 1060 batches in 984.63s\ttraining loss:\t4.508179\n",
      "Done 1065 batches in 989.05s\ttraining loss:\t4.502364\n",
      "Done 1070 batches in 993.61s\ttraining loss:\t4.496612\n",
      "Done 1075 batches in 998.18s\ttraining loss:\t4.492555\n",
      "Done 1080 batches in 1002.88s\ttraining loss:\t4.486466\n",
      "Done 1085 batches in 1006.87s\ttraining loss:\t4.482414\n",
      "Done 1090 batches in 1011.55s\ttraining loss:\t4.480635\n",
      "Done 1095 batches in 1016.02s\ttraining loss:\t4.476911\n",
      "Done 1100 batches in 1021.05s\ttraining loss:\t4.472196\n",
      "Done 1105 batches in 1025.84s\ttraining loss:\t4.467037\n",
      "Done 1110 batches in 1030.41s\ttraining loss:\t4.463610\n",
      "Done 1115 batches in 1035.12s\ttraining loss:\t4.458646\n",
      "Done 1120 batches in 1039.49s\ttraining loss:\t4.455843\n",
      "Done 1125 batches in 1044.18s\ttraining loss:\t4.450539\n",
      "Done 1130 batches in 1049.28s\ttraining loss:\t4.448504\n",
      "Done 1135 batches in 1053.59s\ttraining loss:\t4.444537\n",
      "Done 1140 batches in 1057.44s\ttraining loss:\t4.441582\n",
      "Done 1145 batches in 1062.56s\ttraining loss:\t4.437090\n",
      "Done 1150 batches in 1067.38s\ttraining loss:\t4.432831\n",
      "Done 1155 batches in 1072.02s\ttraining loss:\t4.429942\n",
      "Done 1160 batches in 1076.43s\ttraining loss:\t4.423878\n",
      "Done 1165 batches in 1081.10s\ttraining loss:\t4.418929\n",
      "Done 1170 batches in 1085.85s\ttraining loss:\t4.414416\n",
      "Done 1175 batches in 1090.72s\ttraining loss:\t4.412198\n",
      "Done 1180 batches in 1094.95s\ttraining loss:\t4.408726\n",
      "Done 1185 batches in 1100.12s\ttraining loss:\t4.403964\n",
      "Done 1190 batches in 1104.41s\ttraining loss:\t4.399044\n",
      "Done 1195 batches in 1109.32s\ttraining loss:\t4.394134\n",
      "Done 1200 batches in 1113.83s\ttraining loss:\t4.389951\n",
      "Done 1205 batches in 1118.30s\ttraining loss:\t4.385858\n",
      "Done 1210 batches in 1122.74s\ttraining loss:\t4.383319\n",
      "Done 1215 batches in 1127.51s\ttraining loss:\t4.379726\n",
      "Done 1220 batches in 1132.24s\ttraining loss:\t4.375047\n",
      "Done 1225 batches in 1137.06s\ttraining loss:\t4.369199\n",
      "Done 1230 batches in 1141.87s\ttraining loss:\t4.366074\n",
      "Done 1235 batches in 1146.27s\ttraining loss:\t4.362784\n",
      "Done 1240 batches in 1151.14s\ttraining loss:\t4.361097\n",
      "Done 1245 batches in 1155.94s\ttraining loss:\t4.357004\n",
      "Done 1250 batches in 1160.65s\ttraining loss:\t4.352727\n",
      "Done 1255 batches in 1165.17s\ttraining loss:\t4.349288\n",
      "Done 1260 batches in 1169.12s\ttraining loss:\t4.344732\n",
      "Done 1265 batches in 1173.66s\ttraining loss:\t4.339909\n",
      "Done 1270 batches in 1177.76s\ttraining loss:\t4.335194\n",
      "Done 1275 batches in 1182.31s\ttraining loss:\t4.330736\n",
      "Done 1280 batches in 1187.03s\ttraining loss:\t4.326370\n",
      "Done 1285 batches in 1191.82s\ttraining loss:\t4.323620\n",
      "Done 1290 batches in 1196.62s\ttraining loss:\t4.320241\n",
      "Done 1295 batches in 1201.54s\ttraining loss:\t4.316802\n",
      "Done 1300 batches in 1206.27s\ttraining loss:\t4.312304\n",
      "Done 1305 batches in 1211.37s\ttraining loss:\t4.309771\n",
      "Done 1310 batches in 1215.96s\ttraining loss:\t4.307633\n",
      "Done 1315 batches in 1220.22s\ttraining loss:\t4.303061\n",
      "Done 1320 batches in 1225.17s\ttraining loss:\t4.299680\n",
      "Done 1325 batches in 1229.60s\ttraining loss:\t4.298765\n",
      "Done 1330 batches in 1234.19s\ttraining loss:\t4.296609\n",
      "Done 1335 batches in 1238.76s\ttraining loss:\t4.295922\n",
      "Done 1340 batches in 1243.54s\ttraining loss:\t4.293497\n",
      "Done 1345 batches in 1248.08s\ttraining loss:\t4.291479\n",
      "Done 1350 batches in 1252.90s\ttraining loss:\t4.289674\n",
      "Done 1355 batches in 1257.32s\ttraining loss:\t4.286359\n",
      "Done 1360 batches in 1261.80s\ttraining loss:\t4.284201\n",
      "Done 1365 batches in 1266.81s\ttraining loss:\t4.281419\n",
      "Done 1370 batches in 1271.64s\ttraining loss:\t4.275731\n",
      "Done 1375 batches in 1276.65s\ttraining loss:\t4.273135\n",
      "Done 1380 batches in 1281.40s\ttraining loss:\t4.270596\n",
      "Done 1385 batches in 1286.11s\ttraining loss:\t4.266873\n",
      "Done 1390 batches in 1290.48s\ttraining loss:\t4.262260\n",
      "Done 1395 batches in 1294.90s\ttraining loss:\t4.259632\n",
      "Done 1400 batches in 1299.78s\ttraining loss:\t4.257271\n",
      "Done 1405 batches in 1304.88s\ttraining loss:\t4.253392\n",
      "Done 1410 batches in 1309.45s\ttraining loss:\t4.248695\n",
      "Done 1415 batches in 1314.34s\ttraining loss:\t4.244719\n",
      "Done 1420 batches in 1318.72s\ttraining loss:\t4.241316\n",
      "Done 1425 batches in 1323.12s\ttraining loss:\t4.238878\n",
      "Done 1430 batches in 1328.01s\ttraining loss:\t4.236688\n",
      "Done 1435 batches in 1332.74s\ttraining loss:\t4.233446\n",
      "Done 1440 batches in 1337.29s\ttraining loss:\t4.229113\n",
      "Done 1445 batches in 1341.89s\ttraining loss:\t4.227683\n",
      "Done 1450 batches in 1347.10s\ttraining loss:\t4.225521\n",
      "Done 1455 batches in 1351.85s\ttraining loss:\t4.222471\n",
      "Done 1460 batches in 1356.62s\ttraining loss:\t4.219744\n",
      "Done 1465 batches in 1361.27s\ttraining loss:\t4.216507\n",
      "Done 1470 batches in 1366.42s\ttraining loss:\t4.212381\n",
      "Done 1475 batches in 1370.91s\ttraining loss:\t4.208557\n",
      "Done 1480 batches in 1375.94s\ttraining loss:\t4.204834\n",
      "Done 1485 batches in 1380.45s\ttraining loss:\t4.202173\n",
      "Done 1490 batches in 1385.12s\ttraining loss:\t4.199614\n",
      "Done 1495 batches in 1389.10s\ttraining loss:\t4.196408\n",
      "Done 1500 batches in 1393.65s\ttraining loss:\t4.194361\n",
      "Done 1505 batches in 1398.02s\ttraining loss:\t4.193119\n",
      "Done 1510 batches in 1402.80s\ttraining loss:\t4.188879\n",
      "Done 1515 batches in 1407.65s\ttraining loss:\t4.186639\n",
      "Done 1520 batches in 1412.18s\ttraining loss:\t4.184569\n",
      "Done 1525 batches in 1416.63s\ttraining loss:\t4.181376\n",
      "Done 1530 batches in 1421.47s\ttraining loss:\t4.177747\n",
      "Done 1535 batches in 1426.16s\ttraining loss:\t4.174746\n",
      "Done 1540 batches in 1431.07s\ttraining loss:\t4.171058\n",
      "Done 1545 batches in 1435.68s\ttraining loss:\t4.167263\n",
      "Done 1550 batches in 1440.37s\ttraining loss:\t4.163859\n",
      "Done 1555 batches in 1444.88s\ttraining loss:\t4.159585\n",
      "Done 1560 batches in 1449.62s\ttraining loss:\t4.156182\n",
      "Done 1565 batches in 1454.13s\ttraining loss:\t4.153842\n",
      "Done 1570 batches in 1458.40s\ttraining loss:\t4.149846\n",
      "Done 1575 batches in 1462.94s\ttraining loss:\t4.147238\n",
      "Done 1580 batches in 1467.19s\ttraining loss:\t4.144850\n",
      "Done 1585 batches in 1472.00s\ttraining loss:\t4.141684\n",
      "Done 1590 batches in 1476.27s\ttraining loss:\t4.138670\n",
      "Done 1595 batches in 1481.08s\ttraining loss:\t4.134705\n",
      "Done 1600 batches in 1486.21s\ttraining loss:\t4.131736\n",
      "Done 1605 batches in 1490.50s\ttraining loss:\t4.130009\n",
      "Done 1610 batches in 1495.33s\ttraining loss:\t4.127712\n",
      "Done 1615 batches in 1499.83s\ttraining loss:\t4.125297\n",
      "Done 1620 batches in 1504.44s\ttraining loss:\t4.121647\n",
      "Done 1625 batches in 1509.39s\ttraining loss:\t4.116573\n",
      "Done 1630 batches in 1514.58s\ttraining loss:\t4.114796\n",
      "Done 1635 batches in 1519.39s\ttraining loss:\t4.112453\n",
      "Done 1640 batches in 1524.11s\ttraining loss:\t4.108354\n",
      "Done 1645 batches in 1528.64s\ttraining loss:\t4.104761\n",
      "Done 1650 batches in 1533.04s\ttraining loss:\t4.100510\n",
      "Done 1655 batches in 1537.43s\ttraining loss:\t4.096823\n",
      "Done 1660 batches in 1541.73s\ttraining loss:\t4.095461\n",
      "Done 1665 batches in 1546.65s\ttraining loss:\t4.093191\n",
      "Done 1670 batches in 1551.03s\ttraining loss:\t4.091163\n",
      "Done 1675 batches in 1555.40s\ttraining loss:\t4.089130\n",
      "Done 1680 batches in 1560.51s\ttraining loss:\t4.087741\n",
      "Done 1685 batches in 1565.18s\ttraining loss:\t4.084692\n",
      "Done 1690 batches in 1569.43s\ttraining loss:\t4.082545\n",
      "Done 1695 batches in 1574.31s\ttraining loss:\t4.080734\n",
      "Done 1700 batches in 1579.26s\ttraining loss:\t4.077904\n",
      "Done 1705 batches in 1584.30s\ttraining loss:\t4.075728\n",
      "Done 1710 batches in 1589.30s\ttraining loss:\t4.072375\n",
      "Done 1715 batches in 1594.17s\ttraining loss:\t4.070336\n",
      "Done 1720 batches in 1598.88s\ttraining loss:\t4.067536\n",
      "Done 1725 batches in 1603.61s\ttraining loss:\t4.064677\n",
      "Done 1730 batches in 1608.00s\ttraining loss:\t4.061040\n",
      "Done 1735 batches in 1612.52s\ttraining loss:\t4.057568\n",
      "Done 1740 batches in 1617.03s\ttraining loss:\t4.056211\n",
      "Done 1745 batches in 1621.78s\ttraining loss:\t4.052846\n",
      "Done 1750 batches in 1626.18s\ttraining loss:\t4.050709\n",
      "Done 1755 batches in 1630.52s\ttraining loss:\t4.048176\n",
      "Done 1760 batches in 1634.99s\ttraining loss:\t4.045251\n",
      "Done 1765 batches in 1639.89s\ttraining loss:\t4.043556\n",
      "Done 1770 batches in 1645.05s\ttraining loss:\t4.041975\n",
      "Done 1775 batches in 1649.35s\ttraining loss:\t4.039506\n",
      "Done 1780 batches in 1654.12s\ttraining loss:\t4.036775\n",
      "Done 1785 batches in 1658.46s\ttraining loss:\t4.034006\n",
      "Done 1790 batches in 1662.84s\ttraining loss:\t4.031940\n",
      "Done 1795 batches in 1667.81s\ttraining loss:\t4.030358\n",
      "Done 1800 batches in 1672.06s\ttraining loss:\t4.027467\n",
      "Done 1805 batches in 1677.10s\ttraining loss:\t4.025475\n",
      "Done 1810 batches in 1681.69s\ttraining loss:\t4.022956\n",
      "Done 1815 batches in 1686.13s\ttraining loss:\t4.020281\n",
      "Done 1820 batches in 1690.77s\ttraining loss:\t4.017062\n",
      "Done 1825 batches in 1695.82s\ttraining loss:\t4.014156\n",
      "Done 1830 batches in 1700.04s\ttraining loss:\t4.011793\n",
      "Done 1835 batches in 1704.77s\ttraining loss:\t4.007949\n",
      "Done 1840 batches in 1709.73s\ttraining loss:\t4.005821\n",
      "Done 1845 batches in 1714.44s\ttraining loss:\t4.004713\n",
      "Done 1850 batches in 1719.11s\ttraining loss:\t4.002297\n",
      "Done 1855 batches in 1723.93s\ttraining loss:\t3.999656\n",
      "Done 1860 batches in 1728.81s\ttraining loss:\t3.996891\n",
      "Done 1865 batches in 1733.34s\ttraining loss:\t3.993244\n",
      "Done 1870 batches in 1737.85s\ttraining loss:\t3.990848\n",
      "Done 1875 batches in 1742.85s\ttraining loss:\t3.988568\n",
      "Done 1880 batches in 1747.05s\ttraining loss:\t3.987535\n",
      "Done 1885 batches in 1751.80s\ttraining loss:\t3.985270\n",
      "Done 1890 batches in 1756.24s\ttraining loss:\t3.983822\n",
      "Done 1895 batches in 1761.10s\ttraining loss:\t3.980994\n",
      "Done 1900 batches in 1765.41s\ttraining loss:\t3.979349\n",
      "Done 1905 batches in 1770.20s\ttraining loss:\t3.976453\n",
      "Done 1910 batches in 1774.66s\ttraining loss:\t3.974216\n",
      "Done 1915 batches in 1778.97s\ttraining loss:\t3.972428\n",
      "Done 1920 batches in 1783.69s\ttraining loss:\t3.970708\n",
      "Done 1925 batches in 1788.75s\ttraining loss:\t3.968128\n",
      "Done 1930 batches in 1793.27s\ttraining loss:\t3.966005\n",
      "Done 1935 batches in 1798.05s\ttraining loss:\t3.962170\n",
      "Done 1940 batches in 1802.21s\ttraining loss:\t3.959536\n",
      "Done 1945 batches in 1807.07s\ttraining loss:\t3.957196\n",
      "Done 1950 batches in 1811.70s\ttraining loss:\t3.954878\n",
      "Done 1955 batches in 1816.27s\ttraining loss:\t3.952267\n",
      "Done 1960 batches in 1820.95s\ttraining loss:\t3.949254\n",
      "Done 1965 batches in 1825.65s\ttraining loss:\t3.946538\n",
      "Done 1970 batches in 1830.25s\ttraining loss:\t3.943112\n",
      "Done 1975 batches in 1835.51s\ttraining loss:\t3.940055\n",
      "Done 1980 batches in 1840.13s\ttraining loss:\t3.937838\n",
      "Done 1985 batches in 1844.44s\ttraining loss:\t3.936569\n",
      "Done 1990 batches in 1849.36s\ttraining loss:\t3.934588\n",
      "Done 1995 batches in 1854.21s\ttraining loss:\t3.932572\n",
      "Done 2000 batches in 1858.93s\ttraining loss:\t3.931618\n",
      "Done 2005 batches in 1863.76s\ttraining loss:\t3.930095\n",
      "Done 2010 batches in 1868.44s\ttraining loss:\t3.929055\n",
      "Done 2015 batches in 1872.72s\ttraining loss:\t3.927670\n",
      "Done 2020 batches in 1877.17s\ttraining loss:\t3.925996\n",
      "Done 2025 batches in 1881.70s\ttraining loss:\t3.925446\n",
      "Done 2030 batches in 1886.48s\ttraining loss:\t3.924905\n",
      "Done 2035 batches in 1890.97s\ttraining loss:\t3.922781\n",
      "Done 2040 batches in 1895.58s\ttraining loss:\t3.920142\n",
      "Done 2045 batches in 1900.39s\ttraining loss:\t3.917861\n",
      "Done 2050 batches in 1905.14s\ttraining loss:\t3.915103\n",
      "Done 2055 batches in 1909.90s\ttraining loss:\t3.913654\n",
      "Done 2060 batches in 1914.92s\ttraining loss:\t3.911117\n",
      "Done 2065 batches in 1919.91s\ttraining loss:\t3.909054\n",
      "Done 2070 batches in 1924.50s\ttraining loss:\t3.906746\n",
      "Done 2075 batches in 1929.37s\ttraining loss:\t3.904970\n",
      "Done 2080 batches in 1934.10s\ttraining loss:\t3.902369\n",
      "Done 2085 batches in 1938.61s\ttraining loss:\t3.900315\n",
      "Done 2090 batches in 1943.44s\ttraining loss:\t3.897574\n",
      "Done 2095 batches in 1948.54s\ttraining loss:\t3.895959\n",
      "Done 2100 batches in 1953.17s\ttraining loss:\t3.893764\n",
      "Done 2105 batches in 1957.40s\ttraining loss:\t3.892061\n",
      "Done 2110 batches in 1961.90s\ttraining loss:\t3.889753\n",
      "Done 2115 batches in 1966.79s\ttraining loss:\t3.887678\n",
      "Done 2120 batches in 1971.52s\ttraining loss:\t3.884983\n",
      "Done 2125 batches in 1976.27s\ttraining loss:\t3.884353\n",
      "Done 2130 batches in 1980.72s\ttraining loss:\t3.882738\n",
      "Done 2135 batches in 1985.53s\ttraining loss:\t3.879873\n",
      "Done 2140 batches in 1990.09s\ttraining loss:\t3.877679\n",
      "Done 2145 batches in 1994.75s\ttraining loss:\t3.876019\n",
      "Done 2150 batches in 1999.26s\ttraining loss:\t3.874469\n",
      "Done 2155 batches in 2003.85s\ttraining loss:\t3.872405\n",
      "Done 2160 batches in 2008.53s\ttraining loss:\t3.870905\n",
      "Done 2165 batches in 2013.45s\ttraining loss:\t3.869867\n",
      "Done 2170 batches in 2018.43s\ttraining loss:\t3.868035\n",
      "Done 2175 batches in 2023.13s\ttraining loss:\t3.865976\n",
      "Done 2180 batches in 2027.94s\ttraining loss:\t3.864418\n",
      "Done 2185 batches in 2032.83s\ttraining loss:\t3.862345\n",
      "Done 2190 batches in 2037.42s\ttraining loss:\t3.860812\n",
      "Done 2195 batches in 2042.21s\ttraining loss:\t3.858734\n",
      "Done 2200 batches in 2046.87s\ttraining loss:\t3.856675\n",
      "Done 2205 batches in 2051.30s\ttraining loss:\t3.855357\n",
      "Done 2210 batches in 2056.25s\ttraining loss:\t3.854001\n",
      "Done 2215 batches in 2060.91s\ttraining loss:\t3.851963\n",
      "Done 2220 batches in 2065.28s\ttraining loss:\t3.849084\n",
      "Done 2225 batches in 2070.17s\ttraining loss:\t3.847402\n",
      "Done 2230 batches in 2075.31s\ttraining loss:\t3.845617\n",
      "Done 2235 batches in 2079.71s\ttraining loss:\t3.844101\n",
      "Done 2240 batches in 2084.51s\ttraining loss:\t3.841939\n",
      "Done 2245 batches in 2089.60s\ttraining loss:\t3.840045\n",
      "Done 2250 batches in 2094.23s\ttraining loss:\t3.838207\n",
      "Done 2255 batches in 2098.36s\ttraining loss:\t3.837472\n",
      "Done 2260 batches in 2102.99s\ttraining loss:\t3.835386\n",
      "Done 2265 batches in 2107.90s\ttraining loss:\t3.832123\n",
      "Done 2270 batches in 2112.84s\ttraining loss:\t3.830732\n",
      "Done 2275 batches in 2117.38s\ttraining loss:\t3.828004\n",
      "Done 2280 batches in 2122.24s\ttraining loss:\t3.826042\n",
      "Done 2285 batches in 2126.91s\ttraining loss:\t3.824291\n",
      "Done 2290 batches in 2131.45s\ttraining loss:\t3.822559\n",
      "Done 2295 batches in 2135.67s\ttraining loss:\t3.820368\n",
      "Done 2300 batches in 2140.58s\ttraining loss:\t3.818081\n",
      "Done 2305 batches in 2145.16s\ttraining loss:\t3.816828\n",
      "Done 2310 batches in 2150.00s\ttraining loss:\t3.814408\n",
      "Done 2315 batches in 2154.64s\ttraining loss:\t3.813137\n",
      "Done 2320 batches in 2159.05s\ttraining loss:\t3.811297\n",
      "Done 2325 batches in 2163.70s\ttraining loss:\t3.810200\n",
      "Done 2330 batches in 2168.42s\ttraining loss:\t3.808847\n",
      "Done 2335 batches in 2173.24s\ttraining loss:\t3.806808\n",
      "Done 2340 batches in 2177.92s\ttraining loss:\t3.804910\n",
      "Done 2345 batches in 2182.58s\ttraining loss:\t3.802961\n",
      "Done 2350 batches in 2187.11s\ttraining loss:\t3.801377\n",
      "Done 2355 batches in 2191.28s\ttraining loss:\t3.799812\n",
      "Done 2360 batches in 2195.66s\ttraining loss:\t3.797643\n",
      "Done 2365 batches in 2200.28s\ttraining loss:\t3.795453\n",
      "Done 2370 batches in 2205.25s\ttraining loss:\t3.793205\n",
      "Done 2375 batches in 2209.79s\ttraining loss:\t3.791367\n",
      "Done 2380 batches in 2214.44s\ttraining loss:\t3.789821\n",
      "Done 2385 batches in 2219.20s\ttraining loss:\t3.787525\n",
      "Done 2390 batches in 2223.49s\ttraining loss:\t3.785978\n",
      "Done 2395 batches in 2227.99s\ttraining loss:\t3.784630\n",
      "Done 2400 batches in 2232.95s\ttraining loss:\t3.783176\n",
      "Done 2405 batches in 2237.59s\ttraining loss:\t3.781702\n",
      "Done 2410 batches in 2242.21s\ttraining loss:\t3.780018\n",
      "Done 2415 batches in 2246.77s\ttraining loss:\t3.777658\n",
      "Done 2420 batches in 2251.89s\ttraining loss:\t3.775926\n",
      "Done 2425 batches in 2257.01s\ttraining loss:\t3.774390\n",
      "Done 2430 batches in 2261.57s\ttraining loss:\t3.773233\n",
      "Done 2435 batches in 2266.59s\ttraining loss:\t3.771538\n",
      "Done 2440 batches in 2271.17s\ttraining loss:\t3.769799\n",
      "Done 2445 batches in 2275.83s\ttraining loss:\t3.768108\n",
      "Done 2450 batches in 2280.70s\ttraining loss:\t3.766447\n",
      "Done 2455 batches in 2285.30s\ttraining loss:\t3.765362\n",
      "Done 2460 batches in 2290.06s\ttraining loss:\t3.764370\n",
      "Done 2465 batches in 2294.96s\ttraining loss:\t3.762305\n",
      "Done 2470 batches in 2299.81s\ttraining loss:\t3.759898\n",
      "Done 2475 batches in 2304.86s\ttraining loss:\t3.758230\n",
      "Done 2480 batches in 2309.10s\ttraining loss:\t3.756377\n",
      "Done 2485 batches in 2313.80s\ttraining loss:\t3.754113\n",
      "Done 2490 batches in 2317.93s\ttraining loss:\t3.752487\n",
      "Done 2495 batches in 2322.65s\ttraining loss:\t3.751173\n",
      "Done 2500 batches in 2326.90s\ttraining loss:\t3.749438\n",
      "Done 2505 batches in 2331.84s\ttraining loss:\t3.748058\n",
      "Done 2510 batches in 2336.65s\ttraining loss:\t3.746463\n",
      "Done 2515 batches in 2341.40s\ttraining loss:\t3.744744\n",
      "Done 2520 batches in 2345.95s\ttraining loss:\t3.743556\n",
      "Done 2525 batches in 2350.72s\ttraining loss:\t3.742425\n",
      "Done 2530 batches in 2355.49s\ttraining loss:\t3.740467\n",
      "Done 2535 batches in 2360.67s\ttraining loss:\t3.739351\n",
      "Done 2540 batches in 2365.30s\ttraining loss:\t3.737309\n",
      "Done 2545 batches in 2370.17s\ttraining loss:\t3.735575\n",
      "Done 2550 batches in 2375.13s\ttraining loss:\t3.733294\n",
      "Done 2555 batches in 2379.88s\ttraining loss:\t3.731983\n",
      "Done 2560 batches in 2384.58s\ttraining loss:\t3.731009\n",
      "Done 2565 batches in 2389.33s\ttraining loss:\t3.729183\n",
      "Done 2570 batches in 2393.83s\ttraining loss:\t3.727003\n",
      "Done 2575 batches in 2398.45s\ttraining loss:\t3.725463\n",
      "Done 2580 batches in 2403.16s\ttraining loss:\t3.722949\n",
      "Done 2585 batches in 2407.91s\ttraining loss:\t3.721879\n",
      "Done 2590 batches in 2412.51s\ttraining loss:\t3.720320\n",
      "Done 2595 batches in 2416.95s\ttraining loss:\t3.718896\n",
      "Done 2600 batches in 2421.84s\ttraining loss:\t3.716787\n",
      "Done 2605 batches in 2426.48s\ttraining loss:\t3.714703\n",
      "Done 2610 batches in 2431.21s\ttraining loss:\t3.713335\n",
      "Done 2615 batches in 2435.92s\ttraining loss:\t3.711318\n",
      "Done 2620 batches in 2440.76s\ttraining loss:\t3.709471\n",
      "Done 2625 batches in 2445.66s\ttraining loss:\t3.708003\n",
      "Done 2630 batches in 2450.89s\ttraining loss:\t3.705438\n",
      "Done 2635 batches in 2455.64s\ttraining loss:\t3.704408\n",
      "Done 2640 batches in 2460.09s\ttraining loss:\t3.702656\n",
      "Done 2645 batches in 2464.74s\ttraining loss:\t3.701029\n",
      "Done 2650 batches in 2468.90s\ttraining loss:\t3.699768\n",
      "Done 2655 batches in 2473.81s\ttraining loss:\t3.699904\n",
      "Done 2660 batches in 2478.33s\ttraining loss:\t3.698333\n",
      "Done 2665 batches in 2482.79s\ttraining loss:\t3.697544\n",
      "Done 2670 batches in 2487.74s\ttraining loss:\t3.696095\n",
      "Done 2675 batches in 2492.45s\ttraining loss:\t3.694599\n",
      "Done 2680 batches in 2497.29s\ttraining loss:\t3.693378\n",
      "Done 2685 batches in 2501.81s\ttraining loss:\t3.691633\n",
      "Done 2690 batches in 2506.75s\ttraining loss:\t3.690563\n",
      "Done 2695 batches in 2511.20s\ttraining loss:\t3.689671\n",
      "Done 2700 batches in 2516.31s\ttraining loss:\t3.688071\n",
      "Done 2705 batches in 2520.75s\ttraining loss:\t3.686399\n",
      "Done 2710 batches in 2525.44s\ttraining loss:\t3.684592\n",
      "Done 2715 batches in 2530.10s\ttraining loss:\t3.683202\n",
      "Done 2720 batches in 2534.92s\ttraining loss:\t3.681538\n",
      "Done 2725 batches in 2539.38s\ttraining loss:\t3.680404\n",
      "Done 2730 batches in 2543.56s\ttraining loss:\t3.679093\n",
      "Done 2735 batches in 2548.15s\ttraining loss:\t3.677952\n",
      "Done 2740 batches in 2553.35s\ttraining loss:\t3.676104\n",
      "Done 2745 batches in 2558.18s\ttraining loss:\t3.674307\n",
      "Done 2750 batches in 2562.91s\ttraining loss:\t3.672276\n",
      "Done 2755 batches in 2567.33s\ttraining loss:\t3.671062\n",
      "Done 2760 batches in 2571.79s\ttraining loss:\t3.670473\n",
      "Done 2765 batches in 2576.28s\ttraining loss:\t3.669143\n",
      "Done 2770 batches in 2580.87s\ttraining loss:\t3.668199\n",
      "Done 2775 batches in 2585.64s\ttraining loss:\t3.667111\n",
      "Done 2780 batches in 2590.59s\ttraining loss:\t3.664963\n",
      "Done 2785 batches in 2595.66s\ttraining loss:\t3.663410\n",
      "Done 2790 batches in 2600.27s\ttraining loss:\t3.662200\n",
      "Done 2795 batches in 2605.18s\ttraining loss:\t3.660416\n",
      "Done 2800 batches in 2609.85s\ttraining loss:\t3.658783\n",
      "Done 2805 batches in 2614.02s\ttraining loss:\t3.657151\n",
      "Done 2810 batches in 2619.15s\ttraining loss:\t3.655550\n",
      "Done 2815 batches in 2623.99s\ttraining loss:\t3.653594\n",
      "Done 2820 batches in 2628.98s\ttraining loss:\t3.651723\n",
      "Done 2825 batches in 2633.70s\ttraining loss:\t3.649850\n",
      "Done 2830 batches in 2637.96s\ttraining loss:\t3.648127\n",
      "Done 2835 batches in 2642.50s\ttraining loss:\t3.646961\n",
      "Done 2840 batches in 2647.58s\ttraining loss:\t3.645107\n",
      "Done 2845 batches in 2651.95s\ttraining loss:\t3.643699\n",
      "Done 2850 batches in 2656.72s\ttraining loss:\t3.642323\n",
      "Done 2855 batches in 2661.27s\ttraining loss:\t3.640497\n",
      "Done 2860 batches in 2665.93s\ttraining loss:\t3.639805\n",
      "Done 2865 batches in 2670.80s\ttraining loss:\t3.638328\n",
      "Done 2870 batches in 2675.93s\ttraining loss:\t3.637207\n",
      "Done 2875 batches in 2680.54s\ttraining loss:\t3.635556\n",
      "Done 2880 batches in 2684.90s\ttraining loss:\t3.634681\n",
      "Done 2885 batches in 2689.44s\ttraining loss:\t3.633082\n",
      "Done 2890 batches in 2694.17s\ttraining loss:\t3.632027\n",
      "Done 2895 batches in 2699.18s\ttraining loss:\t3.631663\n",
      "Done 2900 batches in 2703.73s\ttraining loss:\t3.630124\n",
      "Done 2905 batches in 2708.47s\ttraining loss:\t3.628841\n",
      "Done 2910 batches in 2713.71s\ttraining loss:\t3.628279\n",
      "Done 2915 batches in 2718.28s\ttraining loss:\t3.626862\n",
      "Done 2920 batches in 2723.16s\ttraining loss:\t3.624820\n",
      "Done 2925 batches in 2728.06s\ttraining loss:\t3.623162\n",
      "Done 2930 batches in 2732.81s\ttraining loss:\t3.621931\n",
      "Done 2935 batches in 2737.29s\ttraining loss:\t3.620404\n",
      "Done 2940 batches in 2741.94s\ttraining loss:\t3.619033\n",
      "Done 2945 batches in 2746.84s\ttraining loss:\t3.617801\n",
      "Done 2950 batches in 2751.53s\ttraining loss:\t3.616874\n",
      "Done 2955 batches in 2756.13s\ttraining loss:\t3.615771\n",
      "Done 2960 batches in 2761.01s\ttraining loss:\t3.615554\n",
      "Done 2965 batches in 2765.77s\ttraining loss:\t3.614303\n",
      "Done 2970 batches in 2769.85s\ttraining loss:\t3.613174\n",
      "Done 2975 batches in 2774.49s\ttraining loss:\t3.611805\n",
      "Done 2980 batches in 2779.25s\ttraining loss:\t3.609944\n",
      "Done 2985 batches in 2784.05s\ttraining loss:\t3.609350\n",
      "Done 2990 batches in 2788.68s\ttraining loss:\t3.607856\n",
      "Done 2995 batches in 2793.56s\ttraining loss:\t3.606975\n",
      "Done 3000 batches in 2798.33s\ttraining loss:\t3.605793\n",
      "Done 3005 batches in 2802.99s\ttraining loss:\t3.604960\n",
      "Done 3010 batches in 2807.73s\ttraining loss:\t3.603369\n",
      "Done 3015 batches in 2812.33s\ttraining loss:\t3.602527\n",
      "Done 3020 batches in 2816.95s\ttraining loss:\t3.600832\n",
      "Done 3025 batches in 2821.80s\ttraining loss:\t3.599249\n",
      "Done 3030 batches in 2826.73s\ttraining loss:\t3.598288\n",
      "Done 3035 batches in 2831.49s\ttraining loss:\t3.597226\n",
      "Done 3040 batches in 2835.61s\ttraining loss:\t3.596960\n",
      "Done 3045 batches in 2840.51s\ttraining loss:\t3.596220\n",
      "Done 3050 batches in 2845.11s\ttraining loss:\t3.595029\n",
      "Done 3055 batches in 2849.93s\ttraining loss:\t3.593619\n",
      "Done 3060 batches in 2854.86s\ttraining loss:\t3.592534\n",
      "Done 3065 batches in 2859.68s\ttraining loss:\t3.590782\n",
      "Done 3070 batches in 2864.65s\ttraining loss:\t3.588809\n",
      "Done 3075 batches in 2869.59s\ttraining loss:\t3.588355\n",
      "Done 3080 batches in 2874.05s\ttraining loss:\t3.587411\n",
      "Done 3085 batches in 2878.45s\ttraining loss:\t3.585975\n",
      "Done 3090 batches in 2883.35s\ttraining loss:\t3.584319\n",
      "Done 3095 batches in 2888.61s\ttraining loss:\t3.582730\n",
      "Done 3100 batches in 2893.22s\ttraining loss:\t3.580864\n",
      "Done 3105 batches in 2897.95s\ttraining loss:\t3.580473\n",
      "Done 3110 batches in 2902.51s\ttraining loss:\t3.579273\n",
      "Done 3115 batches in 2907.22s\ttraining loss:\t3.577391\n",
      "Done 3120 batches in 2912.28s\ttraining loss:\t3.576470\n",
      "Done 3125 batches in 2916.20s\ttraining loss:\t3.574787\n",
      "Done 3130 batches in 2920.48s\ttraining loss:\t3.573305\n",
      "Done 3135 batches in 2925.53s\ttraining loss:\t3.572512\n",
      "Done 3140 batches in 2930.40s\ttraining loss:\t3.571155\n",
      "Done 3145 batches in 2935.54s\ttraining loss:\t3.569901\n",
      "Done 3150 batches in 2940.04s\ttraining loss:\t3.568719\n",
      "Done 3155 batches in 2944.45s\ttraining loss:\t3.567694\n",
      "Done 3160 batches in 2949.20s\ttraining loss:\t3.566259\n",
      "Done 3165 batches in 2953.74s\ttraining loss:\t3.565297\n",
      "Done 3170 batches in 2958.30s\ttraining loss:\t3.563696\n",
      "Done 3175 batches in 2963.22s\ttraining loss:\t3.561637\n",
      "Done 3180 batches in 2967.77s\ttraining loss:\t3.560722\n",
      "Done 3185 batches in 2972.22s\ttraining loss:\t3.559660\n",
      "Done 3190 batches in 2977.15s\ttraining loss:\t3.558301\n",
      "Done 3195 batches in 2981.97s\ttraining loss:\t3.557983\n",
      "Done 3200 batches in 2987.13s\ttraining loss:\t3.557016\n",
      "Done 3205 batches in 2991.89s\ttraining loss:\t3.555744\n",
      "Done 3210 batches in 2996.59s\ttraining loss:\t3.554529\n",
      "Done 3215 batches in 3001.70s\ttraining loss:\t3.553482\n",
      "Done 3220 batches in 3006.07s\ttraining loss:\t3.552195\n",
      "Done 3225 batches in 3010.83s\ttraining loss:\t3.551091\n",
      "Done 3230 batches in 3015.66s\ttraining loss:\t3.549067\n",
      "Done 3235 batches in 3020.45s\ttraining loss:\t3.548575\n",
      "Done 3240 batches in 3024.98s\ttraining loss:\t3.547251\n",
      "Done 3245 batches in 3029.30s\ttraining loss:\t3.546295\n",
      "Done 3250 batches in 3033.98s\ttraining loss:\t3.545054\n",
      "Done 3255 batches in 3038.82s\ttraining loss:\t3.544098\n",
      "Done 3260 batches in 3043.20s\ttraining loss:\t3.543595\n",
      "Done 3265 batches in 3048.14s\ttraining loss:\t3.542288\n",
      "Done 3270 batches in 3052.68s\ttraining loss:\t3.541152\n",
      "Done 3275 batches in 3056.74s\ttraining loss:\t3.540648\n",
      "Done 3280 batches in 3061.32s\ttraining loss:\t3.539318\n",
      "Done 3285 batches in 3065.92s\ttraining loss:\t3.537913\n",
      "Done 3290 batches in 3070.55s\ttraining loss:\t3.536287\n",
      "Done 3295 batches in 3074.84s\ttraining loss:\t3.535296\n",
      "Done 3300 batches in 3079.92s\ttraining loss:\t3.533828\n",
      "Done 3305 batches in 3084.23s\ttraining loss:\t3.532521\n",
      "Done 3310 batches in 3088.68s\ttraining loss:\t3.532065\n",
      "Done 3315 batches in 3093.25s\ttraining loss:\t3.531088\n",
      "Done 3320 batches in 3097.69s\ttraining loss:\t3.530166\n",
      "Done 3325 batches in 3102.85s\ttraining loss:\t3.529645\n",
      "Done 3330 batches in 3107.54s\ttraining loss:\t3.528865\n",
      "Done 3335 batches in 3112.27s\ttraining loss:\t3.527937\n",
      "Done 3340 batches in 3117.20s\ttraining loss:\t3.526447\n",
      "Done 3345 batches in 3122.14s\ttraining loss:\t3.524866\n",
      "Done 3350 batches in 3127.10s\ttraining loss:\t3.523832\n",
      "Done 3355 batches in 3131.74s\ttraining loss:\t3.522753\n",
      "Done 3360 batches in 3136.38s\ttraining loss:\t3.521989\n",
      "Done 3365 batches in 3140.79s\ttraining loss:\t3.521491\n",
      "Done 3370 batches in 3145.84s\ttraining loss:\t3.520236\n",
      "Done 3375 batches in 3149.99s\ttraining loss:\t3.519245\n",
      "Done 3380 batches in 3155.12s\ttraining loss:\t3.518413\n",
      "Done 3385 batches in 3159.82s\ttraining loss:\t3.517748\n",
      "Done 3390 batches in 3164.22s\ttraining loss:\t3.516902\n",
      "Done 3395 batches in 3169.24s\ttraining loss:\t3.515416\n",
      "Done 3400 batches in 3174.09s\ttraining loss:\t3.513764\n",
      "Done 3405 batches in 3178.57s\ttraining loss:\t3.512652\n",
      "Done 3410 batches in 3183.81s\ttraining loss:\t3.511396\n",
      "Done 3415 batches in 3188.58s\ttraining loss:\t3.510548\n",
      "Done 3420 batches in 3192.85s\ttraining loss:\t3.509431\n",
      "Done 3425 batches in 3196.98s\ttraining loss:\t3.508642\n",
      "Done 3430 batches in 3201.67s\ttraining loss:\t3.507807\n",
      "Done 3435 batches in 3206.03s\ttraining loss:\t3.506527\n",
      "Done 3440 batches in 3210.87s\ttraining loss:\t3.505620\n",
      "Done 3445 batches in 3215.55s\ttraining loss:\t3.504134\n",
      "Done 3450 batches in 3220.28s\ttraining loss:\t3.502955\n",
      "Done 3455 batches in 3225.05s\ttraining loss:\t3.501830\n",
      "Done 3460 batches in 3229.95s\ttraining loss:\t3.499535\n",
      "Done 3465 batches in 3234.40s\ttraining loss:\t3.498644\n",
      "Done 3470 batches in 3238.51s\ttraining loss:\t3.498119\n",
      "Done 3475 batches in 3243.34s\ttraining loss:\t3.497076\n",
      "Done 3480 batches in 3248.03s\ttraining loss:\t3.496529\n",
      "Done 3485 batches in 3253.08s\ttraining loss:\t3.495280\n",
      "Done 3490 batches in 3257.47s\ttraining loss:\t3.494004\n",
      "Done 3495 batches in 3262.34s\ttraining loss:\t3.493646\n",
      "Done 3500 batches in 3267.12s\ttraining loss:\t3.492306\n",
      "Done 3505 batches in 3271.60s\ttraining loss:\t3.492062\n",
      "Done 3510 batches in 3276.46s\ttraining loss:\t3.491241\n",
      "Done 3515 batches in 3281.23s\ttraining loss:\t3.490609\n",
      "Done 3520 batches in 3286.13s\ttraining loss:\t3.490029\n",
      "Done 3525 batches in 3290.49s\ttraining loss:\t3.488839\n",
      "Done 3530 batches in 3295.22s\ttraining loss:\t3.487699\n",
      "Done 3535 batches in 3299.75s\ttraining loss:\t3.486515\n",
      "Done 3540 batches in 3304.32s\ttraining loss:\t3.485602\n",
      "Done 3545 batches in 3309.22s\ttraining loss:\t3.484226\n",
      "Done 3550 batches in 3313.78s\ttraining loss:\t3.483522\n",
      "Done 3555 batches in 3318.70s\ttraining loss:\t3.482081\n",
      "Done 3560 batches in 3323.40s\ttraining loss:\t3.481258\n",
      "Done 3565 batches in 3328.26s\ttraining loss:\t3.479835\n",
      "Done 3570 batches in 3332.99s\ttraining loss:\t3.478921\n",
      "Done 3575 batches in 3337.90s\ttraining loss:\t3.478038\n",
      "Done 3580 batches in 3342.75s\ttraining loss:\t3.476693\n",
      "Done 3585 batches in 3347.53s\ttraining loss:\t3.476284\n",
      "Done 3590 batches in 3352.38s\ttraining loss:\t3.475631\n",
      "Done 3595 batches in 3357.30s\ttraining loss:\t3.474871\n",
      "Done 3600 batches in 3361.97s\ttraining loss:\t3.474020\n",
      "Done 3605 batches in 3366.66s\ttraining loss:\t3.472833\n",
      "Done 3610 batches in 3371.54s\ttraining loss:\t3.472460\n",
      "Done 3615 batches in 3376.40s\ttraining loss:\t3.471347\n",
      "Done 3620 batches in 3380.94s\ttraining loss:\t3.469844\n",
      "Done 3625 batches in 3385.39s\ttraining loss:\t3.469136\n",
      "Done 3630 batches in 3390.11s\ttraining loss:\t3.468886\n",
      "Done 3635 batches in 3394.85s\ttraining loss:\t3.468435\n",
      "Done 3640 batches in 3399.52s\ttraining loss:\t3.467383\n",
      "Done 3645 batches in 3404.47s\ttraining loss:\t3.466457\n",
      "Done 3650 batches in 3409.35s\ttraining loss:\t3.465477\n",
      "Done 3655 batches in 3414.02s\ttraining loss:\t3.464366\n",
      "Done 3660 batches in 3418.68s\ttraining loss:\t3.463538\n",
      "Done 3665 batches in 3423.81s\ttraining loss:\t3.462372\n",
      "Done 3670 batches in 3428.73s\ttraining loss:\t3.460695\n",
      "Done 3675 batches in 3433.44s\ttraining loss:\t3.460134\n",
      "Done 3680 batches in 3438.13s\ttraining loss:\t3.458949\n",
      "Done 3685 batches in 3443.06s\ttraining loss:\t3.458276\n",
      "Done 3690 batches in 3447.48s\ttraining loss:\t3.457613\n",
      "Done 3695 batches in 3451.81s\ttraining loss:\t3.457328\n",
      "Done 3700 batches in 3456.15s\ttraining loss:\t3.456211\n",
      "Done 3705 batches in 3460.60s\ttraining loss:\t3.455324\n",
      "Done 3710 batches in 3465.37s\ttraining loss:\t3.454276\n",
      "Done 3715 batches in 3470.26s\ttraining loss:\t3.453293\n",
      "Done 3720 batches in 3474.96s\ttraining loss:\t3.451686\n",
      "Done 3725 batches in 3479.67s\ttraining loss:\t3.450327\n",
      "Done 3730 batches in 3484.55s\ttraining loss:\t3.448845\n",
      "Done 3735 batches in 3489.18s\ttraining loss:\t3.447593\n",
      "Done 3740 batches in 3493.85s\ttraining loss:\t3.447132\n",
      "Done 3745 batches in 3498.41s\ttraining loss:\t3.446110\n",
      "Done 3750 batches in 3503.43s\ttraining loss:\t3.444432\n",
      "Done 3755 batches in 3508.32s\ttraining loss:\t3.443526\n",
      "Done 3760 batches in 3512.75s\ttraining loss:\t3.442682\n",
      "Done 3765 batches in 3517.15s\ttraining loss:\t3.441983\n",
      "Done 3770 batches in 3523.38s\ttraining loss:\t3.441297\n",
      "Done 3775 batches in 3529.53s\ttraining loss:\t3.440118\n",
      "Done 3780 batches in 3533.91s\ttraining loss:\t3.439189\n",
      "Done 3785 batches in 3540.39s\ttraining loss:\t3.438504\n",
      "Done 3790 batches in 3545.49s\ttraining loss:\t3.437900\n",
      "Done 3795 batches in 3550.30s\ttraining loss:\t3.436649\n",
      "Done 3800 batches in 3555.47s\ttraining loss:\t3.435964\n",
      "Done 3805 batches in 3560.06s\ttraining loss:\t3.435070\n",
      "Done 3810 batches in 3564.74s\ttraining loss:\t3.433882\n",
      "Done 3815 batches in 3569.68s\ttraining loss:\t3.433349\n",
      "Done 3820 batches in 3574.78s\ttraining loss:\t3.432407\n",
      "Done 3825 batches in 3579.61s\ttraining loss:\t3.431520\n",
      "Done 3830 batches in 3583.83s\ttraining loss:\t3.430760\n",
      "Done 3835 batches in 3588.06s\ttraining loss:\t3.429982\n",
      "Done 3840 batches in 3592.75s\ttraining loss:\t3.429218\n",
      "Done 3845 batches in 3597.52s\ttraining loss:\t3.428794\n",
      "Done 3850 batches in 3602.00s\ttraining loss:\t3.427600\n",
      "Done 3855 batches in 3606.70s\ttraining loss:\t3.426474\n",
      "Done 3860 batches in 3611.72s\ttraining loss:\t3.426118\n",
      "Done 3865 batches in 3616.06s\ttraining loss:\t3.425467\n",
      "Done 3870 batches in 3620.99s\ttraining loss:\t3.424263\n",
      "Done 3875 batches in 3625.60s\ttraining loss:\t3.423704\n",
      "Done 3880 batches in 3630.15s\ttraining loss:\t3.422554\n",
      "Done 3885 batches in 3634.85s\ttraining loss:\t3.421011\n",
      "Done 3890 batches in 3639.29s\ttraining loss:\t3.419506\n",
      "Done 3895 batches in 3644.10s\ttraining loss:\t3.418901\n",
      "Done 3900 batches in 3648.88s\ttraining loss:\t3.418252\n",
      "Done 3905 batches in 3653.81s\ttraining loss:\t3.416973\n",
      "Done 3910 batches in 3658.40s\ttraining loss:\t3.416020\n",
      "Done 3915 batches in 3662.71s\ttraining loss:\t3.414884\n",
      "Done 3920 batches in 3667.51s\ttraining loss:\t3.414124\n",
      "Done 3925 batches in 3672.15s\ttraining loss:\t3.413571\n",
      "Done 3930 batches in 3676.86s\ttraining loss:\t3.412711\n",
      "Done 3935 batches in 3681.14s\ttraining loss:\t3.412089\n",
      "Done 3940 batches in 3685.30s\ttraining loss:\t3.411539\n",
      "Done 3945 batches in 3689.90s\ttraining loss:\t3.410653\n",
      "Done 3950 batches in 3694.78s\ttraining loss:\t3.409676\n",
      "Done 3955 batches in 3699.24s\ttraining loss:\t3.408553\n",
      "Done 3960 batches in 3703.90s\ttraining loss:\t3.407542\n",
      "Done 3965 batches in 3708.57s\ttraining loss:\t3.406180\n",
      "Done 3970 batches in 3713.02s\ttraining loss:\t3.405051\n",
      "Done 3975 batches in 3717.97s\ttraining loss:\t3.404182\n",
      "Done 3980 batches in 3722.60s\ttraining loss:\t3.403427\n",
      "Done 3985 batches in 3727.26s\ttraining loss:\t3.402897\n",
      "Done 3990 batches in 3732.05s\ttraining loss:\t3.402026\n",
      "Done 3995 batches in 3736.19s\ttraining loss:\t3.400692\n",
      "Done 4000 batches in 3740.81s\ttraining loss:\t3.399602\n",
      "Done 4005 batches in 3745.13s\ttraining loss:\t3.398332\n",
      "Done 4010 batches in 3749.87s\ttraining loss:\t3.397106\n",
      "Done 4015 batches in 3754.74s\ttraining loss:\t3.396314\n",
      "Done 4020 batches in 3759.25s\ttraining loss:\t3.395556\n",
      "Done 4025 batches in 3763.65s\ttraining loss:\t3.394837\n",
      "Done 4030 batches in 3768.43s\ttraining loss:\t3.394091\n",
      "Done 4035 batches in 3773.40s\ttraining loss:\t3.393398\n",
      "Done 4040 batches in 3778.18s\ttraining loss:\t3.392294\n",
      "Done 4045 batches in 3782.54s\ttraining loss:\t3.391536\n",
      "Done 4050 batches in 3787.68s\ttraining loss:\t3.390702\n",
      "Done 4055 batches in 3792.70s\ttraining loss:\t3.389474\n",
      "Done 4060 batches in 3797.54s\ttraining loss:\t3.388164\n",
      "Done 4065 batches in 3802.57s\ttraining loss:\t3.387386\n",
      "Done 4070 batches in 3806.96s\ttraining loss:\t3.386246\n",
      "Done 4075 batches in 3811.60s\ttraining loss:\t3.385830\n",
      "Done 4080 batches in 3816.46s\ttraining loss:\t3.385236\n",
      "Done 4085 batches in 3821.18s\ttraining loss:\t3.384539\n",
      "Done 4090 batches in 3825.91s\ttraining loss:\t3.383682\n",
      "Done 4095 batches in 3829.99s\ttraining loss:\t3.382792\n",
      "Done 4100 batches in 3834.36s\ttraining loss:\t3.381432\n",
      "Done 4105 batches in 3838.90s\ttraining loss:\t3.380089\n",
      "Done 4110 batches in 3843.62s\ttraining loss:\t3.379891\n",
      "Done 4115 batches in 3848.28s\ttraining loss:\t3.378736\n",
      "Done 4120 batches in 3852.96s\ttraining loss:\t3.378036\n",
      "Done 4125 batches in 3857.61s\ttraining loss:\t3.376850\n",
      "Done 4130 batches in 3862.65s\ttraining loss:\t3.376005\n",
      "Done 4135 batches in 3867.62s\ttraining loss:\t3.375163\n",
      "Done 4140 batches in 3871.85s\ttraining loss:\t3.374306\n",
      "Done 4145 batches in 3876.62s\ttraining loss:\t3.373660\n",
      "Done 4150 batches in 3881.04s\ttraining loss:\t3.372641\n",
      "Done 4155 batches in 3885.78s\ttraining loss:\t3.372334\n",
      "Done 4160 batches in 3890.42s\ttraining loss:\t3.371391\n",
      "Done 4165 batches in 3895.15s\ttraining loss:\t3.370390\n",
      "Done 4170 batches in 3899.53s\ttraining loss:\t3.369479\n",
      "Done 4175 batches in 3904.33s\ttraining loss:\t3.368918\n",
      "Done 4180 batches in 3908.82s\ttraining loss:\t3.367858\n",
      "Done 4185 batches in 3913.01s\ttraining loss:\t3.367131\n",
      "Done 4190 batches in 3917.61s\ttraining loss:\t3.366149\n",
      "Done 4195 batches in 3921.86s\ttraining loss:\t3.365281\n",
      "Done 4200 batches in 3926.09s\ttraining loss:\t3.364724\n",
      "Done 4205 batches in 3930.94s\ttraining loss:\t3.363619\n",
      "Done 4210 batches in 3935.61s\ttraining loss:\t3.363254\n",
      "Done 4215 batches in 3940.43s\ttraining loss:\t3.362464\n",
      "Done 4220 batches in 3945.25s\ttraining loss:\t3.361421\n",
      "Done 4225 batches in 3949.78s\ttraining loss:\t3.360889\n",
      "Done 4230 batches in 3954.62s\ttraining loss:\t3.360050\n",
      "Done 4235 batches in 3959.32s\ttraining loss:\t3.359253\n",
      "Done 4240 batches in 3964.10s\ttraining loss:\t3.358370\n",
      "Done 4245 batches in 3968.51s\ttraining loss:\t3.357067\n",
      "Done 4250 batches in 3973.20s\ttraining loss:\t3.355339\n",
      "Done 4255 batches in 3978.32s\ttraining loss:\t3.355026\n",
      "Done 4260 batches in 3983.26s\ttraining loss:\t3.354371\n",
      "Done 4265 batches in 3988.22s\ttraining loss:\t3.354139\n",
      "Done 4270 batches in 3992.76s\ttraining loss:\t3.353422\n",
      "Done 4275 batches in 3997.29s\ttraining loss:\t3.353147\n",
      "Done 4280 batches in 4001.72s\ttraining loss:\t3.351904\n",
      "Done 4285 batches in 4006.18s\ttraining loss:\t3.351169\n",
      "Done 4290 batches in 4010.82s\ttraining loss:\t3.350480\n",
      "Done 4295 batches in 4015.04s\ttraining loss:\t3.350311\n",
      "Done 4300 batches in 4020.10s\ttraining loss:\t3.349316\n",
      "Done 4305 batches in 4025.14s\ttraining loss:\t3.348605\n",
      "Done 4310 batches in 4029.66s\ttraining loss:\t3.347442\n",
      "Done 4315 batches in 4034.57s\ttraining loss:\t3.346430\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.3460889364562991"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1ep train set na słowniku z glove\n",
    "\n",
    "# dane są przycięte do długości 300 (jeśli odpowiedź się nie mieści, to pytanie jest usuwane z danych)\n",
    "# przycięto około 1400 próbek, usunięto 119\n",
    "\n",
    "# zanurzenia z glove, unk to średnie słowo, nie trenujemy zanurzeń\n",
    "\n",
    "qa_net.train_one_epoch(data, 20, log_interval=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qa_net.save_params('trained_models/glove_unks/simplified_glove_unks_all_fixed_1ep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QANet tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "1ep all fixed : {\"f1\": 63.93851222573222, \"exact_match\": 53.33964049195837}\n",
    "1ep train unk : {\"f1\": 59.56790117987117, \"exact_match\": 49.10122989593188}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_spans(data, beam=10, batch_size=10):\n",
    "    num_examples = len(data)\n",
    "    \n",
    "    start_probs = qa_net.get_start_probs(data, batch_size)\n",
    "    best_starts = start_probs.argpartition(-beam, axis=1)[:, -beam:].astype(np.int32)\n",
    "    \n",
    "    scores = start_probs[np.arange(num_examples)[:, np.newaxis], best_starts]\n",
    "    scores = np.tile(scores[:, np.newaxis], (beam, 1)).transpose(0, 2, 1)\n",
    "    \n",
    "    best_ends_all = []\n",
    "    for i in xrange(beam):\n",
    "        end_probs = qa_net.get_end_probs(data, best_starts[:, i], batch_size)\n",
    "        best_ends = end_probs.argpartition(-beam, axis=1)[:, -beam:]\n",
    "        scores[:, i, :] *= end_probs[np.arange(num_examples)[:, np.newaxis], best_ends]\n",
    "        best_ends_all.append(best_ends)\n",
    "        \n",
    "    best_ends_all = np.hstack(best_ends_all)\n",
    "        \n",
    "    scores = scores.reshape(num_examples, beam**2)\n",
    "    best_spans = scores.argmax(axis=1)\n",
    "    starts = [i / beam for i in best_spans]\n",
    "    \n",
    "    starts = best_starts[np.arange(num_examples), starts]\n",
    "    ends = best_ends_all[np.arange(num_examples), best_spans]\n",
    "    \n",
    "    return starts, ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 examples\n",
      "2000 examples\n",
      "3000 examples\n",
      "4000 examples\n",
      "5000 examples\n",
      "6000 examples\n",
      "7000 examples\n",
      "8000 examples\n",
      "9000 examples\n",
      "10000 examples\n",
      "Predictions done\n",
      "CPU times: user 13min 27s, sys: 28min 45s, total: 42min 12s\n",
      "Wall time: 6min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predicted_spans = []\n",
    "batch_size = 10\n",
    "\n",
    "idx = 0\n",
    "while idx < len(data_dev):\n",
    "    spans = predict_spans(data_dev[idx:idx + batch_size], beam=1)\n",
    "    predicted_spans.append(np.vstack(spans))\n",
    "    idx += batch_size\n",
    "    if not idx % 1000:\n",
    "        print idx, 'examples'\n",
    "        \n",
    "print 'Predictions done'\n",
    "    \n",
    "predicted_spans = np.hstack(predicted_spans).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.savez('evaluate/glove_vocab/dev_with_glove_vocab_predictions_simplified_all_fixed3_1ep', predicted_spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json, nltk, io, pickle\n",
    "import numpy as np\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with io.open('/pio/data/data/squad/train-v1.1.json', 'r', encoding='utf-8') as f:\n",
    "    train = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with io.open('/pio/data/data/squad/dev-v1.1.json', 'r', encoding='utf-8') as f:\n",
    "    dev = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{u'answer_start': 177, u'text': u'Denver Broncos'},\n",
       " {u'answer_start': 177, u'text': u'Denver Broncos'},\n",
       " {u'answer_start': 177, u'text': u'Denver Broncos'}]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev['data'][0]['paragraphs'][0]['qas'][0]['answers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u\"The State Council declared a three-day period of national mourning for the quake victims starting from May 19 , 2008 ; the PRC 's National Flag and Regional Flags of Hong Kong and Macau Special Administrative Regions flown at half mast\",\n",
       " u'It was the first time that a national mourning period had been declared for something other than the death of a state leader , and many have called it the biggest display of mourning since the death of Mao Zedong',\n",
       " u'At 14:28 CST on May 19 , 2008 , a week after the earthquake , the Chinese public held a moment of silence',\n",
       " u'People stood silent for three minutes while air defense , police and fire sirens , and the horns of vehicles , vessels and trains sounded',\n",
       " u\"Cars and trucks on Beijing 's roads also came to a halt\",\n",
       " u\"People spontaneously burst into cheering `` Zhongguo jiayou ! '' ( Let 's go , China ! ) and `` Sichuan jiayou '' ( Let 's go , Sichuan ! ) afterwards .\"]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(nltk.word_tokenize(train['data'][10]['paragraphs'][60]['context'])).split(' . ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab all the question-answer pairs and create a wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = set()\n",
    "data = []\n",
    "lower = lambda x: x.lower()\n",
    "\n",
    "for par in train['data']:\n",
    "    title = par['title']\n",
    "    \n",
    "    for con in par['paragraphs']:\n",
    "        context = con['context']\n",
    "        context_tok = map(lower, nltk.word_tokenize(context))\n",
    "        words |= set(context_tok)\n",
    "        \n",
    "        for q in con['qas']:\n",
    "            question = q['question']\n",
    "            question_tok = map(lower, nltk.word_tokenize(question))\n",
    "            words |= set(question_tok)\n",
    "            \n",
    "            Id = q['id']\n",
    "            \n",
    "            answers = []\n",
    "            \n",
    "            for ans in q['answers']:\n",
    "                text = ans['text']\n",
    "                text_tok = map(lower, nltk.word_tokenize(text))\n",
    "                ans_start = ans['answer_start']\n",
    "                \n",
    "                answers.append((ans_start, text_tok))\n",
    "                \n",
    "            data.append([answers, question_tok, context_tok])\n",
    "            \n",
    "words.add('<unk>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87599"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-f14e42198858>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'words' is not defined"
     ]
    }
   ],
   "source": [
    "print len(data), len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for d in data:\n",
    "    if len(d[0]) > 1:\n",
    "        print d\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn words into numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_on_dot(s):\n",
    "    res = [[]]\n",
    "    for w in s:\n",
    "        res[-1].append(w)\n",
    "        if w == u'.':\n",
    "            res.append([])\n",
    "    return res if res[-1] else res[:-1]\n",
    "\n",
    "def words_to_num(s):\n",
    "    return map(lambda x: w_to_i.get(x, w_to_i['<unk>']), s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i_to_w = dict(enumerate(words))\n",
    "w_to_i = {v:k for (k,v) in i_to_w.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in xrange(len(data)):\n",
    "    data[i][2] = split_on_dot(data[i][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_num = []\n",
    "\n",
    "for a, q, c in data:\n",
    "    answers = []\n",
    "    for ans in a:\n",
    "        answers.append((ans[0], words_to_num(ans[1])))        \n",
    "    data_num.append([answers, words_to_num(q), map(words_to_num, c)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some answers don't work, because of the tokenizer\n",
    "\n",
    "bugged_answers = 0\n",
    "\n",
    "for ans,_,_ in data_num:\n",
    "    for _,a in ans:\n",
    "        if w_to_i['<unk>'] in a:\n",
    "            bugged_answers += 1\n",
    "bugged_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_num = map(lambda l: [l[0], [l[1]] + l[2]], data_num)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_num = map(lambda l: [map(lambda t: t[1], l[0]), l[1]], data_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1028"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are more broken answers, because I tag words instead of characters\n",
    "\n",
    "k = 0\n",
    "for a, q in data_num:\n",
    "    for w in a[0]:\n",
    "        if w not in list(chain(*q[1:])):\n",
    "            k += 1\n",
    "k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find answer indices on words, not characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inds = []\n",
    "\n",
    "for a, q in data_num:\n",
    "    ans = []\n",
    "    tot_q = list(chain(*q[1:]))\n",
    "    for x in a:\n",
    "        for i in xrange(len(tot_q)):\n",
    "            if x == tot_q[i:i+len(x)]:\n",
    "                ans.append(list(xrange(i, i + len(x))))\n",
    "                break\n",
    "    inds.append(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in xrange(len(data_num)):\n",
    "    data_num[i][0] = inds[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_words = map(lambda x: x[0], sorted(w_to_i.items(), key=lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with io.open('/pio/data/data/squad/wordlist.txt', 'w', encoding='utf-8') as f:\n",
    "    for w in sorted_words:\n",
    "        f.write(unicode(w + '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[102, 103, 104]],\n",
       " [[78406,\n",
       "   50177,\n",
       "   45612,\n",
       "   67711,\n",
       "   87146,\n",
       "   71884,\n",
       "   58619,\n",
       "   29587,\n",
       "   94338,\n",
       "   55795,\n",
       "   94338,\n",
       "   72312,\n",
       "   97133,\n",
       "   83077],\n",
       "  [100780, 44968, 67711, 60695, 608, 43315, 89195, 32610, 492],\n",
       "  [40701,\n",
       "   67711,\n",
       "   45830,\n",
       "   54332,\n",
       "   55792,\n",
       "   78791,\n",
       "   78506,\n",
       "   19554,\n",
       "   43315,\n",
       "   51341,\n",
       "   10820,\n",
       "   23764,\n",
       "   67711,\n",
       "   87146,\n",
       "   71884,\n",
       "   492],\n",
       "  [83032,\n",
       "   94338,\n",
       "   95628,\n",
       "   23764,\n",
       "   67711,\n",
       "   45830,\n",
       "   54332,\n",
       "   49698,\n",
       "   33470,\n",
       "   19557,\n",
       "   44968,\n",
       "   19554,\n",
       "   43315,\n",
       "   85100,\n",
       "   10820,\n",
       "   23764,\n",
       "   99569,\n",
       "   1485,\n",
       "   96317,\n",
       "   37478,\n",
       "   1485,\n",
       "   67711,\n",
       "   78483,\n",
       "   64851,\n",
       "   101002,\n",
       "   14122,\n",
       "   32833,\n",
       "   66547,\n",
       "   77561,\n",
       "   492],\n",
       "  [19445,\n",
       "   78406,\n",
       "   67711,\n",
       "   45830,\n",
       "   54332,\n",
       "   19554,\n",
       "   67711,\n",
       "   32756,\n",
       "   23764,\n",
       "   67711,\n",
       "   7991,\n",
       "   80913,\n",
       "   492],\n",
       "  [83032,\n",
       "   71615,\n",
       "   67711,\n",
       "   32756,\n",
       "   19554,\n",
       "   67711,\n",
       "   16921,\n",
       "   44968,\n",
       "   43315,\n",
       "   50307,\n",
       "   87507,\n",
       "   23764,\n",
       "   55111,\n",
       "   49698,\n",
       "   18814,\n",
       "   492],\n",
       "  [19557,\n",
       "   19554,\n",
       "   43315,\n",
       "   41529,\n",
       "   23764,\n",
       "   67711,\n",
       "   16921,\n",
       "   14138,\n",
       "   72312,\n",
       "   44968,\n",
       "   97133,\n",
       "   39657,\n",
       "   67711,\n",
       "   87146,\n",
       "   71884,\n",
       "   52733,\n",
       "   15575,\n",
       "   78406,\n",
       "   1880,\n",
       "   11294,\n",
       "   70764,\n",
       "   94338,\n",
       "   55795,\n",
       "   492],\n",
       "  [14138,\n",
       "   67711,\n",
       "   22438,\n",
       "   23764,\n",
       "   67711,\n",
       "   45830,\n",
       "   58251,\n",
       "   87309,\n",
       "   49698,\n",
       "   94338,\n",
       "   43315,\n",
       "   9478,\n",
       "   66270,\n",
       "   94548,\n",
       "   44767,\n",
       "   59287,\n",
       "   42821,\n",
       "   9093,\n",
       "   49698,\n",
       "   67711,\n",
       "   78791,\n",
       "   78506,\n",
       "   60426,\n",
       "   44968,\n",
       "   19554,\n",
       "   43315,\n",
       "   85325,\n",
       "   44968,\n",
       "   83589,\n",
       "   91160,\n",
       "   10820,\n",
       "   23764,\n",
       "   71884,\n",
       "   492]]]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_num[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This file has a lot of redundant parts, context is repeated for each question.\n",
    "# It only slows down the initial loading.\n",
    "\n",
    "with open('/pio/data/data/squad/train.pkl', 'w') as f:\n",
    "    pickle.dump(data_num, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.load('/pio/data/data/squad/train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[102, 103, 104]],\n",
       " [[78406,\n",
       "   50177,\n",
       "   45612,\n",
       "   67711,\n",
       "   87146,\n",
       "   71884,\n",
       "   58619,\n",
       "   29587,\n",
       "   94338,\n",
       "   55795,\n",
       "   94338,\n",
       "   72312,\n",
       "   97133,\n",
       "   83077],\n",
       "  [100780, 44968, 67711, 60695, 608, 43315, 89195, 32610, 492],\n",
       "  [40701,\n",
       "   67711,\n",
       "   45830,\n",
       "   54332,\n",
       "   55792,\n",
       "   78791,\n",
       "   78506,\n",
       "   19554,\n",
       "   43315,\n",
       "   51341,\n",
       "   10820,\n",
       "   23764,\n",
       "   67711,\n",
       "   87146,\n",
       "   71884,\n",
       "   492],\n",
       "  [83032,\n",
       "   94338,\n",
       "   95628,\n",
       "   23764,\n",
       "   67711,\n",
       "   45830,\n",
       "   54332,\n",
       "   49698,\n",
       "   33470,\n",
       "   19557,\n",
       "   44968,\n",
       "   19554,\n",
       "   43315,\n",
       "   85100,\n",
       "   10820,\n",
       "   23764,\n",
       "   99569,\n",
       "   1485,\n",
       "   96317,\n",
       "   37478,\n",
       "   1485,\n",
       "   67711,\n",
       "   78483,\n",
       "   64851,\n",
       "   101002,\n",
       "   14122,\n",
       "   32833,\n",
       "   66547,\n",
       "   77561,\n",
       "   492],\n",
       "  [19445,\n",
       "   78406,\n",
       "   67711,\n",
       "   45830,\n",
       "   54332,\n",
       "   19554,\n",
       "   67711,\n",
       "   32756,\n",
       "   23764,\n",
       "   67711,\n",
       "   7991,\n",
       "   80913,\n",
       "   492],\n",
       "  [83032,\n",
       "   71615,\n",
       "   67711,\n",
       "   32756,\n",
       "   19554,\n",
       "   67711,\n",
       "   16921,\n",
       "   44968,\n",
       "   43315,\n",
       "   50307,\n",
       "   87507,\n",
       "   23764,\n",
       "   55111,\n",
       "   49698,\n",
       "   18814,\n",
       "   492],\n",
       "  [19557,\n",
       "   19554,\n",
       "   43315,\n",
       "   41529,\n",
       "   23764,\n",
       "   67711,\n",
       "   16921,\n",
       "   14138,\n",
       "   72312,\n",
       "   44968,\n",
       "   97133,\n",
       "   39657,\n",
       "   67711,\n",
       "   87146,\n",
       "   71884,\n",
       "   52733,\n",
       "   15575,\n",
       "   78406,\n",
       "   1880,\n",
       "   11294,\n",
       "   70764,\n",
       "   94338,\n",
       "   55795,\n",
       "   492],\n",
       "  [14138,\n",
       "   67711,\n",
       "   22438,\n",
       "   23764,\n",
       "   67711,\n",
       "   45830,\n",
       "   58251,\n",
       "   87309,\n",
       "   49698,\n",
       "   94338,\n",
       "   43315,\n",
       "   9478,\n",
       "   66270,\n",
       "   94548,\n",
       "   44767,\n",
       "   59287,\n",
       "   42821,\n",
       "   9093,\n",
       "   49698,\n",
       "   67711,\n",
       "   78791,\n",
       "   78506,\n",
       "   60426,\n",
       "   44968,\n",
       "   19554,\n",
       "   43315,\n",
       "   85325,\n",
       "   44968,\n",
       "   83589,\n",
       "   91160,\n",
       "   10820,\n",
       "   23764,\n",
       "   71884,\n",
       "   492]]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87599"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w_to_i = {}\n",
    "idx = 0\n",
    "\n",
    "with io.open('/pio/data/data/squad/wordlist.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        w_to_i[line[:-1]] = idx\n",
    "        idx += 1\n",
    "        \n",
    "i_to_w = {v:k for (k,v) in w_to_i.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'it'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_to_w[19557]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lens = np.array(map(lambda x: len(x[1]), data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def show_data(idx):\n",
    "    for s in data[idx][1]:\n",
    "        print ' '.join([i_to_w[x] for x in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is the largest hottest continuously large area worldwide ?\n",
      "the sky is usually clear above the desert and the sunshine duration is extremely high everywhere in the sahara .\n",
      "most of the desert enjoys more than 3,600 h of bright sunshine annually or over 82 % of the time and a wide area in the eastern part experiences in excess of 4,000 h of bright sunshine a year or over 91 % of the time , and the highest values are very close to the theoretical maximum value .\n",
      "a value of 4,300 h or 98 % of the time would be recorded in upper egypt ( aswan , luxor ) and in the nubian desert ( wadi halfa ) .\n",
      "the annual average direct solar irradiation is around 2,800 kwh/ ( m2 year ) in the great desert .\n",
      "the sahara has a huge potential for solar energy production .\n",
      "the constantly high position of the sun , the extremely low relative humidity , the lack of vegetation and rainfall make the great desert the hottest continuously large area worldwide and certainly the hottest place on earth during summertime in some spots .\n",
      "the average high temperature exceeds 38 °c ( 100.4 °f ) - 40 °c ( 104 °f ) during the hottest month nearly everywhere in the desert except at very high mountainous areas .\n",
      "the highest officially recorded average high temperature was 47 °c ( 116.6 °f ) in a remote desert town in the algerian desert called bou bernous with an elevation of 378 meters above sea level .\n",
      "it 's the world 's highest recorded average high temperature and only death valley , california rivals it .\n",
      "other hot spots in algeria such as adrar , timimoun , in salah , ouallene , aoulef , reggane with an elevation between 200 and 400 meters above sea level get slightly lower summer average highs around 46 °c ( 114.8 °f ) during the hottest months of the year .\n",
      "salah , well known in algeria for its extreme heat , has an average high temperature of 43.8 °c ( 110.8 °f ) , 46.4 °c ( 115.5 °f ) , 45.5 ( 113.9 °f ) .\n",
      "furthermore , 41.9 °c ( 107.4 °f ) in june , july , august and september .\n",
      "in fact , there are even hotter spots in the sahara , but they are located in extremely remote areas , especially in the azalai , lying in northern mali .\n",
      "the major part of the desert experiences around 3 – 5 months when the average high strictly exceeds 40 °c ( 104 °f ) .\n",
      "the southern central part of the desert experiences up to 6 – 7 months when the average high temperature strictly exceeds 40 °c ( 104 °f ) which shows the constancy and the length of the really hot season in the sahara .\n",
      "some examples of this are bilma , niger and faya-largeau , chad .\n",
      "the annual average daily temperature exceeds 20 °c ( 68 °f ) everywhere and can approach 30 °c ( 86 °f ) in the hottest regions year-round .\n",
      "however , most of the desert has a value in excess of 25 °c ( 77 °f ) .\n",
      "the sand and ground temperatures are even more extreme .\n",
      "during daytime , the sand temperature is extremely high as it can easily reach 80 °c ( 176 °f ) or more .\n",
      "a sand temperature of 83.5 °c ( 182.3 °f ) has been recorded in port sudan .\n",
      "ground temperatures of 72 °c ( 161.6 °f ) have been recorded in the adrar of mauritania and a value of 75 °c ( 167 °f ) has been measured in borkou , northern chad .\n",
      "due to lack of cloud cover and very low humidity , the desert usually features high diurnal temperature variations between days and nights .\n",
      "however , it 's a myth that the nights are cold after extremely hot days in the sahara .\n",
      "the average diurnal temperature range is typically between 13 °c ( 55.4 °f ) and 20 °c ( 68 °f ) .\n",
      "the lowest values are found along the coastal regions due to high humidity and are often even lower than 10 °c ( 50 °f ) , while the highest values are found in inland desert areas where the humidity is the lowest , mainly in the southern sahara .\n",
      "still , it 's true that winter nights can be cold as it can drop to the freezing point and even below , especially in high-elevation areas .\n"
     ]
    }
   ],
   "source": [
    "show_data(60023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([    0,     0,  1665,  6015, 12958, 18663, 16891, 12176,  7727,\n",
       "        4709,  2711,  1677,   859,   636,   352,   223,   166,    60,\n",
       "          39,    24,     9,     0,     5,    19,     5,     0,     0,\n",
       "           0,    10])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print max(lens)\n",
    "np.bincount(lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified HRED test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: GeForce GTX 780 (CNMeM is enabled with initial size: 30.0% of memory, cuDNN 5105)\n"
     ]
    }
   ],
   "source": [
    "from HRED_v2 import HRED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.load('/pio/data/data/squad/train.pkl')\n",
    "w_to_i = {}\n",
    "idx = 0\n",
    "\n",
    "with io.open('/pio/data/data/squad/wordlist.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        w_to_i[line[:-1]] = idx\n",
    "        idx += 1\n",
    "        \n",
    "i_to_w = {v:k for (k,v) in w_to_i.items()}\n",
    "\n",
    "w_to_i['<pad_value>'] = -1\n",
    "i_to_w[-1] = '<pad_value>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "voc_size = len(w_to_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model...\n",
      "Compiling theano functions...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "hred_net = HRED(voc_size=voc_size,\n",
    "                emb_size=300,\n",
    "                lv1_rec_size=300, \n",
    "                lv2_rec_size=300, \n",
    "                out_emb_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 100 batches in 6.13s\ttraining loss:\t0.010142\n",
      "Done 200 batches in 13.17s\ttraining loss:\t0.005333\n",
      "Done 300 batches in 20.51s\ttraining loss:\t0.003530\n",
      "Done 400 batches in 27.01s\ttraining loss:\t0.002560\n",
      "Done 500 batches in 34.08s\ttraining loss:\t0.001999\n",
      "Done 600 batches in 40.29s\ttraining loss:\t0.001697\n",
      "Done 700 batches in 47.30s\ttraining loss:\t0.001468\n",
      "Done 800 batches in 53.77s\ttraining loss:\t0.001322\n",
      "Done 900 batches in 61.37s\ttraining loss:\t0.001182\n",
      "Done 1000 batches in 67.53s\ttraining loss:\t0.001084\n",
      "Done 1100 batches in 74.21s\ttraining loss:\t0.000988\n",
      "Done 1200 batches in 81.73s\ttraining loss:\t0.000907\n",
      "Done 1300 batches in 89.97s\ttraining loss:\t0.000848\n",
      "Done 1400 batches in 97.03s\ttraining loss:\t0.000790\n",
      "Done 1500 batches in 103.61s\ttraining loss:\t0.000743\n",
      "Done 1600 batches in 109.75s\ttraining loss:\t0.000706\n",
      "Done 1700 batches in 116.80s\ttraining loss:\t0.000668\n",
      "Done 1800 batches in 124.28s\ttraining loss:\t0.000624\n",
      "Done 1900 batches in 132.12s\ttraining loss:\t0.000592\n",
      "Done 2000 batches in 137.93s\ttraining loss:\t0.000576\n",
      "Done 2100 batches in 143.94s\ttraining loss:\t0.000556\n",
      "Done 2200 batches in 151.05s\ttraining loss:\t0.000535\n",
      "Done 2300 batches in 158.49s\ttraining loss:\t0.000514\n",
      "Done 2400 batches in 165.55s\ttraining loss:\t0.000493\n",
      "Done 2500 batches in 172.23s\ttraining loss:\t0.000474\n",
      "Done 2600 batches in 178.24s\ttraining loss:\t0.000459\n",
      "Done 2700 batches in 184.21s\ttraining loss:\t0.000446\n",
      "Done 2800 batches in 190.23s\ttraining loss:\t0.000435\n",
      "Done 2900 batches in 196.80s\ttraining loss:\t0.000420\n",
      "Done 3000 batches in 202.82s\ttraining loss:\t0.000406\n",
      "Done 3100 batches in 209.13s\ttraining loss:\t0.000394\n",
      "Done 3200 batches in 215.64s\ttraining loss:\t0.000384\n",
      "Done 3300 batches in 222.33s\ttraining loss:\t0.000373\n",
      "Done 3400 batches in 229.19s\ttraining loss:\t0.000363\n",
      "Done 3500 batches in 234.49s\ttraining loss:\t0.000356\n",
      "Done 3600 batches in 240.42s\ttraining loss:\t0.000350\n",
      "Done 3700 batches in 246.84s\ttraining loss:\t0.000342\n",
      "Done 3800 batches in 253.60s\ttraining loss:\t0.000335\n",
      "Done 3900 batches in 260.34s\ttraining loss:\t0.000328\n",
      "Done 4000 batches in 266.86s\ttraining loss:\t0.000322\n",
      "Done 4100 batches in 273.73s\ttraining loss:\t0.000315\n",
      "Done 4200 batches in 281.33s\ttraining loss:\t0.000308\n",
      "Done 4300 batches in 289.37s\ttraining loss:\t0.000301\n",
      "Done 4400 batches in 295.17s\ttraining loss:\t0.000296\n",
      "Done 4500 batches in 301.59s\ttraining loss:\t0.000290\n",
      "Done 4600 batches in 308.09s\ttraining loss:\t0.000284\n",
      "Done 4700 batches in 314.11s\ttraining loss:\t0.000278\n",
      "Done 4800 batches in 321.25s\ttraining loss:\t0.000271\n",
      "Done 4900 batches in 327.98s\ttraining loss:\t0.000265\n",
      "Done 5000 batches in 333.94s\ttraining loss:\t0.000260\n",
      "Done 5100 batches in 339.46s\ttraining loss:\t0.000256\n",
      "Done 5200 batches in 346.30s\ttraining loss:\t0.000252\n",
      "Done 5300 batches in 353.01s\ttraining loss:\t0.000247\n",
      "Done 5400 batches in 360.00s\ttraining loss:\t0.000243\n",
      "Done 5500 batches in 366.62s\ttraining loss:\t0.000239\n",
      "Done 5600 batches in 373.09s\ttraining loss:\t0.000235\n",
      "Done 5700 batches in 380.74s\ttraining loss:\t0.000232\n",
      "Done 5800 batches in 387.92s\ttraining loss:\t0.000228\n",
      "Done 5900 batches in 395.07s\ttraining loss:\t0.000224\n",
      "Done 6000 batches in 401.39s\ttraining loss:\t0.000220\n",
      "Done 6100 batches in 408.59s\ttraining loss:\t0.000215\n",
      "Done 6200 batches in 415.48s\ttraining loss:\t0.000211\n",
      "Done 6300 batches in 422.11s\ttraining loss:\t0.000207\n",
      "Done 6400 batches in 429.35s\ttraining loss:\t0.000204\n",
      "Done 6500 batches in 436.16s\ttraining loss:\t0.000201\n",
      "Done 6600 batches in 442.07s\ttraining loss:\t0.000199\n",
      "Done 6700 batches in 448.11s\ttraining loss:\t0.000197\n",
      "Done 6800 batches in 454.70s\ttraining loss:\t0.000195\n",
      "Done 6900 batches in 460.89s\ttraining loss:\t0.000193\n",
      "Done 7000 batches in 467.35s\ttraining loss:\t0.000191\n",
      "Done 7100 batches in 473.98s\ttraining loss:\t0.000188\n",
      "Done 7200 batches in 479.93s\ttraining loss:\t0.000186\n",
      "Done 7300 batches in 485.68s\ttraining loss:\t0.000184\n",
      "Done 7400 batches in 491.32s\ttraining loss:\t0.000183\n",
      "Done 7500 batches in 496.26s\ttraining loss:\t0.000181\n",
      "Done 7600 batches in 502.53s\ttraining loss:\t0.000179\n",
      "Done 7700 batches in 510.14s\ttraining loss:\t0.000177\n",
      "Done 7800 batches in 516.09s\ttraining loss:\t0.000175\n",
      "Done 7900 batches in 522.31s\ttraining loss:\t0.000173\n",
      "Done 8000 batches in 529.40s\ttraining loss:\t0.000171\n",
      "Done 8100 batches in 535.55s\ttraining loss:\t0.000169\n",
      "Done 8200 batches in 541.38s\ttraining loss:\t0.000168\n",
      "Done 8300 batches in 547.76s\ttraining loss:\t0.000165\n",
      "Done 8400 batches in 554.50s\ttraining loss:\t0.000163\n",
      "Done 8500 batches in 560.80s\ttraining loss:\t0.000160\n",
      "Done 8600 batches in 568.21s\ttraining loss:\t0.000158\n",
      "Done 8700 batches in 575.46s\ttraining loss:\t0.000156\n",
      "Done 8800 batches in 581.95s\ttraining loss:\t0.000154\n",
      "Done 8900 batches in 589.31s\ttraining loss:\t0.000152\n",
      "Done 9000 batches in 596.01s\ttraining loss:\t0.000151\n",
      "Done 9100 batches in 602.51s\ttraining loss:\t0.000149\n",
      "Done 9200 batches in 609.77s\ttraining loss:\t0.000147\n",
      "Done 9300 batches in 617.75s\ttraining loss:\t0.000146\n",
      "Done 9400 batches in 625.14s\ttraining loss:\t0.000144\n",
      "Done 9500 batches in 631.23s\ttraining loss:\t0.000143\n",
      "Done 9600 batches in 636.74s\ttraining loss:\t0.000142\n",
      "Done 9700 batches in 642.51s\ttraining loss:\t0.000141\n",
      "Done 9800 batches in 648.05s\ttraining loss:\t0.000140\n",
      "Done 9900 batches in 654.53s\ttraining loss:\t0.000139\n",
      "Done 10000 batches in 660.15s\ttraining loss:\t0.000138\n",
      "Done 10100 batches in 666.29s\ttraining loss:\t0.000137\n",
      "Done 10200 batches in 672.16s\ttraining loss:\t0.000136\n",
      "Done 10300 batches in 678.67s\ttraining loss:\t0.000135\n",
      "Done 10400 batches in 685.97s\ttraining loss:\t0.000134\n",
      "Done 10500 batches in 693.18s\ttraining loss:\t0.000133\n",
      "Done 10600 batches in 699.26s\ttraining loss:\t0.000131\n",
      "Done 10700 batches in 704.68s\ttraining loss:\t0.000130\n",
      "Done 10800 batches in 710.95s\ttraining loss:\t0.000129\n",
      "Done 10900 batches in 717.34s\ttraining loss:\t0.000129\n",
      "Done 11000 batches in 723.48s\ttraining loss:\t0.000128\n",
      "Done 11100 batches in 729.70s\ttraining loss:\t0.000127\n",
      "Done 11200 batches in 735.63s\ttraining loss:\t0.000126\n",
      "Done 11300 batches in 742.06s\ttraining loss:\t0.000125\n",
      "Done 11400 batches in 748.54s\ttraining loss:\t0.000124\n",
      "Done 11500 batches in 754.85s\ttraining loss:\t0.000123\n",
      "Done 11600 batches in 761.39s\ttraining loss:\t0.000122\n",
      "Done 11700 batches in 767.49s\ttraining loss:\t0.000121\n",
      "Done 11800 batches in 773.24s\ttraining loss:\t0.000120\n",
      "Done 11900 batches in 778.73s\ttraining loss:\t0.000119\n",
      "Done 12000 batches in 785.19s\ttraining loss:\t0.000119\n",
      "Done 12100 batches in 791.96s\ttraining loss:\t0.000118\n",
      "Done 12200 batches in 798.46s\ttraining loss:\t0.000117\n",
      "Done 12300 batches in 805.26s\ttraining loss:\t0.000116\n",
      "Done 12400 batches in 811.55s\ttraining loss:\t0.000115\n",
      "Done 12500 batches in 818.08s\ttraining loss:\t0.000114\n",
      "Done 12600 batches in 824.31s\ttraining loss:\t0.000113\n",
      "Done 12700 batches in 830.76s\ttraining loss:\t0.000113\n",
      "Done 12800 batches in 836.78s\ttraining loss:\t0.000112\n",
      "Done 12900 batches in 842.97s\ttraining loss:\t0.000111\n",
      "Done 13000 batches in 849.22s\ttraining loss:\t0.000111\n",
      "Done 13100 batches in 855.13s\ttraining loss:\t0.000110\n",
      "Done 13200 batches in 861.32s\ttraining loss:\t0.000110\n",
      "Done 13300 batches in 867.63s\ttraining loss:\t0.000109\n",
      "Done 13400 batches in 873.55s\ttraining loss:\t0.000108\n",
      "Done 13500 batches in 879.44s\ttraining loss:\t0.000108\n",
      "Done 13600 batches in 886.13s\ttraining loss:\t0.000107\n",
      "Done 13700 batches in 893.02s\ttraining loss:\t0.000106\n",
      "Done 13800 batches in 899.80s\ttraining loss:\t0.000105\n",
      "Done 13900 batches in 907.07s\ttraining loss:\t0.000104\n",
      "Done 14000 batches in 913.08s\ttraining loss:\t0.000104\n",
      "Done 14100 batches in 919.28s\ttraining loss:\t0.000103\n",
      "Done 14200 batches in 925.53s\ttraining loss:\t0.000103\n",
      "Done 14300 batches in 932.13s\ttraining loss:\t0.000102\n",
      "Done 14400 batches in 938.00s\ttraining loss:\t0.000102\n",
      "Done 14500 batches in 943.17s\ttraining loss:\t0.000101\n",
      "Done 14600 batches in 949.05s\ttraining loss:\t0.000101\n",
      "Done 14700 batches in 955.01s\ttraining loss:\t0.000100\n",
      "Done 14800 batches in 960.85s\ttraining loss:\t0.000100\n",
      "Done 14900 batches in 967.41s\ttraining loss:\t0.000099\n",
      "Done 15000 batches in 974.59s\ttraining loss:\t0.000098\n",
      "Done 15100 batches in 981.08s\ttraining loss:\t0.000098\n",
      "Done 15200 batches in 987.54s\ttraining loss:\t0.000097\n",
      "Done 15300 batches in 993.00s\ttraining loss:\t0.000097\n",
      "Done 15400 batches in 1000.32s\ttraining loss:\t0.000096\n",
      "Done 15500 batches in 1005.74s\ttraining loss:\t0.000096\n",
      "Done 15600 batches in 1011.42s\ttraining loss:\t0.000095\n",
      "Done 15700 batches in 1017.47s\ttraining loss:\t0.000095\n",
      "Done 15800 batches in 1023.57s\ttraining loss:\t0.000095\n",
      "Done 15900 batches in 1029.03s\ttraining loss:\t0.000094\n",
      "Done 16000 batches in 1034.93s\ttraining loss:\t0.000094\n",
      "Done 16100 batches in 1040.74s\ttraining loss:\t0.000093\n",
      "Done 16200 batches in 1047.39s\ttraining loss:\t0.000093\n",
      "Done 16300 batches in 1053.72s\ttraining loss:\t0.000092\n",
      "Done 16400 batches in 1059.82s\ttraining loss:\t0.000092\n",
      "Done 16500 batches in 1066.89s\ttraining loss:\t0.000091\n",
      "Done 16600 batches in 1073.93s\ttraining loss:\t0.000091\n",
      "Done 16700 batches in 1080.47s\ttraining loss:\t0.000090\n",
      "Done 16800 batches in 1086.50s\ttraining loss:\t0.000090\n",
      "Done 16900 batches in 1093.72s\ttraining loss:\t0.000089\n",
      "Done 17000 batches in 1100.16s\ttraining loss:\t0.000089\n",
      "Done 17100 batches in 1106.51s\ttraining loss:\t0.000088\n",
      "Done 17200 batches in 1112.31s\ttraining loss:\t0.000088\n",
      "Done 17300 batches in 1118.82s\ttraining loss:\t0.000088\n",
      "Done 17400 batches in 1126.30s\ttraining loss:\t0.000087\n",
      "Done 17500 batches in 1132.77s\ttraining loss:\t0.000087\n",
      "Done 17600 batches in 1139.26s\ttraining loss:\t0.000086\n",
      "Done 17700 batches in 1145.64s\ttraining loss:\t0.000086\n",
      "Done 17800 batches in 1151.33s\ttraining loss:\t0.000086\n",
      "Done 17900 batches in 1157.37s\ttraining loss:\t0.000085\n",
      "Done 18000 batches in 1163.82s\ttraining loss:\t0.000085\n",
      "Done 18100 batches in 1169.55s\ttraining loss:\t0.000085\n",
      "Done 18200 batches in 1176.77s\ttraining loss:\t0.000084\n",
      "Done 18300 batches in 1183.45s\ttraining loss:\t0.000084\n",
      "Done 18400 batches in 1191.24s\ttraining loss:\t0.000083\n",
      "Done 18500 batches in 1198.37s\ttraining loss:\t0.000083\n",
      "Done 18600 batches in 1204.65s\ttraining loss:\t0.000082\n",
      "Done 18700 batches in 1211.50s\ttraining loss:\t0.000082\n",
      "Done 18800 batches in 1218.02s\ttraining loss:\t0.000082\n",
      "Done 18900 batches in 1224.88s\ttraining loss:\t0.000081\n",
      "Done 19000 batches in 1230.34s\ttraining loss:\t0.000081\n",
      "Done 19100 batches in 1236.54s\ttraining loss:\t0.000080\n",
      "Done 19200 batches in 1243.04s\ttraining loss:\t0.000080\n",
      "Done 19300 batches in 1248.81s\ttraining loss:\t0.000080\n",
      "Done 19400 batches in 1254.96s\ttraining loss:\t0.000079\n",
      "Done 19500 batches in 1260.91s\ttraining loss:\t0.000079\n",
      "Done 19600 batches in 1267.22s\ttraining loss:\t0.000079\n",
      "Done 19700 batches in 1273.37s\ttraining loss:\t0.000078\n",
      "Done 19800 batches in 1280.46s\ttraining loss:\t0.000078\n",
      "Done 19900 batches in 1286.63s\ttraining loss:\t0.000077\n",
      "Done 20000 batches in 1293.26s\ttraining loss:\t0.000077\n",
      "Done 20100 batches in 1299.10s\ttraining loss:\t0.000077\n",
      "Done 20200 batches in 1304.50s\ttraining loss:\t0.000076\n",
      "Done 20300 batches in 1311.40s\ttraining loss:\t0.000076\n",
      "Done 20400 batches in 1318.28s\ttraining loss:\t0.000076\n",
      "Done 20500 batches in 1324.65s\ttraining loss:\t0.000075\n",
      "Done 20600 batches in 1331.66s\ttraining loss:\t0.000075\n",
      "Done 20700 batches in 1337.84s\ttraining loss:\t0.000075\n",
      "Done 20800 batches in 1343.98s\ttraining loss:\t0.000074\n",
      "Done 20900 batches in 1350.18s\ttraining loss:\t0.000074\n",
      "Done 21000 batches in 1356.51s\ttraining loss:\t0.000074\n",
      "Done 21100 batches in 1361.90s\ttraining loss:\t0.000073\n",
      "Done 21200 batches in 1367.86s\ttraining loss:\t0.000073\n",
      "Done 21300 batches in 1373.25s\ttraining loss:\t0.000073\n",
      "Done 21400 batches in 1378.23s\ttraining loss:\t0.000072\n",
      "Done 21500 batches in 1383.42s\ttraining loss:\t0.000072\n",
      "Done 21600 batches in 1388.97s\ttraining loss:\t0.000072\n",
      "Done 21700 batches in 1394.96s\ttraining loss:\t0.000072\n",
      "Done 21800 batches in 1401.19s\ttraining loss:\t0.000071\n",
      "Done 21900 batches in 1407.70s\ttraining loss:\t0.000071\n",
      "Done 22000 batches in 1413.77s\ttraining loss:\t0.000071\n",
      "Done 22100 batches in 1419.59s\ttraining loss:\t0.000070\n",
      "Done 22200 batches in 1426.40s\ttraining loss:\t0.000070\n",
      "Done 22300 batches in 1432.33s\ttraining loss:\t0.000070\n",
      "Done 22400 batches in 1439.13s\ttraining loss:\t0.000069\n",
      "Done 22500 batches in 1445.10s\ttraining loss:\t0.000069\n",
      "Done 22600 batches in 1451.58s\ttraining loss:\t0.000069\n",
      "Done 22700 batches in 1457.71s\ttraining loss:\t0.000069\n",
      "Done 22800 batches in 1463.81s\ttraining loss:\t0.000068\n",
      "Done 22900 batches in 1469.40s\ttraining loss:\t0.000068\n",
      "Done 23000 batches in 1476.01s\ttraining loss:\t0.000068\n",
      "Done 23100 batches in 1482.99s\ttraining loss:\t0.000068\n",
      "Done 23200 batches in 1488.32s\ttraining loss:\t0.000067\n",
      "Done 23300 batches in 1494.14s\ttraining loss:\t0.000067\n",
      "Done 23400 batches in 1500.63s\ttraining loss:\t0.000067\n",
      "Done 23500 batches in 1507.20s\ttraining loss:\t0.000066\n",
      "Done 23600 batches in 1513.47s\ttraining loss:\t0.000066\n",
      "Done 23700 batches in 1520.41s\ttraining loss:\t0.000065\n",
      "Done 23800 batches in 1526.94s\ttraining loss:\t0.000065\n",
      "Done 23900 batches in 1532.52s\ttraining loss:\t0.000065\n",
      "Done 24000 batches in 1538.29s\ttraining loss:\t0.000065\n",
      "Done 24100 batches in 1544.00s\ttraining loss:\t0.000065\n",
      "Done 24200 batches in 1550.28s\ttraining loss:\t0.000064\n",
      "Done 24300 batches in 1557.61s\ttraining loss:\t0.000064\n",
      "Done 24400 batches in 1564.16s\ttraining loss:\t0.000064\n",
      "Done 24500 batches in 1569.91s\ttraining loss:\t0.000064\n",
      "Done 24600 batches in 1576.17s\ttraining loss:\t0.000063\n",
      "Done 24700 batches in 1581.81s\ttraining loss:\t0.000063\n",
      "Done 24800 batches in 1588.11s\ttraining loss:\t0.000063\n",
      "Done 24900 batches in 1594.79s\ttraining loss:\t0.000063\n",
      "Done 25000 batches in 1600.50s\ttraining loss:\t0.000062\n",
      "Done 25100 batches in 1606.36s\ttraining loss:\t0.000062\n",
      "Done 25200 batches in 1612.27s\ttraining loss:\t0.000062\n",
      "Done 25300 batches in 1618.95s\ttraining loss:\t0.000062\n",
      "Done 25400 batches in 1625.31s\ttraining loss:\t0.000062\n",
      "Done 25500 batches in 1632.03s\ttraining loss:\t0.000061\n",
      "Done 25600 batches in 1637.92s\ttraining loss:\t0.000061\n",
      "Done 25700 batches in 1644.10s\ttraining loss:\t0.000061\n",
      "Done 25800 batches in 1651.97s\ttraining loss:\t0.000061\n",
      "Done 25900 batches in 1658.93s\ttraining loss:\t0.000060\n",
      "Done 26000 batches in 1666.13s\ttraining loss:\t0.000060\n",
      "Done 26100 batches in 1672.86s\ttraining loss:\t0.000060\n",
      "Done 26200 batches in 1679.38s\ttraining loss:\t0.000059\n",
      "Done 26300 batches in 1686.04s\ttraining loss:\t0.000059\n",
      "Done 26400 batches in 1692.07s\ttraining loss:\t0.000059\n",
      "Done 26500 batches in 1698.12s\ttraining loss:\t0.000059\n",
      "Done 26600 batches in 1704.54s\ttraining loss:\t0.000058\n",
      "Done 26700 batches in 1710.91s\ttraining loss:\t0.000058\n",
      "Done 26800 batches in 1716.88s\ttraining loss:\t0.000058\n",
      "Done 26900 batches in 1724.47s\ttraining loss:\t0.000058\n",
      "Done 27000 batches in 1730.88s\ttraining loss:\t0.000057\n",
      "Done 27100 batches in 1737.25s\ttraining loss:\t0.000057\n",
      "Done 27200 batches in 1744.29s\ttraining loss:\t0.000057\n",
      "Done 27300 batches in 1751.53s\ttraining loss:\t0.000057\n",
      "Done 27400 batches in 1757.77s\ttraining loss:\t0.000056\n",
      "Done 27500 batches in 1764.32s\ttraining loss:\t0.000056\n",
      "Done 27600 batches in 1770.84s\ttraining loss:\t0.000056\n",
      "Done 27700 batches in 1778.38s\ttraining loss:\t0.000056\n",
      "Done 27800 batches in 1785.09s\ttraining loss:\t0.000055\n",
      "Done 27900 batches in 1793.28s\ttraining loss:\t0.000055\n",
      "Done 28000 batches in 1799.94s\ttraining loss:\t0.000055\n",
      "Done 28100 batches in 1806.58s\ttraining loss:\t0.000055\n",
      "Done 28200 batches in 1812.90s\ttraining loss:\t0.000054\n",
      "Done 28300 batches in 1819.95s\ttraining loss:\t0.000054\n",
      "Done 28400 batches in 1826.50s\ttraining loss:\t0.000054\n",
      "Done 28500 batches in 1832.75s\ttraining loss:\t0.000054\n",
      "Done 28600 batches in 1839.24s\ttraining loss:\t0.000054\n",
      "Done 28700 batches in 1845.51s\ttraining loss:\t0.000053\n",
      "Done 28800 batches in 1852.21s\ttraining loss:\t0.000053\n",
      "Done 28900 batches in 1858.65s\ttraining loss:\t0.000053\n",
      "Done 29000 batches in 1865.71s\ttraining loss:\t0.000053\n",
      "Done 29100 batches in 1872.05s\ttraining loss:\t0.000052\n",
      "Done 29200 batches in 1878.76s\ttraining loss:\t0.000052\n",
      "Done 29300 batches in 1885.15s\ttraining loss:\t0.000052\n",
      "Done 29400 batches in 1891.99s\ttraining loss:\t0.000052\n",
      "Done 29500 batches in 1899.45s\ttraining loss:\t0.000052\n",
      "Done 29600 batches in 1906.10s\ttraining loss:\t0.000051\n",
      "Done 29700 batches in 1913.03s\ttraining loss:\t0.000051\n",
      "Done 29800 batches in 1919.86s\ttraining loss:\t0.000051\n",
      "Done 29900 batches in 1928.50s\ttraining loss:\t0.000051\n",
      "Done 30000 batches in 1935.11s\ttraining loss:\t0.000050\n",
      "Done 30100 batches in 1941.06s\ttraining loss:\t0.000050\n",
      "Done 30200 batches in 1947.50s\ttraining loss:\t0.000050\n",
      "Done 30300 batches in 1954.19s\ttraining loss:\t0.000050\n",
      "Done 30400 batches in 1960.27s\ttraining loss:\t0.000050\n",
      "Done 30500 batches in 1967.72s\ttraining loss:\t0.000050\n",
      "Done 30600 batches in 1974.43s\ttraining loss:\t0.000049\n",
      "Done 30700 batches in 1980.80s\ttraining loss:\t0.000049\n",
      "Done 30800 batches in 1987.61s\ttraining loss:\t0.000049\n",
      "Done 30900 batches in 1993.95s\ttraining loss:\t0.000049\n",
      "Done 31000 batches in 2000.20s\ttraining loss:\t0.000048\n",
      "Done 31100 batches in 2006.71s\ttraining loss:\t0.000048\n",
      "Done 31200 batches in 2013.87s\ttraining loss:\t0.000048\n",
      "Done 31300 batches in 2021.23s\ttraining loss:\t0.000048\n",
      "Done 31400 batches in 2028.28s\ttraining loss:\t0.000048\n",
      "Done 31500 batches in 2035.14s\ttraining loss:\t0.000048\n",
      "Done 31600 batches in 2042.60s\ttraining loss:\t0.000047\n",
      "Done 31700 batches in 2049.78s\ttraining loss:\t0.000047\n",
      "Done 31800 batches in 2056.71s\ttraining loss:\t0.000047\n",
      "Done 31900 batches in 2063.31s\ttraining loss:\t0.000047\n",
      "Done 32000 batches in 2070.16s\ttraining loss:\t0.000047\n",
      "Done 32100 batches in 2077.03s\ttraining loss:\t0.000046\n",
      "Done 32200 batches in 2083.42s\ttraining loss:\t0.000046\n",
      "Done 32300 batches in 2090.33s\ttraining loss:\t0.000046\n",
      "Done 32400 batches in 2096.65s\ttraining loss:\t0.000046\n",
      "Done 32500 batches in 2103.28s\ttraining loss:\t0.000046\n",
      "Done 32600 batches in 2109.13s\ttraining loss:\t0.000046\n",
      "Done 32700 batches in 2116.08s\ttraining loss:\t0.000045\n",
      "Done 32800 batches in 2124.22s\ttraining loss:\t0.000045\n",
      "Done 32900 batches in 2131.66s\ttraining loss:\t0.000045\n",
      "Done 33000 batches in 2138.51s\ttraining loss:\t0.000045\n",
      "Done 33100 batches in 2145.34s\ttraining loss:\t0.000045\n",
      "Done 33200 batches in 2151.99s\ttraining loss:\t0.000045\n",
      "Done 33300 batches in 2158.55s\ttraining loss:\t0.000044\n",
      "Done 33400 batches in 2164.75s\ttraining loss:\t0.000044\n",
      "Done 33500 batches in 2171.09s\ttraining loss:\t0.000044\n",
      "Done 33600 batches in 2177.24s\ttraining loss:\t0.000044\n",
      "Done 33700 batches in 2184.51s\ttraining loss:\t0.000044\n",
      "Done 33800 batches in 2190.93s\ttraining loss:\t0.000044\n",
      "Done 33900 batches in 2197.91s\ttraining loss:\t0.000044\n",
      "Done 34000 batches in 2204.37s\ttraining loss:\t0.000043\n",
      "Done 34100 batches in 2211.28s\ttraining loss:\t0.000043\n",
      "Done 34200 batches in 2217.83s\ttraining loss:\t0.000043\n",
      "Done 34300 batches in 2224.03s\ttraining loss:\t0.000043\n",
      "Done 34400 batches in 2230.66s\ttraining loss:\t0.000043\n",
      "Done 34500 batches in 2237.53s\ttraining loss:\t0.000043\n",
      "Done 34600 batches in 2243.94s\ttraining loss:\t0.000043\n",
      "Done 34700 batches in 2251.10s\ttraining loss:\t0.000042\n",
      "Done 34800 batches in 2258.05s\ttraining loss:\t0.000042\n",
      "Done 34900 batches in 2265.07s\ttraining loss:\t0.000042\n",
      "Done 35000 batches in 2272.03s\ttraining loss:\t0.000042\n",
      "Done 35100 batches in 2278.90s\ttraining loss:\t0.000042\n",
      "Done 35200 batches in 2285.96s\ttraining loss:\t0.000042\n",
      "Done 35300 batches in 2293.52s\ttraining loss:\t0.000042\n",
      "Done 35400 batches in 2300.41s\ttraining loss:\t0.000041\n",
      "Done 35500 batches in 2307.03s\ttraining loss:\t0.000041\n",
      "Done 35600 batches in 2315.24s\ttraining loss:\t0.000041\n",
      "Done 35700 batches in 2322.05s\ttraining loss:\t0.000041\n",
      "Done 35800 batches in 2329.01s\ttraining loss:\t0.000041\n",
      "Done 35900 batches in 2336.00s\ttraining loss:\t0.000041\n",
      "Done 36000 batches in 2343.97s\ttraining loss:\t0.000041\n",
      "Done 36100 batches in 2350.93s\ttraining loss:\t0.000040\n",
      "Done 36200 batches in 2357.55s\ttraining loss:\t0.000040\n",
      "Done 36300 batches in 2364.21s\ttraining loss:\t0.000040\n",
      "Done 36400 batches in 2370.42s\ttraining loss:\t0.000040\n",
      "Done 36500 batches in 2377.36s\ttraining loss:\t0.000040\n",
      "Done 36600 batches in 2384.23s\ttraining loss:\t0.000040\n",
      "Done 36700 batches in 2391.54s\ttraining loss:\t0.000040\n",
      "Done 36800 batches in 2398.15s\ttraining loss:\t0.000040\n",
      "Done 36900 batches in 2404.84s\ttraining loss:\t0.000039\n",
      "Done 37000 batches in 2411.04s\ttraining loss:\t0.000039\n",
      "Done 37100 batches in 2417.93s\ttraining loss:\t0.000039\n",
      "Done 37200 batches in 2424.31s\ttraining loss:\t0.000039\n",
      "Done 37300 batches in 2431.01s\ttraining loss:\t0.000039\n",
      "Done 37400 batches in 2437.92s\ttraining loss:\t0.000039\n",
      "Done 37500 batches in 2443.86s\ttraining loss:\t0.000039\n",
      "Done 37600 batches in 2450.10s\ttraining loss:\t0.000039\n",
      "Done 37700 batches in 2456.97s\ttraining loss:\t0.000038\n",
      "Done 37800 batches in 2463.45s\ttraining loss:\t0.000038\n",
      "Done 37900 batches in 2469.55s\ttraining loss:\t0.000038\n",
      "Done 38000 batches in 2476.40s\ttraining loss:\t0.000038\n",
      "Done 38100 batches in 2482.47s\ttraining loss:\t0.000038\n",
      "Done 38200 batches in 2489.33s\ttraining loss:\t0.000038\n",
      "Done 38300 batches in 2495.80s\ttraining loss:\t0.000038\n",
      "Done 38400 batches in 2502.14s\ttraining loss:\t0.000038\n",
      "Done 38500 batches in 2508.90s\ttraining loss:\t0.000038\n",
      "Done 38600 batches in 2515.10s\ttraining loss:\t0.000037\n",
      "Done 38700 batches in 2521.53s\ttraining loss:\t0.000037\n",
      "Done 38800 batches in 2528.40s\ttraining loss:\t0.000037\n",
      "Done 38900 batches in 2534.95s\ttraining loss:\t0.000037\n",
      "Done 39000 batches in 2540.94s\ttraining loss:\t0.000037\n",
      "Done 39100 batches in 2547.04s\ttraining loss:\t0.000037\n",
      "Done 39200 batches in 2552.81s\ttraining loss:\t0.000037\n",
      "Done 39300 batches in 2559.53s\ttraining loss:\t0.000037\n",
      "Done 39400 batches in 2566.58s\ttraining loss:\t0.000036\n",
      "Done 39500 batches in 2573.61s\ttraining loss:\t0.000036\n",
      "Done 39600 batches in 2580.67s\ttraining loss:\t0.000036\n",
      "Done 39700 batches in 2590.63s\ttraining loss:\t0.000036\n",
      "Done 39800 batches in 2597.67s\ttraining loss:\t0.000036\n",
      "Done 39900 batches in 2604.03s\ttraining loss:\t0.000036\n",
      "Done 40000 batches in 2610.26s\ttraining loss:\t0.000036\n",
      "Done 40100 batches in 2616.50s\ttraining loss:\t0.000036\n",
      "Done 40200 batches in 2622.84s\ttraining loss:\t0.000036\n",
      "Done 40300 batches in 2629.84s\ttraining loss:\t0.000036\n",
      "Done 40400 batches in 2636.51s\ttraining loss:\t0.000035\n",
      "Done 40500 batches in 2643.33s\ttraining loss:\t0.000035\n",
      "Done 40600 batches in 2651.07s\ttraining loss:\t0.000035\n",
      "Done 40700 batches in 2656.91s\ttraining loss:\t0.000035\n",
      "Done 40800 batches in 2663.93s\ttraining loss:\t0.000035\n",
      "Done 40900 batches in 2670.25s\ttraining loss:\t0.000035\n",
      "Done 41000 batches in 2675.98s\ttraining loss:\t0.000035\n",
      "Done 41100 batches in 2681.97s\ttraining loss:\t0.000035\n",
      "Done 41200 batches in 2688.48s\ttraining loss:\t0.000035\n",
      "Done 41300 batches in 2695.67s\ttraining loss:\t0.000035\n",
      "Done 41400 batches in 2703.56s\ttraining loss:\t0.000034\n",
      "Done 41500 batches in 2710.63s\ttraining loss:\t0.000034\n",
      "Done 41600 batches in 2717.14s\ttraining loss:\t0.000034\n",
      "Done 41700 batches in 2723.96s\ttraining loss:\t0.000034\n",
      "Done 41800 batches in 2730.93s\ttraining loss:\t0.000034\n",
      "Done 41900 batches in 2737.90s\ttraining loss:\t0.000034\n",
      "Done 42000 batches in 2745.34s\ttraining loss:\t0.000034\n",
      "Done 42100 batches in 2753.19s\ttraining loss:\t0.000034\n",
      "Done 42200 batches in 2760.43s\ttraining loss:\t0.000034\n",
      "Done 42300 batches in 2767.35s\ttraining loss:\t0.000034\n",
      "Done 42400 batches in 2774.86s\ttraining loss:\t0.000034\n",
      "Done 42500 batches in 2781.92s\ttraining loss:\t0.000034\n",
      "Done 42600 batches in 2788.52s\ttraining loss:\t0.000033\n",
      "Done 42700 batches in 2795.13s\ttraining loss:\t0.000033\n",
      "Done 42800 batches in 2802.12s\ttraining loss:\t0.000033\n",
      "Done 42900 batches in 2808.15s\ttraining loss:\t0.000033\n",
      "Done 43000 batches in 2815.40s\ttraining loss:\t0.000033\n",
      "Done 43100 batches in 2822.57s\ttraining loss:\t0.000033\n",
      "Done 43200 batches in 2829.71s\ttraining loss:\t0.000033\n",
      "Done 43300 batches in 2836.45s\ttraining loss:\t0.000033\n",
      "Done 43400 batches in 2842.91s\ttraining loss:\t0.000033\n",
      "Done 43500 batches in 2850.23s\ttraining loss:\t0.000033\n",
      "Done 43600 batches in 2858.01s\ttraining loss:\t0.000033\n",
      "Done 43700 batches in 2865.39s\ttraining loss:\t0.000033\n",
      "Done 43800 batches in 2872.51s\ttraining loss:\t0.000032\n",
      "Done 43900 batches in 2878.61s\ttraining loss:\t0.000032\n",
      "Done 44000 batches in 2885.09s\ttraining loss:\t0.000032\n",
      "Done 44100 batches in 2893.63s\ttraining loss:\t0.000032\n",
      "Done 44200 batches in 2900.90s\ttraining loss:\t0.000032\n",
      "Done 44300 batches in 2907.73s\ttraining loss:\t0.000032\n",
      "Done 44400 batches in 2915.21s\ttraining loss:\t0.000032\n",
      "Done 44500 batches in 2922.24s\ttraining loss:\t0.000032\n",
      "Done 44600 batches in 2927.99s\ttraining loss:\t0.000032\n",
      "Done 44700 batches in 2935.00s\ttraining loss:\t0.000032\n",
      "Done 44800 batches in 2942.10s\ttraining loss:\t0.000032\n",
      "Done 44900 batches in 2948.12s\ttraining loss:\t0.000032\n",
      "Done 45000 batches in 2955.48s\ttraining loss:\t0.000032\n",
      "Done 45100 batches in 2961.62s\ttraining loss:\t0.000031\n",
      "Done 45200 batches in 2968.22s\ttraining loss:\t0.000031\n",
      "Done 45300 batches in 2975.43s\ttraining loss:\t0.000031\n",
      "Done 45400 batches in 2983.12s\ttraining loss:\t0.000031\n",
      "Done 45500 batches in 2990.72s\ttraining loss:\t0.000031\n",
      "Done 45600 batches in 2996.99s\ttraining loss:\t0.000031\n",
      "Done 45700 batches in 3003.65s\ttraining loss:\t0.000031\n",
      "Done 45800 batches in 3010.07s\ttraining loss:\t0.000031\n",
      "Done 45900 batches in 3017.45s\ttraining loss:\t0.000031\n",
      "Done 46000 batches in 3024.26s\ttraining loss:\t0.000031\n",
      "Done 46100 batches in 3031.34s\ttraining loss:\t0.000031\n",
      "Done 46200 batches in 3038.28s\ttraining loss:\t0.000031\n",
      "Done 46300 batches in 3045.30s\ttraining loss:\t0.000030\n",
      "Done 46400 batches in 3051.55s\ttraining loss:\t0.000030\n",
      "Done 46500 batches in 3057.83s\ttraining loss:\t0.000030\n",
      "Done 46600 batches in 3064.04s\ttraining loss:\t0.000030\n",
      "Done 46700 batches in 3070.13s\ttraining loss:\t0.000030\n",
      "Done 46800 batches in 3077.02s\ttraining loss:\t0.000030\n",
      "Done 46900 batches in 3083.69s\ttraining loss:\t0.000030\n",
      "Done 47000 batches in 3089.74s\ttraining loss:\t0.000030\n",
      "Done 47100 batches in 3096.30s\ttraining loss:\t0.000030\n",
      "Done 47200 batches in 3104.06s\ttraining loss:\t0.000030\n",
      "Done 47300 batches in 3110.65s\ttraining loss:\t0.000030\n",
      "Done 47400 batches in 3118.64s\ttraining loss:\t0.000030\n",
      "Done 47500 batches in 3124.53s\ttraining loss:\t0.000030\n",
      "Done 47600 batches in 3132.05s\ttraining loss:\t0.000030\n",
      "Done 47700 batches in 3138.61s\ttraining loss:\t0.000029\n",
      "Done 47800 batches in 3146.14s\ttraining loss:\t0.000029\n",
      "Done 47900 batches in 3153.49s\ttraining loss:\t0.000029\n",
      "Done 48000 batches in 3160.12s\ttraining loss:\t0.000029\n",
      "Done 48100 batches in 3166.26s\ttraining loss:\t0.000029\n",
      "Done 48200 batches in 3173.65s\ttraining loss:\t0.000029\n",
      "Done 48300 batches in 3181.08s\ttraining loss:\t0.000029\n",
      "Done 48400 batches in 3187.77s\ttraining loss:\t0.000029\n",
      "Done 48500 batches in 3194.75s\ttraining loss:\t0.000029\n",
      "Done 48600 batches in 3200.73s\ttraining loss:\t0.000029\n",
      "Done 48700 batches in 3207.36s\ttraining loss:\t0.000029\n",
      "Done 48800 batches in 3213.50s\ttraining loss:\t0.000029\n",
      "Done 48900 batches in 3220.73s\ttraining loss:\t0.000029\n",
      "Done 49000 batches in 3227.84s\ttraining loss:\t0.000029\n",
      "Done 49100 batches in 3236.43s\ttraining loss:\t0.000029\n",
      "Done 49200 batches in 3242.65s\ttraining loss:\t0.000028\n",
      "Done 49300 batches in 3248.68s\ttraining loss:\t0.000028\n",
      "Done 49400 batches in 3255.46s\ttraining loss:\t0.000028\n",
      "Done 49500 batches in 3262.31s\ttraining loss:\t0.000028\n",
      "Done 49600 batches in 3269.37s\ttraining loss:\t0.000028\n",
      "Done 49700 batches in 3277.29s\ttraining loss:\t0.000028\n",
      "Done 49800 batches in 3283.96s\ttraining loss:\t0.000028\n",
      "Done 49900 batches in 3290.11s\ttraining loss:\t0.000028\n",
      "Done 50000 batches in 3296.50s\ttraining loss:\t0.000028\n",
      "Done 50100 batches in 3303.53s\ttraining loss:\t0.000028\n",
      "Done 50200 batches in 3310.86s\ttraining loss:\t0.000028\n",
      "Done 50300 batches in 3317.33s\ttraining loss:\t0.000028\n",
      "Done 50400 batches in 3323.65s\ttraining loss:\t0.000028\n",
      "Done 50500 batches in 3330.66s\ttraining loss:\t0.000028\n",
      "Done 50600 batches in 3337.24s\ttraining loss:\t0.000028\n",
      "Done 50700 batches in 3343.61s\ttraining loss:\t0.000028\n",
      "Done 50800 batches in 3350.23s\ttraining loss:\t0.000027\n",
      "Done 50900 batches in 3356.62s\ttraining loss:\t0.000027\n",
      "Done 51000 batches in 3363.21s\ttraining loss:\t0.000027\n",
      "Done 51100 batches in 3369.65s\ttraining loss:\t0.000027\n",
      "Done 51200 batches in 3376.18s\ttraining loss:\t0.000027\n",
      "Done 51300 batches in 3382.39s\ttraining loss:\t0.000027\n",
      "Done 51400 batches in 3388.44s\ttraining loss:\t0.000027\n",
      "Done 51500 batches in 3395.39s\ttraining loss:\t0.000027\n",
      "Done 51600 batches in 3402.15s\ttraining loss:\t0.000027\n",
      "Done 51700 batches in 3408.87s\ttraining loss:\t0.000027\n",
      "Done 51800 batches in 3415.73s\ttraining loss:\t0.000027\n",
      "Done 51900 batches in 3422.57s\ttraining loss:\t0.000027\n",
      "Done 52000 batches in 3428.86s\ttraining loss:\t0.000027\n",
      "Done 52100 batches in 3435.11s\ttraining loss:\t0.000027\n",
      "Done 52200 batches in 3441.32s\ttraining loss:\t0.000027\n",
      "Done 52300 batches in 3448.01s\ttraining loss:\t0.000027\n",
      "Done 52400 batches in 3454.40s\ttraining loss:\t0.000027\n",
      "Done 52500 batches in 3462.37s\ttraining loss:\t0.000027\n",
      "Done 52600 batches in 3470.18s\ttraining loss:\t0.000026\n",
      "Done 52700 batches in 3477.64s\ttraining loss:\t0.000026\n",
      "Done 52800 batches in 3484.42s\ttraining loss:\t0.000026\n",
      "Done 52900 batches in 3490.66s\ttraining loss:\t0.000026\n",
      "Done 53000 batches in 3496.68s\ttraining loss:\t0.000026\n",
      "Done 53100 batches in 3503.29s\ttraining loss:\t0.000026\n",
      "Done 53200 batches in 3509.70s\ttraining loss:\t0.000026\n",
      "Done 53300 batches in 3516.13s\ttraining loss:\t0.000026\n",
      "Done 53400 batches in 3522.78s\ttraining loss:\t0.000026\n",
      "Done 53500 batches in 3530.06s\ttraining loss:\t0.000026\n",
      "Done 53600 batches in 3537.07s\ttraining loss:\t0.000026\n",
      "Done 53700 batches in 3543.28s\ttraining loss:\t0.000026\n",
      "Done 53800 batches in 3549.54s\ttraining loss:\t0.000026\n",
      "Done 53900 batches in 3557.00s\ttraining loss:\t0.000026\n",
      "Done 54000 batches in 3564.49s\ttraining loss:\t0.000026\n",
      "Done 54100 batches in 3570.51s\ttraining loss:\t0.000026\n",
      "Done 54200 batches in 3577.31s\ttraining loss:\t0.000026\n",
      "Done 54300 batches in 3583.61s\ttraining loss:\t0.000026\n",
      "Done 54400 batches in 3590.76s\ttraining loss:\t0.000026\n",
      "Done 54500 batches in 3596.39s\ttraining loss:\t0.000026\n",
      "Done 54600 batches in 3603.33s\ttraining loss:\t0.000025\n",
      "Done 54700 batches in 3610.38s\ttraining loss:\t0.000025\n",
      "Done 54800 batches in 3616.67s\ttraining loss:\t0.000025\n",
      "Done 54900 batches in 3622.45s\ttraining loss:\t0.000025\n",
      "Done 55000 batches in 3629.00s\ttraining loss:\t0.000025\n",
      "Done 55100 batches in 3636.17s\ttraining loss:\t0.000025\n",
      "Done 55200 batches in 3643.44s\ttraining loss:\t0.000025\n",
      "Done 55300 batches in 3651.37s\ttraining loss:\t0.000025\n",
      "Done 55400 batches in 3658.22s\ttraining loss:\t0.000025\n",
      "Done 55500 batches in 3664.75s\ttraining loss:\t0.000025\n",
      "Done 55600 batches in 3670.97s\ttraining loss:\t0.000025\n",
      "Done 55700 batches in 3677.12s\ttraining loss:\t0.000025\n",
      "Done 55800 batches in 3683.79s\ttraining loss:\t0.000025\n",
      "Done 55900 batches in 3689.72s\ttraining loss:\t0.000025\n",
      "Done 56000 batches in 3697.28s\ttraining loss:\t0.000025\n",
      "Done 56100 batches in 3704.33s\ttraining loss:\t0.000025\n",
      "Done 56200 batches in 3710.56s\ttraining loss:\t0.000025\n",
      "Done 56300 batches in 3718.33s\ttraining loss:\t0.000025\n",
      "Done 56400 batches in 3725.93s\ttraining loss:\t0.000025\n",
      "Done 56500 batches in 3732.81s\ttraining loss:\t0.000025\n",
      "Done 56600 batches in 3739.41s\ttraining loss:\t0.000024\n",
      "Done 56700 batches in 3745.95s\ttraining loss:\t0.000024\n",
      "Done 56800 batches in 3753.67s\ttraining loss:\t0.000024\n",
      "Done 56900 batches in 3760.03s\ttraining loss:\t0.000024\n",
      "Done 57000 batches in 3767.48s\ttraining loss:\t0.000024\n",
      "Done 57100 batches in 3774.55s\ttraining loss:\t0.000024\n",
      "Done 57200 batches in 3781.08s\ttraining loss:\t0.000024\n",
      "Done 57300 batches in 3787.85s\ttraining loss:\t0.000024\n",
      "Done 57400 batches in 3795.73s\ttraining loss:\t0.000024\n",
      "Done 57500 batches in 3803.02s\ttraining loss:\t0.000024\n",
      "Done 57600 batches in 3810.15s\ttraining loss:\t0.000024\n",
      "Done 57700 batches in 3816.24s\ttraining loss:\t0.000024\n",
      "Done 57800 batches in 3824.40s\ttraining loss:\t0.000024\n",
      "Done 57900 batches in 3831.97s\ttraining loss:\t0.000024\n",
      "Done 58000 batches in 3839.05s\ttraining loss:\t0.000024\n",
      "Done 58100 batches in 3845.92s\ttraining loss:\t0.000024\n",
      "Done 58200 batches in 3853.51s\ttraining loss:\t0.000024\n",
      "Done 58300 batches in 3860.04s\ttraining loss:\t0.000024\n",
      "Done 58400 batches in 3866.77s\ttraining loss:\t0.000024\n",
      "Done 58500 batches in 3874.50s\ttraining loss:\t0.000024\n",
      "Done 58600 batches in 3882.08s\ttraining loss:\t0.000024\n",
      "Done 58700 batches in 3888.22s\ttraining loss:\t0.000024\n",
      "Done 58800 batches in 3895.09s\ttraining loss:\t0.000023\n",
      "Done 58900 batches in 3902.50s\ttraining loss:\t0.000023\n",
      "Done 59000 batches in 3910.40s\ttraining loss:\t0.000023\n",
      "Done 59100 batches in 3916.96s\ttraining loss:\t0.000023\n",
      "Done 59200 batches in 3923.65s\ttraining loss:\t0.000023\n",
      "Done 59300 batches in 3930.30s\ttraining loss:\t0.000023\n",
      "Done 59400 batches in 3936.92s\ttraining loss:\t0.000023\n",
      "Done 59500 batches in 3944.17s\ttraining loss:\t0.000023\n",
      "Done 59600 batches in 3951.41s\ttraining loss:\t0.000023\n",
      "Done 59700 batches in 3957.74s\ttraining loss:\t0.000023\n",
      "Done 59800 batches in 3964.39s\ttraining loss:\t0.000023\n",
      "Done 59900 batches in 3971.20s\ttraining loss:\t0.000023\n",
      "Done 60000 batches in 3977.89s\ttraining loss:\t0.000023\n",
      "Done 60100 batches in 3984.76s\ttraining loss:\t0.000023\n",
      "Done 60200 batches in 3992.41s\ttraining loss:\t0.000023\n",
      "Done 60300 batches in 3998.67s\ttraining loss:\t0.000023\n",
      "Done 60400 batches in 4005.48s\ttraining loss:\t0.000023\n",
      "Done 60500 batches in 4013.03s\ttraining loss:\t0.000023\n",
      "Done 60600 batches in 4021.26s\ttraining loss:\t0.000023\n",
      "Done 60700 batches in 4028.48s\ttraining loss:\t0.000023\n",
      "Done 60800 batches in 4035.11s\ttraining loss:\t0.000023\n",
      "Done 60900 batches in 4042.70s\ttraining loss:\t0.000023\n",
      "Done 61000 batches in 4048.58s\ttraining loss:\t0.000023\n",
      "Done 61100 batches in 4055.14s\ttraining loss:\t0.000023\n",
      "Done 61200 batches in 4062.06s\ttraining loss:\t0.000023\n",
      "Done 61300 batches in 4068.39s\ttraining loss:\t0.000023\n",
      "Done 61400 batches in 4074.76s\ttraining loss:\t0.000022\n",
      "Done 61500 batches in 4081.08s\ttraining loss:\t0.000022\n",
      "Done 61600 batches in 4087.00s\ttraining loss:\t0.000022\n",
      "Done 61700 batches in 4092.89s\ttraining loss:\t0.000022\n",
      "Done 61800 batches in 4099.90s\ttraining loss:\t0.000022\n",
      "Done 61900 batches in 4106.83s\ttraining loss:\t0.000022\n",
      "Done 62000 batches in 4114.17s\ttraining loss:\t0.000022\n",
      "Done 62100 batches in 4120.63s\ttraining loss:\t0.000022\n",
      "Done 62200 batches in 4127.18s\ttraining loss:\t0.000022\n",
      "Done 62300 batches in 4133.91s\ttraining loss:\t0.000022\n",
      "Done 62400 batches in 4141.31s\ttraining loss:\t0.000022\n",
      "Done 62500 batches in 4149.38s\ttraining loss:\t0.000022\n",
      "Done 62600 batches in 4157.28s\ttraining loss:\t0.000022\n",
      "Done 62700 batches in 4164.57s\ttraining loss:\t0.000022\n",
      "Done 62800 batches in 4171.34s\ttraining loss:\t0.000022\n",
      "Done 62900 batches in 4176.97s\ttraining loss:\t0.000022\n",
      "Done 63000 batches in 4183.61s\ttraining loss:\t0.000022\n",
      "Done 63100 batches in 4190.63s\ttraining loss:\t0.000022\n",
      "Done 63200 batches in 4197.72s\ttraining loss:\t0.000022\n",
      "Done 63300 batches in 4204.38s\ttraining loss:\t0.000022\n",
      "Done 63400 batches in 4210.84s\ttraining loss:\t0.000022\n",
      "Done 63500 batches in 4217.32s\ttraining loss:\t0.000022\n",
      "Done 63600 batches in 4225.02s\ttraining loss:\t0.000022\n",
      "Done 63700 batches in 4231.95s\ttraining loss:\t0.000022\n",
      "Done 63800 batches in 4238.90s\ttraining loss:\t0.000022\n",
      "Done 63900 batches in 4245.51s\ttraining loss:\t0.000022\n",
      "Done 64000 batches in 4251.74s\ttraining loss:\t0.000022\n",
      "Done 64100 batches in 4258.76s\ttraining loss:\t0.000021\n",
      "Done 64200 batches in 4264.94s\ttraining loss:\t0.000021\n",
      "Done 64300 batches in 4271.66s\ttraining loss:\t0.000021\n",
      "Done 64400 batches in 4279.35s\ttraining loss:\t0.000021\n",
      "Done 64500 batches in 4285.63s\ttraining loss:\t0.000021\n",
      "Done 64600 batches in 4292.04s\ttraining loss:\t0.000021\n",
      "Done 64700 batches in 4299.00s\ttraining loss:\t0.000021\n",
      "Done 64800 batches in 4304.52s\ttraining loss:\t0.000021\n",
      "Done 64900 batches in 4310.37s\ttraining loss:\t0.000021\n",
      "Done 65000 batches in 4316.27s\ttraining loss:\t0.000021\n",
      "Done 65100 batches in 4323.77s\ttraining loss:\t0.000021\n",
      "Done 65200 batches in 4331.61s\ttraining loss:\t0.000021\n",
      "Done 65300 batches in 4339.54s\ttraining loss:\t0.000021\n",
      "Done 65400 batches in 4346.64s\ttraining loss:\t0.000021\n",
      "Done 65500 batches in 4354.48s\ttraining loss:\t0.000021\n",
      "Done 65600 batches in 4361.50s\ttraining loss:\t0.000021\n",
      "Done 65700 batches in 4369.01s\ttraining loss:\t0.000021\n",
      "Done 65800 batches in 4376.97s\ttraining loss:\t0.000021\n",
      "Done 65900 batches in 4383.25s\ttraining loss:\t0.000021\n",
      "Done 66000 batches in 4389.96s\ttraining loss:\t0.000021\n",
      "Done 66100 batches in 4397.21s\ttraining loss:\t0.000021\n",
      "Done 66200 batches in 4403.65s\ttraining loss:\t0.000021\n",
      "Done 66300 batches in 4411.49s\ttraining loss:\t0.000021\n",
      "Done 66400 batches in 4418.50s\ttraining loss:\t0.000021\n",
      "Done 66500 batches in 4424.91s\ttraining loss:\t0.000021\n",
      "Done 66600 batches in 4431.72s\ttraining loss:\t0.000021\n",
      "Done 66700 batches in 4437.95s\ttraining loss:\t0.000021\n",
      "Done 66800 batches in 4444.78s\ttraining loss:\t0.000021\n",
      "Done 66900 batches in 4451.11s\ttraining loss:\t0.000021\n",
      "Done 67000 batches in 4457.11s\ttraining loss:\t0.000020\n",
      "Done 67100 batches in 4463.05s\ttraining loss:\t0.000020\n",
      "Done 67200 batches in 4469.91s\ttraining loss:\t0.000020\n",
      "Done 67300 batches in 4477.45s\ttraining loss:\t0.000020\n",
      "Done 67400 batches in 4484.79s\ttraining loss:\t0.000020\n",
      "Done 67500 batches in 4491.93s\ttraining loss:\t0.000020\n",
      "Done 67600 batches in 4498.63s\ttraining loss:\t0.000020\n",
      "Done 67700 batches in 4505.35s\ttraining loss:\t0.000020\n",
      "Done 67800 batches in 4511.87s\ttraining loss:\t0.000020\n",
      "Done 67900 batches in 4519.68s\ttraining loss:\t0.000020\n",
      "Done 68000 batches in 4526.25s\ttraining loss:\t0.000020\n",
      "Done 68100 batches in 4532.82s\ttraining loss:\t0.000020\n",
      "Done 68200 batches in 4540.00s\ttraining loss:\t0.000020\n",
      "Done 68300 batches in 4547.38s\ttraining loss:\t0.000020\n",
      "Done 68400 batches in 4553.60s\ttraining loss:\t0.000020\n",
      "Done 68500 batches in 4561.71s\ttraining loss:\t0.000020\n",
      "Done 68600 batches in 4568.33s\ttraining loss:\t0.000020\n",
      "Done 68700 batches in 4575.89s\ttraining loss:\t0.000020\n",
      "Done 68800 batches in 4582.10s\ttraining loss:\t0.000020\n",
      "Done 68900 batches in 4588.99s\ttraining loss:\t0.000020\n",
      "Done 69000 batches in 4596.15s\ttraining loss:\t0.000020\n",
      "Done 69100 batches in 4602.39s\ttraining loss:\t0.000020\n",
      "Done 69200 batches in 4608.53s\ttraining loss:\t0.000020\n",
      "Done 69300 batches in 4615.04s\ttraining loss:\t0.000020\n",
      "Done 69400 batches in 4622.01s\ttraining loss:\t0.000020\n",
      "Done 69500 batches in 4629.11s\ttraining loss:\t0.000020\n",
      "Done 69600 batches in 4635.61s\ttraining loss:\t0.000020\n",
      "Done 69700 batches in 4642.10s\ttraining loss:\t0.000020\n",
      "Done 69800 batches in 4649.31s\ttraining loss:\t0.000020\n",
      "Done 69900 batches in 4655.79s\ttraining loss:\t0.000020\n",
      "Done 70000 batches in 4663.40s\ttraining loss:\t0.000020\n",
      "Done 70100 batches in 4671.25s\ttraining loss:\t0.000020\n",
      "Done 70200 batches in 4678.26s\ttraining loss:\t0.000019\n",
      "Done 70300 batches in 4684.33s\ttraining loss:\t0.000019\n",
      "Done 70400 batches in 4690.62s\ttraining loss:\t0.000019\n",
      "Done 70500 batches in 4697.54s\ttraining loss:\t0.000019\n",
      "Done 70600 batches in 4704.81s\ttraining loss:\t0.000019\n",
      "Done 70700 batches in 4711.70s\ttraining loss:\t0.000019\n",
      "Done 70800 batches in 4718.07s\ttraining loss:\t0.000019\n",
      "Done 70900 batches in 4724.71s\ttraining loss:\t0.000019\n",
      "Done 71000 batches in 4731.56s\ttraining loss:\t0.000019\n",
      "Done 71100 batches in 4738.11s\ttraining loss:\t0.000019\n",
      "Done 71200 batches in 4745.52s\ttraining loss:\t0.000019\n",
      "Done 71300 batches in 4752.60s\ttraining loss:\t0.000019\n",
      "Done 71400 batches in 4759.09s\ttraining loss:\t0.000019\n",
      "Done 71500 batches in 4765.42s\ttraining loss:\t0.000019\n",
      "Done 71600 batches in 4772.93s\ttraining loss:\t0.000019\n",
      "Done 71700 batches in 4779.86s\ttraining loss:\t0.000019\n",
      "Done 71800 batches in 4785.99s\ttraining loss:\t0.000019\n",
      "Done 71900 batches in 4791.91s\ttraining loss:\t0.000019\n",
      "Done 72000 batches in 4798.02s\ttraining loss:\t0.000019\n",
      "Done 72100 batches in 4804.46s\ttraining loss:\t0.000019\n",
      "Done 72200 batches in 4811.73s\ttraining loss:\t0.000019\n",
      "Done 72300 batches in 4819.06s\ttraining loss:\t0.000019\n",
      "Done 72400 batches in 4825.90s\ttraining loss:\t0.000019\n",
      "Done 72500 batches in 4832.25s\ttraining loss:\t0.000019\n",
      "Done 72600 batches in 4838.57s\ttraining loss:\t0.000019\n",
      "Done 72700 batches in 4844.96s\ttraining loss:\t0.000019\n",
      "Done 72800 batches in 4851.80s\ttraining loss:\t0.000019\n",
      "Done 72900 batches in 4858.48s\ttraining loss:\t0.000019\n",
      "Done 73000 batches in 4864.58s\ttraining loss:\t0.000019\n",
      "Done 73100 batches in 4871.04s\ttraining loss:\t0.000019\n",
      "Done 73200 batches in 4876.65s\ttraining loss:\t0.000019\n",
      "Done 73300 batches in 4883.47s\ttraining loss:\t0.000019\n",
      "Done 73400 batches in 4890.55s\ttraining loss:\t0.000019\n",
      "Done 73500 batches in 4896.62s\ttraining loss:\t0.000019\n",
      "Done 73600 batches in 4903.16s\ttraining loss:\t0.000019\n",
      "Done 73700 batches in 4909.30s\ttraining loss:\t0.000019\n",
      "Done 73800 batches in 4915.60s\ttraining loss:\t0.000018\n",
      "Done 73900 batches in 4923.37s\ttraining loss:\t0.000018\n",
      "Done 74000 batches in 4930.69s\ttraining loss:\t0.000018\n",
      "Done 74100 batches in 4937.87s\ttraining loss:\t0.000018\n",
      "Done 74200 batches in 4944.47s\ttraining loss:\t0.000018\n",
      "Done 74300 batches in 4951.23s\ttraining loss:\t0.000018\n",
      "Done 74400 batches in 4957.66s\ttraining loss:\t0.000018\n",
      "Done 74500 batches in 4963.61s\ttraining loss:\t0.000018\n",
      "Done 74600 batches in 4970.31s\ttraining loss:\t0.000018\n",
      "Done 74700 batches in 4976.79s\ttraining loss:\t0.000018\n",
      "Done 74800 batches in 4983.59s\ttraining loss:\t0.000018\n",
      "Done 74900 batches in 4990.03s\ttraining loss:\t0.000018\n",
      "Done 75000 batches in 4996.52s\ttraining loss:\t0.000018\n",
      "Done 75100 batches in 5002.83s\ttraining loss:\t0.000018\n",
      "Done 75200 batches in 5009.93s\ttraining loss:\t0.000018\n",
      "Done 75300 batches in 5016.57s\ttraining loss:\t0.000018\n",
      "Done 75400 batches in 5022.72s\ttraining loss:\t0.000018\n",
      "Done 75500 batches in 5029.10s\ttraining loss:\t0.000018\n",
      "Done 75600 batches in 5035.38s\ttraining loss:\t0.000018\n",
      "Done 75700 batches in 5042.68s\ttraining loss:\t0.000018\n",
      "Done 75800 batches in 5050.29s\ttraining loss:\t0.000018\n",
      "Done 75900 batches in 5057.05s\ttraining loss:\t0.000018\n",
      "Done 76000 batches in 5064.06s\ttraining loss:\t0.000018\n",
      "Done 76100 batches in 5070.56s\ttraining loss:\t0.000018\n",
      "Done 76200 batches in 5077.78s\ttraining loss:\t0.000018\n",
      "Done 76300 batches in 5084.78s\ttraining loss:\t0.000018\n",
      "Done 76400 batches in 5091.76s\ttraining loss:\t0.000018\n",
      "Done 76500 batches in 5098.07s\ttraining loss:\t0.000018\n",
      "Done 76600 batches in 5104.45s\ttraining loss:\t0.000018\n",
      "Done 76700 batches in 5111.14s\ttraining loss:\t0.000018\n",
      "Done 76800 batches in 5117.46s\ttraining loss:\t0.000018\n",
      "Done 76900 batches in 5123.94s\ttraining loss:\t0.000018\n",
      "Done 77000 batches in 5130.15s\ttraining loss:\t0.000018\n",
      "Done 77100 batches in 5136.10s\ttraining loss:\t0.000018\n",
      "Done 77200 batches in 5142.04s\ttraining loss:\t0.000018\n",
      "Done 77300 batches in 5148.23s\ttraining loss:\t0.000018\n",
      "Done 77400 batches in 5153.58s\ttraining loss:\t0.000018\n",
      "Done 77500 batches in 5159.95s\ttraining loss:\t0.000018\n",
      "Done 77600 batches in 5167.35s\ttraining loss:\t0.000018\n",
      "Done 77700 batches in 5174.49s\ttraining loss:\t0.000018\n",
      "Done 77800 batches in 5181.28s\ttraining loss:\t0.000018\n",
      "Done 77900 batches in 5188.23s\ttraining loss:\t0.000017\n",
      "Done 78000 batches in 5194.23s\ttraining loss:\t0.000017\n",
      "Done 78100 batches in 5199.47s\ttraining loss:\t0.000017\n",
      "Done 78200 batches in 5205.01s\ttraining loss:\t0.000017\n",
      "Done 78300 batches in 5211.80s\ttraining loss:\t0.000017\n",
      "Done 78400 batches in 5218.77s\ttraining loss:\t0.000017\n",
      "Done 78500 batches in 5225.67s\ttraining loss:\t0.000017\n",
      "Done 78600 batches in 5231.98s\ttraining loss:\t0.000017\n",
      "Done 78700 batches in 5239.15s\ttraining loss:\t0.000017\n",
      "Done 78800 batches in 5245.75s\ttraining loss:\t0.000017\n",
      "Done 78900 batches in 5252.98s\ttraining loss:\t0.000017\n",
      "Done 79000 batches in 5259.57s\ttraining loss:\t0.000017\n",
      "Done 79100 batches in 5266.21s\ttraining loss:\t0.000017\n",
      "Done 79200 batches in 5273.67s\ttraining loss:\t0.000017\n",
      "Done 79300 batches in 5280.78s\ttraining loss:\t0.000017\n",
      "Done 79400 batches in 5287.81s\ttraining loss:\t0.000017\n",
      "Done 79500 batches in 5294.64s\ttraining loss:\t0.000017\n",
      "Done 79600 batches in 5300.83s\ttraining loss:\t0.000017\n",
      "Done 79700 batches in 5307.47s\ttraining loss:\t0.000017\n",
      "Done 79800 batches in 5314.01s\ttraining loss:\t0.000017\n",
      "Done 79900 batches in 5320.39s\ttraining loss:\t0.000017\n",
      "Done 80000 batches in 5326.82s\ttraining loss:\t0.000017\n",
      "Done 80100 batches in 5332.84s\ttraining loss:\t0.000017\n",
      "Done 80200 batches in 5340.32s\ttraining loss:\t0.000017\n",
      "Done 80300 batches in 5347.95s\ttraining loss:\t0.000017\n",
      "Done 80400 batches in 5353.72s\ttraining loss:\t0.000017\n",
      "Done 80500 batches in 5360.84s\ttraining loss:\t0.000017\n",
      "Done 80600 batches in 5367.59s\ttraining loss:\t0.000017\n",
      "Done 80700 batches in 5374.10s\ttraining loss:\t0.000017\n",
      "Done 80800 batches in 5380.80s\ttraining loss:\t0.000017\n",
      "Done 80900 batches in 5388.28s\ttraining loss:\t0.000017\n",
      "Done 81000 batches in 5395.49s\ttraining loss:\t0.000017\n",
      "Done 81100 batches in 5402.19s\ttraining loss:\t0.000017\n",
      "Done 81200 batches in 5408.78s\ttraining loss:\t0.000017\n",
      "Done 81300 batches in 5415.26s\ttraining loss:\t0.000017\n",
      "Done 81400 batches in 5423.12s\ttraining loss:\t0.000017\n",
      "Done 81500 batches in 5431.58s\ttraining loss:\t0.000017\n",
      "Done 81600 batches in 5438.22s\ttraining loss:\t0.000017\n",
      "Done 81700 batches in 5444.59s\ttraining loss:\t0.000017\n",
      "Done 81800 batches in 5452.37s\ttraining loss:\t0.000017\n",
      "Done 81900 batches in 5459.28s\ttraining loss:\t0.000017\n",
      "Done 82000 batches in 5467.03s\ttraining loss:\t0.000017\n",
      "Done 82100 batches in 5473.77s\ttraining loss:\t0.000017\n",
      "Done 82200 batches in 5480.08s\ttraining loss:\t0.000017\n",
      "Done 82300 batches in 5486.16s\ttraining loss:\t0.000017\n",
      "Done 82400 batches in 5492.60s\ttraining loss:\t0.000016\n",
      "Done 82500 batches in 5499.62s\ttraining loss:\t0.000016\n",
      "Done 82600 batches in 5506.82s\ttraining loss:\t0.000016\n",
      "Done 82700 batches in 5513.55s\ttraining loss:\t0.000016\n",
      "Done 82800 batches in 5520.00s\ttraining loss:\t0.000016\n",
      "Done 82900 batches in 5526.50s\ttraining loss:\t0.000016\n",
      "Done 83000 batches in 5533.43s\ttraining loss:\t0.000016\n",
      "Done 83100 batches in 5540.12s\ttraining loss:\t0.000016\n",
      "Done 83200 batches in 5546.33s\ttraining loss:\t0.000016\n",
      "Done 83300 batches in 5552.96s\ttraining loss:\t0.000016\n",
      "Done 83400 batches in 5559.34s\ttraining loss:\t0.000016\n",
      "Done 83500 batches in 5565.30s\ttraining loss:\t0.000016\n",
      "Done 83600 batches in 5571.18s\ttraining loss:\t0.000016\n",
      "Done 83700 batches in 5578.16s\ttraining loss:\t0.000016\n",
      "Done 83800 batches in 5585.28s\ttraining loss:\t0.000016\n",
      "Done 83900 batches in 5592.03s\ttraining loss:\t0.000016\n",
      "Done 84000 batches in 5598.59s\ttraining loss:\t0.000016\n",
      "Done 84100 batches in 5605.35s\ttraining loss:\t0.000016\n",
      "Done 84200 batches in 5611.63s\ttraining loss:\t0.000016\n",
      "Done 84300 batches in 5618.60s\ttraining loss:\t0.000016\n",
      "Done 84400 batches in 5626.70s\ttraining loss:\t0.000016\n",
      "Done 84500 batches in 5634.61s\ttraining loss:\t0.000016\n",
      "Done 84600 batches in 5641.65s\ttraining loss:\t0.000016\n",
      "Done 84700 batches in 5647.45s\ttraining loss:\t0.000016\n",
      "Done 84800 batches in 5653.99s\ttraining loss:\t0.000016\n",
      "Done 84900 batches in 5661.38s\ttraining loss:\t0.000016\n",
      "Done 85000 batches in 5667.62s\ttraining loss:\t0.000016\n",
      "Done 85100 batches in 5674.76s\ttraining loss:\t0.000016\n",
      "Done 85200 batches in 5681.82s\ttraining loss:\t0.000016\n",
      "Done 85300 batches in 5688.77s\ttraining loss:\t0.000016\n",
      "Done 85400 batches in 5695.40s\ttraining loss:\t0.000016\n",
      "Done 85500 batches in 5702.10s\ttraining loss:\t0.000016\n",
      "Done 85600 batches in 5709.94s\ttraining loss:\t0.000016\n",
      "Done 85700 batches in 5716.59s\ttraining loss:\t0.000016\n",
      "Done 85800 batches in 5723.83s\ttraining loss:\t0.000016\n",
      "Done 85900 batches in 5730.26s\ttraining loss:\t0.000016\n",
      "Done 86000 batches in 5736.97s\ttraining loss:\t0.000016\n",
      "Done 86100 batches in 5743.87s\ttraining loss:\t0.000016\n",
      "Done 86200 batches in 5751.40s\ttraining loss:\t0.000016\n",
      "Done 86300 batches in 5759.08s\ttraining loss:\t0.000016\n",
      "Done 86400 batches in 5766.44s\ttraining loss:\t0.000016\n",
      "Done 86500 batches in 5772.47s\ttraining loss:\t0.000016\n",
      "Done 86600 batches in 5778.93s\ttraining loss:\t0.000016\n",
      "Done 86700 batches in 5786.30s\ttraining loss:\t0.000016\n",
      "Done 86800 batches in 5792.10s\ttraining loss:\t0.000016\n",
      "Done 86900 batches in 5798.67s\ttraining loss:\t0.000016\n",
      "Done 87000 batches in 5804.59s\ttraining loss:\t0.000016\n",
      "Done 87100 batches in 5810.92s\ttraining loss:\t0.000016\n",
      "Done 87200 batches in 5817.11s\ttraining loss:\t0.000016\n",
      "Done 87300 batches in 5823.38s\ttraining loss:\t0.000016\n",
      "Done 87400 batches in 5828.97s\ttraining loss:\t0.000016\n",
      "Done 87500 batches in 5834.94s\ttraining loss:\t0.000015\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.5474093184784871e-05"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hred_net.train_one_epoch(data, 1, log_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hred_net.save_params()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

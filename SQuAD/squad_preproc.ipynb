{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json, nltk, io, pickle\n",
    "import numpy as np\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with io.open('/pio/data/data/squad/train-v1.1.json', 'r', encoding='utf-8') as f:\n",
    "    train = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with io.open('/pio/data/data/squad/dev-v1.1.json', 'r', encoding='utf-8') as f:\n",
    "    dev = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{u'answer_start': 177, u'text': u'Denver Broncos'},\n",
       " {u'answer_start': 177, u'text': u'Denver Broncos'},\n",
       " {u'answer_start': 177, u'text': u'Denver Broncos'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev['data'][0]['paragraphs'][0]['qas'][0]['answers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev['data'][0]['paragraphs'][0]['qas'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'question', u'id', u'answers']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev['data'][0]['paragraphs'][0]['qas'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u\"The State Council declared a three-day period of national mourning for the quake victims starting from May 19 , 2008 ; the PRC 's National Flag and Regional Flags of Hong Kong and Macau Special Administrative Regions flown at half mast\",\n",
       " u'It was the first time that a national mourning period had been declared for something other than the death of a state leader , and many have called it the biggest display of mourning since the death of Mao Zedong',\n",
       " u'At 14:28 CST on May 19 , 2008 , a week after the earthquake , the Chinese public held a moment of silence',\n",
       " u'People stood silent for three minutes while air defense , police and fire sirens , and the horns of vehicles , vessels and trains sounded',\n",
       " u\"Cars and trucks on Beijing 's roads also came to a halt\",\n",
       " u\"People spontaneously burst into cheering `` Zhongguo jiayou ! '' ( Let 's go , China ! ) and `` Sichuan jiayou '' ( Let 's go , Sichuan ! ) afterwards .\"]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(nltk.word_tokenize(train['data'][10]['paragraphs'][60]['context'])).split(' . ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save glove vectors as npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "glove_vec = []\n",
    "\n",
    "with io.open('/pio/data/data/glove_vec/6B/glove.6B.300d.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        glove_vec.append(np.matrix(str(' '.join(line.split()[1:]))))\n",
    "        \n",
    "glove_vec = np.vstack(glove_vec).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('/pio/data/data/glove_vec/6B/glove.6B.300d', glove_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a glove wordlist\n",
    "\n",
    "glove_words = []\n",
    "\n",
    "with io.open('/pio/data/data/glove_vec/6B/glove.6B.300d.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        glove_words.append(line.split()[0])\n",
    "        \n",
    "with io.open('/pio/data/data/glove_vec/6B/glove.6B.wordlist.txt', 'w', encoding='utf-8') as f:\n",
    "    for w in glove_words:\n",
    "        f.write(unicode(w + '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab all the question-answer pairs and create a wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = set()\n",
    "data = []\n",
    "lower = lambda x: x.lower()\n",
    "\n",
    "for par in train['data']:\n",
    "    title = par['title']\n",
    "    \n",
    "    for con in par['paragraphs']:\n",
    "        context = con['context']\n",
    "        context_tok = map(lower, nltk.word_tokenize(context))\n",
    "        words |= set(context_tok)\n",
    "        \n",
    "        for q in con['qas']:\n",
    "            question = q['question']\n",
    "            question_tok = map(lower, nltk.word_tokenize(question))\n",
    "            words |= set(question_tok)\n",
    "            \n",
    "            Id = q['id']\n",
    "            \n",
    "            answers = []\n",
    "            \n",
    "            for ans in q['answers']:\n",
    "                text = ans['text']\n",
    "                text_tok = map(lower, nltk.word_tokenize(text))\n",
    "                ans_start = ans['answer_start']\n",
    "                \n",
    "                answers.append((ans_start, text_tok))\n",
    "                \n",
    "            data.append([answers, question_tok, context_tok])\n",
    "            \n",
    "words.add('<unk>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn words into numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i_to_w = dict(enumerate(words))\n",
    "w_to_i = {v:k for (k,v) in i_to_w.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_on_dot(s):\n",
    "    res = [[]]\n",
    "    for w in s:\n",
    "        res[-1].append(w)\n",
    "        if w == u'.':\n",
    "            res.append([])\n",
    "    return res if res[-1] else res[:-1]\n",
    "\n",
    "def words_to_num(s):\n",
    "    return map(lambda x: w_to_i.get(x, w_to_i['<unk>']), s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in xrange(len(data)):\n",
    "    data[i][2] = split_on_dot(data[i][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_num = []\n",
    "\n",
    "for a, q, c in data:\n",
    "    answers = []\n",
    "    for ans in a:\n",
    "        answers.append((ans[0], words_to_num(ans[1])))        \n",
    "    data_num.append([answers, words_to_num(q), map(words_to_num, c)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_num = [[l[0], [l[1]] + l[2]] for l in data_num]\n",
    "data_num = [[[t[1] for t in l[0]], l[1]] for l in data_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1028"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are some broken answers, because of the tokenizer (I count words instead of characters)\n",
    "\n",
    "k = 0\n",
    "for a, q in data_num:\n",
    "    for w in a[0]:\n",
    "        if w not in list(chain(*q[1:])):\n",
    "            k += 1\n",
    "k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find answer indices on words, not characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inds = []\n",
    "\n",
    "for a, q in data_num:\n",
    "    ans = []\n",
    "    tot_q = list(chain(*q[1:]))\n",
    "    for x in a:\n",
    "        for i in xrange(len(tot_q)):\n",
    "            if x == tot_q[i:i+len(x)]:\n",
    "                ans.append(list(xrange(i, i + len(x))))\n",
    "                break\n",
    "    inds.append(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in xrange(len(data_num)):\n",
    "    data_num[i][0] = inds[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = np.load('evaluate/glove_vocab/dev_with_glove_vocab_predictions_charemb_all_fixed_ep3.npz')['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with io.open('evaluate/glove_vocab/dev_with_glove_vocab_predictions_charemb_all_fixed_ep3.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(u'{')\n",
    "    for i in xrange(len(data_dev)):\n",
    "        ans = ' '.join(data_dev[i][2][preds[i][0]:preds[i][1] + 1])\n",
    "        Id = data_dev[i][3]\n",
    "        f.write(u'\"{}\": \"{}\"'.format(Id, ans))\n",
    "        if i < len(data_dev) - 1:\n",
    "            f.write(u', ')\n",
    "    f.write(u'}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Preprocess dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_dev = set()\n",
    "data_dev = []\n",
    "lower = lambda x: x.lower()\n",
    "\n",
    "for par in dev['data']:\n",
    "    title = par['title']\n",
    "    \n",
    "    for con in par['paragraphs']:\n",
    "        context = con['context']\n",
    "        context_tok = map(lower, nltk.word_tokenize(context))\n",
    "        words_dev |= set(context_tok)\n",
    "        \n",
    "        for q in con['qas']:\n",
    "            question = q['question']\n",
    "            question_tok = map(lower, nltk.word_tokenize(question))\n",
    "            words_dev |= set(question_tok)\n",
    "            \n",
    "            Id = q['id']\n",
    "            \n",
    "            answers = []\n",
    "            \n",
    "            for ans in q['answers']:\n",
    "                text = ans['text']\n",
    "                text_tok = map(lower, nltk.word_tokenize(text))\n",
    "                ans_start = ans['answer_start']\n",
    "                \n",
    "                answers.append((ans_start, text_tok))\n",
    "                \n",
    "            data_dev.append([answers, question_tok, context_tok, Id])\n",
    "            \n",
    "words_dev.add('<unk>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in xrange(len(data_dev)):\n",
    "    data_dev[i][2] = [w if w in words else '<unk>' for w in data_dev[i][2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in xrange(len(data_dev)):\n",
    "    data_dev[i][2] = split_on_dot(data_dev[i][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def words_to_num(s):\n",
    "    return map(lambda x: glove_w_to_i.get(x, 0), s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_num_dev = []\n",
    "\n",
    "for a, q, c, _ in data_dev:\n",
    "    answers = []\n",
    "    for ans in a:\n",
    "        answers.append((ans[0], words_to_num(ans[1])))        \n",
    "    data_num_dev.append([answers, words_to_num(q), map(words_to_num, c)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_num_dev = [[l[0], [l[1]] + l[2]] for l in data_num_dev]\n",
    "data_num_dev = [[[t[1] for t in l[0]], l[1]] for l in data_num_dev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inds = []\n",
    "\n",
    "for a, q in data_num_dev:\n",
    "    ans = []\n",
    "    tot_q = list(chain(*q[1:]))\n",
    "    for x in a:\n",
    "        for i in xrange(len(tot_q)):\n",
    "            if x == tot_q[i:i+len(x)]:\n",
    "                ans.append(list(xrange(i, i + len(x))))\n",
    "                break\n",
    "    inds.append(ans)\n",
    "    \n",
    "for i in xrange(len(data_num_dev)):\n",
    "    data_num_dev[i][0] = inds[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data_num_dev = [[d[0]] + map(words_to_num, d[1:]) for d in data_dev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/pio/data/data/squad/dev_with_glove_vocab.pkl', 'w') as f:\n",
    "    pickle.dump(data_num_dev, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Get Glove vectors for words in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_vec = np.load('/pio/data/data/glove_vec/6B/glove.6B.300d.npy')\n",
    "\n",
    "glove_words = []\n",
    "\n",
    "with io.open('/pio/data/data/glove_vec/6B/glove.6B.wordlist.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        glove_words.append(line.split()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "glove_i_to_w = glove_words\n",
    "glove_w_to_i = {v:k for (k,v) in list(enumerate(glove_words))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102802, 300)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs = np.zeros((len(w_to_i), 300), dtype=np.float32)\n",
    "embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73351"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "known_inds = [i for i in xrange(len(w_to_i)) if i_to_w[i] in glove_w_to_i]\n",
    "len(known_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s_known = set(known_inds)\n",
    "unknown_inds = [i for i in xrange(len(w_to_i)) if i not in s_known]\n",
    "s_unknown = set(unknown_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_num = np.load('/pio/data/data/squad/dev_with_training_vocab.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_num = np.load('/pio/data/data/squad/train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "not_in_dev = words - words_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4445"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_to_i['<unk>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for di in xrange(len(train_num)):\n",
    "    for si in xrange(len(train_num[di][1])):\n",
    "        for wi in xrange(len(train_num[di][1][si])):\n",
    "            w = train_num[di][1][si][wi]\n",
    "            if i_to_w[w] in not_in_dev:\n",
    "                train_num[di][1][si][wi] = 4445"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/pio/data/data/squad/train_with_unks.pkl', 'w') as f:\n",
    "    pickle.dump(train_num, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embs[known_inds] = glove_vec[[glove_w_to_i[i_to_w[i]] for i in known_inds]]\n",
    "embs[unknown_inds] = L.init.Normal()((len(unknown_inds), 300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1569040"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([w for d in data_dev for w in list(chain(*d[1:3])) if w in w_to_i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03201188947819429"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# percentage of <unk> in dev\n",
    "51889. / 1620929"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9679881105218057"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# % of dev set in train vocabulary\n",
    "1569040. / 1620929"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9874362171322741"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# % of dev set in glove\n",
    "1600564. / 1620929"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.021761058288548987"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# <unk> in dev questions\n",
    "2632. / 120950"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08194192478236054"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no-devs in train\n",
    "1070257. / 13061165"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01261426526653633"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no-gloves in train\n",
    "164757. / 13061165"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_words = map(lambda x: x[0], sorted(w_to_i.items(), key=lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with io.open('/pio/data/data/squad/wordlist.txt', 'w', encoding='utf-8') as f:\n",
    "    for w in sorted_words:\n",
    "        f.write(unicode(w + '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This file has a lot of redundant parts, context is repeated for each question.\n",
    "# It only slows down the initial loading.\n",
    "\n",
    "with open('/pio/data/data/squad/train.pkl', 'w') as f:\n",
    "    pickle.dump(data_num, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.load('/pio/data/data/squad/train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w_to_i = {}\n",
    "idx = 0\n",
    "\n",
    "with io.open('/pio/data/data/squad/train_wordlist.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        w_to_i[line[:-1]] = idx\n",
    "        idx += 1\n",
    "        \n",
    "i_to_w = {v:k for (k,v) in w_to_i.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lens = np.array(map(lambda x: len(x[1]), data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def show_data(idx):\n",
    "    for s in data[idx][1]:\n",
    "        print ' '.join([i_to_w[x] for x in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is the largest hottest continuously large area worldwide ?\n",
      "the sky is usually clear above the desert and the sunshine duration is extremely high everywhere in the sahara .\n",
      "most of the desert enjoys more than 3,600 h of bright sunshine annually or over 82 % of the time and a wide area in the eastern part experiences in excess of 4,000 h of bright sunshine a year or over 91 % of the time , and the highest values are very close to the theoretical maximum value .\n",
      "a value of 4,300 h or 98 % of the time would be recorded in upper egypt ( aswan , luxor ) and in the nubian desert ( wadi halfa ) .\n",
      "the annual average direct solar irradiation is around 2,800 kwh/ ( m2 year ) in the great desert .\n",
      "the sahara has a huge potential for solar energy production .\n",
      "the constantly high position of the sun , the extremely low relative humidity , the lack of vegetation and rainfall make the great desert the hottest continuously large area worldwide and certainly the hottest place on earth during summertime in some spots .\n",
      "the average high temperature exceeds 38 °c ( 100.4 °f ) - 40 °c ( 104 °f ) during the hottest month nearly everywhere in the desert except at very high mountainous areas .\n",
      "the highest officially recorded average high temperature was 47 °c ( 116.6 °f ) in a remote desert town in the algerian desert called bou bernous with an elevation of 378 meters above sea level .\n",
      "it 's the world 's highest recorded average high temperature and only death valley , california rivals it .\n",
      "other hot spots in algeria such as adrar , timimoun , in salah , ouallene , aoulef , reggane with an elevation between 200 and 400 meters above sea level get slightly lower summer average highs around 46 °c ( 114.8 °f ) during the hottest months of the year .\n",
      "salah , well known in algeria for its extreme heat , has an average high temperature of 43.8 °c ( 110.8 °f ) , 46.4 °c ( 115.5 °f ) , 45.5 ( 113.9 °f ) .\n",
      "furthermore , 41.9 °c ( 107.4 °f ) in june , july , august and september .\n",
      "in fact , there are even hotter spots in the sahara , but they are located in extremely remote areas , especially in the azalai , lying in northern mali .\n",
      "the major part of the desert experiences around 3 – 5 months when the average high strictly exceeds 40 °c ( 104 °f ) .\n",
      "the southern central part of the desert experiences up to 6 – 7 months when the average high temperature strictly exceeds 40 °c ( 104 °f ) which shows the constancy and the length of the really hot season in the sahara .\n",
      "some examples of this are bilma , niger and faya-largeau , chad .\n",
      "the annual average daily temperature exceeds 20 °c ( 68 °f ) everywhere and can approach 30 °c ( 86 °f ) in the hottest regions year-round .\n",
      "however , most of the desert has a value in excess of 25 °c ( 77 °f ) .\n",
      "the sand and ground temperatures are even more extreme .\n",
      "during daytime , the sand temperature is extremely high as it can easily reach 80 °c ( 176 °f ) or more .\n",
      "a sand temperature of 83.5 °c ( 182.3 °f ) has been recorded in port sudan .\n",
      "ground temperatures of 72 °c ( 161.6 °f ) have been recorded in the adrar of mauritania and a value of 75 °c ( 167 °f ) has been measured in borkou , northern chad .\n",
      "due to lack of cloud cover and very low humidity , the desert usually features high diurnal temperature variations between days and nights .\n",
      "however , it 's a myth that the nights are cold after extremely hot days in the sahara .\n",
      "the average diurnal temperature range is typically between 13 °c ( 55.4 °f ) and 20 °c ( 68 °f ) .\n",
      "the lowest values are found along the coastal regions due to high humidity and are often even lower than 10 °c ( 50 °f ) , while the highest values are found in inland desert areas where the humidity is the lowest , mainly in the southern sahara .\n",
      "still , it 's true that winter nights can be cold as it can drop to the freezing point and even below , especially in high-elevation areas .\n"
     ]
    }
   ],
   "source": [
    "show_data(60023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([    0,     0,  1665,  6015, 12958, 18663, 16891, 12176,  7727,\n",
       "        4709,  2711,  1677,   859,   636,   352,   223,   166,    60,\n",
       "          39,    24,     9,     0,     5,    19,     5,     0,     0,\n",
       "           0,    10])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print max(lens)\n",
    "np.bincount(lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fun with characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# i_to_c = list(glove_chars)\n",
    "# c_to_i = {v:k for (k,v) in i_to_c.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data powinno być bezpośrednio po wykonaniu okienka, w którym jest inicjowane words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# chars = {c for d in data for s in d[1:] for w in s for c in w}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# chars_dev = {c for d in data_dev for s in d[1:3] for w in s for c in w}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# chars.add('<unk>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 0 - unk\n",
    "# 1 - start\n",
    "# 2 - end\n",
    "# there are no 1s or 2s in data, so these are safe\n",
    "\n",
    "chars = [unichr(i) for i in xrange(128)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i_to_c = chars\n",
    "c_to_i = {v:k for (k,v) in list(enumerate(chars))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_char = []\n",
    "\n",
    "for _, q, x in data:\n",
    "    q_char = [[1] + [c_to_i.get(c, 0) for c in w] + [2] for w in q]\n",
    "    x_char = [[1] + [c_to_i.get(c, 0) for c in w] + [2] for w in x]\n",
    "    data_char.append([q_char, x_char])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_dev_char = []\n",
    "\n",
    "for _, q, x, _ in data_dev:\n",
    "    q_char = [[1] + [c_to_i.get(c, 0) for c in w] + [2] for w in q]\n",
    "    x_char = [[1] + [c_to_i.get(c, 0) for c in w] + [2] for w in x]\n",
    "    data_dev_char.append([q_char, x_char])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'which',\n",
       "  u'nfl',\n",
       "  u'team',\n",
       "  u'represented',\n",
       "  u'the',\n",
       "  u'afc',\n",
       "  u'at',\n",
       "  u'super',\n",
       "  u'bowl',\n",
       "  u'50',\n",
       "  u'?'],\n",
       " [u'super',\n",
       "  u'bowl',\n",
       "  u'50',\n",
       "  u'was',\n",
       "  u'an',\n",
       "  u'american',\n",
       "  u'football',\n",
       "  u'game',\n",
       "  u'to',\n",
       "  u'determine',\n",
       "  u'the',\n",
       "  u'champion',\n",
       "  u'of',\n",
       "  u'the',\n",
       "  u'national',\n",
       "  u'football',\n",
       "  u'league',\n",
       "  u'(',\n",
       "  u'nfl',\n",
       "  u')',\n",
       "  u'for',\n",
       "  u'the',\n",
       "  u'2015',\n",
       "  u'season',\n",
       "  u'.',\n",
       "  u'the',\n",
       "  u'american',\n",
       "  u'football',\n",
       "  u'conference',\n",
       "  u'(',\n",
       "  u'afc',\n",
       "  u')',\n",
       "  u'champion',\n",
       "  u'denver',\n",
       "  u'broncos',\n",
       "  u'defeated',\n",
       "  u'the',\n",
       "  u'national',\n",
       "  u'football',\n",
       "  u'conference',\n",
       "  u'(',\n",
       "  u'nfc',\n",
       "  u')',\n",
       "  u'champion',\n",
       "  u'carolina',\n",
       "  u'panthers',\n",
       "  u'24\\u201310',\n",
       "  u'to',\n",
       "  u'earn',\n",
       "  u'their',\n",
       "  u'third',\n",
       "  u'super',\n",
       "  u'bowl',\n",
       "  u'title',\n",
       "  u'.',\n",
       "  u'the',\n",
       "  u'game',\n",
       "  u'was',\n",
       "  u'played',\n",
       "  u'on',\n",
       "  u'february',\n",
       "  u'7',\n",
       "  u',',\n",
       "  u'2016',\n",
       "  u',',\n",
       "  u'at',\n",
       "  u'levi',\n",
       "  u\"'s\",\n",
       "  u'stadium',\n",
       "  u'in',\n",
       "  u'the',\n",
       "  u'san',\n",
       "  u'francisco',\n",
       "  u'bay',\n",
       "  u'area',\n",
       "  u'at',\n",
       "  u'santa',\n",
       "  u'clara',\n",
       "  u',',\n",
       "  u'california',\n",
       "  u'.',\n",
       "  u'as',\n",
       "  u'this',\n",
       "  u'was',\n",
       "  u'the',\n",
       "  u'50th',\n",
       "  u'super',\n",
       "  u'bowl',\n",
       "  u',',\n",
       "  u'the',\n",
       "  u'league',\n",
       "  u'emphasized',\n",
       "  u'the',\n",
       "  u'``',\n",
       "  u'golden',\n",
       "  u'anniversary',\n",
       "  u\"''\",\n",
       "  u'with',\n",
       "  u'various',\n",
       "  u'gold-themed',\n",
       "  u'initiatives',\n",
       "  u',',\n",
       "  u'as',\n",
       "  u'well',\n",
       "  u'as',\n",
       "  u'temporarily',\n",
       "  u'suspending',\n",
       "  u'the',\n",
       "  u'tradition',\n",
       "  u'of',\n",
       "  u'naming',\n",
       "  u'each',\n",
       "  u'super',\n",
       "  u'bowl',\n",
       "  u'game',\n",
       "  u'with',\n",
       "  u'roman',\n",
       "  u'numerals',\n",
       "  u'(',\n",
       "  u'under',\n",
       "  u'which',\n",
       "  u'the',\n",
       "  u'game',\n",
       "  u'would',\n",
       "  u'have',\n",
       "  u'been',\n",
       "  u'known',\n",
       "  u'as',\n",
       "  u'``',\n",
       "  u'super',\n",
       "  u'bowl',\n",
       "  u'l',\n",
       "  u\"''\",\n",
       "  u')',\n",
       "  u',',\n",
       "  u'so',\n",
       "  u'that',\n",
       "  u'the',\n",
       "  u'logo',\n",
       "  u'could',\n",
       "  u'prominently',\n",
       "  u'feature',\n",
       "  u'the',\n",
       "  u'arabic',\n",
       "  u'numerals',\n",
       "  u'50',\n",
       "  u'.']]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dev[0][1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[1, 119, 104, 105, 99, 104, 2],\n",
       "  [1, 110, 102, 108, 2],\n",
       "  [1, 116, 101, 97, 109, 2],\n",
       "  [1, 114, 101, 112, 114, 101, 115, 101, 110, 116, 101, 100, 2],\n",
       "  [1, 116, 104, 101, 2],\n",
       "  [1, 97, 102, 99, 2],\n",
       "  [1, 97, 116, 2],\n",
       "  [1, 115, 117, 112, 101, 114, 2],\n",
       "  [1, 98, 111, 119, 108, 2],\n",
       "  [1, 53, 48, 2],\n",
       "  [1, 63, 2]],\n",
       " [[1, 115, 117, 112, 101, 114, 2],\n",
       "  [1, 98, 111, 119, 108, 2],\n",
       "  [1, 53, 48, 2],\n",
       "  [1, 119, 97, 115, 2],\n",
       "  [1, 97, 110, 2],\n",
       "  [1, 97, 109, 101, 114, 105, 99, 97, 110, 2],\n",
       "  [1, 102, 111, 111, 116, 98, 97, 108, 108, 2],\n",
       "  [1, 103, 97, 109, 101, 2],\n",
       "  [1, 116, 111, 2],\n",
       "  [1, 100, 101, 116, 101, 114, 109, 105, 110, 101, 2],\n",
       "  [1, 116, 104, 101, 2],\n",
       "  [1, 99, 104, 97, 109, 112, 105, 111, 110, 2],\n",
       "  [1, 111, 102, 2],\n",
       "  [1, 116, 104, 101, 2],\n",
       "  [1, 110, 97, 116, 105, 111, 110, 97, 108, 2],\n",
       "  [1, 102, 111, 111, 116, 98, 97, 108, 108, 2],\n",
       "  [1, 108, 101, 97, 103, 117, 101, 2],\n",
       "  [1, 40, 2],\n",
       "  [1, 110, 102, 108, 2],\n",
       "  [1, 41, 2],\n",
       "  [1, 102, 111, 114, 2],\n",
       "  [1, 116, 104, 101, 2],\n",
       "  [1, 50, 48, 49, 53, 2],\n",
       "  [1, 115, 101, 97, 115, 111, 110, 2],\n",
       "  [1, 46, 2],\n",
       "  [1, 116, 104, 101, 2],\n",
       "  [1, 97, 109, 101, 114, 105, 99, 97, 110, 2],\n",
       "  [1, 102, 111, 111, 116, 98, 97, 108, 108, 2],\n",
       "  [1, 99, 111, 110, 102, 101, 114, 101, 110, 99, 101, 2],\n",
       "  [1, 40, 2],\n",
       "  [1, 97, 102, 99, 2],\n",
       "  [1, 41, 2],\n",
       "  [1, 99, 104, 97, 109, 112, 105, 111, 110, 2],\n",
       "  [1, 100, 101, 110, 118, 101, 114, 2],\n",
       "  [1, 98, 114, 111, 110, 99, 111, 115, 2],\n",
       "  [1, 100, 101, 102, 101, 97, 116, 101, 100, 2],\n",
       "  [1, 116, 104, 101, 2],\n",
       "  [1, 110, 97, 116, 105, 111, 110, 97, 108, 2],\n",
       "  [1, 102, 111, 111, 116, 98, 97, 108, 108, 2],\n",
       "  [1, 99, 111, 110, 102, 101, 114, 101, 110, 99, 101, 2],\n",
       "  [1, 40, 2],\n",
       "  [1, 110, 102, 99, 2],\n",
       "  [1, 41, 2],\n",
       "  [1, 99, 104, 97, 109, 112, 105, 111, 110, 2],\n",
       "  [1, 99, 97, 114, 111, 108, 105, 110, 97, 2],\n",
       "  [1, 112, 97, 110, 116, 104, 101, 114, 115, 2],\n",
       "  [1, 50, 52, 0, 49, 48, 2],\n",
       "  [1, 116, 111, 2],\n",
       "  [1, 101, 97, 114, 110, 2],\n",
       "  [1, 116, 104, 101, 105, 114, 2],\n",
       "  [1, 116, 104, 105, 114, 100, 2],\n",
       "  [1, 115, 117, 112, 101, 114, 2],\n",
       "  [1, 98, 111, 119, 108, 2],\n",
       "  [1, 116, 105, 116, 108, 101, 2],\n",
       "  [1, 46, 2],\n",
       "  [1, 116, 104, 101, 2],\n",
       "  [1, 103, 97, 109, 101, 2],\n",
       "  [1, 119, 97, 115, 2],\n",
       "  [1, 112, 108, 97, 121, 101, 100, 2],\n",
       "  [1, 111, 110, 2],\n",
       "  [1, 102, 101, 98, 114, 117, 97, 114, 121, 2],\n",
       "  [1, 55, 2],\n",
       "  [1, 44, 2],\n",
       "  [1, 50, 48, 49, 54, 2],\n",
       "  [1, 44, 2],\n",
       "  [1, 97, 116, 2],\n",
       "  [1, 108, 101, 118, 105, 2],\n",
       "  [1, 39, 115, 2],\n",
       "  [1, 115, 116, 97, 100, 105, 117, 109, 2],\n",
       "  [1, 105, 110, 2],\n",
       "  [1, 116, 104, 101, 2],\n",
       "  [1, 115, 97, 110, 2],\n",
       "  [1, 102, 114, 97, 110, 99, 105, 115, 99, 111, 2],\n",
       "  [1, 98, 97, 121, 2],\n",
       "  [1, 97, 114, 101, 97, 2],\n",
       "  [1, 97, 116, 2],\n",
       "  [1, 115, 97, 110, 116, 97, 2],\n",
       "  [1, 99, 108, 97, 114, 97, 2],\n",
       "  [1, 44, 2],\n",
       "  [1, 99, 97, 108, 105, 102, 111, 114, 110, 105, 97, 2],\n",
       "  [1, 46, 2],\n",
       "  [1, 97, 115, 2],\n",
       "  [1, 116, 104, 105, 115, 2],\n",
       "  [1, 119, 97, 115, 2],\n",
       "  [1, 116, 104, 101, 2],\n",
       "  [1, 53, 48, 116, 104, 2],\n",
       "  [1, 115, 117, 112, 101, 114, 2],\n",
       "  [1, 98, 111, 119, 108, 2],\n",
       "  [1, 44, 2],\n",
       "  [1, 116, 104, 101, 2],\n",
       "  [1, 108, 101, 97, 103, 117, 101, 2],\n",
       "  [1, 101, 109, 112, 104, 97, 115, 105, 122, 101, 100, 2],\n",
       "  [1, 116, 104, 101, 2],\n",
       "  [1, 96, 96, 2],\n",
       "  [1, 103, 111, 108, 100, 101, 110, 2],\n",
       "  [1, 97, 110, 110, 105, 118, 101, 114, 115, 97, 114, 121, 2],\n",
       "  [1, 39, 39, 2],\n",
       "  [1, 119, 105, 116, 104, 2],\n",
       "  [1, 118, 97, 114, 105, 111, 117, 115, 2],\n",
       "  [1, 103, 111, 108, 100, 45, 116, 104, 101, 109, 101, 100, 2],\n",
       "  [1, 105, 110, 105, 116, 105, 97, 116, 105, 118, 101, 115, 2],\n",
       "  [1, 44, 2],\n",
       "  [1, 97, 115, 2],\n",
       "  [1, 119, 101, 108, 108, 2],\n",
       "  [1, 97, 115, 2],\n",
       "  [1, 116, 101, 109, 112, 111, 114, 97, 114, 105, 108, 121, 2],\n",
       "  [1, 115, 117, 115, 112, 101, 110, 100, 105, 110, 103, 2],\n",
       "  [1, 116, 104, 101, 2],\n",
       "  [1, 116, 114, 97, 100, 105, 116, 105, 111, 110, 2],\n",
       "  [1, 111, 102, 2],\n",
       "  [1, 110, 97, 109, 105, 110, 103, 2],\n",
       "  [1, 101, 97, 99, 104, 2],\n",
       "  [1, 115, 117, 112, 101, 114, 2],\n",
       "  [1, 98, 111, 119, 108, 2],\n",
       "  [1, 103, 97, 109, 101, 2],\n",
       "  [1, 119, 105, 116, 104, 2],\n",
       "  [1, 114, 111, 109, 97, 110, 2],\n",
       "  [1, 110, 117, 109, 101, 114, 97, 108, 115, 2],\n",
       "  [1, 40, 2],\n",
       "  [1, 117, 110, 100, 101, 114, 2],\n",
       "  [1, 119, 104, 105, 99, 104, 2],\n",
       "  [1, 116, 104, 101, 2],\n",
       "  [1, 103, 97, 109, 101, 2],\n",
       "  [1, 119, 111, 117, 108, 100, 2],\n",
       "  [1, 104, 97, 118, 101, 2],\n",
       "  [1, 98, 101, 101, 110, 2],\n",
       "  [1, 107, 110, 111, 119, 110, 2],\n",
       "  [1, 97, 115, 2],\n",
       "  [1, 96, 96, 2],\n",
       "  [1, 115, 117, 112, 101, 114, 2],\n",
       "  [1, 98, 111, 119, 108, 2],\n",
       "  [1, 108, 2],\n",
       "  [1, 39, 39, 2],\n",
       "  [1, 41, 2],\n",
       "  [1, 44, 2],\n",
       "  [1, 115, 111, 2],\n",
       "  [1, 116, 104, 97, 116, 2],\n",
       "  [1, 116, 104, 101, 2],\n",
       "  [1, 108, 111, 103, 111, 2],\n",
       "  [1, 99, 111, 117, 108, 100, 2],\n",
       "  [1, 112, 114, 111, 109, 105, 110, 101, 110, 116, 108, 121, 2],\n",
       "  [1, 102, 101, 97, 116, 117, 114, 101, 2],\n",
       "  [1, 116, 104, 101, 2],\n",
       "  [1, 97, 114, 97, 98, 105, 99, 2],\n",
       "  [1, 110, 117, 109, 101, 114, 97, 108, 115, 2],\n",
       "  [1, 53, 48, 2],\n",
       "  [1, 46, 2]]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dev_char[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_chars = map(lambda x: x[0], sorted(c_to_i.items(), key=lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with io.open('/pio/data/data/squad/train_charlist.txt', 'w', encoding='utf-8') as f:\n",
    "    for w in sorted_chars:\n",
    "        f.write(unicode(w + '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/pio/data/data/squad/train_char_ascii.pkl', 'w') as f:\n",
    "    pickle.dump(data_char, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/pio/data/data/squad/dev_char_ascii.pkl', 'w') as f:\n",
    "    pickle.dump(data_dev_char, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQuAD data with glove dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add unk to glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_vec = np.load('/pio/data/data/glove_vec/6B/glove.6B.300d.npy')\n",
    "\n",
    "glove_words = []\n",
    "\n",
    "with io.open('/pio/data/data/glove_vec/6B/glove.6B.wordlist.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        glove_words.append(line.split()[0])\n",
    "        \n",
    "glove_words.insert(0, '<unk>')\n",
    "glove_vec = np.vstack([glove_vec.mean(axis=0), glove_vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_i_to_w = glove_words\n",
    "glove_w_to_i = {v:k for (k,v) in list(enumerate(glove_words))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('/pio/data/data/glove_vec/6B/glove.6B.300d', glove_vec)\n",
    "\n",
    "with io.open('/pio/data/data/glove_vec/6B/glove.6B.wordlist.txt', 'w', encoding='utf-8') as f:\n",
    "    for w in glove_words:\n",
    "        f.write(unicode(w + '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make train and dev set with glove dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_set = np.load('/pio/data/data/squad/train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Originally contexts are split into sentences, this reverses that.\n",
    "for i in xrange(len(train_set)):\n",
    "    train_set[i].append(list(chain(*train_set[i][1][1:])))\n",
    "    train_set[i][1] = train_set[i][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for di in xrange(len(train_set)):\n",
    "    for si in xrange(len(train_set[di][1:])):\n",
    "        for ii in xrange(len(train_set[di][1:][si])):\n",
    "            i = train_set[di][1:][si][ii]\n",
    "            train_set[di][1:][si][ii] = glove_w_to_i.get(i_to_w[i], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('/pio/data/data/squad/train_with_glove_vocab.pkl', 'w') as f:\n",
    "    pickle.dump(train_set, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# zrobione wyżej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_set = np.load('/pio/data/data/squad/train_with_glove_vocab.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dev_set = np.load('/pio/data/data/squad/dev_with_glove_vocab.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Originally contexts are split into sentences, this reverses that.\n",
    "for i in xrange(len(dev_set)):\n",
    "    dev_set[i].append(list(chain(*dev_set[i][1][1:])))\n",
    "    dev_set[i][1] = dev_set[i][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('/pio/data/data/squad/dev_with_glove_vocab.pkl', 'w') as f:\n",
    "    pickle.dump(dev_set, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data powinno być bezpośrednio po wykonaniu okienka, w którym jest inicjowane words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_chars = sorted({c for w in glove_w_to_i for c in w})\n",
    "glove_chars.insert(0, '<unk_char>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_i_to_c = glove_chars\n",
    "glove_c_to_i = {v:k for (k,v) in list(enumerate(glove_chars))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with io.open('/pio/data/data/glove_vec/6B/glove.6B.charlist.txt', 'w', encoding='utf-8') as f:\n",
    "    for w in glove_chars:\n",
    "        f.write(unicode(w + '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_char = []\n",
    "\n",
    "for _, q, x in data:\n",
    "    q_char = [[glove_c_to_i.get(c, 0) for c in w] for w in q]\n",
    "    x_char = [[glove_c_to_i.get(c, 0) for c in w] for w in x]\n",
    "    data_char.append([q_char, x_char])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dev_char = []\n",
    "\n",
    "for _, q, x, _ in data_dev:\n",
    "    q_char = [[glove_c_to_i.get(c, 0) for c in w] for w in q]\n",
    "    x_char = [[glove_c_to_i.get(c, 0) for c in w] for w in x]\n",
    "    data_dev_char.append([q_char, x_char])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO?\n",
    "# here i wanted to add artificial tokens for beginning and end of a word (as in https://arxiv.org/abs/1508.06615)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/pio/data/data/squad/train_char_with_glove_alphabet.pkl', 'w') as f:\n",
    "    pickle.dump(data_char, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/pio/data/data/squad/dev_char_with_glove_alphabet.pkl', 'w') as f:\n",
    "    pickle.dump(data_dev_char, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

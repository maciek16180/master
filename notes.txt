When calculating freqs for sampled softmax in train scripts I used the length of a trimmed
train set instead of a full train set.
result: the <utt_end> token will have *slightly* lower chance that it should have
        (but it probably doesn't matter at all)

####
TODO
####

# Run pretrain_simple_rnnlm.py to do Subtle bootstrapping for that model.
# Didn't help (with the old version anyway).

In beam search: merging branches with similar decoder states.
I don't think it solves the problem of very short resposes getting the best scores though.

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 780 (CNMeM is enabled with initial size: 30.0% of memory, cuDNN 4007)\n",
      "/home/i258346/.local/lib/python2.7/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import lasagne as L\n",
    "\n",
    "sys.path.insert(0, '../HSoftmaxLayerLasagne/')\n",
    "\n",
    "from HSoftmaxLayer import HierarchicalSoftmaxDenseLayer\n",
    "from SampledSoftmaxLayer import SampledSoftmaxDenseLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DODAĆ DO SŁOWNIKA UTT_END (-1), INACZEJ LICZBA KLAS W SOFTMAKSIE SIĘ NIE ZGADZA!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mt_path = \"/pio/data/data/mtriples/\"\n",
    "\n",
    "def get_mt_voc(path=mt_path):\n",
    "    word_list = np.load(mt_path + 'Training.dict.pkl')\n",
    "    word_list = map(lambda x: x[:2], word_list)\n",
    "    wc = len(word_list)\n",
    "    \n",
    "    w_to_idx = dict(word_list)\n",
    "    idx_to_w = {v : k for (k,v) in w_to_idx.items()}\n",
    "    \n",
    "    return idx_to_w, w_to_idx, wc\n",
    "\n",
    "idx_to_w, w_to_idx, voc_size = get_mt_voc()\n",
    "\n",
    "\n",
    "def load_mt(path=mt_path):\n",
    "    tr = np.load(mt_path + 'Training.triples.pkl')\n",
    "    vl = np.load(mt_path + 'Validation.triples.pkl')\n",
    "    ts = np.load(mt_path + 'Test.triples.pkl')\n",
    "    \n",
    "    return tr, vl, ts\n",
    "\n",
    "train, valid, test = load_mt()\n",
    "\n",
    "train = [utt for utt in train if len(utt) < 200]\n",
    "valid = [utt for utt in valid if len(utt) < 200]\n",
    "test  = [utt for utt in test  if len(utt) < 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec_embs, word2vec_embs_mask = np.load(mt_path + 'Word2Vec_WordEmb.pkl')\n",
    "word2vec_embs = np.vstack([word2vec_embs, L.init.GlorotUniform()((1,300))]).astype(np.float32)\n",
    "word2vec_embs_mask = np.vstack([word2vec_embs_mask, np.zeros((1,300))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Similar to Lasagne mnist.py example, added input mask and different sequence lengths\n",
    "\n",
    "def iterate_minibatches(inputs, batchsize, pad=-1):\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):        \n",
    "        excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        inp = inputs[excerpt]\n",
    "        \n",
    "        inp_max_len = len(max(inp, key=len))\n",
    "        inp = map(lambda l: l + [pad]*(inp_max_len-len(l)), inp)\n",
    "        inp = np.asarray(inp, dtype=np.int32)\n",
    "        tar = np.hstack([inp[:,1:], np.zeros((batchsize,1), dtype=np.int32) + pad])\n",
    "        def not_pad(x):\n",
    "            return x != pad\n",
    "        v_not_pad = np.vectorize(not_pad, otypes=[np.float32])\n",
    "        mask = v_not_pad(inp) # there is no separate value for the end of an utterance right now, just pad\n",
    "        \n",
    "        yield inp, tar, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_simple_rnnlm(input_var, mask_input_var, voc_size, emb_size, rec_size, emb_init=None):\n",
    "    l_in = L.layers.InputLayer(shape=(None, None), input_var=input_var)  \n",
    "    batch_size, seq_len = l_in.input_var.shape\n",
    "    \n",
    "    l_mask = None\n",
    "    if mask_input_var is not None:\n",
    "        print 'setting up input mask...'\n",
    "        l_mask = L.layers.InputLayer(shape=(batch_size, seq_len), input_var=mask_input_var)\n",
    "    \n",
    "    if emb_init is None:\n",
    "        l_emb = L.layers.EmbeddingLayer(l_in,\n",
    "                                        input_size=voc_size+1, \n",
    "                                        output_size=emb_size)\n",
    "    else:\n",
    "        l_emb = L.layers.EmbeddingLayer(l_in,\n",
    "                                        input_size=voc_size+1, \n",
    "                                        output_size=emb_size,\n",
    "                                        W=emb_init)\n",
    "        l_emb.params[l_emb.W].remove('trainable')\n",
    "    \n",
    "    l_lstm1 = L.layers.LSTMLayer(l_emb,\n",
    "                                 num_units=rec_size,\n",
    "                                 nonlinearity=L.nonlinearities.tanh,\n",
    "                                 grad_clipping=100,\n",
    "                                 mask_input=l_mask)\n",
    "    \n",
    "    l_lstm2 = L.layers.LSTMLayer(l_lstm1,\n",
    "                                 num_units=rec_size,\n",
    "                                 nonlinearity=L.nonlinearities.tanh,\n",
    "                                 grad_clipping=100,\n",
    "                                 mask_input=l_mask)\n",
    "    \n",
    "    l_resh = L.layers.ReshapeLayer(l_lstm2, shape=(-1, rec_size))\n",
    "    \n",
    "    l_soft = L.layers.DenseLayer(l_resh,\n",
    "                                num_units=voc_size,\n",
    "                                nonlinearity=L.nonlinearities.softmax)\n",
    "    \n",
    "    l_out = L.layers.ReshapeLayer(l_soft, shape=(batch_size, seq_len, voc_size))\n",
    "    \n",
    "    return l_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_hsoft_rnnlm(input_var, target_var, mask_input_var, voc_size, emb_size, rec_size):\n",
    "    l_in = L.layers.InputLayer(shape=(None, None), input_var=input_var)    \n",
    "    batch_size, seq_len = l_in.input_var.shape\n",
    "    l_mask = None\n",
    "    if mask_input_var is not None:\n",
    "        print 'setting up input mask...'\n",
    "        l_mask = L.layers.InputLayer(shape=(batch_size, seq_len), input_var=mask_input_var)\n",
    "    \n",
    "    l_emb = L.layers.EmbeddingLayer(l_in,\n",
    "                                    input_size=voc_size+1, \n",
    "                                    output_size=emb_size)\n",
    "    \n",
    "    l_lstm1 = L.layers.LSTMLayer(l_emb,\n",
    "                                 num_units=rec_size,\n",
    "                                 nonlinearity=L.nonlinearities.tanh,\n",
    "                                 grad_clipping=100,\n",
    "                                 mask_input=l_mask)    \n",
    "    \n",
    "#     l_lstm2 = L.layers.LSTMLayer(l_lstm1,\n",
    "#                                  num_units=rec_size,\n",
    "#                                  nonlinearity=L.nonlinearities.tanh,\n",
    "#                                  grad_clipping=100,\n",
    "#                                  mask_input=l_mask)\n",
    "    \n",
    "    l_resh = L.layers.ReshapeLayer(l_lstm1, shape=(-1, rec_size))\n",
    "    \n",
    "    # hierarchical softmax\n",
    "    \n",
    "    l_resh_tar = None\n",
    "    if target_var is not None:\n",
    "        print 'setting up targets for hsoftmax...'\n",
    "        l_tar = L.layers.InputLayer(shape=(None, None), input_var=target_var)\n",
    "        l_resh_tar = L.layers.ReshapeLayer(l_tar, shape=(-1, 1))\n",
    "        \n",
    "    l_hsoft = HierarchicalSoftmaxDenseLayer(l_resh,\n",
    "                                            num_units=voc_size,\n",
    "                                            target=l_resh_tar)\n",
    "    l_out = None\n",
    "    if target_var is not None:\n",
    "        l_out = L.layers.ReshapeLayer(l_hsoft, shape=(batch_size, seq_len))\n",
    "    else:\n",
    "        l_out = L.layers.ReshapeLayer(l_hsoft, shape=(batch_size, seq_len, voc_size))\n",
    "    \n",
    "    return l_out\n",
    "\n",
    "# 1 epoch on gpu with hsoft took about 700s, batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_sampledsoft_rnnlm(input_var, mask_input_var, num_sampled, voc_size, \n",
    "                            emb_size, rec_size, target_var=None, use_all_words=False):\n",
    "    l_in = L.layers.InputLayer(shape=(None, None), input_var=input_var)    \n",
    "    batch_size, seq_len = l_in.input_var.shape\n",
    "    l_mask = None\n",
    "    if mask_input_var != None:\n",
    "        print 'setting up input mask...'\n",
    "        l_mask = L.layers.InputLayer(shape=(batch_size, seq_len), input_var=mask_input_var)\n",
    "    \n",
    "    l_emb = L.layers.EmbeddingLayer(l_in,\n",
    "                                    input_size=voc_size+1, \n",
    "                                    output_size=emb_size)\n",
    "    \n",
    "    l_lstm1 = L.layers.LSTMLayer(l_emb,\n",
    "                                 num_units=rec_size,\n",
    "                                 nonlinearity=L.nonlinearities.tanh,\n",
    "                                 grad_clipping=100,\n",
    "                                 mask_input=l_mask)\n",
    "    \n",
    "    l_lstm2 = L.layers.LSTMLayer(l_lstm1,\n",
    "                                 num_units=rec_size,\n",
    "                                 nonlinearity=L.nonlinearities.tanh,\n",
    "                                 grad_clipping=100,\n",
    "                                 mask_input=l_mask)\n",
    "      \n",
    "    l_resh = L.layers.ReshapeLayer(l_lstm2, shape=(-1, rec_size))\n",
    "    \n",
    "    if target_var is not None:\n",
    "        print 'setting up targets for sampled softmax...'\n",
    "        target_var = target_var.ravel()\n",
    "    \n",
    "    l_ssoft = SampledSoftmaxDenseLayer(l_resh, num_sampled, voc_size, \n",
    "                                       targets=target_var, \n",
    "                                       use_all_words=use_all_words)\n",
    "    \n",
    "    if target_var is not None:\n",
    "        l_out = L.layers.ReshapeLayer(l_ssoft, shape=(batch_size, seq_len))\n",
    "    else:\n",
    "        l_out = L.layers.ReshapeLayer(l_ssoft, shape=(batch_size, seq_len, voc_size))\n",
    "    \n",
    "    return l_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emb_size = 300\n",
    "rec_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clone_param_values(net_from, net_to):\n",
    "    L.layers.set_all_param_values(net_to, L.layers.get_all_param_values(net_from))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up input mask...\n"
     ]
    }
   ],
   "source": [
    "# full softmax test\n",
    "\n",
    "input_var = T.imatrix('inputs')\n",
    "targets = T.imatrix('targets') # these will be inputs shifted by 1\n",
    "mask_input_var = T.matrix('input_mask')\n",
    "\n",
    "net = build_simple_rnnlm(input_var, mask_input_var, voc_size, emb_size, rec_size, \n",
    "                         emb_init=word2vec_embs)\n",
    "out = L.layers.get_output(net)\n",
    "\n",
    "mask_idx = mask_input_var.nonzero()\n",
    "loss = L.objectives.categorical_crossentropy(out[mask_idx], targets[mask_idx])\n",
    "loss = loss.mean() # mean batch loss\n",
    "\n",
    "params = L.layers.get_all_params(net, trainable=True)\n",
    "updates = L.updates.adagrad(loss, params, learning_rate=.01)\n",
    "\n",
    "train_fn = theano.function([input_var, targets, mask_input_var], loss, updates=updates)\n",
    "\n",
    "### for validation\n",
    "\n",
    "test_net = net # this line is just for compatibility later\n",
    "\n",
    "test_out = L.layers.get_output(net, deterministic=True)\n",
    "test_loss = L.objectives.categorical_crossentropy(test_out[mask_idx], targets[mask_idx])\n",
    "test_loss = test_loss.mean()\n",
    "# test_acc = T.mean(T.eq(T.argmax(test_out, axis=1), targets), dtype=theano.config.floatX)\n",
    "\n",
    "val_fn = theano.function([input_var, targets, mask_input_var], test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up input mask...\n",
      "setting up targets for sampled softmax...\n",
      "setting up input mask...\n",
      "setting up targets for sampled softmax...\n"
     ]
    }
   ],
   "source": [
    "# sampled softmax test (with targets!)\n",
    "\n",
    "num_sampled = None\n",
    "\n",
    "input_var = T.imatrix('inputs')\n",
    "targets = T.imatrix('targets') # these will be inputs shifted by 1\n",
    "mask_input_var = T.matrix('input_mask')\n",
    "\n",
    "net = build_sampledsoft_rnnlm(input_var, mask_input_var, num_sampled, voc_size, \n",
    "                              emb_size, rec_size, target_var=targets)\n",
    "out = L.layers.get_output(net)\n",
    "\n",
    "mask_idx = mask_input_var.nonzero()\n",
    "loss = -T.sum(T.log(out[mask_idx])) / T.sum(mask_input_var)\n",
    "\n",
    "params = L.layers.get_all_params(net, trainable=True)\n",
    "updates = L.updates.adagrad(loss, params, learning_rate=.01)\n",
    "# updates = L.updates.rmsprop(loss, params, learning_rate=.001, rho=.9, epsilon=1e-06)\n",
    "\n",
    "train_fn = theano.function([input_var, targets, mask_input_var], loss, updates=updates)\n",
    "\n",
    "### for validation\n",
    "\n",
    "test_input_var = T.imatrix('inputs')\n",
    "test_targets = T.imatrix('targets')\n",
    "test_mask_input_var = T.matrix('input_mask')\n",
    "\n",
    "test_net = build_sampledsoft_rnnlm(test_input_var, test_mask_input_var, num_sampled, voc_size, \n",
    "                                   emb_size, rec_size, target_var=test_targets, use_all_words=True)\n",
    "\n",
    "test_mask_idx = test_mask_input_var.nonzero()\n",
    "\n",
    "test_out = L.layers.get_output(test_net, deterministic=True)\n",
    "test_loss = -T.sum(T.log(test_out[test_mask_idx])) / T.sum(test_mask_input_var)\n",
    "\n",
    "# test_acc = T.mean(T.eq(T.argmax(test_out, axis=1), targets), dtype=theano.config.floatX)\n",
    "\n",
    "val_fn = theano.function([test_input_var, test_targets, test_mask_input_var], test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up input mask...\n",
      "setting up targets for hsoftmax...\n",
      "setting up input mask...\n",
      "setting up targets for hsoftmax...\n"
     ]
    }
   ],
   "source": [
    "# hierarchical softmax test\n",
    "\n",
    "input_var = T.imatrix('inputs')\n",
    "targets = T.imatrix('targets') # these will be inputs shifted by 1\n",
    "mask_input_var = T.matrix('input_mask')\n",
    "\n",
    "net = build_hsoft_rnnlm(input_var, targets, mask_input_var, voc_size, emb_size, rec_size)\n",
    "out = L.layers.get_output(net)\n",
    "\n",
    "mask_idx = mask_input_var.nonzero()\n",
    "loss = -T.sum(T.log(out[mask_idx])) / T.sum(mask_input_var)\n",
    "\n",
    "params = L.layers.get_all_params(net, trainable=True)\n",
    "#updates = L.updates.rmsprop(loss, params, learning_rate=.001, rho=.9, epsilon=1e-06)\n",
    "updates = L.updates.adagrad(loss, params, learning_rate=.01)\n",
    "\n",
    "train_fn = theano.function([input_var, targets, mask_input_var], loss, updates=updates)\n",
    "\n",
    "#### for validation\n",
    "\n",
    "test_net = net # this line is just for compatibility later\n",
    "\n",
    "test_out = L.layers.get_output(net, deterministic=True)\n",
    "test_loss = -T.sum(T.log(test_out[mask_idx])) / T.sum(mask_input_var)\n",
    "\n",
    "#test_acc = T.mean(T.eq(T.argmax(test_out, axis=1), targets), dtype=theano.config.floatX)\n",
    "\n",
    "val_fn = theano.function([input_var, targets, mask_input_var], test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 10 batches in 4.65 sec.    training loss:\t\t7.42406930923\n",
      "Done 20 batches in 9.59 sec.    training loss:\t\t6.59261679649\n",
      "Done 30 batches in 14.75 sec.    training loss:\t\t6.25511074066\n",
      "Done 40 batches in 19.32 sec.    training loss:\t\t6.07375824451\n",
      "Done 50 batches in 23.67 sec.    training loss:\t\t5.9530665493\n",
      "Done 60 batches in 28.58 sec.    training loss:\t\t5.85336194833\n",
      "Done 70 batches in 33.40 sec.    training loss:\t\t5.76497023446\n",
      "Done 80 batches in 37.91 sec.    training loss:\t\t5.68909730911\n",
      "Done 90 batches in 42.80 sec.    training loss:\t\t5.6094874435\n",
      "Done 100 batches in 48.01 sec.    training loss:\t\t5.54523957729\n",
      "Done 110 batches in 52.53 sec.    training loss:\t\t5.48311381774\n",
      "Done 120 batches in 57.07 sec.    training loss:\t\t5.42250680923\n",
      "Done 130 batches in 61.77 sec.    training loss:\t\t5.37028982456\n",
      "Done 140 batches in 66.42 sec.    training loss:\t\t5.32293986934\n",
      "Done 150 batches in 71.15 sec.    training loss:\t\t5.2794783783\n",
      "Done 160 batches in 75.55 sec.    training loss:\t\t5.23645240366\n",
      "Done 170 batches in 80.21 sec.    training loss:\t\t5.19687960008\n",
      "Done 180 batches in 85.15 sec.    training loss:\t\t5.16199208101\n",
      "Done 190 batches in 89.62 sec.    training loss:\t\t5.13053916379\n",
      "Done 200 batches in 94.14 sec.    training loss:\t\t5.09959283113\n",
      "Done 210 batches in 98.71 sec.    training loss:\t\t5.07118217605\n",
      "Done 220 batches in 103.74 sec.    training loss:\t\t5.04517651905\n",
      "Done 230 batches in 108.58 sec.    training loss:\t\t5.0189359271\n",
      "Done 240 batches in 113.26 sec.    training loss:\t\t4.99566968878\n",
      "Done 250 batches in 118.31 sec.    training loss:\t\t4.97060261917\n",
      "Done 260 batches in 122.88 sec.    training loss:\t\t4.94611130311\n",
      "Done 270 batches in 127.52 sec.    training loss:\t\t4.92483537109\n",
      "Done 280 batches in 132.29 sec.    training loss:\t\t4.90421564238\n",
      "Done 290 batches in 136.88 sec.    training loss:\t\t4.88515113304\n",
      "Done 300 batches in 141.19 sec.    training loss:\t\t4.86402647654\n",
      "Done 310 batches in 145.75 sec.    training loss:\t\t4.84660037102\n",
      "Done 320 batches in 150.26 sec.    training loss:\t\t4.82873048782\n",
      "Done 330 batches in 155.31 sec.    training loss:\t\t4.81271823825\n",
      "Done 340 batches in 159.66 sec.    training loss:\t\t4.7957747796\n",
      "Done 350 batches in 164.21 sec.    training loss:\t\t4.77984410422\n",
      "Done 360 batches in 168.86 sec.    training loss:\t\t4.76302394602\n",
      "Done 370 batches in 173.37 sec.    training loss:\t\t4.74913377246\n",
      "Done 380 batches in 178.01 sec.    training loss:\t\t4.73860850334\n",
      "Done 390 batches in 182.89 sec.    training loss:\t\t4.72667052196\n",
      "Done 400 batches in 187.80 sec.    training loss:\t\t4.71366610587\n",
      "Done 410 batches in 192.41 sec.    training loss:\t\t4.70035117836\n",
      "Done 420 batches in 197.11 sec.    training loss:\t\t4.68870645818\n",
      "Done 430 batches in 201.46 sec.    training loss:\t\t4.67672216171\n",
      "Done 440 batches in 206.24 sec.    training loss:\t\t4.66516898545\n",
      "Done 450 batches in 211.20 sec.    training loss:\t\t4.6558686871\n",
      "Done 460 batches in 215.31 sec.    training loss:\t\t4.64256659228\n",
      "Done 470 batches in 219.82 sec.    training loss:\t\t4.63114334522\n",
      "Done 480 batches in 224.39 sec.    training loss:\t\t4.62061059773\n",
      "Done 490 batches in 229.35 sec.    training loss:\t\t4.61238986862\n",
      "Done 500 batches in 234.29 sec.    training loss:\t\t4.60436216974\n",
      "Done 510 batches in 239.07 sec.    training loss:\t\t4.59621947466\n",
      "Done 520 batches in 243.71 sec.    training loss:\t\t4.58635223508\n",
      "Done 530 batches in 248.45 sec.    training loss:\t\t4.57802451557\n",
      "Done 540 batches in 253.13 sec.    training loss:\t\t4.56970322353\n",
      "Done 550 batches in 258.04 sec.    training loss:\t\t4.56212030454\n",
      "Done 560 batches in 262.63 sec.    training loss:\t\t4.55310546117\n",
      "Done 570 batches in 267.21 sec.    training loss:\t\t4.54465724962\n",
      "Done 580 batches in 272.10 sec.    training loss:\t\t4.53828108393\n",
      "Done 590 batches in 276.72 sec.    training loss:\t\t4.53011653706\n",
      "Done 600 batches in 281.25 sec.    training loss:\t\t4.52191088796\n",
      "Done 610 batches in 285.96 sec.    training loss:\t\t4.51547151941\n",
      "Done 620 batches in 290.38 sec.    training loss:\t\t4.50732708016\n",
      "Done 630 batches in 294.90 sec.    training loss:\t\t4.50035435661\n",
      "Done 640 batches in 299.20 sec.    training loss:\t\t4.49323785529\n",
      "Done 650 batches in 303.95 sec.    training loss:\t\t4.486957323\n",
      "Done 660 batches in 308.55 sec.    training loss:\t\t4.48113425464\n",
      "Done 670 batches in 313.49 sec.    training loss:\t\t4.47502318532\n",
      "Done 680 batches in 317.94 sec.    training loss:\t\t4.46829913259\n",
      "Done 690 batches in 322.99 sec.    training loss:\t\t4.46313709556\n",
      "Done 700 batches in 327.75 sec.    training loss:\t\t4.45762908527\n",
      "Done 710 batches in 332.71 sec.    training loss:\t\t4.45214377894\n",
      "Done 720 batches in 337.15 sec.    training loss:\t\t4.44630991386\n",
      "Done 730 batches in 341.87 sec.    training loss:\t\t4.44023174194\n",
      "Done 740 batches in 346.30 sec.    training loss:\t\t4.43522394863\n",
      "Done 750 batches in 351.11 sec.    training loss:\t\t4.43023326874\n",
      "Done 760 batches in 355.81 sec.    training loss:\t\t4.42477545205\n",
      "Done 770 batches in 360.41 sec.    training loss:\t\t4.41975188379\n",
      "Done 780 batches in 364.90 sec.    training loss:\t\t4.41417005123\n",
      "Done 790 batches in 369.26 sec.    training loss:\t\t4.40830649756\n",
      "Done 800 batches in 373.90 sec.    training loss:\t\t4.40341181219\n",
      "Done 810 batches in 378.24 sec.    training loss:\t\t4.39762951975\n",
      "Done 820 batches in 382.99 sec.    training loss:\t\t4.39275943506\n",
      "Done 830 batches in 387.42 sec.    training loss:\t\t4.38625241475\n",
      "Done 840 batches in 392.54 sec.    training loss:\t\t4.38242610296\n",
      "Done 850 batches in 397.31 sec.    training loss:\t\t4.37806426189\n",
      "Done 860 batches in 401.81 sec.    training loss:\t\t4.37320031288\n",
      "Done 870 batches in 406.83 sec.    training loss:\t\t4.36789544451\n",
      "Done 880 batches in 411.74 sec.    training loss:\t\t4.36270523694\n",
      "Done 890 batches in 416.42 sec.    training loss:\t\t4.35753880003\n",
      "Done 900 batches in 421.07 sec.    training loss:\t\t4.35254023049\n",
      "Done 910 batches in 425.58 sec.    training loss:\t\t4.34833019723\n",
      "Done 920 batches in 430.34 sec.    training loss:\t\t4.3443533703\n",
      "Done 930 batches in 435.16 sec.    training loss:\t\t4.34037108729\n",
      "Done 940 batches in 440.30 sec.    training loss:\t\t4.33674316482\n",
      "Done 950 batches in 444.76 sec.    training loss:\t\t4.33237136188\n",
      "Done 960 batches in 449.53 sec.    training loss:\t\t4.32806588933\n",
      "Done 970 batches in 454.06 sec.    training loss:\t\t4.32441146054\n",
      "Done 980 batches in 458.55 sec.    training loss:\t\t4.32039259186\n",
      "Done 990 batches in 463.46 sec.    training loss:\t\t4.31597821183\n",
      "Done 1000 batches in 467.91 sec.    training loss:\t\t4.31159377337\n",
      "Done 1010 batches in 472.56 sec.    training loss:\t\t4.30751396736\n",
      "Done 1020 batches in 477.10 sec.    training loss:\t\t4.30311528514\n",
      "Done 1030 batches in 481.79 sec.    training loss:\t\t4.29970737036\n",
      "Done 1040 batches in 487.04 sec.    training loss:\t\t4.29657816474\n",
      "Done 1050 batches in 491.66 sec.    training loss:\t\t4.29332538082\n",
      "Done 1060 batches in 496.10 sec.    training loss:\t\t4.28998776202\n",
      "Done 1070 batches in 500.14 sec.    training loss:\t\t4.28615743535\n",
      "Done 1080 batches in 505.40 sec.    training loss:\t\t4.28273118138\n",
      "Done 1090 batches in 510.07 sec.    training loss:\t\t4.27959263325\n",
      "Done 1100 batches in 514.52 sec.    training loss:\t\t4.27609364033\n",
      "Done 1110 batches in 519.60 sec.    training loss:\t\t4.27354763628\n",
      "Done 1120 batches in 524.31 sec.    training loss:\t\t4.27001799835\n",
      "Done 1130 batches in 528.91 sec.    training loss:\t\t4.26632162596\n",
      "Done 1140 batches in 533.74 sec.    training loss:\t\t4.26348848657\n",
      "Done 1150 batches in 537.89 sec.    training loss:\t\t4.26023911642\n",
      "Done 1160 batches in 542.69 sec.    training loss:\t\t4.25778237828\n",
      "Done 1170 batches in 547.49 sec.    training loss:\t\t4.25498479289\n",
      "Done 1180 batches in 552.23 sec.    training loss:\t\t4.25106617798\n",
      "Done 1190 batches in 556.80 sec.    training loss:\t\t4.24850541323\n",
      "Done 1200 batches in 561.71 sec.    training loss:\t\t4.24607892891\n",
      "Done 1210 batches in 566.11 sec.    training loss:\t\t4.24287122675\n",
      "Done 1220 batches in 570.82 sec.    training loss:\t\t4.23992162005\n",
      "Done 1230 batches in 575.88 sec.    training loss:\t\t4.23707789406\n",
      "Done 1240 batches in 580.94 sec.    training loss:\t\t4.23422428793\n",
      "Done 1250 batches in 585.36 sec.    training loss:\t\t4.23127424679\n",
      "Done 1260 batches in 589.82 sec.    training loss:\t\t4.22808836793\n",
      "Done 1270 batches in 594.23 sec.    training loss:\t\t4.22530383726\n",
      "Done 1280 batches in 598.73 sec.    training loss:\t\t4.22234759945\n",
      "Done 1290 batches in 603.58 sec.    training loss:\t\t4.2194028067\n",
      "Done 1300 batches in 608.20 sec.    training loss:\t\t4.21598641231\n",
      "Done 1310 batches in 612.95 sec.    training loss:\t\t4.21372066036\n",
      "Done 1320 batches in 617.25 sec.    training loss:\t\t4.21056973031\n",
      "Done 1330 batches in 622.28 sec.    training loss:\t\t4.20864082638\n",
      "Done 1340 batches in 626.94 sec.    training loss:\t\t4.20607729705\n",
      "Done 1350 batches in 631.42 sec.    training loss:\t\t4.20350092994\n",
      "Done 1360 batches in 635.96 sec.    training loss:\t\t4.20093508608\n",
      "Done 1370 batches in 641.09 sec.    training loss:\t\t4.19892409892\n",
      "Done 1380 batches in 645.45 sec.    training loss:\t\t4.19599796136\n",
      "Done 1390 batches in 649.99 sec.    training loss:\t\t4.19287730001\n",
      "Done 1400 batches in 654.31 sec.    training loss:\t\t4.18972738436\n",
      "Done 1410 batches in 658.86 sec.    training loss:\t\t4.18794571829\n",
      "Done 1420 batches in 663.82 sec.    training loss:\t\t4.18606318286\n",
      "Done 1430 batches in 668.57 sec.    training loss:\t\t4.18372095865\n",
      "Done 1440 batches in 673.16 sec.    training loss:\t\t4.18110185911\n",
      "Done 1450 batches in 677.94 sec.    training loss:\t\t4.17866601895\n",
      "Done 1460 batches in 682.13 sec.    training loss:\t\t4.17571672544\n",
      "Done 1470 batches in 686.99 sec.    training loss:\t\t4.17330878932\n",
      "Done 1480 batches in 691.43 sec.    training loss:\t\t4.17108564618\n",
      "Done 1490 batches in 696.08 sec.    training loss:\t\t4.16858257975\n",
      "Done 1500 batches in 701.14 sec.    training loss:\t\t4.16684508038\n",
      "Done 1510 batches in 705.77 sec.    training loss:\t\t4.16487344669\n",
      "Done 1520 batches in 710.31 sec.    training loss:\t\t4.16301808561\n",
      "Done 1530 batches in 715.70 sec.    training loss:\t\t4.16137921888\n",
      "Done 1540 batches in 720.08 sec.    training loss:\t\t4.15870758961\n",
      "Done 1550 batches in 725.00 sec.    training loss:\t\t4.15644311705\n",
      "Done 1560 batches in 729.31 sec.    training loss:\t\t4.15424623719\n",
      "Done 1570 batches in 733.73 sec.    training loss:\t\t4.1519987571\n",
      "Done 1580 batches in 739.12 sec.    training loss:\t\t4.14999292757\n",
      "Done 1590 batches in 743.72 sec.    training loss:\t\t4.14780360618\n",
      "Done 1600 batches in 748.55 sec.    training loss:\t\t4.14562956288\n",
      "Done 1610 batches in 752.69 sec.    training loss:\t\t4.14308244264\n",
      "Done 1620 batches in 757.31 sec.    training loss:\t\t4.14129662955\n",
      "Done 1630 batches in 761.35 sec.    training loss:\t\t4.13870792199\n",
      "Done 1640 batches in 766.24 sec.    training loss:\t\t4.1368050459\n",
      "Done 1650 batches in 770.75 sec.    training loss:\t\t4.13522927024\n",
      "Done 1660 batches in 775.20 sec.    training loss:\t\t4.13324401781\n",
      "Done 1670 batches in 779.95 sec.    training loss:\t\t4.13155131526\n",
      "Done 1680 batches in 784.75 sec.    training loss:\t\t4.12964249168\n",
      "Done 1690 batches in 789.00 sec.    training loss:\t\t4.12746540781\n",
      "Done 1700 batches in 793.81 sec.    training loss:\t\t4.12552543808\n",
      "Done 1710 batches in 798.81 sec.    training loss:\t\t4.12378090153\n",
      "Done 1720 batches in 803.20 sec.    training loss:\t\t4.12172407924\n",
      "Done 1730 batches in 807.44 sec.    training loss:\t\t4.11952751303\n",
      "Done 1740 batches in 812.15 sec.    training loss:\t\t4.11765658897\n",
      "Done 1750 batches in 816.61 sec.    training loss:\t\t4.11583399269\n",
      "Done 1760 batches in 821.09 sec.    training loss:\t\t4.11397731209\n",
      "Done 1770 batches in 825.90 sec.    training loss:\t\t4.11260968965\n",
      "Done 1780 batches in 830.89 sec.    training loss:\t\t4.11136121402\n",
      "Done 1790 batches in 835.55 sec.    training loss:\t\t4.1095263633\n",
      "Done 1800 batches in 840.31 sec.    training loss:\t\t4.10800639166\n",
      "Done 1810 batches in 844.51 sec.    training loss:\t\t4.105851653\n",
      "Done 1820 batches in 848.85 sec.    training loss:\t\t4.10380741384\n",
      "Done 1830 batches in 853.62 sec.    training loss:\t\t4.10218532268\n",
      "Done 1840 batches in 857.86 sec.    training loss:\t\t4.10039252898\n",
      "Done 1850 batches in 862.49 sec.    training loss:\t\t4.0987206057\n",
      "Done 1860 batches in 867.21 sec.    training loss:\t\t4.09708210166\n",
      "Done 1870 batches in 871.53 sec.    training loss:\t\t4.09515938695\n",
      "Done 1880 batches in 876.36 sec.    training loss:\t\t4.09382873216\n",
      "Done 1890 batches in 881.20 sec.    training loss:\t\t4.09215796347\n",
      "Done 1900 batches in 885.78 sec.    training loss:\t\t4.09046977633\n",
      "Done 1910 batches in 890.18 sec.    training loss:\t\t4.08845604739\n",
      "Done 1920 batches in 894.89 sec.    training loss:\t\t4.08696400014\n",
      "Done 1930 batches in 899.32 sec.    training loss:\t\t4.08542646077\n",
      "Done 1940 batches in 904.02 sec.    training loss:\t\t4.08353017593\n",
      "Done 1950 batches in 908.33 sec.    training loss:\t\t4.08177372859\n",
      "Done 1960 batches in 913.19 sec.    training loss:\t\t4.08022112287\n",
      "Done 1970 batches in 918.04 sec.    training loss:\t\t4.07861461071\n",
      "Done 1980 batches in 921.99 sec.    training loss:\t\t4.07668695992\n",
      "Done 1990 batches in 926.83 sec.    training loss:\t\t4.07522428779\n",
      "Done 2000 batches in 931.24 sec.    training loss:\t\t4.07326815021\n",
      "Done 2010 batches in 935.84 sec.    training loss:\t\t4.07174641967\n",
      "Done 2020 batches in 940.52 sec.    training loss:\t\t4.07039910779\n",
      "Done 2030 batches in 944.94 sec.    training loss:\t\t4.06869693977\n",
      "Done 2040 batches in 949.08 sec.    training loss:\t\t4.06655121551\n",
      "Done 2050 batches in 953.68 sec.    training loss:\t\t4.06499494146\n",
      "Done 2060 batches in 958.55 sec.    training loss:\t\t4.06376226157\n",
      "Done 2070 batches in 963.59 sec.    training loss:\t\t4.06252678973\n",
      "Done 2080 batches in 968.39 sec.    training loss:\t\t4.06093898588\n",
      "Done 2090 batches in 973.02 sec.    training loss:\t\t4.05940406596\n",
      "Done 2100 batches in 977.67 sec.    training loss:\t\t4.05795467592\n",
      "Done 2110 batches in 982.37 sec.    training loss:\t\t4.05638392254\n",
      "Done 2120 batches in 986.93 sec.    training loss:\t\t4.05504502958\n",
      "Done 2130 batches in 991.71 sec.    training loss:\t\t4.05361810469\n",
      "Done 2140 batches in 996.04 sec.    training loss:\t\t4.05203225256\n",
      "Done 2150 batches in 1000.35 sec.    training loss:\t\t4.05040911342\n",
      "Done 2160 batches in 1005.20 sec.    training loss:\t\t4.04903012878\n",
      "Done 2170 batches in 1010.10 sec.    training loss:\t\t4.04811502712\n",
      "Done 2180 batches in 1014.84 sec.    training loss:\t\t4.04676455773\n",
      "Done 2190 batches in 1019.36 sec.    training loss:\t\t4.04548056866\n",
      "Done 2200 batches in 1024.20 sec.    training loss:\t\t4.04403651725\n",
      "Done 2210 batches in 1029.12 sec.    training loss:\t\t4.042552458\n",
      "Done 2220 batches in 1033.70 sec.    training loss:\t\t4.04132320795\n",
      "Done 2230 batches in 1038.58 sec.    training loss:\t\t4.04027808322\n",
      "Done 2240 batches in 1043.37 sec.    training loss:\t\t4.03903471742\n",
      "Done 2250 batches in 1047.92 sec.    training loss:\t\t4.03750426727\n",
      "Done 2260 batches in 1052.90 sec.    training loss:\t\t4.03644407682\n",
      "Done 2270 batches in 1057.68 sec.    training loss:\t\t4.03496420005\n",
      "Done 2280 batches in 1062.29 sec.    training loss:\t\t4.03337226688\n",
      "Done 2290 batches in 1066.91 sec.    training loss:\t\t4.03229741353\n",
      "Done 2300 batches in 1071.68 sec.    training loss:\t\t4.03144087698\n",
      "Done 2310 batches in 1076.40 sec.    training loss:\t\t4.0300704313\n",
      "Done 2320 batches in 1081.01 sec.    training loss:\t\t4.02861496107\n",
      "Done 2330 batches in 1085.86 sec.    training loss:\t\t4.02742085007\n",
      "Done 2340 batches in 1090.95 sec.    training loss:\t\t4.02604039133\n",
      "Done 2350 batches in 1095.59 sec.    training loss:\t\t4.02492887213\n",
      "Done 2360 batches in 1100.01 sec.    training loss:\t\t4.02383158379\n",
      "Done 2370 batches in 1105.10 sec.    training loss:\t\t4.0228088565\n",
      "Done 2380 batches in 1110.26 sec.    training loss:\t\t4.02176720866\n",
      "Done 2390 batches in 1114.97 sec.    training loss:\t\t4.02060010433\n",
      "Done 2400 batches in 1119.49 sec.    training loss:\t\t4.01901844035\n",
      "Done 2410 batches in 1124.40 sec.    training loss:\t\t4.01779711682\n",
      "Done 2420 batches in 1129.26 sec.    training loss:\t\t4.01653188043\n",
      "Done 2430 batches in 1133.99 sec.    training loss:\t\t4.01522371916\n",
      "Done 2440 batches in 1138.58 sec.    training loss:\t\t4.01423126059\n",
      "Done 2450 batches in 1143.26 sec.    training loss:\t\t4.01304596337\n",
      "Done 2460 batches in 1148.15 sec.    training loss:\t\t4.0117531853\n",
      "Done 2470 batches in 1152.89 sec.    training loss:\t\t4.01063831644\n",
      "Done 2480 batches in 1157.61 sec.    training loss:\t\t4.00947953789\n",
      "Done 2490 batches in 1162.53 sec.    training loss:\t\t4.00835239638\n",
      "Done 2500 batches in 1167.50 sec.    training loss:\t\t4.0074591136\n",
      "Done 2510 batches in 1172.20 sec.    training loss:\t\t4.00637901333\n",
      "Done 2520 batches in 1176.90 sec.    training loss:\t\t4.00540701312\n",
      "Done 2530 batches in 1181.51 sec.    training loss:\t\t4.00429292083\n",
      "Done 2540 batches in 1186.52 sec.    training loss:\t\t4.003238543\n",
      "Done 2550 batches in 1191.26 sec.    training loss:\t\t4.00207160033\n",
      "Done 2560 batches in 1195.99 sec.    training loss:\t\t4.00095354626\n",
      "Done 2570 batches in 1200.36 sec.    training loss:\t\t3.9997252968\n",
      "Done 2580 batches in 1205.34 sec.    training loss:\t\t3.99860351049\n",
      "Done 2590 batches in 1210.16 sec.    training loss:\t\t3.99765971232\n",
      "Done 2600 batches in 1214.82 sec.    training loss:\t\t3.99622750548\n",
      "Done 2610 batches in 1219.83 sec.    training loss:\t\t3.99532814903\n",
      "Done 2620 batches in 1224.55 sec.    training loss:\t\t3.99422546707\n",
      "Done 2630 batches in 1229.06 sec.    training loss:\t\t3.99315124791\n",
      "Done 2640 batches in 1233.52 sec.    training loss:\t\t3.99218389274\n",
      "Done 2650 batches in 1238.07 sec.    training loss:\t\t3.99117188256\n",
      "Done 2660 batches in 1242.57 sec.    training loss:\t\t3.98994848692\n",
      "Done 2670 batches in 1247.09 sec.    training loss:\t\t3.98858727346\n",
      "Done 2680 batches in 1251.92 sec.    training loss:\t\t3.98747890414\n",
      "Done 2690 batches in 1256.66 sec.    training loss:\t\t3.98610274331\n",
      "Done 2700 batches in 1261.28 sec.    training loss:\t\t3.98499866803\n",
      "Done 2710 batches in 1266.21 sec.    training loss:\t\t3.98409610604\n",
      "Done 2720 batches in 1271.14 sec.    training loss:\t\t3.98303681963\n",
      "Done 2730 batches in 1275.81 sec.    training loss:\t\t3.98198613649\n",
      "Done 2740 batches in 1280.43 sec.    training loss:\t\t3.98090317771\n",
      "Done 2750 batches in 1284.91 sec.    training loss:\t\t3.98004500458\n",
      "Done 2760 batches in 1289.39 sec.    training loss:\t\t3.97919450316\n",
      "Done 2770 batches in 1294.26 sec.    training loss:\t\t3.97836201062\n",
      "Done 2780 batches in 1299.09 sec.    training loss:\t\t3.97735779826\n",
      "Done 2790 batches in 1303.84 sec.    training loss:\t\t3.97622728843\n",
      "Done 2800 batches in 1308.64 sec.    training loss:\t\t3.97512249129\n",
      "Done 2810 batches in 1313.69 sec.    training loss:\t\t3.97435829529\n",
      "Done 2820 batches in 1318.16 sec.    training loss:\t\t3.97318840636\n",
      "Done 2830 batches in 1322.74 sec.    training loss:\t\t3.97224942492\n",
      "Done 2840 batches in 1326.83 sec.    training loss:\t\t3.97115478583\n",
      "Done 2850 batches in 1331.31 sec.    training loss:\t\t3.97002566329\n",
      "Done 2860 batches in 1336.11 sec.    training loss:\t\t3.96927759431\n",
      "Done 2870 batches in 1340.56 sec.    training loss:\t\t3.96843966399\n",
      "Done 2880 batches in 1345.47 sec.    training loss:\t\t3.96766570293\n",
      "Done 2890 batches in 1350.14 sec.    training loss:\t\t3.96672384879\n",
      "Done 2900 batches in 1355.60 sec.    training loss:\t\t3.96597113757\n",
      "Done 2910 batches in 1360.04 sec.    training loss:\t\t3.96491224168\n",
      "Done 2920 batches in 1364.77 sec.    training loss:\t\t3.963779747\n",
      "Done 2930 batches in 1369.05 sec.    training loss:\t\t3.96270566857\n",
      "Done 2940 batches in 1373.68 sec.    training loss:\t\t3.96176354163\n",
      "Done 2950 batches in 1378.37 sec.    training loss:\t\t3.96095721447\n",
      "Done 2960 batches in 1383.49 sec.    training loss:\t\t3.96004382074\n",
      "Done 2970 batches in 1388.10 sec.    training loss:\t\t3.95925909919\n",
      "Done 2980 batches in 1393.17 sec.    training loss:\t\t3.95811617086\n",
      "Done 2990 batches in 1397.67 sec.    training loss:\t\t3.95718679915\n",
      "Done 3000 batches in 1402.52 sec.    training loss:\t\t3.95625681162\n",
      "Done 3010 batches in 1407.22 sec.    training loss:\t\t3.95562054865\n",
      "Done 3020 batches in 1412.08 sec.    training loss:\t\t3.95473079445\n",
      "Done 3030 batches in 1416.62 sec.    training loss:\t\t3.95387761475\n",
      "Done 3040 batches in 1421.44 sec.    training loss:\t\t3.95290619864\n",
      "Done 3050 batches in 1426.37 sec.    training loss:\t\t3.95214140931\n",
      "Done 3060 batches in 1431.41 sec.    training loss:\t\t3.95129060831\n",
      "Done 3070 batches in 1435.83 sec.    training loss:\t\t3.95033074924\n",
      "Done 3080 batches in 1440.61 sec.    training loss:\t\t3.94957699048\n",
      "Done 3090 batches in 1444.89 sec.    training loss:\t\t3.94857043502\n",
      "Done 3100 batches in 1449.84 sec.    training loss:\t\t3.94782216895\n",
      "Done 3110 batches in 1455.03 sec.    training loss:\t\t3.9472103217\n",
      "Done 3120 batches in 1459.41 sec.    training loss:\t\t3.94622505498\n",
      "Done 3130 batches in 1464.22 sec.    training loss:\t\t3.9455104837\n",
      "Done 3140 batches in 1468.93 sec.    training loss:\t\t3.944855138\n",
      "Done 3150 batches in 1473.34 sec.    training loss:\t\t3.94409966302\n",
      "Done 3160 batches in 1477.74 sec.    training loss:\t\t3.94311831277\n",
      "Done 3170 batches in 1482.32 sec.    training loss:\t\t3.94216155622\n",
      "Done 3180 batches in 1487.67 sec.    training loss:\t\t3.94139267529\n",
      "Done 3190 batches in 1492.05 sec.    training loss:\t\t3.94019900317\n",
      "Done 3200 batches in 1496.87 sec.    training loss:\t\t3.93963036209\n",
      "Done 3210 batches in 1501.18 sec.    training loss:\t\t3.93888678922\n",
      "Done 3220 batches in 1505.30 sec.    training loss:\t\t3.93786608632\n",
      "Done 3230 batches in 1510.36 sec.    training loss:\t\t3.93720297393\n",
      "Done 3240 batches in 1515.27 sec.    training loss:\t\t3.93637473576\n",
      "Done 3250 batches in 1520.17 sec.    training loss:\t\t3.93535881006\n",
      "Done 3260 batches in 1524.72 sec.    training loss:\t\t3.93460552297\n",
      "Done 3270 batches in 1529.59 sec.    training loss:\t\t3.93392246163\n",
      "Done 3280 batches in 1534.69 sec.    training loss:\t\t3.93333962189\n",
      "Done 3290 batches in 1539.75 sec.    training loss:\t\t3.93269513219\n",
      "Done 3300 batches in 1544.38 sec.    training loss:\t\t3.9316801556\n",
      "Done 3310 batches in 1549.11 sec.    training loss:\t\t3.9310246621\n",
      "Done 3320 batches in 1553.62 sec.    training loss:\t\t3.93005354318\n",
      "Done 3330 batches in 1558.31 sec.    training loss:\t\t3.92935071707\n",
      "Done 3340 batches in 1563.29 sec.    training loss:\t\t3.92845984333\n",
      "Done 3350 batches in 1567.85 sec.    training loss:\t\t3.92765690739\n",
      "Done 3360 batches in 1573.09 sec.    training loss:\t\t3.9270206274\n",
      "Done 3370 batches in 1578.15 sec.    training loss:\t\t3.92643447453\n",
      "Done 3380 batches in 1582.95 sec.    training loss:\t\t3.92548326113\n",
      "Done 3390 batches in 1586.99 sec.    training loss:\t\t3.92485830903\n",
      "Done 3400 batches in 1591.74 sec.    training loss:\t\t3.92404813213\n",
      "Done 3410 batches in 1596.26 sec.    training loss:\t\t3.92319311613\n",
      "Done 3420 batches in 1601.26 sec.    training loss:\t\t3.92248751434\n",
      "Done 3430 batches in 1605.58 sec.    training loss:\t\t3.92159347333\n",
      "Done 3440 batches in 1610.29 sec.    training loss:\t\t3.92084296935\n",
      "Done 3450 batches in 1615.29 sec.    training loss:\t\t3.92017687265\n",
      "Done 3460 batches in 1620.14 sec.    training loss:\t\t3.91943985417\n",
      "Done 3470 batches in 1624.53 sec.    training loss:\t\t3.91881236996\n",
      "Done 3480 batches in 1629.02 sec.    training loss:\t\t3.91793714734\n",
      "Done 3490 batches in 1633.82 sec.    training loss:\t\t3.91741149583\n",
      "Done 3500 batches in 1638.58 sec.    training loss:\t\t3.91672339405\n",
      "Done 3510 batches in 1643.53 sec.    training loss:\t\t3.9160485646\n",
      "Done 3520 batches in 1648.24 sec.    training loss:\t\t3.91535146006\n",
      "Done 3530 batches in 1652.58 sec.    training loss:\t\t3.91449235526\n",
      "Done 3540 batches in 1656.89 sec.    training loss:\t\t3.91366967549\n",
      "Done 3550 batches in 1662.13 sec.    training loss:\t\t3.91309352559\n",
      "Done 3560 batches in 1666.76 sec.    training loss:\t\t3.91245220417\n",
      "Done 3570 batches in 1671.41 sec.    training loss:\t\t3.91191016512\n",
      "Done 3580 batches in 1676.08 sec.    training loss:\t\t3.91135543978\n",
      "Done 3590 batches in 1680.80 sec.    training loss:\t\t3.91070650686\n",
      "Done 3600 batches in 1685.29 sec.    training loss:\t\t3.91003116866\n",
      "Done 3610 batches in 1690.07 sec.    training loss:\t\t3.90948576934\n",
      "Done 3620 batches in 1694.93 sec.    training loss:\t\t3.90890236836\n",
      "Done 3630 batches in 1700.06 sec.    training loss:\t\t3.9081452924\n",
      "Done 3640 batches in 1704.92 sec.    training loss:\t\t3.90746819783\n",
      "Done 3650 batches in 1710.02 sec.    training loss:\t\t3.90685590123\n",
      "Done 3660 batches in 1714.67 sec.    training loss:\t\t3.90613049555\n",
      "Done 3670 batches in 1719.19 sec.    training loss:\t\t3.90540345046\n",
      "Done 3680 batches in 1723.99 sec.    training loss:\t\t3.90466103483\n",
      "Done 3690 batches in 1728.37 sec.    training loss:\t\t3.90386719936\n",
      "Done 3700 batches in 1733.13 sec.    training loss:\t\t3.90323172254\n",
      "Done 3710 batches in 1737.24 sec.    training loss:\t\t3.90220210205\n",
      "Done 3720 batches in 1741.55 sec.    training loss:\t\t3.90138455777\n",
      "Done 3730 batches in 1746.35 sec.    training loss:\t\t3.90097381231\n",
      "Done 3740 batches in 1750.72 sec.    training loss:\t\t3.90027101116\n",
      "Done 3750 batches in 1755.08 sec.    training loss:\t\t3.89947487583\n",
      "Done 3760 batches in 1759.45 sec.    training loss:\t\t3.89884517624\n",
      "Done 3770 batches in 1763.85 sec.    training loss:\t\t3.89830032672\n",
      "Done 3780 batches in 1769.25 sec.    training loss:\t\t3.89782960655\n",
      "Done 3790 batches in 1773.77 sec.    training loss:\t\t3.89718137354\n",
      "Done 3800 batches in 1778.23 sec.    training loss:\t\t3.89658654621\n",
      "Done 3810 batches in 1783.17 sec.    training loss:\t\t3.89589093338\n",
      "Done 3820 batches in 1787.73 sec.    training loss:\t\t3.89518427549\n",
      "Done 3830 batches in 1792.34 sec.    training loss:\t\t3.8943528712\n",
      "Done 3840 batches in 1796.77 sec.    training loss:\t\t3.89364553789\n",
      "Done 3850 batches in 1801.69 sec.    training loss:\t\t3.89312559908\n",
      "Done 3860 batches in 1806.47 sec.    training loss:\t\t3.89240154504\n",
      "Done 3870 batches in 1811.68 sec.    training loss:\t\t3.89178171035\n",
      "Done 3880 batches in 1816.82 sec.    training loss:\t\t3.89117431426\n",
      "Done 3890 batches in 1821.42 sec.    training loss:\t\t3.89054405174\n",
      "Done 3900 batches in 1825.90 sec.    training loss:\t\t3.88987398441\n",
      "Done 3910 batches in 1830.60 sec.    training loss:\t\t3.88923086327\n",
      "Done 3920 batches in 1835.38 sec.    training loss:\t\t3.8886626479\n",
      "Done 3930 batches in 1839.49 sec.    training loss:\t\t3.88806697443\n",
      "Done 3940 batches in 1843.66 sec.    training loss:\t\t3.88747582466\n",
      "Done 3950 batches in 1848.38 sec.    training loss:\t\t3.88673087893\n",
      "Done 3960 batches in 1852.61 sec.    training loss:\t\t3.88608008894\n",
      "Done 3970 batches in 1857.21 sec.    training loss:\t\t3.88560825079\n",
      "Done 3980 batches in 1861.86 sec.    training loss:\t\t3.88500668613\n",
      "Done 3990 batches in 1866.33 sec.    training loss:\t\t3.88435716061\n",
      "Done 4000 batches in 1871.61 sec.    training loss:\t\t3.88375054765\n",
      "Done 4010 batches in 1876.58 sec.    training loss:\t\t3.88323394164\n",
      "Done 4020 batches in 1881.45 sec.    training loss:\t\t3.88250091159\n",
      "Done 4030 batches in 1885.67 sec.    training loss:\t\t3.881708244\n",
      "Done 4040 batches in 1890.46 sec.    training loss:\t\t3.88114802218\n",
      "Done 4050 batches in 1895.36 sec.    training loss:\t\t3.88050651303\n",
      "Done 4060 batches in 1900.10 sec.    training loss:\t\t3.87976001971\n",
      "Done 4070 batches in 1904.86 sec.    training loss:\t\t3.87933033391\n",
      "Done 4080 batches in 1909.30 sec.    training loss:\t\t3.87864431277\n",
      "Done 4090 batches in 1913.52 sec.    training loss:\t\t3.87799712213\n",
      "Done 4100 batches in 1918.25 sec.    training loss:\t\t3.87746032412\n",
      "Done 4110 batches in 1922.81 sec.    training loss:\t\t3.87689449398\n",
      "Done 4120 batches in 1927.53 sec.    training loss:\t\t3.87636836883\n",
      "Done 4130 batches in 1932.25 sec.    training loss:\t\t3.87573799833\n",
      "Done 4140 batches in 1937.06 sec.    training loss:\t\t3.875070946\n",
      "Done 4150 batches in 1941.43 sec.    training loss:\t\t3.87445578788\n",
      "Done 4160 batches in 1946.35 sec.    training loss:\t\t3.87391985236\n",
      "Done 4170 batches in 1950.91 sec.    training loss:\t\t3.87332535213\n",
      "Done 4180 batches in 1955.64 sec.    training loss:\t\t3.87266885573\n",
      "Done 4190 batches in 1960.46 sec.    training loss:\t\t3.87208125916\n",
      "Done 4200 batches in 1965.01 sec.    training loss:\t\t3.87142492448\n",
      "Done 4210 batches in 1969.45 sec.    training loss:\t\t3.87073130993\n",
      "Done 4220 batches in 1973.96 sec.    training loss:\t\t3.87027054777\n",
      "Done 4230 batches in 1978.50 sec.    training loss:\t\t3.86971021691\n",
      "Done 4240 batches in 1982.85 sec.    training loss:\t\t3.86907548095\n",
      "Done 4250 batches in 1987.35 sec.    training loss:\t\t3.86858394067\n",
      "Done 4260 batches in 1991.69 sec.    training loss:\t\t3.86784926328\n",
      "Done 4270 batches in 1996.49 sec.    training loss:\t\t3.86725765425\n",
      "Done 4280 batches in 2000.76 sec.    training loss:\t\t3.86657632417\n",
      "Done 4290 batches in 2006.10 sec.    training loss:\t\t3.86588694238\n",
      "Done 4300 batches in 2010.54 sec.    training loss:\t\t3.86526711325\n",
      "Done 4310 batches in 2015.13 sec.    training loss:\t\t3.86478363632\n",
      "Done 4320 batches in 2019.84 sec.    training loss:\t\t3.86417093316\n",
      "Done 4330 batches in 2024.48 sec.    training loss:\t\t3.86362138373\n",
      "Done 4340 batches in 2029.79 sec.    training loss:\t\t3.86312044386\n",
      "Done 4350 batches in 2034.19 sec.    training loss:\t\t3.86256266342\n",
      "Done 4360 batches in 2038.57 sec.    training loss:\t\t3.86204290806\n",
      "Done 4370 batches in 2043.22 sec.    training loss:\t\t3.86151352038\n",
      "Done 4380 batches in 2047.63 sec.    training loss:\t\t3.86093608326\n",
      "Done 4390 batches in 2052.29 sec.    training loss:\t\t3.86025288583\n",
      "Done 4400 batches in 2057.02 sec.    training loss:\t\t3.85962320745\n",
      "Done 4410 batches in 2061.74 sec.    training loss:\t\t3.85924544097\n",
      "Done 4420 batches in 2066.48 sec.    training loss:\t\t3.85859934938\n",
      "Done 4430 batches in 2071.25 sec.    training loss:\t\t3.85805759215\n",
      "Done 4440 batches in 2075.83 sec.    training loss:\t\t3.85742095806\n",
      "Done 4450 batches in 2079.93 sec.    training loss:\t\t3.85669512074\n",
      "Done 4460 batches in 2084.36 sec.    training loss:\t\t3.85618753513\n",
      "Done 4470 batches in 2089.36 sec.    training loss:\t\t3.85560415548\n",
      "Done 4480 batches in 2093.94 sec.    training loss:\t\t3.85507832803\n",
      "Done 4490 batches in 2098.73 sec.    training loss:\t\t3.85458195682\n",
      "Done 4500 batches in 2103.53 sec.    training loss:\t\t3.85406773027\n",
      "Done 4510 batches in 2107.85 sec.    training loss:\t\t3.85347120931\n",
      "Done 4520 batches in 2112.38 sec.    training loss:\t\t3.85291684291\n",
      "Done 4530 batches in 2117.24 sec.    training loss:\t\t3.85262596365\n",
      "Done 4540 batches in 2121.43 sec.    training loss:\t\t3.85207663922\n",
      "Done 4550 batches in 2126.16 sec.    training loss:\t\t3.85143442379\n",
      "Done 4560 batches in 2130.71 sec.    training loss:\t\t3.85089879825\n",
      "Done 4570 batches in 2135.85 sec.    training loss:\t\t3.85033426488\n",
      "Done 4580 batches in 2140.77 sec.    training loss:\t\t3.84986394076\n",
      "Done 4590 batches in 2145.28 sec.    training loss:\t\t3.84940416849\n",
      "Done 4600 batches in 2150.64 sec.    training loss:\t\t3.84912058421\n",
      "Done 4610 batches in 2155.86 sec.    training loss:\t\t3.84880459691\n",
      "Done 4620 batches in 2160.67 sec.    training loss:\t\t3.84823447801\n",
      "Done 4630 batches in 2165.57 sec.    training loss:\t\t3.8476869339\n",
      "Done 4640 batches in 2170.05 sec.    training loss:\t\t3.84710346306\n",
      "Done 4650 batches in 2174.46 sec.    training loss:\t\t3.84659466313\n",
      "Done 4660 batches in 2179.72 sec.    training loss:\t\t3.84623999069\n",
      "Done 4670 batches in 2184.61 sec.    training loss:\t\t3.84578366719\n",
      "Done 4680 batches in 2189.34 sec.    training loss:\t\t3.84521149871\n",
      "Done 4690 batches in 2194.08 sec.    training loss:\t\t3.8446993534\n",
      "Done 4700 batches in 2198.92 sec.    training loss:\t\t3.84418143217\n",
      "Done 4710 batches in 2203.48 sec.    training loss:\t\t3.84369354375\n",
      "Done 4720 batches in 2207.96 sec.    training loss:\t\t3.84319016837\n",
      "Done 4730 batches in 2212.93 sec.    training loss:\t\t3.84290415706\n",
      "Done 4740 batches in 2217.55 sec.    training loss:\t\t3.84233717893\n",
      "Done 4750 batches in 2222.18 sec.    training loss:\t\t3.84177914479\n",
      "Done 4760 batches in 2226.61 sec.    training loss:\t\t3.84122223924\n",
      "Done 4770 batches in 2231.28 sec.    training loss:\t\t3.84065743887\n",
      "Done 4780 batches in 2235.85 sec.    training loss:\t\t3.84034141237\n",
      "Done 4790 batches in 2240.87 sec.    training loss:\t\t3.84016350356\n",
      "Done 4800 batches in 2245.13 sec.    training loss:\t\t3.83959315245\n",
      "Done 4810 batches in 2249.90 sec.    training loss:\t\t3.83910415163\n",
      "Done 4820 batches in 2254.60 sec.    training loss:\t\t3.83875247299\n",
      "Done 4830 batches in 2259.53 sec.    training loss:\t\t3.83827950782\n",
      "Done 4840 batches in 2264.40 sec.    training loss:\t\t3.83808011408\n",
      "Done 4850 batches in 2269.00 sec.    training loss:\t\t3.83774636578\n",
      "Done 4860 batches in 2273.60 sec.    training loss:\t\t3.83728510244\n",
      "Done 4870 batches in 2277.99 sec.    training loss:\t\t3.83681430469\n",
      "Done 4880 batches in 2282.52 sec.    training loss:\t\t3.83639842374\n",
      "Done 4890 batches in 2287.76 sec.    training loss:\t\t3.83593992217\n",
      "Done 4900 batches in 2291.95 sec.    training loss:\t\t3.83549083578\n",
      "Done 4910 batches in 2296.68 sec.    training loss:\t\t3.83512667541\n",
      "Done 4920 batches in 2301.02 sec.    training loss:\t\t3.83465049126\n",
      "Done 4930 batches in 2305.99 sec.    training loss:\t\t3.83426219426\n",
      "Done 4940 batches in 2310.42 sec.    training loss:\t\t3.83386914527\n",
      "Done 4950 batches in 2315.17 sec.    training loss:\t\t3.83341174208\n",
      "Done 4960 batches in 2320.15 sec.    training loss:\t\t3.83301066016\n",
      "Done 4970 batches in 2325.13 sec.    training loss:\t\t3.8326074262\n",
      "Done 4980 batches in 2329.87 sec.    training loss:\t\t3.83206247067\n",
      "Done 4990 batches in 2334.49 sec.    training loss:\t\t3.83168027745\n",
      "Done 5000 batches in 2339.36 sec.    training loss:\t\t3.83124885254\n",
      "Done 5010 batches in 2344.13 sec.    training loss:\t\t3.83092217474\n",
      "Done 5020 batches in 2348.49 sec.    training loss:\t\t3.83037699546\n",
      "Done 5030 batches in 2353.15 sec.    training loss:\t\t3.82993379497\n",
      "Done 5040 batches in 2357.84 sec.    training loss:\t\t3.82938836458\n",
      "Done 5050 batches in 2362.83 sec.    training loss:\t\t3.82895596452\n",
      "Done 5060 batches in 2367.95 sec.    training loss:\t\t3.82850625859\n",
      "Done 5070 batches in 2372.73 sec.    training loss:\t\t3.8279545974\n",
      "Done 5080 batches in 2377.44 sec.    training loss:\t\t3.82762483964\n",
      "Done 5090 batches in 2382.16 sec.    training loss:\t\t3.82724951236\n",
      "Done 5100 batches in 2387.11 sec.    training loss:\t\t3.82702349247\n",
      "Done 5110 batches in 2391.98 sec.    training loss:\t\t3.82665754984\n",
      "Done 5120 batches in 2396.57 sec.    training loss:\t\t3.82621756759\n",
      "Done 5130 batches in 2401.16 sec.    training loss:\t\t3.82577142432\n",
      "Done 5140 batches in 2405.84 sec.    training loss:\t\t3.82532008812\n",
      "Done 5150 batches in 2410.64 sec.    training loss:\t\t3.82473881569\n",
      "Done 5160 batches in 2415.82 sec.    training loss:\t\t3.8244191015\n",
      "Done 5170 batches in 2420.07 sec.    training loss:\t\t3.8239130319\n",
      "Done 5180 batches in 2425.25 sec.    training loss:\t\t3.82354715869\n",
      "Done 5190 batches in 2430.24 sec.    training loss:\t\t3.82312898218\n",
      "Done 5200 batches in 2434.98 sec.    training loss:\t\t3.82271580522\n",
      "Done 5210 batches in 2439.88 sec.    training loss:\t\t3.82230212103\n",
      "Done 5220 batches in 2444.50 sec.    training loss:\t\t3.8219379531\n",
      "Done 5230 batches in 2449.29 sec.    training loss:\t\t3.82153198678\n",
      "Done 5240 batches in 2454.10 sec.    training loss:\t\t3.8210419608\n",
      "Done 5250 batches in 2458.61 sec.    training loss:\t\t3.82054095909\n",
      "Done 5260 batches in 2463.36 sec.    training loss:\t\t3.82015487209\n",
      "Done 5270 batches in 2468.17 sec.    training loss:\t\t3.81981986089\n",
      "Done 5280 batches in 2473.38 sec.    training loss:\t\t3.81954033163\n",
      "Done 5290 batches in 2478.25 sec.    training loss:\t\t3.81920683853\n",
      "Done 5300 batches in 2482.65 sec.    training loss:\t\t3.81876916818\n",
      "Done 5310 batches in 2487.24 sec.    training loss:\t\t3.81837853267\n",
      "Done 5320 batches in 2491.74 sec.    training loss:\t\t3.81786722456\n",
      "Done 5330 batches in 2496.20 sec.    training loss:\t\t3.81740316396\n",
      "Done 5340 batches in 2500.54 sec.    training loss:\t\t3.81689995706\n",
      "Done 5350 batches in 2505.67 sec.    training loss:\t\t3.8164520097\n",
      "Done 5360 batches in 2510.53 sec.    training loss:\t\t3.81615589363\n",
      "Done 5370 batches in 2515.40 sec.    training loss:\t\t3.8159460973\n",
      "Done 5380 batches in 2520.08 sec.    training loss:\t\t3.81554656809\n",
      "Done 5390 batches in 2525.19 sec.    training loss:\t\t3.81501358431\n",
      "Done 5400 batches in 2529.87 sec.    training loss:\t\t3.81473310113\n",
      "Done 5410 batches in 2534.40 sec.    training loss:\t\t3.81437620579\n",
      "Done 5420 batches in 2538.52 sec.    training loss:\t\t3.81396068942\n",
      "Done 5430 batches in 2542.76 sec.    training loss:\t\t3.8134939368\n",
      "Done 5440 batches in 2547.66 sec.    training loss:\t\t3.81308823456\n",
      "Done 5450 batches in 2552.07 sec.    training loss:\t\t3.81269599648\n",
      "Done 5460 batches in 2556.79 sec.    training loss:\t\t3.81229955451\n",
      "Done 5470 batches in 2561.24 sec.    training loss:\t\t3.81184241161\n",
      "Done 5480 batches in 2566.06 sec.    training loss:\t\t3.81139151676\n",
      "Done 5490 batches in 2571.20 sec.    training loss:\t\t3.81099892792\n",
      "Done 5500 batches in 2575.69 sec.    training loss:\t\t3.81051137677\n",
      "Done 5510 batches in 2580.09 sec.    training loss:\t\t3.81017088475\n",
      "Done 5520 batches in 2585.02 sec.    training loss:\t\t3.80996019192\n",
      "Done 5530 batches in 2589.86 sec.    training loss:\t\t3.80958047685\n",
      "Done 5540 batches in 2594.97 sec.    training loss:\t\t3.80929423704\n",
      "Done 5550 batches in 2599.15 sec.    training loss:\t\t3.80875863183\n",
      "Done 5560 batches in 2603.60 sec.    training loss:\t\t3.80845190896\n",
      "Done 5570 batches in 2608.39 sec.    training loss:\t\t3.8081111057\n",
      "Done 5580 batches in 2612.93 sec.    training loss:\t\t3.80765563344\n",
      "Done 5590 batches in 2617.38 sec.    training loss:\t\t3.80729545485\n",
      "Done 5600 batches in 2621.84 sec.    training loss:\t\t3.80693972375\n",
      "Done 5610 batches in 2626.86 sec.    training loss:\t\t3.80660518678\n",
      "Done 5620 batches in 2631.20 sec.    training loss:\t\t3.80624778572\n",
      "Done 5630 batches in 2635.89 sec.    training loss:\t\t3.8057511076\n",
      "Done 5640 batches in 2640.63 sec.    training loss:\t\t3.80552634132\n",
      "Done 5650 batches in 2645.95 sec.    training loss:\t\t3.80521570113\n",
      "Done 5660 batches in 2650.74 sec.    training loss:\t\t3.80484014008\n",
      "Done 5670 batches in 2655.19 sec.    training loss:\t\t3.80438306243\n",
      "Done 5680 batches in 2660.37 sec.    training loss:\t\t3.804051577\n",
      "Done 5690 batches in 2665.19 sec.    training loss:\t\t3.80357667876\n",
      "Done 5700 batches in 2669.60 sec.    training loss:\t\t3.80310840289\n",
      "Done 5710 batches in 2674.45 sec.    training loss:\t\t3.80282997709\n",
      "Done 5720 batches in 2679.54 sec.    training loss:\t\t3.80247520729\n",
      "Done 5730 batches in 2683.93 sec.    training loss:\t\t3.80199606739\n",
      "Done 5740 batches in 2688.92 sec.    training loss:\t\t3.80174657297\n",
      "Done 5750 batches in 2693.39 sec.    training loss:\t\t3.80146048873\n",
      "Done 5760 batches in 2697.75 sec.    training loss:\t\t3.80112296554\n",
      "Done 5770 batches in 2702.49 sec.    training loss:\t\t3.80064779778\n",
      "Done 5780 batches in 2707.36 sec.    training loss:\t\t3.80024990206\n",
      "Done 5790 batches in 2712.11 sec.    training loss:\t\t3.79987426484\n",
      "Done 5800 batches in 2717.00 sec.    training loss:\t\t3.79939769983\n",
      "Done 5810 batches in 2721.23 sec.    training loss:\t\t3.79899638971\n",
      "Done 5820 batches in 2725.56 sec.    training loss:\t\t3.79852076052\n",
      "Done 5830 batches in 2730.30 sec.    training loss:\t\t3.79808192662\n",
      "Done 5840 batches in 2735.08 sec.    training loss:\t\t3.79772118689\n",
      "Done 5850 batches in 2739.81 sec.    training loss:\t\t3.79738039616\n",
      "Done 5860 batches in 2743.95 sec.    training loss:\t\t3.79692367576\n",
      "Done 5870 batches in 2748.32 sec.    training loss:\t\t3.79653931692\n",
      "Done 5880 batches in 2753.16 sec.    training loss:\t\t3.79609830384\n",
      "Done 5890 batches in 2757.98 sec.    training loss:\t\t3.79573112392\n",
      "Done 5900 batches in 2762.76 sec.    training loss:\t\t3.79528270047\n",
      "Done 5910 batches in 2767.40 sec.    training loss:\t\t3.7949221131\n",
      "Done 5920 batches in 2772.23 sec.    training loss:\t\t3.79472613697\n",
      "Done 5930 batches in 2776.84 sec.    training loss:\t\t3.79428660451\n",
      "Done 5940 batches in 2781.45 sec.    training loss:\t\t3.79403317207\n",
      "Done 5950 batches in 2786.21 sec.    training loss:\t\t3.79370862031\n",
      "Done 5960 batches in 2790.88 sec.    training loss:\t\t3.79339955437\n",
      "Done 5970 batches in 2795.41 sec.    training loss:\t\t3.79298062396\n",
      "Done 5980 batches in 2800.26 sec.    training loss:\t\t3.79257544478\n",
      "Done 5990 batches in 2805.01 sec.    training loss:\t\t3.7922041819\n",
      "Done 6000 batches in 2809.90 sec.    training loss:\t\t3.7919127576\n",
      "Done 6010 batches in 2814.49 sec.    training loss:\t\t3.79154166314\n",
      "Done 6020 batches in 2819.00 sec.    training loss:\t\t3.79124684868\n",
      "Done 6030 batches in 2823.80 sec.    training loss:\t\t3.79095073498\n",
      "Done 6040 batches in 2827.97 sec.    training loss:\t\t3.79052461283\n",
      "Done 6050 batches in 2832.18 sec.    training loss:\t\t3.79021734533\n",
      "Done 6060 batches in 2837.42 sec.    training loss:\t\t3.78989546094\n",
      "Done 6070 batches in 2842.26 sec.    training loss:\t\t3.78953165438\n",
      "Done 6080 batches in 2847.05 sec.    training loss:\t\t3.78917247124\n",
      "Done 6090 batches in 2851.70 sec.    training loss:\t\t3.78870381917\n",
      "Done 6100 batches in 2856.81 sec.    training loss:\t\t3.78834374119\n",
      "Done 6110 batches in 2861.24 sec.    training loss:\t\t3.78795180426\n",
      "Done 6120 batches in 2865.83 sec.    training loss:\t\t3.78768032894\n",
      "Done 6130 batches in 2870.38 sec.    training loss:\t\t3.78727582979\n",
      "Done 6140 batches in 2874.95 sec.    training loss:\t\t3.78686332345\n",
      "Done 6150 batches in 2879.78 sec.    training loss:\t\t3.78653026054\n",
      "Done 6160 batches in 2884.35 sec.    training loss:\t\t3.78619535604\n",
      "Done 6170 batches in 2888.95 sec.    training loss:\t\t3.78575759865\n",
      "Done 6180 batches in 2893.97 sec.    training loss:\t\t3.78545675332\n",
      "Done 6190 batches in 2898.31 sec.    training loss:\t\t3.78506934073\n",
      "Done 6200 batches in 2902.77 sec.    training loss:\t\t3.78468734218\n",
      "Done 6210 batches in 2907.38 sec.    training loss:\t\t3.78446210099\n",
      "Done 6220 batches in 2912.12 sec.    training loss:\t\t3.78421079806\n",
      "Done 6230 batches in 2916.87 sec.    training loss:\t\t3.78390460431\n",
      "Done 6240 batches in 2921.41 sec.    training loss:\t\t3.7834610636\n",
      "Done 6250 batches in 2926.27 sec.    training loss:\t\t3.78307852768\n",
      "Done 6260 batches in 2931.23 sec.    training loss:\t\t3.78270317324\n",
      "Done 6270 batches in 2935.88 sec.    training loss:\t\t3.78230939193\n",
      "Done 6280 batches in 2940.37 sec.    training loss:\t\t3.78204165055\n",
      "Done 6290 batches in 2944.44 sec.    training loss:\t\t3.78162734152\n",
      "Done 6300 batches in 2948.74 sec.    training loss:\t\t3.78124583312\n",
      "Done 6310 batches in 2953.58 sec.    training loss:\t\t3.78093229368\n",
      "Done 6320 batches in 2958.28 sec.    training loss:\t\t3.78060048238\n",
      "Done 6330 batches in 2962.91 sec.    training loss:\t\t3.78002865687\n",
      "Done 6340 batches in 2967.31 sec.    training loss:\t\t3.77959309433\n",
      "Done 6350 batches in 2972.13 sec.    training loss:\t\t3.77931683859\n",
      "Done 6360 batches in 2977.15 sec.    training loss:\t\t3.77890772685\n",
      "Done 6370 batches in 2981.77 sec.    training loss:\t\t3.77853553714\n",
      "Done 6380 batches in 2986.32 sec.    training loss:\t\t3.7781682985\n",
      "Done 6390 batches in 2990.80 sec.    training loss:\t\t3.77797278086\n",
      "Done 6400 batches in 2995.64 sec.    training loss:\t\t3.77765130363\n",
      "Done 6410 batches in 3000.30 sec.    training loss:\t\t3.77740990959\n",
      "Done 6420 batches in 3005.05 sec.    training loss:\t\t3.77728348651\n",
      "Done 6430 batches in 3010.16 sec.    training loss:\t\t3.77705341835\n",
      "Done 6440 batches in 3014.92 sec.    training loss:\t\t3.77669350072\n",
      "Done 6450 batches in 3019.89 sec.    training loss:\t\t3.7763265979\n",
      "Done 6460 batches in 3024.50 sec.    training loss:\t\t3.77590316261\n",
      "Done 6470 batches in 3029.58 sec.    training loss:\t\t3.77560494724\n",
      "Done 6480 batches in 3034.09 sec.    training loss:\t\t3.77528458201\n",
      "Done 6490 batches in 3038.72 sec.    training loss:\t\t3.77504430159\n",
      "Done 6500 batches in 3043.87 sec.    training loss:\t\t3.77471097381\n",
      "Done 6510 batches in 3048.19 sec.    training loss:\t\t3.77431504027\n",
      "Done 6520 batches in 3052.90 sec.    training loss:\t\t3.77402283447\n",
      "Done 6530 batches in 3058.36 sec.    training loss:\t\t3.7737968956\n",
      "Done 6540 batches in 3062.80 sec.    training loss:\t\t3.7734460562\n",
      "Done 6550 batches in 3067.62 sec.    training loss:\t\t3.77306247285\n",
      "Done 6560 batches in 3072.54 sec.    training loss:\t\t3.77288564124\n",
      "Done 6570 batches in 3076.99 sec.    training loss:\t\t3.77256751129\n",
      "Done 6580 batches in 3081.39 sec.    training loss:\t\t3.77210309027\n",
      "Done 6590 batches in 3086.35 sec.    training loss:\t\t3.77174755756\n",
      "Done 6600 batches in 3091.18 sec.    training loss:\t\t3.77144063079\n",
      "Done 6610 batches in 3096.15 sec.    training loss:\t\t3.77120631773\n",
      "Done 6620 batches in 3100.29 sec.    training loss:\t\t3.77086133888\n",
      "Done 6630 batches in 3104.96 sec.    training loss:\t\t3.77056962023\n",
      "Done 6640 batches in 3108.95 sec.    training loss:\t\t3.77017892847\n",
      "Done 6650 batches in 3113.46 sec.    training loss:\t\t3.7698258605\n",
      "Done 6660 batches in 3118.64 sec.    training loss:\t\t3.76957262808\n",
      "Done 6670 batches in 3123.22 sec.    training loss:\t\t3.76920315351\n",
      "Done 6680 batches in 3127.77 sec.    training loss:\t\t3.76879106035\n",
      "Done 6690 batches in 3132.42 sec.    training loss:\t\t3.76852530439\n",
      "Done 6700 batches in 3137.16 sec.    training loss:\t\t3.76831041884\n",
      "Done 6710 batches in 3141.63 sec.    training loss:\t\t3.76809936671\n",
      "Done 6720 batches in 3146.56 sec.    training loss:\t\t3.76773110252\n",
      "Done 6730 batches in 3151.39 sec.    training loss:\t\t3.76751545623\n",
      "Done 6740 batches in 3155.90 sec.    training loss:\t\t3.76717873203\n",
      "Done 6750 batches in 3160.49 sec.    training loss:\t\t3.76686282546\n",
      "Done 6760 batches in 3165.23 sec.    training loss:\t\t3.76650695984\n",
      "Done 6770 batches in 3169.65 sec.    training loss:\t\t3.76621951067\n",
      "Done 6780 batches in 3174.29 sec.    training loss:\t\t3.76592924957\n",
      "Done 6790 batches in 3179.33 sec.    training loss:\t\t3.76569051472\n",
      "Done 6800 batches in 3183.93 sec.    training loss:\t\t3.76537271233\n",
      "Done 6810 batches in 3188.87 sec.    training loss:\t\t3.76522571711\n",
      "Done 6820 batches in 3193.39 sec.    training loss:\t\t3.76491489508\n",
      "Done 6830 batches in 3197.72 sec.    training loss:\t\t3.76461223111\n",
      "Done 6840 batches in 3202.37 sec.    training loss:\t\t3.76430500592\n",
      "Done 6850 batches in 3207.25 sec.    training loss:\t\t3.76401099031\n",
      "Done 6860 batches in 3211.42 sec.    training loss:\t\t3.76363954669\n",
      "Done 6870 batches in 3216.65 sec.    training loss:\t\t3.76333939422\n",
      "Done 6880 batches in 3221.00 sec.    training loss:\t\t3.76306582703\n",
      "Done 6890 batches in 3225.51 sec.    training loss:\t\t3.76282430418\n",
      "Done 6900 batches in 3230.07 sec.    training loss:\t\t3.76261332906\n",
      "Done 6910 batches in 3234.79 sec.    training loss:\t\t3.76233985528\n",
      "Done 6920 batches in 3239.76 sec.    training loss:\t\t3.76198627883\n",
      "Done 6930 batches in 3244.15 sec.    training loss:\t\t3.76164453504\n",
      "Done 6940 batches in 3249.10 sec.    training loss:\t\t3.76136729082\n",
      "Done 6950 batches in 3253.89 sec.    training loss:\t\t3.76114102727\n",
      "Done 6960 batches in 3258.82 sec.    training loss:\t\t3.76083175054\n",
      "Done 6970 batches in 3263.25 sec.    training loss:\t\t3.76053777456\n",
      "Done 6980 batches in 3268.35 sec.    training loss:\t\t3.76034709625\n",
      "Done 6990 batches in 3273.39 sec.    training loss:\t\t3.76007351446\n",
      "Done 7000 batches in 3277.92 sec.    training loss:\t\t3.7596698856\n",
      "Done 7010 batches in 3282.70 sec.    training loss:\t\t3.75939373661\n",
      "Done 7020 batches in 3287.27 sec.    training loss:\t\t3.7591694894\n",
      "Done 7030 batches in 3291.41 sec.    training loss:\t\t3.75888745303\n",
      "Done 7040 batches in 3297.01 sec.    training loss:\t\t3.75872511721\n",
      "Done 7050 batches in 3301.63 sec.    training loss:\t\t3.75841359071\n",
      "Done 7060 batches in 3305.71 sec.    training loss:\t\t3.7581137439\n",
      "Done 7070 batches in 3310.69 sec.    training loss:\t\t3.75789475492\n",
      "Done 7080 batches in 3315.16 sec.    training loss:\t\t3.7574847117\n",
      "Done 7090 batches in 3319.55 sec.    training loss:\t\t3.75707275363\n",
      "Done 7100 batches in 3324.01 sec.    training loss:\t\t3.75679077138\n",
      "Done 7110 batches in 3329.06 sec.    training loss:\t\t3.75649704058\n",
      "Done 7120 batches in 3333.60 sec.    training loss:\t\t3.75613320272\n",
      "Done 7130 batches in 3338.72 sec.    training loss:\t\t3.75588373493\n",
      "Done 7140 batches in 3343.45 sec.    training loss:\t\t3.75559017181\n",
      "Done 7150 batches in 3348.18 sec.    training loss:\t\t3.75536813269\n",
      "Done 7160 batches in 3353.24 sec.    training loss:\t\t3.7551084966\n",
      "Done 7170 batches in 3357.86 sec.    training loss:\t\t3.75481105712\n",
      "Done 7180 batches in 3362.83 sec.    training loss:\t\t3.75451322469\n",
      "Done 7190 batches in 3367.81 sec.    training loss:\t\t3.7541358327\n",
      "Done 7200 batches in 3373.03 sec.    training loss:\t\t3.7539450865\n",
      "Done 7210 batches in 3377.70 sec.    training loss:\t\t3.75365872079\n",
      "Done 7220 batches in 3382.99 sec.    training loss:\t\t3.75342751012\n",
      "Done 7230 batches in 3387.85 sec.    training loss:\t\t3.75309113431\n",
      "Done 7240 batches in 3392.16 sec.    training loss:\t\t3.75277045962\n",
      "Done 7250 batches in 3396.70 sec.    training loss:\t\t3.75258372912\n",
      "Done 7260 batches in 3401.12 sec.    training loss:\t\t3.75222899973\n",
      "Done 7270 batches in 3406.02 sec.    training loss:\t\t3.75201475092\n",
      "Done 7280 batches in 3410.81 sec.    training loss:\t\t3.75166305133\n",
      "Done 7290 batches in 3415.24 sec.    training loss:\t\t3.75139276582\n",
      "Done 7300 batches in 3420.39 sec.    training loss:\t\t3.75115123501\n",
      "Done 7310 batches in 3424.71 sec.    training loss:\t\t3.75091204744\n",
      "Done 7320 batches in 3429.57 sec.    training loss:\t\t3.75061109998\n",
      "Done 7330 batches in 3434.16 sec.    training loss:\t\t3.75029632486\n",
      "Done 7340 batches in 3438.76 sec.    training loss:\t\t3.75009884324\n",
      "Done 7350 batches in 3443.43 sec.    training loss:\t\t3.74975495277\n",
      "Done 7360 batches in 3447.82 sec.    training loss:\t\t3.74944223418\n",
      "Done 7370 batches in 3452.30 sec.    training loss:\t\t3.74923211522\n",
      "Done 7380 batches in 3456.74 sec.    training loss:\t\t3.74893090001\n",
      "Done 7390 batches in 3461.47 sec.    training loss:\t\t3.74873899688\n",
      "Done 7400 batches in 3466.84 sec.    training loss:\t\t3.74850373742\n",
      "Done 7410 batches in 3471.02 sec.    training loss:\t\t3.74817902598\n",
      "Done 7420 batches in 3475.55 sec.    training loss:\t\t3.74779305959\n",
      "Done 7430 batches in 3480.03 sec.    training loss:\t\t3.74753334368\n",
      "Done 7440 batches in 3484.56 sec.    training loss:\t\t3.74728413458\n",
      "Done 7450 batches in 3489.22 sec.    training loss:\t\t3.74694220325\n",
      "Done 7460 batches in 3493.72 sec.    training loss:\t\t3.74660782312\n",
      "Done 7470 batches in 3498.26 sec.    training loss:\t\t3.7463388976\n",
      "Done 7480 batches in 3503.05 sec.    training loss:\t\t3.74603041127\n",
      "Done 7490 batches in 3507.81 sec.    training loss:\t\t3.74582737533\n",
      "Done 7500 batches in 3512.18 sec.    training loss:\t\t3.74554200134\n",
      "Done 7510 batches in 3516.80 sec.    training loss:\t\t3.74514144391\n",
      "Done 7520 batches in 3521.65 sec.    training loss:\t\t3.74488853987\n",
      "Done 7530 batches in 3526.03 sec.    training loss:\t\t3.7447287829\n",
      "Done 7540 batches in 3531.05 sec.    training loss:\t\t3.74452191866\n",
      "Done 7550 batches in 3535.99 sec.    training loss:\t\t3.74427209264\n",
      "Done 7560 batches in 3540.39 sec.    training loss:\t\t3.74399051903\n",
      "Done 7570 batches in 3544.84 sec.    training loss:\t\t3.74368235296\n",
      "Done 7580 batches in 3549.63 sec.    training loss:\t\t3.74346792223\n",
      "Done 7590 batches in 3554.50 sec.    training loss:\t\t3.74317765471\n",
      "Done 7600 batches in 3559.01 sec.    training loss:\t\t3.7428647888\n",
      "Done 7610 batches in 3563.33 sec.    training loss:\t\t3.74260584462\n",
      "Done 7620 batches in 3568.09 sec.    training loss:\t\t3.74235159803\n",
      "Done 7630 batches in 3573.27 sec.    training loss:\t\t3.74209367092\n",
      "Done 7640 batches in 3578.01 sec.    training loss:\t\t3.74185872271\n",
      "Done 7650 batches in 3582.37 sec.    training loss:\t\t3.74152052315\n",
      "Done 7660 batches in 3587.04 sec.    training loss:\t\t3.74127529184\n",
      "Done 7670 batches in 3591.91 sec.    training loss:\t\t3.74100861997\n",
      "Done 7680 batches in 3596.63 sec.    training loss:\t\t3.74077161771\n",
      "Done 7690 batches in 3601.63 sec.    training loss:\t\t3.74047776639\n",
      "Done 7700 batches in 3606.64 sec.    training loss:\t\t3.74023187907\n",
      "Done 7710 batches in 3611.53 sec.    training loss:\t\t3.74001177854\n",
      "Done 7720 batches in 3615.90 sec.    training loss:\t\t3.73970243353\n",
      "Done 7730 batches in 3620.58 sec.    training loss:\t\t3.73941464581\n",
      "Done 7740 batches in 3625.61 sec.    training loss:\t\t3.73919132905\n",
      "Done 7750 batches in 3630.33 sec.    training loss:\t\t3.73890173592\n",
      "Done 7760 batches in 3634.93 sec.    training loss:\t\t3.73859188421\n",
      "Done 7770 batches in 3639.43 sec.    training loss:\t\t3.73832329028\n",
      "Done 7780 batches in 3643.73 sec.    training loss:\t\t3.73809110806\n",
      "Done 7790 batches in 3648.71 sec.    training loss:\t\t3.73787191049\n",
      "Done 100 batches in 8.43 sec.\n",
      "Done 200 batches in 16.74 sec.\n",
      "Done 300 batches in 24.92 sec.\n",
      "Done 400 batches in 33.20 sec.\n",
      "Done 500 batches in 41.92 sec.\n",
      "Done 600 batches in 50.04 sec.\n",
      "Done 700 batches in 58.50 sec.\n",
      "Done 800 batches in 66.87 sec.\n",
      "Done 900 batches in 75.45 sec.\n",
      "Epoch 1 of 1 took 3735.237s\n",
      "  training loss:\t\t3.737686\n",
      "  validation loss:\t\t3.578042\n"
     ]
    }
   ],
   "source": [
    "# training, taken from mnist.py in lasagne examples\n",
    "\n",
    "num_epochs = 1\n",
    "batch_size = 25\n",
    "val_batch_size = 25\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch in iterate_minibatches(train, batch_size):\n",
    "        \n",
    "        inputs, targets, mask = batch\n",
    "        batch_err = train_fn(inputs, targets, mask)\n",
    "\n",
    "        train_err += batch_err\n",
    "        train_batches += 1\n",
    "        \n",
    "        if not train_batches % 10:\n",
    "            print \"Done {} batches in {:.2f} sec.    training loss:\\t\\t{}\".format(\n",
    "                train_batches, time.time() - start_time, train_err / train_batches)\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_err = 0\n",
    "    val_batches = 0\n",
    "    start_time2 = time.time()\n",
    "    \n",
    "    clone_param_values(net_from=net, net_to=test_net)\n",
    "    \n",
    "    for batch in iterate_minibatches(valid, val_batch_size):\n",
    "        inputs, targets, mask = batch\n",
    "        \n",
    "        err = val_fn(inputs, targets, mask)\n",
    "        val_err += err\n",
    "        val_batches += 1\n",
    "        if not val_batches % 100:\n",
    "            print \"Done {} batches in {:.2f} sec.\".format(\n",
    "                val_batches, time.time() - start_time2)\n",
    "\n",
    "    # Then we print the results for this epoch:\n",
    "    print \"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time)\n",
    "    print \"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches)\n",
    "    print \"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches)\n",
    "    #print \"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "    #    val_acc / val_batches * 100)\n",
    "    \n",
    "np.savez('1ep_w2vFixed_300_300_fullsoft_bs25_cut200.npz', *L.layers.get_all_param_values(net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with np.load('test_1ep_params.npz') as f:\n",
    "    param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "    L.layers.set_all_param_values(test_net, param_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_var = T.imatrix('inputs')\n",
    "gen_net = build_hsoft_rnnlm(input_var, None, None, voc_size, emb_size, rec_size)\n",
    "probs = L.layers.get_output(gen_net)[:,-1,:]\n",
    "get_probs = theano.function([input_var], probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_var = T.imatrix('inputs')\n",
    "gen_net = build_sampledsoft_rnnlm(input_var, None, -1, voc_size, emb_size, rec_size)\n",
    "probs = L.layers.get_output(gen_net)[:,-1,:]\n",
    "get_probs = theano.function([input_var], probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_var = T.imatrix('inputs')\n",
    "gen_net = build_simple_rnnlm(input_var, None, voc_size, emb_size, rec_size)\n",
    "probs = L.layers.get_output(gen_net)[:,-1,:]\n",
    "get_probs = theano.function([input_var], probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clone_param_values(net_from=net, net_to=gen_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with np.load('test_1ep_params_1000_sampled_unique_bs40.npz') as f:\n",
    "    param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "    L.layers.set_all_param_values(gen_net, param_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnd_next_word(probs, size=1):\n",
    "    return np.random.choice(np.arange(probs.shape[0], dtype=np.int32), size=size, p=probs)\n",
    "\n",
    "def beam_search(get_probs_fun, beam=10, init_seq='', mode='rr'):\n",
    "    utt = map(lambda w: w_to_idx.get(w, w_to_idx['<unk>']), init_seq.split())\n",
    "    if len(utt) == 0 or utt[0] != 1:\n",
    "        utt = [1] + utt\n",
    "    utt = np.asarray(utt, dtype=np.int32)[np.newaxis]\n",
    "    \n",
    "    if mode[0] == 's':\n",
    "        words = get_probs_fun(utt)[0].argpartition(-beam)[-beam:].astype(np.int32)\n",
    "    elif mode[0] == 'r':\n",
    "        words = rnd_next_word(get_probs_fun(utt)[0], beam)\n",
    "    \n",
    "    candidates = utt.repeat(beam, axis=0)\n",
    "    candidates = np.hstack([candidates, words[np.newaxis].T])\n",
    "    scores = np.zeros(beam)\n",
    "    \n",
    "#     print candidates\n",
    "    \n",
    "    while voc_size-1 not in candidates[:,-1] and candidates.shape[1] < 100:\n",
    "        \n",
    "        if mode[1] == 's':\n",
    "            log_probs = np.log(get_probs_fun(candidates))\n",
    "            tot_scores = log_probs + scores[np.newaxis].T\n",
    "\n",
    "            idx = tot_scores.ravel().argpartition(-beam)[-beam:]\n",
    "            i,j = idx / tot_scores.shape[1], (idx % tot_scores.shape[1]).astype(np.int32)\n",
    "\n",
    "            scores = tot_scores[i,j]\n",
    "\n",
    "            candidates = np.hstack([candidates[i], j[np.newaxis].T])\n",
    "            \n",
    "        elif mode[1] == 'r':\n",
    "            probs = get_probs_fun(candidates)\n",
    "            words = []\n",
    "            for k in xrange(beam):\n",
    "                words.append(rnd_next_word(probs[k], beam)) # this doesn't have to be exactly 'beam'\n",
    "            words = np.array(words)\n",
    "            idx = np.indices((beam, words.shape[1]))[0]\n",
    "            tot_scores = scores[np.newaxis].T + np.log(probs)[idx, words]\n",
    "                \n",
    "            idx = tot_scores.ravel().argpartition(-beam)[-beam:]\n",
    "            i,j = idx / tot_scores.shape[1], (idx % tot_scores.shape[1])\n",
    "\n",
    "            scores = tot_scores[i,j]\n",
    "\n",
    "            candidates = np.hstack([candidates[i], words[i,j][np.newaxis].T])\n",
    "            \n",
    "#     print candidates[:,:10]\n",
    "#     print scores[:10]\n",
    "        \n",
    "    cands = candidates[candidates[:,-1] == 0]\n",
    "    if cands.size > 0:\n",
    "        return candidates[candidates[:,-1] == 0][0]\n",
    "    return candidates[scores.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"it ' s the matter . </s> <s> what ' s the matter ? </s> <s> this is a man . </s>\""
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utt = beam_search(get_probs, init_seq='', beam=2, mode='rr')\n",
    "\n",
    "text = map(lambda i: idx_to_w[i], list(utt))\n",
    "' '.join(text[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"but i can ' t let him introduce her a while , much been all a long special respect to things . </s> <s> it ' s simple , mr . <person> to you . i ' m glad you ' re mad , tell me ? </s> <s> yes . this no isn ' t who they aren ' t speak of him . </s>\""
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rnd_next_word(probs):\n",
    "    return np.random.choice(np.arange(len(probs[0])), p=probs[0])\n",
    "\n",
    "init_seq = ''\n",
    "utt = [1] + map(lambda w: w_to_idx.get(w, w_to_idx['<unk>']), init_seq.split())\n",
    "utt = np.asarray(utt, dtype=np.int32)[np.newaxis]\n",
    "\n",
    "i = 0\n",
    "while utt[0,-1] != voc_size-1 and i < 100:\n",
    "    word_probs = get_probs(utt)\n",
    "    next_idx = rnd_next_word(word_probs)\n",
    "    utt = np.append(utt, next_idx)[np.newaxis].astype(np.int32)\n",
    "    i += 1\n",
    "    \n",
    "text = map(lambda i: idx_to_w[i], list(utt[0]))\n",
    "' '.join(text[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

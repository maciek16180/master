{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/i258346/.local/lib/python2.7/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import lasagne as L\n",
    "\n",
    "sys.path.insert(0, '../HSoftmaxLayerLasagne/')\n",
    "\n",
    "import HSoftmaxLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mt_path = \"/pio/data/data/mtriples/\"\n",
    "\n",
    "beg_token = '<utt_beg>'\n",
    "end_token = '<utt_end>'\n",
    "\n",
    "def get_mt_voc(path=mt_path):\n",
    "    i_to_w, w_to_i = {}, {}\n",
    "    \n",
    "    i_to_w[0] = end_token   # separate tokens for beginning and ending of an utterance\n",
    "    w_to_i[end_token] = 0   # <utt_end> serves only as a target for the last word in the input sequence\n",
    "    i_to_w[1] = beg_token   # <utt_beg> will always be the first generated word\n",
    "    w_to_i[beg_token] = 1    \n",
    "    wc = 2\n",
    "    \n",
    "    with open(path + \"WordsList.txt\", \"r\") as wl:\n",
    "        for w in wl:\n",
    "            i_to_w[wc] = w[:-1]\n",
    "            w_to_i[w[:-1]] = wc\n",
    "            wc += 1\n",
    "    \n",
    "    return i_to_w, w_to_i, wc\n",
    "\n",
    "mt_i_to_w, mt_w_to_i, mt_voc_size = get_mt_voc()\n",
    "\n",
    "\n",
    "def load_mt(path=mt_path):\n",
    "    tr = None\n",
    "    vl = None\n",
    "    ts = None\n",
    "    \n",
    "    with open(path + \"Training_Shuffled_Dataset.txt\") as f:\n",
    "        tr = []\n",
    "        for l in f:\n",
    "            tr.insert(0, [1] + map(lambda w: mt_w_to_i.get(w, mt_w_to_i['<unk>']), l.split()) + [0])\n",
    "        \n",
    "    with open(path + \"Validation_Shuffled_Dataset.txt\") as f:\n",
    "        vl = []\n",
    "        for l in f:\n",
    "            vl.insert(0, [1] + map(lambda w: mt_w_to_i.get(w, mt_w_to_i['<unk>']), l.split()) + [0])\n",
    "            \n",
    "    with open(path + \"Test_Shuffled_Dataset.txt\") as f:\n",
    "        ts = []\n",
    "        for l in f:\n",
    "            ts.insert(0, [1] + map(lambda w: mt_w_to_i.get(w, mt_w_to_i['<unk>']), l.split()) + [0])\n",
    "    \n",
    "    return tr, vl, ts\n",
    "\n",
    "mt_train, mt_val, mt_test = load_mt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Similar to Lasagne mnist.py example, added input mask and different sequence lengths\n",
    "\n",
    "def iterate_minibatches(inputs, batchsize, shuffle=False):\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        t0 = time.time() # time wasted preparing data, just for the info\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        inp = inputs[excerpt]\n",
    "        \n",
    "        inp_max_len = len(max(inp, key=len))\n",
    "        inp = map(lambda l: l + [-1]*(inp_max_len-len(l)), inp)\n",
    "        inp = np.asarray(inp, dtype=np.int32)\n",
    "        tar = np.hstack((inp[:,1:], np.asarray([-1]*batchsize, dtype=np.int32).reshape((-1,1))))\n",
    "        def gr_zero(x):\n",
    "            if x > 0:\n",
    "                return 1.\n",
    "            return 0.\n",
    "        v_gr_zero = np.vectorize(gr_zero, otypes=[np.float32])\n",
    "        mask = v_gr_zero(inp) # 0 in vocabulary represents <utt_end>, we don't feed that into the net\n",
    "        \n",
    "        yield inp, tar, mask, (time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_simple_rnnlm(input_var, mask_input_var, voc_size, emb_size, rec_size):\n",
    "    l_in = L.layers.InputLayer(shape=(None, None), input_var=input_var)    \n",
    "    batch_size, seq_len = l_in.input_var.shape\n",
    "    l_mask = L.layers.InputLayer(shape=(batch_size, seq_len), input_var=mask_input_var)\n",
    "    \n",
    "    l_emb = L.layers.EmbeddingLayer(l_in,\n",
    "                                    input_size=voc_size+1, \n",
    "                                    output_size=emb_size)\n",
    "    \n",
    "    l_rec = L.layers.RecurrentLayer(l_emb,\n",
    "                                    num_units=rec_size, \n",
    "                                    W_in_to_hid=L.init.Orthogonal(), \n",
    "                                    W_hid_to_hid=L.init.Orthogonal(),\n",
    "                                    mask_input=l_mask)\n",
    "    \n",
    "    l_resh = L.layers.ReshapeLayer(l_rec, shape=(-1, rec_size))\n",
    "    \n",
    "    l_soft = L.layers.DenseLayer(l_resh,\n",
    "                                num_units=voc_size,\n",
    "                                nonlinearity=L.nonlinearities.softmax)\n",
    "    \n",
    "    l_out = L.layers.ReshapeLayer(l_soft, shape=(batch_size, seq_len, voc_size))\n",
    "    \n",
    "    return l_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#voc_size = mt_voc_size\n",
    "#emb_size = 50\n",
    "#rec_size = 100\n",
    "#\n",
    "#input_var = T.imatrix('inputs')\n",
    "#targets = T.imatrix('targets') # these will be inputs shifted by 1\n",
    "#mask_input_var = T.matrix('input_mask')\n",
    "#\n",
    "#net = build_simple_rnnlm(input_var, mask_input_var, voc_size, emb_size, rec_size)\n",
    "#out = L.layers.get_output(net)\n",
    "#\n",
    "#loss = L.objectives.categorical_crossentropy(out.reshape((-1,voc_size)), targets.ravel())\n",
    "#loss = loss.mean() # mean batch loss\n",
    "#\n",
    "#params = L.layers.get_all_params(net, trainable=True)\n",
    "#updates = L.updates.rmsprop(loss, params)\n",
    "#\n",
    "#train_fn = theano.function([input_var, targets, mask_input_var], loss, updates=updates)\n",
    "#\n",
    "#### for validation\n",
    "#\n",
    "#test_out = L.layers.get_output(net, deterministic=True)\n",
    "#test_loss = L.objectives.categorical_crossentropy(test_out.reshape((-1,voc_size)), targets.ravel())\n",
    "#test_loss = test_loss.mean()\n",
    "#test_acc = T.mean(T.eq(T.argmax(test_out, axis=1), targets), dtype=theano.config.floatX)\n",
    "#\n",
    "#val_fn = theano.function([input_var, targets, mask_input_var], [test_loss, test_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_hsoft_rnnlm(input_var, target_var, mask_input_var, voc_size, emb_size, rec_size):\n",
    "    l_in = L.layers.InputLayer(shape=(None, None), input_var=input_var)    \n",
    "    batch_size, seq_len = l_in.input_var.shape\n",
    "    l_mask = L.layers.InputLayer(shape=(batch_size, seq_len), input_var=mask_input_var)\n",
    "    l_tar = L.layers.InputLayer(shape=(None, None), input_var=target_var)\n",
    "    \n",
    "    l_emb = L.layers.EmbeddingLayer(l_in,\n",
    "                                    input_size=voc_size+1, \n",
    "                                    output_size=emb_size)\n",
    "    \n",
    "    l_rec = L.layers.RecurrentLayer(l_emb,\n",
    "                                    num_units=rec_size, \n",
    "                                    W_in_to_hid=L.init.Orthogonal(), \n",
    "                                    W_hid_to_hid=L.init.Orthogonal(),\n",
    "                                    mask_input=l_mask)\n",
    "    \n",
    "    l_resh = L.layers.ReshapeLayer(l_rec, shape=(-1, rec_size))\n",
    "    \n",
    "    l_resh_tar = L.layers.ReshapeLayer(l_tar, shape=(-1, 1))\n",
    "    \n",
    "    l_hsoft = HSoftmaxLayer.HierarchicalSoftmaxDenseLayer(l_resh,\n",
    "                                                          num_units=voc_size,\n",
    "                                                          target=l_resh_tar)\n",
    "    l_out = L.layers.ReshapeLayer(l_hsoft, shape=(batch_size, seq_len))\n",
    "    \n",
    "    return l_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "voc_size = mt_voc_size\n",
    "emb_size = 10\n",
    "rec_size = 10\n",
    "\n",
    "input_var = T.imatrix('inputs')\n",
    "targets = T.imatrix('targets') # these will be inputs shifted by 1\n",
    "mask_input_var = T.matrix('input_mask')\n",
    "\n",
    "net = build_hsoft_rnnlm(input_var, targets, mask_input_var, voc_size, emb_size, rec_size)\n",
    "out = L.layers.get_output(net)\n",
    "\n",
    "mask_idx = mask_input_var.nonzero()\n",
    "loss = -T.sum(T.log(out[mask_idx])) / T.sum(mask_input_var)\n",
    "#loss = loss.mean() # mean batch loss\n",
    "\n",
    "params = L.layers.get_all_params(net, trainable=True)\n",
    "updates = L.updates.rmsprop(loss, params, learning_rate=1., rho=.9, epsilon=1e-06)\n",
    "\n",
    "train_fn = theano.function([input_var, targets, mask_input_var], loss, updates=updates)\n",
    "\n",
    "#### for validation\n",
    "\n",
    "#test_out = L.layers.get_output(net, deterministic=True)\n",
    "#test_loss = -T.sum(T.log(test_out[mask_idx])) / T.sum(mask_input_var)\n",
    "\n",
    "#test_acc = T.mean(T.eq(T.argmax(test_out, axis=1), targets), dtype=theano.config.floatX)\n",
    "\n",
    "#val_fn = theano.function([input_var, targets, mask_input_var], test_loss)\n",
    "\n",
    "\n",
    "### dump weights\n",
    "\n",
    "np.savez('model.npz', *L.layers.get_all_param_values(net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with np.load('model.npz') as f:\n",
    "    param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "    L.layers.set_all_param_values(net, param_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 1 batches in 0.02 sec.    batch training loss:\t\t9.21974754333\n",
      "Done 2 batches in 0.02 sec.    batch training loss:\t\tinf\n",
      "Done 3 batches in 0.03 sec.    batch training loss:\t\tnan\n",
      "Done 4 batches in 0.03 sec.    batch training loss:\t\tnan\n",
      "Done 5 batches in 0.03 sec.    batch training loss:\t\tnan\n",
      "Done 6 batches in 0.04 sec.    batch training loss:\t\tnan\n",
      "Done 7 batches in 0.04 sec.    batch training loss:\t\tnan\n",
      "Done 8 batches in 0.05 sec.    batch training loss:\t\tnan\n",
      "Done 9 batches in 0.05 sec.    batch training loss:\t\tnan\n",
      "Done 10 batches in 0.06 sec.    batch training loss:\t\tnan\n",
      "Done 11 batches in 0.07 sec.    batch training loss:\t\tnan\n",
      "Done 12 batches in 0.07 sec.    batch training loss:\t\tnan\n",
      "Done 13 batches in 0.08 sec.    batch training loss:\t\tnan\n",
      "Done 14 batches in 0.08 sec.    batch training loss:\t\tnan\n",
      "Done 15 batches in 0.09 sec.    batch training loss:\t\tnan\n",
      "Done 16 batches in 0.09 sec.    batch training loss:\t\tnan\n",
      "Done 17 batches in 0.10 sec.    batch training loss:\t\tnan\n",
      "Done 18 batches in 0.11 sec.    batch training loss:\t\tnan\n",
      "Done 19 batches in 0.12 sec.    batch training loss:\t\tnan\n",
      "Done 20 batches in 0.12 sec.    batch training loss:\t\tnan\n",
      "Done 21 batches in 0.12 sec.    batch training loss:\t\tnan\n",
      "Done 22 batches in 0.13 sec.    batch training loss:\t\tnan\n",
      "Done 23 batches in 0.14 sec.    batch training loss:\t\tnan\n",
      "Done 24 batches in 0.14 sec.    batch training loss:\t\tnan\n",
      "Done 25 batches in 0.14 sec.    batch training loss:\t\tnan\n",
      "Done 26 batches in 0.15 sec.    batch training loss:\t\tnan\n",
      "Done 27 batches in 0.16 sec.    batch training loss:\t\tnan\n",
      "Done 28 batches in 0.17 sec.    batch training loss:\t\tnan\n",
      "Done 29 batches in 0.17 sec.    batch training loss:\t\tnan\n",
      "Done 30 batches in 0.18 sec.    batch training loss:\t\tnan\n",
      "Done 31 batches in 0.18 sec.    batch training loss:\t\tnan\n",
      "Done 32 batches in 0.19 sec.    batch training loss:\t\tnan\n",
      "Done 33 batches in 0.20 sec.    batch training loss:\t\tnan\n",
      "Done 34 batches in 0.20 sec.    batch training loss:\t\tnan\n",
      "Done 35 batches in 0.21 sec.    batch training loss:\t\tnan\n",
      "Done 36 batches in 0.21 sec.    batch training loss:\t\tnan\n",
      "Done 37 batches in 0.22 sec.    batch training loss:\t\tnan\n",
      "Done 38 batches in 0.24 sec.    batch training loss:\t\tnan\n",
      "Done 39 batches in 0.24 sec.    batch training loss:\t\tnan\n",
      "Done 40 batches in 0.25 sec.    batch training loss:\t\tnan\n",
      "Done 41 batches in 0.26 sec.    batch training loss:\t\tnan\n",
      "Done 42 batches in 0.26 sec.    batch training loss:\t\tnan\n",
      "Done 43 batches in 0.27 sec.    batch training loss:\t\tnan\n",
      "Done 44 batches in 0.27 sec.    batch training loss:\t\tnan\n",
      "Done 45 batches in 0.28 sec.    batch training loss:\t\tnan\n",
      "Done 46 batches in 0.28 sec.    batch training loss:\t\tnan\n",
      "Done 47 batches in 0.29 sec.    batch training loss:\t\tnan\n",
      "Done 48 batches in 0.30 sec.    batch training loss:\t\tnan\n",
      "Done 49 batches in 0.30 sec.    batch training loss:\t\tnan\n",
      "Done 50 batches in 0.31 sec.    batch training loss:\t\tnan\n",
      "Done 51 batches in 0.31 sec.    batch training loss:\t\tnan\n",
      "Done 52 batches in 0.32 sec.    batch training loss:\t\tnan\n",
      "Done 53 batches in 0.32 sec.    batch training loss:\t\tnan\n",
      "Done 54 batches in 0.33 sec.    batch training loss:\t\tnan\n",
      "Done 55 batches in 0.34 sec.    batch training loss:\t\tnan\n",
      "Done 56 batches in 0.34 sec.    batch training loss:\t\tnan\n",
      "Done 57 batches in 0.35 sec.    batch training loss:\t\tnan\n",
      "Done 58 batches in 0.35 sec.    batch training loss:\t\tnan\n",
      "Done 59 batches in 0.36 sec.    batch training loss:\t\tnan\n",
      "Done 60 batches in 0.36 sec.    batch training loss:\t\tnan\n",
      "Done 61 batches in 0.37 sec.    batch training loss:\t\tnan\n",
      "Done 62 batches in 0.38 sec.    batch training loss:\t\tnan\n",
      "Done 63 batches in 0.38 sec.    batch training loss:\t\tnan\n",
      "Done 64 batches in 0.39 sec.    batch training loss:\t\tnan\n",
      "Done 65 batches in 0.39 sec.    batch training loss:\t\tnan\n",
      "Done 66 batches in 0.40 sec.    batch training loss:\t\tnan\n",
      "Done 67 batches in 0.40 sec.    batch training loss:\t\tnan\n",
      "Done 68 batches in 0.41 sec.    batch training loss:\t\tnan\n",
      "Done 69 batches in 0.41 sec.    batch training loss:\t\tnan\n",
      "Done 70 batches in 0.42 sec.    batch training loss:\t\tnan\n",
      "Done 71 batches in 0.43 sec.    batch training loss:\t\tnan\n",
      "Done 72 batches in 0.44 sec.    batch training loss:\t\tnan\n",
      "Done 73 batches in 0.45 sec.    batch training loss:\t\tnan\n",
      "Done 74 batches in 0.45 sec.    batch training loss:\t\tnan\n",
      "Done 75 batches in 0.46 sec.    batch training loss:\t\tnan\n",
      "Done 76 batches in 0.46 sec.    batch training loss:\t\tnan\n",
      "Done 77 batches in 0.46 sec.    batch training loss:\t\tnan\n",
      "Done 78 batches in 0.47 sec.    batch training loss:\t\tnan\n",
      "Done 79 batches in 0.48 sec.    batch training loss:\t\tnan\n",
      "Done 80 batches in 0.48 sec.    batch training loss:\t\tnan\n",
      "Done 81 batches in 0.49 sec.    batch training loss:\t\tnan\n",
      "Done 82 batches in 0.49 sec.    batch training loss:\t\tnan\n",
      "Done 83 batches in 0.50 sec.    batch training loss:\t\tnan\n",
      "Done 84 batches in 0.50 sec.    batch training loss:\t\tnan\n",
      "Done 85 batches in 0.51 sec.    batch training loss:\t\tnan\n",
      "Done 86 batches in 0.51 sec.    batch training loss:\t\tnan\n",
      "Done 87 batches in 0.52 sec.    batch training loss:\t\tnan\n",
      "Done 88 batches in 0.52 sec.    batch training loss:\t\tnan\n",
      "Done 89 batches in 0.53 sec.    batch training loss:\t\tnan\n",
      "Done 90 batches in 0.53 sec.    batch training loss:\t\tnan\n",
      "Done 91 batches in 0.55 sec.    batch training loss:\t\tnan\n",
      "Done 92 batches in 0.55 sec.    batch training loss:\t\tnan\n",
      "Done 93 batches in 0.56 sec.    batch training loss:\t\tnan\n",
      "Done 94 batches in 0.56 sec.    batch training loss:\t\tnan\n",
      "Done 95 batches in 0.56 sec.    batch training loss:\t\tnan\n",
      "Done 96 batches in 0.57 sec.    batch training loss:\t\tnan\n",
      "Done 97 batches in 0.57 sec.    batch training loss:\t\tnan\n",
      "Done 98 batches in 0.58 sec.    batch training loss:\t\tnan\n",
      "Done 99 batches in 0.59 sec.    batch training loss:\t\tnan\n",
      "Done 100 batches in 0.59 sec.    batch training loss:\t\tnan\n",
      "Done 101 batches in 0.59 sec.    batch training loss:\t\tnan\n",
      "Done 102 batches in 0.60 sec.    batch training loss:\t\tnan\n",
      "Done 103 batches in 0.60 sec.    batch training loss:\t\tnan\n",
      "Done 104 batches in 0.61 sec.    batch training loss:\t\tnan\n",
      "Done 105 batches in 0.62 sec.    batch training loss:\t\tnan\n",
      "Done 106 batches in 0.63 sec.    batch training loss:\t\tnan\n",
      "Done 107 batches in 0.63 sec.    batch training loss:\t\tnan\n",
      "Done 108 batches in 0.64 sec.    batch training loss:\t\tnan\n",
      "Done 109 batches in 0.65 sec.    batch training loss:\t\tnan\n",
      "Done 110 batches in 0.66 sec.    batch training loss:\t\tnan\n",
      "Done 111 batches in 0.66 sec.    batch training loss:\t\tnan\n",
      "Done 112 batches in 0.67 sec.    batch training loss:\t\tnan\n",
      "Done 113 batches in 0.67 sec.    batch training loss:\t\tnan\n",
      "Done 114 batches in 0.68 sec.    batch training loss:\t\tnan\n",
      "Done 115 batches in 0.68 sec.    batch training loss:\t\tnan\n",
      "Done 116 batches in 0.69 sec.    batch training loss:\t\tnan\n",
      "Done 117 batches in 0.70 sec.    batch training loss:\t\tnan\n",
      "Done 118 batches in 0.71 sec.    batch training loss:\t\tnan\n",
      "Done 119 batches in 0.71 sec.    batch training loss:\t\tnan\n",
      "Done 120 batches in 0.72 sec.    batch training loss:\t\tnan\n",
      "Done 121 batches in 0.73 sec.    batch training loss:\t\tnan\n",
      "Done 122 batches in 0.73 sec.    batch training loss:\t\tnan\n",
      "Done 123 batches in 0.73 sec.    batch training loss:\t\tnan\n",
      "Done 124 batches in 0.74 sec.    batch training loss:\t\tnan\n",
      "Done 125 batches in 0.74 sec.    batch training loss:\t\tnan\n",
      "Done 126 batches in 0.74 sec.    batch training loss:\t\tnan\n",
      "Done 127 batches in 0.75 sec.    batch training loss:\t\tnan\n",
      "Done 128 batches in 0.76 sec.    batch training loss:\t\tnan\n",
      "Done 129 batches in 0.77 sec.    batch training loss:\t\tnan\n",
      "Done 130 batches in 0.77 sec.    batch training loss:\t\tnan\n",
      "Done 131 batches in 0.78 sec.    batch training loss:\t\tnan\n",
      "Done 132 batches in 0.78 sec.    batch training loss:\t\tnan\n",
      "Done 133 batches in 0.79 sec.    batch training loss:\t\tnan\n",
      "Done 134 batches in 0.80 sec.    batch training loss:\t\tnan\n",
      "Done 135 batches in 0.80 sec.    batch training loss:\t\tnan\n",
      "Done 136 batches in 0.81 sec.    batch training loss:\t\tnan\n",
      "Done 137 batches in 0.81 sec.    batch training loss:\t\tnan\n",
      "Done 138 batches in 0.82 sec.    batch training loss:\t\tnan\n",
      "Done 139 batches in 0.82 sec.    batch training loss:\t\tnan\n",
      "Done 140 batches in 0.83 sec.    batch training loss:\t\tnan\n",
      "Done 141 batches in 0.84 sec.    batch training loss:\t\tnan\n",
      "Done 142 batches in 0.85 sec.    batch training loss:\t\tnan\n",
      "Done 143 batches in 0.85 sec.    batch training loss:\t\tnan\n",
      "Done 144 batches in 0.86 sec.    batch training loss:\t\tnan\n",
      "Done 145 batches in 0.87 sec.    batch training loss:\t\tnan\n",
      "Done 146 batches in 0.87 sec.    batch training loss:\t\tnan\n",
      "Done 147 batches in 0.88 sec.    batch training loss:\t\tnan\n",
      "Done 148 batches in 0.88 sec.    batch training loss:\t\tnan\n",
      "Done 149 batches in 0.89 sec.    batch training loss:\t\tnan\n",
      "Done 150 batches in 0.89 sec.    batch training loss:\t\tnan\n",
      "Done 151 batches in 0.90 sec.    batch training loss:\t\tnan\n",
      "Done 152 batches in 0.90 sec.    batch training loss:\t\tnan\n",
      "Done 153 batches in 0.90 sec.    batch training loss:\t\tnan\n",
      "Done 154 batches in 0.91 sec.    batch training loss:\t\tnan\n",
      "Done 155 batches in 0.91 sec.    batch training loss:\t\tnan\n",
      "Done 156 batches in 0.92 sec.    batch training loss:\t\tnan\n",
      "Done 157 batches in 0.93 sec.    batch training loss:\t\tnan\n",
      "Done 158 batches in 0.93 sec.    batch training loss:\t\tnan\n",
      "Done 159 batches in 0.94 sec.    batch training loss:\t\tnan\n",
      "Done 160 batches in 0.94 sec.    batch training loss:\t\tnan\n",
      "Done 161 batches in 0.95 sec.    batch training loss:\t\tnan\n",
      "Done 162 batches in 0.95 sec.    batch training loss:\t\tnan\n",
      "Done 163 batches in 0.96 sec.    batch training loss:\t\tnan\n",
      "Done 164 batches in 0.97 sec.    batch training loss:\t\tnan\n",
      "Done 165 batches in 0.97 sec.    batch training loss:\t\tnan\n",
      "Done 166 batches in 0.98 sec.    batch training loss:\t\tnan\n",
      "Done 167 batches in 0.99 sec.    batch training loss:\t\tnan\n",
      "Done 168 batches in 1.00 sec.    batch training loss:\t\tnan\n",
      "Done 169 batches in 1.00 sec.    batch training loss:\t\tnan\n",
      "Done 170 batches in 1.01 sec.    batch training loss:\t\tnan\n",
      "Done 171 batches in 1.02 sec.    batch training loss:\t\tnan\n",
      "Done 172 batches in 1.03 sec.    batch training loss:\t\tnan\n",
      "Done 173 batches in 1.03 sec.    batch training loss:\t\tnan\n",
      "Done 174 batches in 1.04 sec.    batch training loss:\t\tnan\n",
      "Done 175 batches in 1.04 sec.    batch training loss:\t\tnan\n",
      "Done 176 batches in 1.04 sec.    batch training loss:\t\tnan\n",
      "Done 177 batches in 1.05 sec.    batch training loss:\t\tnan\n",
      "Done 178 batches in 1.05 sec.    batch training loss:\t\tnan\n",
      "Done 179 batches in 1.06 sec.    batch training loss:\t\tnan\n",
      "Done 180 batches in 1.07 sec.    batch training loss:\t\tnan\n",
      "Done 181 batches in 1.07 sec.    batch training loss:\t\tnan\n",
      "Done 182 batches in 1.08 sec.    batch training loss:\t\tnan\n",
      "Done 183 batches in 1.09 sec.    batch training loss:\t\tnan\n",
      "Done 184 batches in 1.09 sec.    batch training loss:\t\tnan\n",
      "Done 185 batches in 1.10 sec.    batch training loss:\t\tnan\n",
      "Done 186 batches in 1.10 sec.    batch training loss:\t\tnan\n",
      "Done 187 batches in 1.11 sec.    batch training loss:\t\tnan\n",
      "Done 188 batches in 1.12 sec.    batch training loss:\t\tnan\n",
      "Done 189 batches in 1.12 sec.    batch training loss:\t\tnan\n",
      "Done 190 batches in 1.13 sec.    batch training loss:\t\tnan\n",
      "Done 191 batches in 1.13 sec.    batch training loss:\t\tnan\n",
      "Done 192 batches in 1.14 sec.    batch training loss:\t\tnan\n",
      "Done 193 batches in 1.14 sec.    batch training loss:\t\tnan\n",
      "Done 194 batches in 1.15 sec.    batch training loss:\t\tnan\n",
      "Done 195 batches in 1.15 sec.    batch training loss:\t\tnan\n",
      "Done 196 batches in 1.16 sec.    batch training loss:\t\tnan\n",
      "Done 197 batches in 1.16 sec.    batch training loss:\t\tnan\n",
      "Done 198 batches in 1.17 sec.    batch training loss:\t\tnan\n",
      "Done 199 batches in 1.17 sec.    batch training loss:\t\tnan\n",
      "Done 200 batches in 1.18 sec.    batch training loss:\t\tnan\n",
      "Done 201 batches in 1.18 sec.    batch training loss:\t\tnan\n",
      "Done 202 batches in 1.19 sec.    batch training loss:\t\tnan\n",
      "Done 203 batches in 1.20 sec.    batch training loss:\t\tnan\n",
      "Done 204 batches in 1.20 sec.    batch training loss:\t\tnan\n",
      "Done 205 batches in 1.21 sec.    batch training loss:\t\tnan\n",
      "Done 206 batches in 1.21 sec.    batch training loss:\t\tnan\n",
      "Done 207 batches in 1.22 sec.    batch training loss:\t\tnan\n",
      "Done 208 batches in 1.22 sec.    batch training loss:\t\tnan\n",
      "Done 209 batches in 1.23 sec.    batch training loss:\t\tnan\n",
      "Done 210 batches in 1.23 sec.    batch training loss:\t\tnan\n",
      "Done 211 batches in 1.24 sec.    batch training loss:\t\tnan\n",
      "Done 212 batches in 1.25 sec.    batch training loss:\t\tnan\n",
      "Done 213 batches in 1.25 sec.    batch training loss:\t\tnan\n",
      "Done 214 batches in 1.26 sec.    batch training loss:\t\tnan\n",
      "Done 215 batches in 1.27 sec.    batch training loss:\t\tnan\n",
      "Done 216 batches in 1.27 sec.    batch training loss:\t\tnan\n",
      "Done 217 batches in 1.28 sec.    batch training loss:\t\tnan\n",
      "Done 218 batches in 1.28 sec.    batch training loss:\t\tnan\n",
      "Done 219 batches in 1.29 sec.    batch training loss:\t\tnan\n",
      "Done 220 batches in 1.30 sec.    batch training loss:\t\tnan\n",
      "Done 221 batches in 1.30 sec.    batch training loss:\t\tnan\n",
      "Done 222 batches in 1.31 sec.    batch training loss:\t\tnan\n",
      "Done 223 batches in 1.31 sec.    batch training loss:\t\tnan\n",
      "Done 224 batches in 1.32 sec.    batch training loss:\t\tnan\n",
      "Done 225 batches in 1.32 sec.    batch training loss:\t\tnan\n",
      "Done 226 batches in 1.32 sec.    batch training loss:\t\tnan\n",
      "Done 227 batches in 1.33 sec.    batch training loss:\t\tnan\n",
      "Done 228 batches in 1.33 sec.    batch training loss:\t\tnan\n",
      "Done 229 batches in 1.34 sec.    batch training loss:\t\tnan\n",
      "Done 230 batches in 1.34 sec.    batch training loss:\t\tnan\n",
      "Done 231 batches in 1.35 sec.    batch training loss:\t\tnan\n",
      "Done 232 batches in 1.35 sec.    batch training loss:\t\tnan\n",
      "Done 233 batches in 1.36 sec.    batch training loss:\t\tnan\n",
      "Done 234 batches in 1.36 sec.    batch training loss:\t\tnan\n",
      "Done 235 batches in 1.36 sec.    batch training loss:\t\tnan\n",
      "Done 236 batches in 1.37 sec.    batch training loss:\t\tnan\n",
      "Done 237 batches in 1.38 sec.    batch training loss:\t\tnan\n",
      "Done 238 batches in 1.38 sec.    batch training loss:\t\tnan\n",
      "Done 239 batches in 1.39 sec.    batch training loss:\t\tnan\n",
      "Done 240 batches in 1.40 sec.    batch training loss:\t\tnan\n",
      "Done 241 batches in 1.41 sec.    batch training loss:\t\tnan\n",
      "Done 242 batches in 1.41 sec.    batch training loss:\t\tnan\n",
      "Done 243 batches in 1.41 sec.    batch training loss:\t\tnan\n",
      "Done 244 batches in 1.42 sec.    batch training loss:\t\tnan\n",
      "Done 245 batches in 1.43 sec.    batch training loss:\t\tnan\n",
      "Done 246 batches in 1.43 sec.    batch training loss:\t\tnan\n",
      "Done 247 batches in 1.44 sec.    batch training loss:\t\tnan\n",
      "Done 248 batches in 1.44 sec.    batch training loss:\t\tnan\n",
      "Done 249 batches in 1.45 sec.    batch training loss:\t\tnan\n",
      "Done 250 batches in 1.46 sec.    batch training loss:\t\tnan\n",
      "Done 251 batches in 1.46 sec.    batch training loss:\t\tnan\n",
      "Done 252 batches in 1.47 sec.    batch training loss:\t\tnan\n",
      "Done 253 batches in 1.48 sec.    batch training loss:\t\tnan\n",
      "Done 254 batches in 1.48 sec.    batch training loss:\t\tnan\n",
      "Done 255 batches in 1.48 sec.    batch training loss:\t\tnan\n",
      "Done 256 batches in 1.49 sec.    batch training loss:\t\tnan\n",
      "Done 257 batches in 1.49 sec.    batch training loss:\t\tnan\n",
      "Done 258 batches in 1.50 sec.    batch training loss:\t\tnan\n",
      "Done 259 batches in 1.50 sec.    batch training loss:\t\tnan\n",
      "Done 260 batches in 1.51 sec.    batch training loss:\t\tnan\n",
      "Done 261 batches in 1.52 sec.    batch training loss:\t\tnan\n",
      "Done 262 batches in 1.52 sec.    batch training loss:\t\tnan\n",
      "Done 263 batches in 1.53 sec.    batch training loss:\t\tnan\n",
      "Done 264 batches in 1.53 sec.    batch training loss:\t\tnan\n",
      "Done 265 batches in 1.54 sec.    batch training loss:\t\tnan\n",
      "Done 266 batches in 1.54 sec.    batch training loss:\t\tnan\n",
      "Done 267 batches in 1.55 sec.    batch training loss:\t\tnan\n",
      "Done 268 batches in 1.55 sec.    batch training loss:\t\tnan\n",
      "Done 269 batches in 1.57 sec.    batch training loss:\t\tnan\n",
      "Done 270 batches in 1.57 sec.    batch training loss:\t\tnan\n",
      "Done 271 batches in 1.58 sec.    batch training loss:\t\tnan\n",
      "Done 272 batches in 1.58 sec.    batch training loss:\t\tnan\n",
      "Done 273 batches in 1.59 sec.    batch training loss:\t\tnan\n",
      "Done 274 batches in 1.59 sec.    batch training loss:\t\tnan\n",
      "Done 275 batches in 1.60 sec.    batch training loss:\t\tnan\n",
      "Done 276 batches in 1.61 sec.    batch training loss:\t\tnan\n",
      "Done 277 batches in 1.61 sec.    batch training loss:\t\tnan\n",
      "Done 278 batches in 1.62 sec.    batch training loss:\t\tnan\n",
      "Done 279 batches in 1.63 sec.    batch training loss:\t\tnan\n",
      "Done 280 batches in 1.63 sec.    batch training loss:\t\tnan\n",
      "Done 281 batches in 1.64 sec.    batch training loss:\t\tnan\n",
      "Done 282 batches in 1.65 sec.    batch training loss:\t\tnan\n",
      "Done 283 batches in 1.65 sec.    batch training loss:\t\tnan\n",
      "Done 284 batches in 1.66 sec.    batch training loss:\t\tnan\n",
      "Done 285 batches in 1.67 sec.    batch training loss:\t\tnan\n",
      "Done 286 batches in 1.67 sec.    batch training loss:\t\tnan\n",
      "Done 287 batches in 1.67 sec.    batch training loss:\t\tnan\n",
      "Done 288 batches in 1.68 sec.    batch training loss:\t\tnan\n",
      "Done 289 batches in 1.69 sec.    batch training loss:\t\tnan\n",
      "Done 290 batches in 1.70 sec.    batch training loss:\t\tnan\n",
      "Done 291 batches in 1.71 sec.    batch training loss:\t\tnan\n",
      "Done 292 batches in 1.71 sec.    batch training loss:\t\tnan\n",
      "Done 293 batches in 1.72 sec.    batch training loss:\t\tnan\n",
      "Done 294 batches in 1.72 sec.    batch training loss:\t\tnan\n",
      "Done 295 batches in 1.73 sec.    batch training loss:\t\tnan\n",
      "Done 296 batches in 1.73 sec.    batch training loss:\t\tnan\n",
      "Done 297 batches in 1.73 sec.    batch training loss:\t\tnan\n",
      "Done 298 batches in 1.74 sec.    batch training loss:\t\tnan\n",
      "Done 299 batches in 1.75 sec.    batch training loss:\t\tnan\n",
      "Done 300 batches in 1.75 sec.    batch training loss:\t\tnan\n",
      "Done 301 batches in 1.76 sec.    batch training loss:\t\tnan\n",
      "Done 302 batches in 1.76 sec.    batch training loss:\t\tnan\n",
      "Done 303 batches in 1.77 sec.    batch training loss:\t\tnan\n",
      "Done 304 batches in 1.78 sec.    batch training loss:\t\tnan\n",
      "Done 305 batches in 1.78 sec.    batch training loss:\t\tnan\n",
      "Done 306 batches in 1.79 sec.    batch training loss:\t\tnan\n",
      "Done 307 batches in 1.80 sec.    batch training loss:\t\tnan\n",
      "Done 308 batches in 1.80 sec.    batch training loss:\t\tnan\n",
      "Done 309 batches in 1.81 sec.    batch training loss:\t\tnan\n",
      "Done 310 batches in 1.81 sec.    batch training loss:\t\tnan\n",
      "Done 311 batches in 1.82 sec.    batch training loss:\t\tnan\n",
      "Done 312 batches in 1.83 sec.    batch training loss:\t\tnan\n",
      "Done 313 batches in 1.83 sec.    batch training loss:\t\tnan\n",
      "Done 314 batches in 1.84 sec.    batch training loss:\t\tnan\n",
      "Done 315 batches in 1.84 sec.    batch training loss:\t\tnan\n",
      "Done 316 batches in 1.85 sec.    batch training loss:\t\tnan\n",
      "Done 317 batches in 1.86 sec.    batch training loss:\t\tnan\n",
      "Done 318 batches in 1.86 sec.    batch training loss:\t\tnan\n",
      "Done 319 batches in 1.87 sec.    batch training loss:\t\tnan\n",
      "Done 320 batches in 1.87 sec.    batch training loss:\t\tnan\n",
      "Done 321 batches in 1.88 sec.    batch training loss:\t\tnan\n",
      "Done 322 batches in 1.88 sec.    batch training loss:\t\tnan\n",
      "Done 323 batches in 1.89 sec.    batch training loss:\t\tnan\n",
      "Done 324 batches in 1.89 sec.    batch training loss:\t\tnan\n",
      "Done 325 batches in 1.90 sec.    batch training loss:\t\tnan\n",
      "Done 326 batches in 1.90 sec.    batch training loss:\t\tnan\n",
      "Done 327 batches in 1.92 sec.    batch training loss:\t\tnan\n",
      "Done 328 batches in 1.93 sec.    batch training loss:\t\tnan\n",
      "Done 329 batches in 1.93 sec.    batch training loss:\t\tnan\n",
      "Done 330 batches in 1.94 sec.    batch training loss:\t\tnan\n",
      "Done 331 batches in 1.94 sec.    batch training loss:\t\tnan\n",
      "Done 332 batches in 1.95 sec.    batch training loss:\t\tnan\n",
      "Done 333 batches in 1.96 sec.    batch training loss:\t\tnan\n",
      "Done 334 batches in 1.96 sec.    batch training loss:\t\tnan\n",
      "Done 335 batches in 1.97 sec.    batch training loss:\t\tnan\n",
      "Done 336 batches in 1.97 sec.    batch training loss:\t\tnan\n",
      "Done 337 batches in 1.98 sec.    batch training loss:\t\tnan\n",
      "Done 338 batches in 1.98 sec.    batch training loss:\t\tnan\n",
      "Done 339 batches in 1.99 sec.    batch training loss:\t\tnan\n",
      "Done 340 batches in 1.99 sec.    batch training loss:\t\tnan\n",
      "Done 341 batches in 2.00 sec.    batch training loss:\t\tnan\n",
      "Done 342 batches in 2.00 sec.    batch training loss:\t\tnan\n",
      "Done 343 batches in 2.01 sec.    batch training loss:\t\tnan\n",
      "Done 344 batches in 2.01 sec.    batch training loss:\t\tnan\n",
      "Done 345 batches in 2.02 sec.    batch training loss:\t\tnan\n",
      "Done 346 batches in 2.02 sec.    batch training loss:\t\tnan\n",
      "Done 347 batches in 2.03 sec.    batch training loss:\t\tnan\n",
      "Done 348 batches in 2.04 sec.    batch training loss:\t\tnan\n",
      "Done 349 batches in 2.04 sec.    batch training loss:\t\tnan\n",
      "Done 350 batches in 2.05 sec.    batch training loss:\t\tnan\n",
      "Done 351 batches in 2.06 sec.    batch training loss:\t\tnan\n",
      "Done 352 batches in 2.06 sec.    batch training loss:\t\tnan\n",
      "Done 353 batches in 2.07 sec.    batch training loss:\t\tnan\n",
      "Done 354 batches in 2.08 sec.    batch training loss:\t\tnan\n",
      "Done 355 batches in 2.08 sec.    batch training loss:\t\tnan\n",
      "Done 356 batches in 2.09 sec.    batch training loss:\t\tnan\n",
      "Done 357 batches in 2.09 sec.    batch training loss:\t\tnan\n",
      "Done 358 batches in 2.10 sec.    batch training loss:\t\tnan\n",
      "Done 359 batches in 2.10 sec.    batch training loss:\t\tnan\n",
      "Done 360 batches in 2.11 sec.    batch training loss:\t\tnan\n",
      "Done 361 batches in 2.12 sec.    batch training loss:\t\tnan\n",
      "Done 362 batches in 2.12 sec.    batch training loss:\t\tnan\n",
      "Done 363 batches in 2.12 sec.    batch training loss:\t\tnan\n",
      "Done 364 batches in 2.13 sec.    batch training loss:\t\tnan\n",
      "Done 365 batches in 2.14 sec.    batch training loss:\t\tnan\n",
      "Done 366 batches in 2.14 sec.    batch training loss:\t\tnan\n",
      "Done 367 batches in 2.15 sec.    batch training loss:\t\tnan\n",
      "Done 368 batches in 2.15 sec.    batch training loss:\t\tnan\n",
      "Done 369 batches in 2.16 sec.    batch training loss:\t\tnan\n",
      "Done 370 batches in 2.16 sec.    batch training loss:\t\tnan\n",
      "Done 371 batches in 2.17 sec.    batch training loss:\t\tnan\n",
      "Done 372 batches in 2.18 sec.    batch training loss:\t\tnan\n",
      "Done 373 batches in 2.18 sec.    batch training loss:\t\tnan\n",
      "Done 374 batches in 2.19 sec.    batch training loss:\t\tnan\n",
      "Done 375 batches in 2.19 sec.    batch training loss:\t\tnan\n",
      "Done 376 batches in 2.20 sec.    batch training loss:\t\tnan\n",
      "Done 377 batches in 2.20 sec.    batch training loss:\t\tnan\n",
      "Done 378 batches in 2.21 sec.    batch training loss:\t\tnan\n",
      "Done 379 batches in 2.21 sec.    batch training loss:\t\tnan\n",
      "Done 380 batches in 2.22 sec.    batch training loss:\t\tnan\n",
      "Done 381 batches in 2.22 sec.    batch training loss:\t\tnan\n",
      "Done 382 batches in 2.23 sec.    batch training loss:\t\tnan\n",
      "Done 383 batches in 2.24 sec.    batch training loss:\t\tnan\n",
      "Done 384 batches in 2.25 sec.    batch training loss:\t\tnan\n",
      "Done 385 batches in 2.25 sec.    batch training loss:\t\tnan\n",
      "Done 386 batches in 2.26 sec.    batch training loss:\t\tnan\n",
      "Done 387 batches in 2.27 sec.    batch training loss:\t\tnan\n",
      "Done 388 batches in 2.28 sec.    batch training loss:\t\tnan\n",
      "Done 389 batches in 2.28 sec.    batch training loss:\t\tnan\n",
      "Done 390 batches in 2.28 sec.    batch training loss:\t\tnan\n",
      "Done 391 batches in 2.29 sec.    batch training loss:\t\tnan\n",
      "Done 392 batches in 2.30 sec.    batch training loss:\t\tnan\n",
      "Done 393 batches in 2.31 sec.    batch training loss:\t\tnan\n",
      "Done 394 batches in 2.32 sec.    batch training loss:\t\tnan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-d27eeafe02fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mbatch_training_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mbatch_err\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mtrain_err\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbatch_err\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mtraining_time\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbatch_training_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/i258346/.local/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/i258346/.local/lib/python2.7/site-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n)\u001b[0m\n\u001b[0;32m    910\u001b[0m             \u001b[1;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 912\u001b[1;33m                 \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    913\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m                     \u001b[0mcompute_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/i258346/.local/lib/python2.7/site-packages/theano/tensor/nnet/blocksparse.pyc\u001b[0m in \u001b[0;36mperform\u001b[1;34m(self, node, inp, out_)\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxIdx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myIdx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m                     o[xIdx[b, i], yIdx[b, j]] += numpy.outer(x[b, i],\n\u001b[0;32m    211\u001b[0m                                                              y[b, j, :])\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training, taken from mnist.py in lasagne examples\n",
    "\n",
    "num_epochs = 50\n",
    "mt_batch_size = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    time_wasted = 0\n",
    "    training_time = 0\n",
    "    \n",
    "    for batch in iterate_minibatches(mt_train, mt_batch_size):\n",
    "        \n",
    "        inputs, targets, mask, t = batch\n",
    "        \n",
    "        batch_training_time = time.time()\n",
    "        batch_err = train_fn(inputs, targets, mask)\n",
    "        train_err += batch_err\n",
    "        training_time += time.time() - batch_training_time\n",
    "        train_batches += 1\n",
    "        \n",
    "        time_wasted += t\n",
    "        if not train_batches % 1:\n",
    "            print(\"Done {} batches in {:.2f} sec.    batch training loss:\\t\\t{}\").format(\n",
    "                train_batches, time.time() - start_time, train_err / train_batches)\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    #val_err = 0\n",
    "    #val_acc = 0\n",
    "    #val_batches = 0\n",
    "    #for batch in iterate_minibatches(mt_val, mt_batch_size):\n",
    "    #    inputs, targets, mask = batch\n",
    "    #    err, acc = val_fn(inputs, targets, mask)\n",
    "    #    val_err += err\n",
    "    #    val_acc += acc\n",
    "    #    val_batches += 1\n",
    "\n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    #print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "    #print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "    #    val_acc / val_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 780 (CNMeM is enabled with initial size: 30.0% of memory, cuDNN Version is too old. Update to v5, was 3007.)\n",
      "/home/i258346/.local/lib/python2.7/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import lasagne as L\n",
    "\n",
    "sys.path.insert(0, '../HSoftmaxLayerLasagne/')\n",
    "\n",
    "import HSoftmaxLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mt_path = \"/pio/data/data/mtriples/\"\n",
    "\n",
    "beg_token = '<utt_beg>'\n",
    "end_token = '<utt_end>'\n",
    "\n",
    "def get_mt_voc(path=mt_path):\n",
    "    i_to_w, w_to_i = {}, {}\n",
    "    \n",
    "    i_to_w[0] = end_token   # separate tokens for beginning and ending of an utterance\n",
    "    w_to_i[end_token] = 0   # <utt_end> serves only as a target for the last word in the input sequence\n",
    "    i_to_w[1] = beg_token   # <utt_beg> will always be the first generated word\n",
    "    w_to_i[beg_token] = 1    \n",
    "    wc = 2\n",
    "    \n",
    "    with open(path + \"WordsList.txt\", \"r\") as wl:\n",
    "        for w in wl:\n",
    "            i_to_w[wc] = w[:-1]\n",
    "            w_to_i[w[:-1]] = wc\n",
    "            wc += 1\n",
    "    \n",
    "    return i_to_w, w_to_i, wc\n",
    "\n",
    "mt_i_to_w, mt_w_to_i, mt_voc_size = get_mt_voc()\n",
    "\n",
    "\n",
    "def load_mt(path=mt_path):\n",
    "    tr = None\n",
    "    vl = None\n",
    "    ts = None\n",
    "    \n",
    "    with open(path + \"Training_Shuffled_Dataset.txt\") as f:\n",
    "        tr = []\n",
    "        for l in f:\n",
    "            if len(l.split()) < 1000:\n",
    "                tr.insert(0, [1] + map(lambda w: mt_w_to_i.get(w, mt_w_to_i['<unk>']), l.split()) + [0])\n",
    "        \n",
    "    with open(path + \"Validation_Shuffled_Dataset.txt\") as f:\n",
    "        vl = []\n",
    "        for l in f:\n",
    "            if len(l.split()) < 1000:\n",
    "                vl.insert(0, [1] + map(lambda w: mt_w_to_i.get(w, mt_w_to_i['<unk>']), l.split()) + [0])\n",
    "            \n",
    "    with open(path + \"Test_Shuffled_Dataset.txt\") as f:\n",
    "        ts = []\n",
    "        for l in f:\n",
    "            if len(l.split()) < 1000:\n",
    "                ts.insert(0, [1] + map(lambda w: mt_w_to_i.get(w, mt_w_to_i['<unk>']), l.split()) + [0])\n",
    "    \n",
    "    return tr, vl, ts\n",
    "\n",
    "mt_train, mt_val, mt_test = load_mt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Similar to Lasagne mnist.py example, added input mask and different sequence lengths\n",
    "\n",
    "def iterate_minibatches(inputs, batchsize):\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        t0 = time.time() # time wasted preparing data, just for the info\n",
    "        \n",
    "        excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        inp = inputs[excerpt]\n",
    "        \n",
    "        inp_max_len = len(max(inp, key=len))\n",
    "        inp = map(lambda l: l + [-1]*(inp_max_len-len(l)), inp)\n",
    "        inp = np.asarray(inp, dtype=np.int32)\n",
    "        tar = np.hstack((inp[:,1:], np.asarray([-1]*batchsize, dtype=np.int32).reshape((-1,1))))\n",
    "        def gr_zero(x):\n",
    "            if x > 0:\n",
    "                return 1.\n",
    "            return 0.\n",
    "        v_gr_zero = np.vectorize(gr_zero, otypes=[np.float32])\n",
    "        mask = v_gr_zero(inp) # 0 in vocabulary represents <utt_end>, we don't feed that into the net\n",
    "        \n",
    "        yield inp, tar, mask, (time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_simple_rnnlm(input_var, mask_input_var, voc_size, emb_size, rec_size):\n",
    "    l_in = L.layers.InputLayer(shape=(None, None), input_var=input_var)    \n",
    "    batch_size, seq_len = l_in.input_var.shape\n",
    "    l_mask = L.layers.InputLayer(shape=(batch_size, seq_len), input_var=mask_input_var)\n",
    "    \n",
    "    l_emb = L.layers.EmbeddingLayer(l_in,\n",
    "                                    input_size=voc_size+1, \n",
    "                                    output_size=emb_size)\n",
    "    \n",
    "    l_rec = L.layers.RecurrentLayer(l_emb,\n",
    "                                    num_units=rec_size, \n",
    "                                    W_in_to_hid=L.init.Orthogonal(), \n",
    "                                    W_hid_to_hid=L.init.Orthogonal(),\n",
    "                                    mask_input=l_mask)\n",
    "    \n",
    "    l_resh = L.layers.ReshapeLayer(l_rec, shape=(-1, rec_size))\n",
    "    \n",
    "    l_soft = L.layers.DenseLayer(l_resh,\n",
    "                                num_units=voc_size,\n",
    "                                nonlinearity=L.nonlinearities.softmax)\n",
    "    \n",
    "    l_out = L.layers.ReshapeLayer(l_soft, shape=(batch_size, seq_len, voc_size))\n",
    "    \n",
    "    return l_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#voc_size = mt_voc_size\n",
    "#emb_size = 50\n",
    "#rec_size = 100\n",
    "#\n",
    "#input_var = T.imatrix('inputs')\n",
    "#targets = T.imatrix('targets') # these will be inputs shifted by 1\n",
    "#mask_input_var = T.matrix('input_mask')\n",
    "#\n",
    "#net = build_simple_rnnlm(input_var, mask_input_var, voc_size, emb_size, rec_size)\n",
    "#out = L.layers.get_output(net)\n",
    "#\n",
    "#loss = L.objectives.categorical_crossentropy(out.reshape((-1,voc_size)), targets.ravel())\n",
    "#loss = loss.mean() # mean batch loss\n",
    "#\n",
    "#params = L.layers.get_all_params(net, trainable=True)\n",
    "#updates = L.updates.rmsprop(loss, params)\n",
    "#\n",
    "#train_fn = theano.function([input_var, targets, mask_input_var], loss, updates=updates)\n",
    "#\n",
    "#### for validation\n",
    "#\n",
    "#test_out = L.layers.get_output(net, deterministic=True)\n",
    "#test_loss = L.objectives.categorical_crossentropy(test_out.reshape((-1,voc_size)), targets.ravel())\n",
    "#test_loss = test_loss.mean()\n",
    "#test_acc = T.mean(T.eq(T.argmax(test_out, axis=1), targets), dtype=theano.config.floatX)\n",
    "#\n",
    "#val_fn = theano.function([input_var, targets, mask_input_var], [test_loss, test_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_hsoft_rnnlm(input_var, target_var, mask_input_var, voc_size, emb_size, rec_size):\n",
    "    l_in = L.layers.InputLayer(shape=(None, None), input_var=input_var)    \n",
    "    batch_size, seq_len = l_in.input_var.shape\n",
    "    l_mask = None\n",
    "    if mask_input_var != None:\n",
    "        print 'setting up input mask...'\n",
    "        l_mask = L.layers.InputLayer(shape=(batch_size, seq_len), input_var=mask_input_var)\n",
    "    \n",
    "    l_emb = L.layers.EmbeddingLayer(l_in,\n",
    "                                    input_size=voc_size+1, \n",
    "                                    output_size=emb_size)\n",
    "    \n",
    "#     l_rec = L.layers.RecurrentLayer(l_emb,\n",
    "#                                    num_units=rec_size,\n",
    "#                                    nonlinearity=L.nonlinearities.tanh,\n",
    "#                                    grad_clipping=100,\n",
    "#                                    mask_input=l_mask)\n",
    "    l_lstm1 = L.layers.LSTMLayer(l_emb,\n",
    "                                 num_units=rec_size,\n",
    "                                 nonlinearity=L.nonlinearities.tanh,\n",
    "                                 grad_clipping=100,\n",
    "                                 mask_input=l_mask)\n",
    "    \n",
    "#     l_lstm2 = L.layers.LSTMLayer(l_lstm1,\n",
    "#                                  num_units=rec_size,\n",
    "#                                  nonlinearity=L.nonlinearities.tanh,\n",
    "#                                  grad_clipping=100,\n",
    "#                                  mask_input=l_mask)\n",
    "    \n",
    "    \n",
    "    l_resh = L.layers.ReshapeLayer(l_lstm1, shape=(-1, rec_size))\n",
    "    \n",
    "    # hierarchical softmax\n",
    "    \n",
    "    l_resh_tar = None\n",
    "    if target_var != None:\n",
    "        print 'setting up targets for hsoftmax...'\n",
    "        l_tar = L.layers.InputLayer(shape=(None, None), input_var=target_var)\n",
    "        l_resh_tar = L.layers.ReshapeLayer(l_tar, shape=(-1, 1))\n",
    "        \n",
    "    l_hsoft = HSoftmaxLayer.HierarchicalSoftmaxDenseLayer(l_resh,\n",
    "                                                          num_units=voc_size,\n",
    "                                                          target=l_resh_tar)\n",
    "    l_out = None\n",
    "    if target_var != None:\n",
    "        l_out = L.layers.ReshapeLayer(l_hsoft, shape=(batch_size, seq_len))\n",
    "    else:\n",
    "        l_out = L.layers.ReshapeLayer(l_hsoft, shape=(batch_size, seq_len, voc_size))\n",
    "    \n",
    "    return l_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up input mask...\n",
      "setting up targets for hsoftmax...\n"
     ]
    }
   ],
   "source": [
    "voc_size = mt_voc_size\n",
    "emb_size = 100\n",
    "rec_size = 300\n",
    "\n",
    "input_var = T.imatrix('inputs')\n",
    "targets = T.imatrix('targets') # these will be inputs shifted by 1\n",
    "mask_input_var = T.matrix('input_mask')\n",
    "\n",
    "net = build_hsoft_rnnlm(input_var, targets, mask_input_var, voc_size, emb_size, rec_size)\n",
    "out = L.layers.get_output(net)\n",
    "\n",
    "mask_idx = mask_input_var.nonzero()\n",
    "loss = -T.sum(T.log(out[mask_idx])) / T.sum(mask_input_var)\n",
    "\n",
    "params = L.layers.get_all_params(net, trainable=True)\n",
    "#updates = L.updates.rmsprop(loss, params, learning_rate=.001, rho=.9, epsilon=1e-06)\n",
    "updates = L.updates.adagrad(loss, params, learning_rate=.01)\n",
    "\n",
    "train_fn = theano.function([input_var, targets, mask_input_var], [loss,out], updates=updates)\n",
    "\n",
    "#### for validation\n",
    "\n",
    "test_out = L.layers.get_output(net, deterministic=True)\n",
    "test_loss = -T.sum(T.log(test_out[mask_idx])) / T.sum(mask_input_var)\n",
    "\n",
    "#test_acc = T.mean(T.eq(T.argmax(test_out, axis=1), targets), dtype=theano.config.floatX)\n",
    "\n",
    "val_fn = theano.function([input_var, targets, mask_input_var], test_loss)\n",
    "\n",
    "\n",
    "### dump weights\n",
    "\n",
    "# np.savez('model.npz', *L.layers.get_all_param_values(net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with np.load('model_trained.npz') as f:\n",
    "#     param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "#     L.layers.set_all_param_values(net, param_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 10 batches in 1.78 sec.    training loss:\t\t7.90764741898\n",
      "Done 20 batches in 3.45 sec.    training loss:\t\t6.96611065865\n",
      "Done 30 batches in 5.13 sec.    training loss:\t\t6.55265278816\n",
      "Done 40 batches in 7.09 sec.    training loss:\t\t6.34841872454\n",
      "Done 50 batches in 9.21 sec.    training loss:\t\t6.21758934975\n",
      "Done 60 batches in 11.08 sec.    training loss:\t\t6.11313880285\n",
      "Done 70 batches in 12.92 sec.    training loss:\t\t6.03942677634\n",
      "Done 80 batches in 15.09 sec.    training loss:\t\t5.98046128154\n",
      "Done 90 batches in 16.68 sec.    training loss:\t\t5.93065799077\n",
      "Done 100 batches in 18.40 sec.    training loss:\t\t5.88557061195\n",
      "Done 110 batches in 19.99 sec.    training loss:\t\t5.84106046937\n",
      "Done 120 batches in 21.82 sec.    training loss:\t\t5.79968442122\n",
      "Done 130 batches in 23.87 sec.    training loss:\t\t5.76270784598\n",
      "Done 140 batches in 25.38 sec.    training loss:\t\t5.72737972736\n",
      "Done 150 batches in 27.20 sec.    training loss:\t\t5.68853348414\n",
      "Done 160 batches in 28.78 sec.    training loss:\t\t5.65396175683\n",
      "Done 170 batches in 30.62 sec.    training loss:\t\t5.62026920038\n",
      "Done 180 batches in 32.59 sec.    training loss:\t\t5.58800759845\n",
      "Done 190 batches in 34.28 sec.    training loss:\t\t5.55942781599\n",
      "Done 200 batches in 35.65 sec.    training loss:\t\t5.52842153549\n",
      "Done 210 batches in 37.55 sec.    training loss:\t\t5.5046331224\n",
      "Done 220 batches in 39.23 sec.    training loss:\t\t5.48089431416\n",
      "Done 230 batches in 40.87 sec.    training loss:\t\t5.45514851031\n",
      "Done 240 batches in 43.02 sec.    training loss:\t\t5.43449742993\n",
      "Done 250 batches in 44.50 sec.    training loss:\t\t5.41168670273\n",
      "Done 260 batches in 46.26 sec.    training loss:\t\t5.39202976777\n",
      "Done 270 batches in 47.89 sec.    training loss:\t\t5.37123238246\n",
      "Done 280 batches in 49.56 sec.    training loss:\t\t5.35213028022\n",
      "Done 290 batches in 51.31 sec.    training loss:\t\t5.3327858037\n",
      "Done 300 batches in 53.24 sec.    training loss:\t\t5.31450595697\n",
      "Done 310 batches in 55.32 sec.    training loss:\t\t5.29669447099\n",
      "Done 320 batches in 57.18 sec.    training loss:\t\t5.28107005209\n",
      "Done 330 batches in 58.81 sec.    training loss:\t\t5.26570999117\n",
      "Done 340 batches in 60.47 sec.    training loss:\t\t5.25055037527\n",
      "Done 350 batches in 62.05 sec.    training loss:\t\t5.23444122451\n",
      "Done 360 batches in 63.57 sec.    training loss:\t\t5.21985667414\n",
      "Done 370 batches in 65.06 sec.    training loss:\t\t5.20590383942\n",
      "Done 380 batches in 66.44 sec.    training loss:\t\t5.19223564675\n",
      "Done 390 batches in 67.96 sec.    training loss:\t\t5.1795593115\n",
      "Done 400 batches in 69.71 sec.    training loss:\t\t5.16829143286\n",
      "Done 410 batches in 71.67 sec.    training loss:\t\t5.15454879505\n",
      "Done 420 batches in 73.41 sec.    training loss:\t\t5.14335650944\n",
      "Done 430 batches in 75.35 sec.    training loss:\t\t5.1321143239\n",
      "Done 440 batches in 76.85 sec.    training loss:\t\t5.12045113824\n",
      "Done 450 batches in 78.34 sec.    training loss:\t\t5.10948785358\n",
      "Done 460 batches in 79.72 sec.    training loss:\t\t5.09980930349\n",
      "Done 470 batches in 81.04 sec.    training loss:\t\t5.08908089273\n",
      "Done 480 batches in 82.47 sec.    training loss:\t\t5.0787433217\n",
      "Done 490 batches in 84.49 sec.    training loss:\t\t5.06978796648\n",
      "Done 500 batches in 86.22 sec.    training loss:\t\t5.06090192604\n",
      "Done 510 batches in 87.78 sec.    training loss:\t\t5.05185416353\n",
      "Done 520 batches in 89.29 sec.    training loss:\t\t5.04352927025\n",
      "Done 530 batches in 90.89 sec.    training loss:\t\t5.03521287846\n",
      "Done 540 batches in 92.60 sec.    training loss:\t\t5.02710495437\n",
      "Done 550 batches in 94.21 sec.    training loss:\t\t5.0185568194\n",
      "Done 560 batches in 95.98 sec.    training loss:\t\t5.01178762061\n",
      "Done 570 batches in 97.70 sec.    training loss:\t\t5.00338459015\n",
      "Done 580 batches in 100.28 sec.    training loss:\t\t4.99607622459\n",
      "Done 590 batches in 102.17 sec.    training loss:\t\t4.9881875709\n",
      "Done 600 batches in 103.88 sec.    training loss:\t\t4.9813191247\n",
      "Done 610 batches in 105.48 sec.    training loss:\t\t4.9736052005\n",
      "Done 620 batches in 107.31 sec.    training loss:\t\t4.96645744924\n",
      "Done 630 batches in 108.88 sec.    training loss:\t\t4.96022988501\n",
      "Done 640 batches in 110.58 sec.    training loss:\t\t4.95360182673\n",
      "Done 650 batches in 112.51 sec.    training loss:\t\t4.94707300626\n",
      "Done 660 batches in 114.08 sec.    training loss:\t\t4.94044052543\n",
      "Done 670 batches in 115.78 sec.    training loss:\t\t4.93426545556\n",
      "Done 680 batches in 117.59 sec.    training loss:\t\t4.92718556699\n",
      "Done 690 batches in 119.14 sec.    training loss:\t\t4.92127161855\n",
      "Done 700 batches in 120.92 sec.    training loss:\t\t4.91645152705\n",
      "Done 710 batches in 122.70 sec.    training loss:\t\t4.91115898616\n",
      "Done 720 batches in 124.52 sec.    training loss:\t\t4.90547492504\n",
      "Done 730 batches in 126.20 sec.    training loss:\t\t4.89958784678\n",
      "Done 740 batches in 127.85 sec.    training loss:\t\t4.89359022411\n",
      "Done 750 batches in 129.43 sec.    training loss:\t\t4.88745376523\n",
      "Done 760 batches in 130.98 sec.    training loss:\t\t4.88222937019\n",
      "Done 770 batches in 132.32 sec.    training loss:\t\t4.87681026521\n",
      "Done 780 batches in 133.91 sec.    training loss:\t\t4.87131887399\n",
      "Done 790 batches in 135.50 sec.    training loss:\t\t4.86592817005\n",
      "Done 800 batches in 137.23 sec.    training loss:\t\t4.86173652112\n",
      "Done 810 batches in 139.01 sec.    training loss:\t\t4.85755919292\n",
      "Done 820 batches in 140.80 sec.    training loss:\t\t4.85255745911\n",
      "Done 830 batches in 142.66 sec.    training loss:\t\t4.84759542511\n",
      "Done 840 batches in 144.18 sec.    training loss:\t\t4.84262959503\n",
      "Done 850 batches in 146.14 sec.    training loss:\t\t4.83861030298\n",
      "Done 860 batches in 147.94 sec.    training loss:\t\t4.83408840091\n",
      "Done 870 batches in 149.56 sec.    training loss:\t\t4.82905931308\n",
      "Done 880 batches in 151.23 sec.    training loss:\t\t4.82479988607\n",
      "Done 890 batches in 153.20 sec.    training loss:\t\t4.82059478813\n",
      "Done 900 batches in 154.99 sec.    training loss:\t\t4.81686203851\n",
      "Done 910 batches in 156.90 sec.    training loss:\t\t4.8132717688\n",
      "Done 920 batches in 158.49 sec.    training loss:\t\t4.80893545825\n",
      "Done 930 batches in 160.36 sec.    training loss:\t\t4.80463246992\n",
      "Done 940 batches in 162.00 sec.    training loss:\t\t4.80109360776\n",
      "Done 950 batches in 164.08 sec.    training loss:\t\t4.79741139914\n",
      "Done 960 batches in 165.55 sec.    training loss:\t\t4.79313088606\n",
      "Done 970 batches in 167.28 sec.    training loss:\t\t4.78904742556\n",
      "Done 980 batches in 168.83 sec.    training loss:\t\t4.78540168976\n",
      "Done 990 batches in 170.51 sec.    training loss:\t\t4.78159654887\n",
      "Done 1000 batches in 172.23 sec.    training loss:\t\t4.77753081942\n",
      "Done 1010 batches in 173.85 sec.    training loss:\t\t4.77363289418\n",
      "Done 1020 batches in 176.05 sec.    training loss:\t\t4.77027155558\n",
      "Done 1030 batches in 177.58 sec.    training loss:\t\t4.76672336939\n",
      "Done 1040 batches in 179.49 sec.    training loss:\t\t4.76385781719\n",
      "Done 1050 batches in 181.08 sec.    training loss:\t\t4.76019902638\n",
      "Done 1060 batches in 182.62 sec.    training loss:\t\t4.75689378235\n",
      "Done 1070 batches in 184.72 sec.    training loss:\t\t4.75325944802\n",
      "Done 1080 batches in 186.28 sec.    training loss:\t\t4.74983208886\n",
      "Done 1090 batches in 188.02 sec.    training loss:\t\t4.74665280045\n",
      "Done 1100 batches in 189.52 sec.    training loss:\t\t4.74339488853\n",
      "Done 1110 batches in 191.30 sec.    training loss:\t\t4.7403705627\n",
      "Done 1120 batches in 193.09 sec.    training loss:\t\t4.73720080001\n",
      "Done 1130 batches in 194.76 sec.    training loss:\t\t4.73430167848\n",
      "Done 1140 batches in 196.29 sec.    training loss:\t\t4.73102320286\n",
      "Done 1150 batches in 197.88 sec.    training loss:\t\t4.72838643655\n",
      "Done 1160 batches in 199.59 sec.    training loss:\t\t4.72541058639\n",
      "Done 1170 batches in 201.57 sec.    training loss:\t\t4.72238021141\n",
      "Done 1180 batches in 203.29 sec.    training loss:\t\t4.71940891258\n",
      "Done 1190 batches in 205.35 sec.    training loss:\t\t4.71650834645\n",
      "Done 1200 batches in 206.75 sec.    training loss:\t\t4.713423388\n",
      "Done 1210 batches in 208.35 sec.    training loss:\t\t4.7105174837\n",
      "Done 1220 batches in 209.86 sec.    training loss:\t\t4.70761036521\n",
      "Done 1230 batches in 212.10 sec.    training loss:\t\t4.70547190372\n",
      "Done 1240 batches in 213.87 sec.    training loss:\t\t4.70294841528\n",
      "Done 1250 batches in 215.80 sec.    training loss:\t\t4.70002358208\n",
      "Done 1260 batches in 217.55 sec.    training loss:\t\t4.69745378116\n",
      "Done 1270 batches in 219.31 sec.    training loss:\t\t4.6950973079\n",
      "Done 1280 batches in 220.99 sec.    training loss:\t\t4.69245579727\n",
      "Done 1290 batches in 222.81 sec.    training loss:\t\t4.68968035935\n",
      "Done 1300 batches in 224.27 sec.    training loss:\t\t4.68711044495\n",
      "Done 1310 batches in 225.84 sec.    training loss:\t\t4.68449344125\n",
      "Done 1320 batches in 228.10 sec.    training loss:\t\t4.68198451273\n",
      "Done 1330 batches in 229.63 sec.    training loss:\t\t4.67972919529\n",
      "Done 1340 batches in 231.53 sec.    training loss:\t\t4.67706997359\n",
      "Done 1350 batches in 233.35 sec.    training loss:\t\t4.67466073637\n",
      "Done 1360 batches in 235.34 sec.    training loss:\t\t4.67256798639\n",
      "Done 1370 batches in 237.41 sec.    training loss:\t\t4.67065476118\n",
      "Done 1380 batches in 239.70 sec.    training loss:\t\t4.66844490369\n",
      "Done 1390 batches in 241.51 sec.    training loss:\t\t4.66586509746\n",
      "Done 1400 batches in 243.14 sec.    training loss:\t\t4.6633475491\n",
      "Done 1410 batches in 245.03 sec.    training loss:\t\t4.66102911597\n",
      "Done 1420 batches in 246.67 sec.    training loss:\t\t4.65893474565\n",
      "Done 1430 batches in 248.21 sec.    training loss:\t\t4.65648808713\n",
      "Done 1440 batches in 249.76 sec.    training loss:\t\t4.65427737898\n",
      "Done 1450 batches in 251.52 sec.    training loss:\t\t4.6522466038\n",
      "Done 1460 batches in 253.14 sec.    training loss:\t\t4.65005894916\n",
      "Done 1470 batches in 254.94 sec.    training loss:\t\t4.64785944588\n",
      "Done 1480 batches in 256.76 sec.    training loss:\t\t4.64566030083\n",
      "Done 1490 batches in 258.36 sec.    training loss:\t\t4.64393272752\n",
      "Done 1500 batches in 259.88 sec.    training loss:\t\t4.64213688564\n",
      "Done 1510 batches in 261.95 sec.    training loss:\t\t4.6402116706\n",
      "Done 1520 batches in 263.52 sec.    training loss:\t\t4.63841629311\n",
      "Done 1530 batches in 265.15 sec.    training loss:\t\t4.63629401525\n",
      "Done 1540 batches in 266.79 sec.    training loss:\t\t4.63403445776\n",
      "Done 1550 batches in 268.42 sec.    training loss:\t\t4.63200235428\n",
      "Done 1560 batches in 270.48 sec.    training loss:\t\t4.63003088694\n",
      "Done 1570 batches in 272.56 sec.    training loss:\t\t4.62787490468\n",
      "Done 1580 batches in 274.30 sec.    training loss:\t\t4.6258564937\n",
      "Done 1590 batches in 276.17 sec.    training loss:\t\t4.62403158302\n",
      "Done 1600 batches in 277.95 sec.    training loss:\t\t4.6219620645\n",
      "Done 1610 batches in 279.83 sec.    training loss:\t\t4.620112027\n",
      "Done 1620 batches in 281.60 sec.    training loss:\t\t4.61839517546\n",
      "Done 1630 batches in 283.23 sec.    training loss:\t\t4.61623065486\n",
      "Done 1640 batches in 284.80 sec.    training loss:\t\t4.61412507935\n",
      "Done 1650 batches in 286.51 sec.    training loss:\t\t4.61240440484\n",
      "Done 1660 batches in 288.33 sec.    training loss:\t\t4.61052913149\n",
      "Done 1670 batches in 290.06 sec.    training loss:\t\t4.60873681086\n",
      "Done 1680 batches in 292.43 sec.    training loss:\t\t4.60696987737\n",
      "Done 1690 batches in 293.93 sec.    training loss:\t\t4.60493511138\n",
      "Done 1700 batches in 295.97 sec.    training loss:\t\t4.60285130024\n",
      "Done 1710 batches in 297.66 sec.    training loss:\t\t4.60112668506\n",
      "Done 1720 batches in 299.62 sec.    training loss:\t\t4.59920885646\n",
      "Done 1730 batches in 301.22 sec.    training loss:\t\t4.59741076486\n",
      "Done 1740 batches in 302.69 sec.    training loss:\t\t4.59562728734\n",
      "Done 1750 batches in 304.35 sec.    training loss:\t\t4.59385137912\n",
      "Done 1760 batches in 306.14 sec.    training loss:\t\t4.59216654707\n",
      "Done 1770 batches in 307.58 sec.    training loss:\t\t4.590118395\n",
      "Done 1780 batches in 309.03 sec.    training loss:\t\t4.58806709547\n",
      "Done 1790 batches in 310.59 sec.    training loss:\t\t4.58622755525\n",
      "Done 1800 batches in 312.28 sec.    training loss:\t\t4.58453405433\n",
      "Done 1810 batches in 314.13 sec.    training loss:\t\t4.5827377077\n",
      "Done 1820 batches in 315.83 sec.    training loss:\t\t4.58086134575\n",
      "Done 1830 batches in 317.39 sec.    training loss:\t\t4.5790836944\n",
      "Done 1840 batches in 319.55 sec.    training loss:\t\t4.57748844416\n",
      "Done 1850 batches in 320.94 sec.    training loss:\t\t4.57578702205\n",
      "Done 1860 batches in 322.60 sec.    training loss:\t\t4.57417707392\n",
      "Done 1870 batches in 324.32 sec.    training loss:\t\t4.57254284639\n",
      "Done 1880 batches in 325.88 sec.    training loss:\t\t4.57093455512\n",
      "Done 1890 batches in 327.85 sec.    training loss:\t\t4.56909678348\n",
      "Done 1900 batches in 329.65 sec.    training loss:\t\t4.56747198381\n",
      "Done 1910 batches in 331.68 sec.    training loss:\t\t4.56603729238\n",
      "Done 1920 batches in 333.78 sec.    training loss:\t\t4.56460908577\n",
      "Done 1930 batches in 335.49 sec.    training loss:\t\t4.56306570083\n",
      "Done 1940 batches in 337.29 sec.    training loss:\t\t4.56148603233\n",
      "Done 1950 batches in 338.57 sec.    training loss:\t\t4.5599874269\n",
      "Done 1960 batches in 340.57 sec.    training loss:\t\t4.55857135593\n",
      "Done 1970 batches in 342.26 sec.    training loss:\t\t4.55705243827\n",
      "Done 1980 batches in 344.23 sec.    training loss:\t\t4.5554509529\n",
      "Done 1990 batches in 345.78 sec.    training loss:\t\t4.5537586967\n",
      "Done 2000 batches in 347.40 sec.    training loss:\t\t4.55223970699\n",
      "Done 2010 batches in 349.10 sec.    training loss:\t\t4.55055391872\n",
      "Done 2020 batches in 351.09 sec.    training loss:\t\t4.54928346176\n",
      "Done 2030 batches in 353.10 sec.    training loss:\t\t4.54798156334\n",
      "Done 2040 batches in 354.80 sec.    training loss:\t\t4.54675505044\n",
      "Done 2050 batches in 357.21 sec.    training loss:\t\t4.54551286604\n",
      "Done 2060 batches in 358.77 sec.    training loss:\t\t4.54385528264\n",
      "Done 2070 batches in 360.17 sec.    training loss:\t\t4.54227088108\n",
      "Done 2080 batches in 362.03 sec.    training loss:\t\t4.54095803912\n",
      "Done 2090 batches in 364.02 sec.    training loss:\t\t4.53963607304\n",
      "Done 2100 batches in 365.90 sec.    training loss:\t\t4.53825287433\n",
      "Done 2110 batches in 367.85 sec.    training loss:\t\t4.53711076642\n",
      "Done 2120 batches in 369.67 sec.    training loss:\t\t4.53570893778\n",
      "Done 2130 batches in 371.25 sec.    training loss:\t\t4.53455032653\n",
      "Done 2140 batches in 372.98 sec.    training loss:\t\t4.5333980714\n",
      "Done 2150 batches in 374.62 sec.    training loss:\t\t4.53195123961\n",
      "Done 2160 batches in 376.25 sec.    training loss:\t\t4.53053592929\n",
      "Done 2170 batches in 377.87 sec.    training loss:\t\t4.52929955298\n",
      "Done 2180 batches in 379.52 sec.    training loss:\t\t4.52791692533\n",
      "Done 2190 batches in 381.74 sec.    training loss:\t\t4.5267474893\n",
      "Done 2200 batches in 383.52 sec.    training loss:\t\t4.52543301344\n",
      "Done 2210 batches in 385.49 sec.    training loss:\t\t4.52428683738\n",
      "Done 2220 batches in 387.25 sec.    training loss:\t\t4.52303333132\n",
      "Done 2230 batches in 388.72 sec.    training loss:\t\t4.52178299994\n",
      "Done 2240 batches in 390.48 sec.    training loss:\t\t4.52054178693\n",
      "Done 2250 batches in 392.10 sec.    training loss:\t\t4.51918983883\n",
      "Done 2260 batches in 393.69 sec.    training loss:\t\t4.51784766311\n",
      "Done 2270 batches in 395.24 sec.    training loss:\t\t4.51644898847\n",
      "Done 2280 batches in 396.88 sec.    training loss:\t\t4.51541763126\n",
      "Done 2290 batches in 398.56 sec.    training loss:\t\t4.51431902715\n",
      "Done 2300 batches in 400.13 sec.    training loss:\t\t4.51285168979\n",
      "Done 2310 batches in 401.96 sec.    training loss:\t\t4.5116196975\n",
      "Done 2320 batches in 403.31 sec.    training loss:\t\t4.51051573548\n",
      "Done 2330 batches in 405.39 sec.    training loss:\t\t4.50910900785\n",
      "Done 2340 batches in 406.86 sec.    training loss:\t\t4.50782990731\n",
      "Done 2350 batches in 409.20 sec.    training loss:\t\t4.50692025215\n",
      "Done 2360 batches in 410.86 sec.    training loss:\t\t4.50578988154\n",
      "Done 2370 batches in 412.40 sec.    training loss:\t\t4.50468748843\n",
      "Done 2380 batches in 413.83 sec.    training loss:\t\t4.50347740159\n",
      "Done 2390 batches in 415.84 sec.    training loss:\t\t4.50224040652\n",
      "Done 2400 batches in 417.62 sec.    training loss:\t\t4.50102907389\n",
      "Done 2410 batches in 419.23 sec.    training loss:\t\t4.49985118219\n",
      "Done 2420 batches in 420.95 sec.    training loss:\t\t4.49884928977\n",
      "Done 2430 batches in 423.05 sec.    training loss:\t\t4.49753436346\n",
      "Done 2440 batches in 424.85 sec.    training loss:\t\t4.49643149141\n",
      "Done 2450 batches in 426.30 sec.    training loss:\t\t4.49535526412\n",
      "Done 2460 batches in 428.10 sec.    training loss:\t\t4.49413442088\n",
      "Done 2470 batches in 430.03 sec.    training loss:\t\t4.49284921368\n",
      "Done 2480 batches in 431.69 sec.    training loss:\t\t4.4917689285\n",
      "Done 2490 batches in 433.76 sec.    training loss:\t\t4.49087231667\n",
      "Done 2500 batches in 435.43 sec.    training loss:\t\t4.48964392319\n",
      "Done 2510 batches in 437.02 sec.    training loss:\t\t4.48842885181\n",
      "Done 2520 batches in 438.53 sec.    training loss:\t\t4.48732459734\n",
      "Done 2530 batches in 440.06 sec.    training loss:\t\t4.48598549526\n",
      "Done 2540 batches in 441.95 sec.    training loss:\t\t4.48505263873\n",
      "Done 2550 batches in 443.53 sec.    training loss:\t\t4.48407505465\n",
      "Done 2560 batches in 445.66 sec.    training loss:\t\t4.48295452502\n",
      "Done 2570 batches in 447.54 sec.    training loss:\t\t4.48179146065\n",
      "Done 2580 batches in 449.39 sec.    training loss:\t\t4.48047587058\n",
      "Done 2590 batches in 451.02 sec.    training loss:\t\t4.47927236244\n",
      "Done 2600 batches in 452.40 sec.    training loss:\t\t4.47823694761\n",
      "Done 2610 batches in 454.21 sec.    training loss:\t\t4.47711555483\n",
      "Done 2620 batches in 456.42 sec.    training loss:\t\t4.47614558316\n",
      "Done 2630 batches in 458.14 sec.    training loss:\t\t4.47513438459\n",
      "Done 2640 batches in 459.60 sec.    training loss:\t\t4.47390574533\n",
      "Done 2650 batches in 461.80 sec.    training loss:\t\t4.47288929426\n",
      "Done 2660 batches in 463.88 sec.    training loss:\t\t4.47187715697\n",
      "Done 2670 batches in 465.36 sec.    training loss:\t\t4.47095730715\n",
      "Done 2680 batches in 467.09 sec.    training loss:\t\t4.46987109389\n",
      "Done 2690 batches in 468.82 sec.    training loss:\t\t4.46879818581\n",
      "Done 2700 batches in 470.48 sec.    training loss:\t\t4.46781548262\n",
      "Done 2710 batches in 471.97 sec.    training loss:\t\t4.46666351218\n",
      "Done 2720 batches in 474.13 sec.    training loss:\t\t4.46580503995\n",
      "Done 2730 batches in 476.22 sec.    training loss:\t\t4.46487452652\n",
      "Done 2740 batches in 478.12 sec.    training loss:\t\t4.46395158498\n",
      "Done 2750 batches in 479.97 sec.    training loss:\t\t4.46314894841\n",
      "Done 2760 batches in 481.52 sec.    training loss:\t\t4.46205095856\n",
      "Done 2770 batches in 483.54 sec.    training loss:\t\t4.46115654238\n",
      "Done 2780 batches in 485.00 sec.    training loss:\t\t4.46030805771\n",
      "Done 2790 batches in 486.95 sec.    training loss:\t\t4.45918220884\n",
      "Done 2800 batches in 489.02 sec.    training loss:\t\t4.45825226554\n",
      "Done 2810 batches in 490.88 sec.    training loss:\t\t4.45737578903\n",
      "Done 2820 batches in 492.49 sec.    training loss:\t\t4.45639153905\n",
      "Done 2830 batches in 493.95 sec.    training loss:\t\t4.45551782539\n",
      "Done 2840 batches in 495.82 sec.    training loss:\t\t4.45477149596\n",
      "Done 2850 batches in 497.33 sec.    training loss:\t\t4.45370882862\n",
      "Done 2860 batches in 498.77 sec.    training loss:\t\t4.45267136355\n",
      "Done 2870 batches in 500.39 sec.    training loss:\t\t4.45172032321\n",
      "Done 2880 batches in 501.91 sec.    training loss:\t\t4.45074219662\n",
      "Done 2890 batches in 503.50 sec.    training loss:\t\t4.44977380586\n",
      "Done 2900 batches in 505.07 sec.    training loss:\t\t4.44890484456\n",
      "Done 2910 batches in 506.94 sec.    training loss:\t\t4.44778000702\n",
      "Done 2920 batches in 508.87 sec.    training loss:\t\t4.44689880748\n",
      "Done 2930 batches in 510.18 sec.    training loss:\t\t4.44581264693\n",
      "Done 2940 batches in 511.95 sec.    training loss:\t\t4.44487085464\n",
      "Done 2950 batches in 513.74 sec.    training loss:\t\t4.44389943487\n",
      "Done 2960 batches in 515.21 sec.    training loss:\t\t4.44294335029\n",
      "Done 2970 batches in 516.93 sec.    training loss:\t\t4.44192484966\n",
      "Done 2980 batches in 518.45 sec.    training loss:\t\t4.44104359254\n",
      "Done 2990 batches in 519.92 sec.    training loss:\t\t4.44021411428\n",
      "Done 3000 batches in 521.92 sec.    training loss:\t\t4.43930153871\n",
      "Done 3010 batches in 523.92 sec.    training loss:\t\t4.43854298924\n",
      "Done 3020 batches in 525.34 sec.    training loss:\t\t4.43748929658\n",
      "Done 3030 batches in 526.90 sec.    training loss:\t\t4.43672173\n",
      "Done 3040 batches in 529.12 sec.    training loss:\t\t4.43610974801\n",
      "Done 3050 batches in 530.92 sec.    training loss:\t\t4.43524806836\n",
      "Done 3060 batches in 532.63 sec.    training loss:\t\t4.43433180183\n",
      "Done 3070 batches in 534.54 sec.    training loss:\t\t4.4334555907\n",
      "Done 3080 batches in 536.03 sec.    training loss:\t\t4.43252716204\n",
      "Done 3090 batches in 537.56 sec.    training loss:\t\t4.4317339053\n",
      "Done 3100 batches in 539.21 sec.    training loss:\t\t4.43102680145\n",
      "Done 3110 batches in 540.88 sec.    training loss:\t\t4.43003917553\n",
      "Done 3120 batches in 543.27 sec.    training loss:\t\t4.42929616953\n",
      "Done 3130 batches in 545.41 sec.    training loss:\t\t4.42848358162\n",
      "Done 3140 batches in 547.45 sec.    training loss:\t\t4.42769977849\n",
      "Done 3150 batches in 549.22 sec.    training loss:\t\t4.42683271628\n",
      "Done 3160 batches in 551.11 sec.    training loss:\t\t4.42598865643\n",
      "Done 3170 batches in 552.52 sec.    training loss:\t\t4.42527204274\n",
      "Done 3180 batches in 554.18 sec.    training loss:\t\t4.42455569941\n",
      "Done 3190 batches in 556.41 sec.    training loss:\t\t4.42379254854\n",
      "Done 3200 batches in 558.12 sec.    training loss:\t\t4.42270164073\n",
      "Done 3210 batches in 559.85 sec.    training loss:\t\t4.42186438807\n",
      "Done 3220 batches in 561.46 sec.    training loss:\t\t4.42126751657\n",
      "Done 3230 batches in 563.18 sec.    training loss:\t\t4.42027508799\n",
      "Done 3240 batches in 565.34 sec.    training loss:\t\t4.41950320307\n",
      "Done 3250 batches in 567.17 sec.    training loss:\t\t4.41877807889\n",
      "Done 3260 batches in 568.94 sec.    training loss:\t\t4.41812962162\n",
      "Done 3270 batches in 571.14 sec.    training loss:\t\t4.41742901736\n",
      "Done 3280 batches in 572.92 sec.    training loss:\t\t4.41654269514\n",
      "Done 3290 batches in 574.88 sec.    training loss:\t\t4.41576835283\n",
      "Done 3300 batches in 576.90 sec.    training loss:\t\t4.41494392012\n",
      "Done 3310 batches in 578.74 sec.    training loss:\t\t4.41418449691\n",
      "Done 3320 batches in 580.27 sec.    training loss:\t\t4.41342402287\n",
      "Done 3330 batches in 581.92 sec.    training loss:\t\t4.41277168098\n",
      "Done 3340 batches in 583.39 sec.    training loss:\t\t4.41179351885\n",
      "Done 3350 batches in 585.00 sec.    training loss:\t\t4.41112555497\n",
      "Done 3360 batches in 586.83 sec.    training loss:\t\t4.41044114211\n",
      "Done 3370 batches in 588.58 sec.    training loss:\t\t4.40987168483\n",
      "Done 3380 batches in 590.23 sec.    training loss:\t\t4.4091380654\n",
      "Done 3390 batches in 591.95 sec.    training loss:\t\t4.40839042171\n",
      "Done 3400 batches in 593.58 sec.    training loss:\t\t4.40759424658\n",
      "Done 3410 batches in 595.41 sec.    training loss:\t\t4.40692276899\n",
      "Done 3420 batches in 597.06 sec.    training loss:\t\t4.40610450841\n",
      "Done 3430 batches in 598.49 sec.    training loss:\t\t4.40512800543\n",
      "Done 3440 batches in 599.92 sec.    training loss:\t\t4.40421557295\n",
      "Done 3450 batches in 601.79 sec.    training loss:\t\t4.40356232975\n",
      "Done 3460 batches in 603.66 sec.    training loss:\t\t4.40279093188\n",
      "Done 3470 batches in 605.46 sec.    training loss:\t\t4.40211018259\n",
      "Done 3480 batches in 606.94 sec.    training loss:\t\t4.4012740179\n",
      "Done 3490 batches in 608.42 sec.    training loss:\t\t4.40035064029\n",
      "Done 3500 batches in 610.10 sec.    training loss:\t\t4.39958327559\n",
      "Done 3510 batches in 611.86 sec.    training loss:\t\t4.39894536336\n",
      "Done 3520 batches in 613.89 sec.    training loss:\t\t4.39816249846\n",
      "Done 3530 batches in 616.29 sec.    training loss:\t\t4.39752774056\n",
      "Done 3540 batches in 618.05 sec.    training loss:\t\t4.39683020445\n",
      "Done 3550 batches in 619.46 sec.    training loss:\t\t4.39621175578\n",
      "Done 3560 batches in 621.16 sec.    training loss:\t\t4.39556175939\n",
      "Done 3570 batches in 622.78 sec.    training loss:\t\t4.39480970734\n",
      "Done 3580 batches in 624.36 sec.    training loss:\t\t4.39423693272\n",
      "Done 3590 batches in 625.98 sec.    training loss:\t\t4.39356599724\n",
      "Done 3600 batches in 627.57 sec.    training loss:\t\t4.39298155255\n",
      "Done 3610 batches in 629.33 sec.    training loss:\t\t4.39238540803\n",
      "Done 3620 batches in 631.33 sec.    training loss:\t\t4.39164820034\n",
      "Done 3630 batches in 632.91 sec.    training loss:\t\t4.39093545895\n",
      "Done 3640 batches in 634.44 sec.    training loss:\t\t4.39027597976\n",
      "Done 3650 batches in 635.97 sec.    training loss:\t\t4.38949507766\n",
      "Done 3660 batches in 637.75 sec.    training loss:\t\t4.3888410606\n",
      "Done 3670 batches in 639.85 sec.    training loss:\t\t4.38822495177\n",
      "Done 3680 batches in 641.56 sec.    training loss:\t\t4.38773409478\n",
      "Done 3690 batches in 643.12 sec.    training loss:\t\t4.38702566398\n",
      "Done 3700 batches in 644.45 sec.    training loss:\t\t4.38618291752\n",
      "Done 3710 batches in 645.94 sec.    training loss:\t\t4.38555202086\n",
      "Done 3720 batches in 647.80 sec.    training loss:\t\t4.38484591489\n",
      "Done 3730 batches in 649.23 sec.    training loss:\t\t4.38420684715\n",
      "Done 3740 batches in 650.89 sec.    training loss:\t\t4.38365642897\n",
      "Done 3750 batches in 652.74 sec.    training loss:\t\t4.38289868787\n",
      "Done 3760 batches in 654.12 sec.    training loss:\t\t4.38214501174\n",
      "Done 3770 batches in 655.80 sec.    training loss:\t\t4.38145129314\n",
      "Done 3780 batches in 657.31 sec.    training loss:\t\t4.38073350059\n",
      "Done 3790 batches in 659.71 sec.    training loss:\t\t4.38019530012\n",
      "Done 3800 batches in 661.36 sec.    training loss:\t\t4.3795083391\n",
      "Done 3810 batches in 662.87 sec.    training loss:\t\t4.37881339472\n",
      "Done 3820 batches in 664.57 sec.    training loss:\t\t4.37814980564\n",
      "Done 3830 batches in 665.93 sec.    training loss:\t\t4.37754262639\n",
      "Done 3840 batches in 667.87 sec.    training loss:\t\t4.37700906973\n",
      "Done 3850 batches in 669.46 sec.    training loss:\t\t4.37631601414\n",
      "Done 3860 batches in 671.03 sec.    training loss:\t\t4.37562491116\n",
      "Done 3870 batches in 673.09 sec.    training loss:\t\t4.37490840747\n",
      "Done 3880 batches in 674.71 sec.    training loss:\t\t4.37424500087\n",
      "Done 3890 batches in 676.32 sec.    training loss:\t\t4.37353331497\n",
      "Done 3900 batches in 677.80 sec.    training loss:\t\t4.37287315625\n",
      "Done 3910 batches in 679.64 sec.    training loss:\t\t4.3722889577\n",
      "Done 3920 batches in 681.36 sec.    training loss:\t\t4.37162685534\n",
      "Epoch 1 of 5 took 697.368s\n",
      "  training loss:\t\t4.371213\n",
      "  validation loss:\t\t4.138967\n",
      "Done 10 batches in 1.75 sec.    training loss:\t\t4.131437397\n",
      "Done 20 batches in 3.42 sec.    training loss:\t\t4.12272338867\n",
      "Done 30 batches in 5.10 sec.    training loss:\t\t4.11286292076\n",
      "Done 40 batches in 7.06 sec.    training loss:\t\t4.11993192434\n",
      "Done 50 batches in 9.18 sec.    training loss:\t\t4.12410499096\n",
      "Done 60 batches in 11.06 sec.    training loss:\t\t4.11887242397\n",
      "Done 70 batches in 12.90 sec.    training loss:\t\t4.12151169436\n",
      "Done 80 batches in 15.06 sec.    training loss:\t\t4.12036801279\n",
      "Done 90 batches in 16.65 sec.    training loss:\t\t4.12148479091\n",
      "Done 100 batches in 18.37 sec.    training loss:\t\t4.12124100208\n",
      "Done 110 batches in 19.96 sec.    training loss:\t\t4.11918438998\n",
      "Done 120 batches in 21.78 sec.    training loss:\t\t4.11842442155\n",
      "Done 130 batches in 23.83 sec.    training loss:\t\t4.1210915107\n",
      "Done 140 batches in 25.34 sec.    training loss:\t\t4.12419922182\n",
      "Done 150 batches in 27.16 sec.    training loss:\t\t4.12130799452\n",
      "Done 160 batches in 28.76 sec.    training loss:\t\t4.12231711298\n",
      "Done 170 batches in 30.64 sec.    training loss:\t\t4.12049743568\n",
      "Done 180 batches in 32.65 sec.    training loss:\t\t4.11937870979\n",
      "Done 190 batches in 34.38 sec.    training loss:\t\t4.12019745927\n",
      "Done 200 batches in 35.78 sec.    training loss:\t\t4.11662561059\n",
      "Done 210 batches in 37.71 sec.    training loss:\t\t4.11829966023\n",
      "Done 220 batches in 39.44 sec.    training loss:\t\t4.11927897063\n",
      "Done 230 batches in 41.11 sec.    training loss:\t\t4.11750434274\n",
      "Done 240 batches in 43.30 sec.    training loss:\t\t4.1185201774\n",
      "Done 250 batches in 44.82 sec.    training loss:\t\t4.1179153595\n",
      "Done 260 batches in 46.62 sec.    training loss:\t\t4.11896643088\n",
      "Done 270 batches in 48.28 sec.    training loss:\t\t4.11782765124\n",
      "Done 280 batches in 49.98 sec.    training loss:\t\t4.11843419501\n",
      "Done 290 batches in 51.77 sec.    training loss:\t\t4.1176932672\n",
      "Done 300 batches in 53.74 sec.    training loss:\t\t4.11741228024\n",
      "Done 310 batches in 55.87 sec.    training loss:\t\t4.11692971876\n",
      "Done 320 batches in 57.77 sec.    training loss:\t\t4.11759484708\n",
      "Done 330 batches in 59.44 sec.    training loss:\t\t4.11774612412\n",
      "Done 340 batches in 61.13 sec.    training loss:\t\t4.11794651396\n",
      "Done 350 batches in 62.75 sec.    training loss:\t\t4.11692997592\n",
      "Done 360 batches in 64.30 sec.    training loss:\t\t4.11645117071\n",
      "Done 370 batches in 65.82 sec.    training loss:\t\t4.11603247926\n",
      "Done 380 batches in 67.23 sec.    training loss:\t\t4.11591200954\n",
      "Done 390 batches in 68.79 sec.    training loss:\t\t4.1159687892\n",
      "Done 400 batches in 70.58 sec.    training loss:\t\t4.11693666756\n",
      "Done 410 batches in 72.58 sec.    training loss:\t\t4.11517553039\n",
      "Done 420 batches in 74.36 sec.    training loss:\t\t4.11580467281\n",
      "Done 430 batches in 76.34 sec.    training loss:\t\t4.11562148305\n",
      "Done 440 batches in 77.87 sec.    training loss:\t\t4.11478547237\n",
      "Done 450 batches in 79.39 sec.    training loss:\t\t4.11425750097\n",
      "Done 460 batches in 80.80 sec.    training loss:\t\t4.11520758504\n",
      "Done 470 batches in 82.15 sec.    training loss:\t\t4.11449125016\n",
      "Done 480 batches in 83.61 sec.    training loss:\t\t4.11383851866\n",
      "Done 490 batches in 85.67 sec.    training loss:\t\t4.11419105919\n",
      "Done 500 batches in 87.44 sec.    training loss:\t\t4.11454383469\n",
      "Done 510 batches in 89.04 sec.    training loss:\t\t4.11450856994\n",
      "Done 520 batches in 90.58 sec.    training loss:\t\t4.11466436478\n",
      "Done 530 batches in 92.22 sec.    training loss:\t\t4.11458855845\n",
      "Done 540 batches in 93.97 sec.    training loss:\t\t4.11465670179\n",
      "Done 550 batches in 95.61 sec.    training loss:\t\t4.11412470861\n",
      "Done 560 batches in 97.42 sec.    training loss:\t\t4.11515101365\n",
      "Done 570 batches in 99.18 sec.    training loss:\t\t4.11436366868\n",
      "Done 580 batches in 101.81 sec.    training loss:\t\t4.11446730433\n",
      "Done 590 batches in 103.74 sec.    training loss:\t\t4.11371378656\n",
      "Done 600 batches in 105.49 sec.    training loss:\t\t4.1138942186\n",
      "Done 610 batches in 107.12 sec.    training loss:\t\t4.11305225716\n",
      "Done 620 batches in 109.00 sec.    training loss:\t\t4.11257136829\n",
      "Done 630 batches in 110.60 sec.    training loss:\t\t4.1129514751\n",
      "Done 640 batches in 112.33 sec.    training loss:\t\t4.11263609529\n",
      "Done 650 batches in 114.30 sec.    training loss:\t\t4.11241790918\n",
      "Done 660 batches in 115.91 sec.    training loss:\t\t4.11199688984\n",
      "Done 670 batches in 117.64 sec.    training loss:\t\t4.11203704165\n",
      "Done 680 batches in 119.49 sec.    training loss:\t\t4.11085299289\n",
      "Done 690 batches in 121.07 sec.    training loss:\t\t4.11084502987\n",
      "Done 700 batches in 122.90 sec.    training loss:\t\t4.11164847817\n",
      "Done 710 batches in 124.71 sec.    training loss:\t\t4.11209582376\n",
      "Done 720 batches in 126.57 sec.    training loss:\t\t4.11178289089\n",
      "Done 730 batches in 128.29 sec.    training loss:\t\t4.11134906044\n",
      "Done 740 batches in 129.98 sec.    training loss:\t\t4.1107007133\n",
      "Done 750 batches in 131.59 sec.    training loss:\t\t4.10968739382\n",
      "Done 760 batches in 133.17 sec.    training loss:\t\t4.10958784787\n",
      "Done 770 batches in 134.54 sec.    training loss:\t\t4.10916447454\n",
      "Done 780 batches in 136.17 sec.    training loss:\t\t4.10847427295\n",
      "Done 790 batches in 137.79 sec.    training loss:\t\t4.10784822989\n",
      "Done 800 batches in 139.56 sec.    training loss:\t\t4.10842924684\n",
      "Done 810 batches in 141.37 sec.    training loss:\t\t4.10889404997\n",
      "Done 820 batches in 143.19 sec.    training loss:\t\t4.10846169896\n",
      "Done 830 batches in 145.09 sec.    training loss:\t\t4.10797672128\n",
      "Done 840 batches in 146.65 sec.    training loss:\t\t4.10739019485\n",
      "Done 850 batches in 148.65 sec.    training loss:\t\t4.10751852036\n",
      "Done 860 batches in 150.49 sec.    training loss:\t\t4.10727016455\n",
      "Done 870 batches in 152.14 sec.    training loss:\t\t4.10636458068\n",
      "Done 880 batches in 153.84 sec.    training loss:\t\t4.10621499013\n",
      "Done 890 batches in 155.85 sec.    training loss:\t\t4.10611480622\n",
      "Done 900 batches in 157.68 sec.    training loss:\t\t4.10636234601\n",
      "Done 910 batches in 159.63 sec.    training loss:\t\t4.10669607409\n",
      "Done 920 batches in 161.26 sec.    training loss:\t\t4.10626230499\n",
      "Done 930 batches in 163.17 sec.    training loss:\t\t4.10578490355\n",
      "Done 940 batches in 164.85 sec.    training loss:\t\t4.1059417963\n",
      "Done 950 batches in 166.98 sec.    training loss:\t\t4.10600423185\n",
      "Done 960 batches in 168.48 sec.    training loss:\t\t4.10542268679\n",
      "Done 970 batches in 170.24 sec.    training loss:\t\t4.10494982041\n",
      "Done 980 batches in 171.84 sec.    training loss:\t\t4.10487578596\n",
      "Done 990 batches in 173.55 sec.    training loss:\t\t4.10455497323\n",
      "Done 1000 batches in 175.31 sec.    training loss:\t\t4.10390257049\n",
      "Done 1010 batches in 176.96 sec.    training loss:\t\t4.10334847162\n",
      "Done 1020 batches in 179.22 sec.    training loss:\t\t4.10328151946\n",
      "Done 1030 batches in 180.78 sec.    training loss:\t\t4.10300111817\n",
      "Done 1040 batches in 182.73 sec.    training loss:\t\t4.10343391849\n",
      "Done 1050 batches in 184.35 sec.    training loss:\t\t4.10297934441\n",
      "Done 1060 batches in 185.93 sec.    training loss:\t\t4.10290441558\n",
      "Done 1070 batches in 188.08 sec.    training loss:\t\t4.10239665575\n",
      "Done 1080 batches in 189.67 sec.    training loss:\t\t4.10198544418\n",
      "Done 1090 batches in 191.45 sec.    training loss:\t\t4.10191243176\n",
      "Done 1100 batches in 192.98 sec.    training loss:\t\t4.10164780118\n",
      "Done 1110 batches in 194.80 sec.    training loss:\t\t4.10156025307\n",
      "Done 1120 batches in 196.62 sec.    training loss:\t\t4.10132771816\n",
      "Done 1130 batches in 198.33 sec.    training loss:\t\t4.10126345727\n",
      "Done 1140 batches in 199.90 sec.    training loss:\t\t4.10090235744\n",
      "Done 1150 batches in 201.52 sec.    training loss:\t\t4.10116647803\n",
      "Done 1160 batches in 203.26 sec.    training loss:\t\t4.10100400016\n",
      "Done 1170 batches in 205.28 sec.    training loss:\t\t4.10070013307\n",
      "Done 1180 batches in 207.05 sec.    training loss:\t\t4.10043290227\n",
      "Done 1190 batches in 209.15 sec.    training loss:\t\t4.10023488357\n",
      "Done 1200 batches in 210.58 sec.    training loss:\t\t4.09987293323\n",
      "Done 1210 batches in 212.21 sec.    training loss:\t\t4.09965523767\n",
      "Done 1220 batches in 213.76 sec.    training loss:\t\t4.09934722029\n",
      "Done 1230 batches in 216.05 sec.    training loss:\t\t4.09976144864\n",
      "Done 1240 batches in 217.86 sec.    training loss:\t\t4.0997178766\n",
      "Done 1250 batches in 219.83 sec.    training loss:\t\t4.0993047966\n",
      "Done 1260 batches in 221.62 sec.    training loss:\t\t4.09922370967\n",
      "Done 1270 batches in 223.42 sec.    training loss:\t\t4.09933854708\n",
      "Done 1280 batches in 225.13 sec.    training loss:\t\t4.09917541258\n",
      "Done 1290 batches in 226.99 sec.    training loss:\t\t4.09881357241\n",
      "Done 1300 batches in 228.49 sec.    training loss:\t\t4.09861152814\n",
      "Done 1310 batches in 230.09 sec.    training loss:\t\t4.09833502278\n",
      "Done 1320 batches in 232.40 sec.    training loss:\t\t4.09817063935\n",
      "Done 1330 batches in 233.97 sec.    training loss:\t\t4.09816776021\n",
      "Done 1340 batches in 235.91 sec.    training loss:\t\t4.09776955772\n",
      "Done 1350 batches in 237.77 sec.    training loss:\t\t4.09760093636\n",
      "Done 1360 batches in 239.80 sec.    training loss:\t\t4.09771890307\n",
      "Done 1370 batches in 241.91 sec.    training loss:\t\t4.09804518188\n",
      "Done 1380 batches in 244.26 sec.    training loss:\t\t4.09803404843\n",
      "Done 1390 batches in 246.11 sec.    training loss:\t\t4.09761778228\n",
      "Done 1400 batches in 247.78 sec.    training loss:\t\t4.09726725987\n",
      "Done 1410 batches in 249.71 sec.    training loss:\t\t4.0971194247\n",
      "Done 1420 batches in 251.38 sec.    training loss:\t\t4.09716556643\n",
      "Done 1430 batches in 252.95 sec.    training loss:\t\t4.09677282563\n",
      "Done 1440 batches in 254.54 sec.    training loss:\t\t4.09655559477\n",
      "Done 1450 batches in 256.34 sec.    training loss:\t\t4.09660013906\n",
      "Done 1460 batches in 257.99 sec.    training loss:\t\t4.0964578297\n",
      "Done 1470 batches in 259.83 sec.    training loss:\t\t4.09630669055\n",
      "Done 1480 batches in 261.69 sec.    training loss:\t\t4.09614484938\n",
      "Done 1490 batches in 263.33 sec.    training loss:\t\t4.09632693153\n",
      "Done 1500 batches in 264.88 sec.    training loss:\t\t4.09653269164\n",
      "Done 1510 batches in 266.99 sec.    training loss:\t\t4.09652753129\n",
      "Done 1520 batches in 268.60 sec.    training loss:\t\t4.09667942869\n",
      "Done 1530 batches in 270.26 sec.    training loss:\t\t4.09645391336\n",
      "Done 1540 batches in 271.94 sec.    training loss:\t\t4.09606716339\n",
      "Done 1550 batches in 273.60 sec.    training loss:\t\t4.0959384063\n",
      "Done 1560 batches in 275.70 sec.    training loss:\t\t4.09582231671\n",
      "Done 1570 batches in 277.84 sec.    training loss:\t\t4.09551146091\n",
      "Done 1580 batches in 279.61 sec.    training loss:\t\t4.0952887941\n",
      "Done 1590 batches in 281.52 sec.    training loss:\t\t4.09526705637\n",
      "Done 1600 batches in 283.34 sec.    training loss:\t\t4.09498065516\n",
      "Done 1610 batches in 285.26 sec.    training loss:\t\t4.09485451168\n",
      "Done 1620 batches in 287.06 sec.    training loss:\t\t4.09489116404\n",
      "Done 1630 batches in 288.73 sec.    training loss:\t\t4.09450314264\n",
      "Done 1640 batches in 290.33 sec.    training loss:\t\t4.09410876108\n",
      "Done 1650 batches in 292.08 sec.    training loss:\t\t4.09409354889\n",
      "Done 1660 batches in 293.93 sec.    training loss:\t\t4.09391515873\n",
      "Done 1670 batches in 295.70 sec.    training loss:\t\t4.09372171499\n",
      "Done 1680 batches in 298.12 sec.    training loss:\t\t4.09359924254\n",
      "Done 1690 batches in 299.65 sec.    training loss:\t\t4.09317476989\n",
      "Done 1700 batches in 301.73 sec.    training loss:\t\t4.0927353678\n",
      "Done 1710 batches in 303.46 sec.    training loss:\t\t4.09265526763\n",
      "Done 1720 batches in 305.46 sec.    training loss:\t\t4.09233002677\n",
      "Done 1730 batches in 307.10 sec.    training loss:\t\t4.0921177135\n",
      "Done 1740 batches in 308.60 sec.    training loss:\t\t4.09193834683\n",
      "Done 1750 batches in 310.30 sec.    training loss:\t\t4.09172719574\n",
      "Done 1760 batches in 312.13 sec.    training loss:\t\t4.0915661419\n",
      "Done 1770 batches in 313.59 sec.    training loss:\t\t4.0910706714\n",
      "Done 1780 batches in 315.08 sec.    training loss:\t\t4.09053030844\n",
      "Done 1790 batches in 316.68 sec.    training loss:\t\t4.0902244163\n",
      "Done 1800 batches in 318.40 sec.    training loss:\t\t4.09003228572\n",
      "Done 1810 batches in 320.29 sec.    training loss:\t\t4.08972879352\n",
      "Done 1820 batches in 322.03 sec.    training loss:\t\t4.0893208881\n",
      "Done 1830 batches in 323.62 sec.    training loss:\t\t4.08900105054\n",
      "Done 1840 batches in 325.83 sec.    training loss:\t\t4.08884971077\n",
      "Done 1850 batches in 327.25 sec.    training loss:\t\t4.08854501982\n",
      "Done 1860 batches in 328.94 sec.    training loss:\t\t4.08837538573\n",
      "Done 1870 batches in 330.70 sec.    training loss:\t\t4.08818730785\n",
      "Done 1880 batches in 332.30 sec.    training loss:\t\t4.08799723425\n",
      "Done 1890 batches in 334.31 sec.    training loss:\t\t4.08757469225\n",
      "Done 1900 batches in 336.15 sec.    training loss:\t\t4.08733407083\n",
      "Done 1910 batches in 338.23 sec.    training loss:\t\t4.08726144985\n",
      "Done 1920 batches in 340.37 sec.    training loss:\t\t4.0872233243\n",
      "Done 1930 batches in 342.13 sec.    training loss:\t\t4.08705242421\n",
      "Done 1940 batches in 343.97 sec.    training loss:\t\t4.08681857193\n",
      "Done 1950 batches in 345.27 sec.    training loss:\t\t4.08663349408\n",
      "Done 1960 batches in 347.31 sec.    training loss:\t\t4.08655538231\n",
      "Done 1970 batches in 349.03 sec.    training loss:\t\t4.08631948493\n",
      "Done 1980 batches in 351.05 sec.    training loss:\t\t4.08609820604\n",
      "Done 1990 batches in 352.63 sec.    training loss:\t\t4.08569329743\n",
      "Done 2000 batches in 354.29 sec.    training loss:\t\t4.08547404015\n",
      "Done 2010 batches in 356.03 sec.    training loss:\t\t4.08509591397\n",
      "Done 2020 batches in 358.06 sec.    training loss:\t\t4.08509630106\n",
      "Done 2030 batches in 360.11 sec.    training loss:\t\t4.08508155487\n",
      "Done 2040 batches in 361.84 sec.    training loss:\t\t4.08508762869\n",
      "Done 2050 batches in 364.31 sec.    training loss:\t\t4.08509055708\n",
      "Done 2060 batches in 365.91 sec.    training loss:\t\t4.08466185933\n",
      "Done 2070 batches in 367.34 sec.    training loss:\t\t4.08429937628\n",
      "Done 2080 batches in 369.24 sec.    training loss:\t\t4.08423874951\n",
      "Done 2090 batches in 371.28 sec.    training loss:\t\t4.08412946069\n",
      "Done 2100 batches in 373.19 sec.    training loss:\t\t4.08397541818\n",
      "Done 2110 batches in 375.19 sec.    training loss:\t\t4.08402248988\n",
      "Done 2120 batches in 377.04 sec.    training loss:\t\t4.08379354162\n",
      "Done 2130 batches in 378.66 sec.    training loss:\t\t4.08384211657\n",
      "Done 2140 batches in 380.42 sec.    training loss:\t\t4.08384834528\n",
      "Done 2150 batches in 382.10 sec.    training loss:\t\t4.08358423854\n",
      "Done 2160 batches in 383.76 sec.    training loss:\t\t4.08335916665\n",
      "Done 2170 batches in 385.42 sec.    training loss:\t\t4.08326628153\n",
      "Done 2180 batches in 387.10 sec.    training loss:\t\t4.08301074002\n",
      "Done 2190 batches in 389.36 sec.    training loss:\t\t4.08299110328\n",
      "Done 2200 batches in 391.18 sec.    training loss:\t\t4.08280467629\n",
      "Done 2210 batches in 393.19 sec.    training loss:\t\t4.08276734298\n",
      "Done 2220 batches in 394.99 sec.    training loss:\t\t4.08262179499\n",
      "Done 2230 batches in 396.49 sec.    training loss:\t\t4.08246185619\n",
      "Done 2240 batches in 398.29 sec.    training loss:\t\t4.08230897358\n",
      "Done 2250 batches in 399.95 sec.    training loss:\t\t4.08205868954\n",
      "Done 2260 batches in 401.57 sec.    training loss:\t\t4.08175944544\n",
      "Done 2270 batches in 403.17 sec.    training loss:\t\t4.08144866664\n",
      "Done 2280 batches in 404.84 sec.    training loss:\t\t4.08147870647\n",
      "Done 2290 batches in 406.55 sec.    training loss:\t\t4.08144632198\n",
      "Done 2300 batches in 408.16 sec.    training loss:\t\t4.0810364745\n",
      "Done 2310 batches in 410.03 sec.    training loss:\t\t4.08085558724\n",
      "Done 2320 batches in 411.41 sec.    training loss:\t\t4.08078469418\n",
      "Done 2330 batches in 413.54 sec.    training loss:\t\t4.0804303598\n",
      "Done 2340 batches in 415.04 sec.    training loss:\t\t4.08016482408\n",
      "Done 2350 batches in 417.44 sec.    training loss:\t\t4.08027249306\n",
      "Done 2360 batches in 419.14 sec.    training loss:\t\t4.08015544657\n",
      "Done 2370 batches in 420.71 sec.    training loss:\t\t4.08009240114\n",
      "Done 2380 batches in 422.17 sec.    training loss:\t\t4.07990576199\n",
      "Done 2390 batches in 424.22 sec.    training loss:\t\t4.07968425182\n",
      "Done 2400 batches in 426.05 sec.    training loss:\t\t4.07947764407\n",
      "Done 2410 batches in 427.68 sec.    training loss:\t\t4.07931462215\n",
      "Done 2420 batches in 429.44 sec.    training loss:\t\t4.0792912699\n",
      "Done 2430 batches in 431.59 sec.    training loss:\t\t4.07896827582\n",
      "Done 2440 batches in 433.42 sec.    training loss:\t\t4.07884988023\n",
      "Done 2450 batches in 434.91 sec.    training loss:\t\t4.07872800097\n",
      "Done 2460 batches in 436.74 sec.    training loss:\t\t4.07850186311\n",
      "Done 2470 batches in 438.71 sec.    training loss:\t\t4.07819087988\n",
      "Done 2480 batches in 440.41 sec.    training loss:\t\t4.07805429668\n",
      "Done 2490 batches in 442.52 sec.    training loss:\t\t4.07809587722\n",
      "Done 2500 batches in 444.23 sec.    training loss:\t\t4.07782351627\n",
      "Done 2510 batches in 445.86 sec.    training loss:\t\t4.07753231734\n",
      "Done 2520 batches in 447.40 sec.    training loss:\t\t4.07736015405\n",
      "Done 2530 batches in 448.96 sec.    training loss:\t\t4.07694670796\n",
      "Done 2540 batches in 450.89 sec.    training loss:\t\t4.07693883875\n",
      "Done 2550 batches in 452.51 sec.    training loss:\t\t4.07689815633\n",
      "Done 2560 batches in 454.69 sec.    training loss:\t\t4.07669819901\n",
      "Done 2570 batches in 456.60 sec.    training loss:\t\t4.07642819455\n",
      "Done 2580 batches in 458.49 sec.    training loss:\t\t4.07600918888\n",
      "Done 2590 batches in 460.15 sec.    training loss:\t\t4.075695789\n",
      "Done 2600 batches in 461.56 sec.    training loss:\t\t4.0755415303\n",
      "Done 2610 batches in 463.42 sec.    training loss:\t\t4.07531802581\n",
      "Done 2620 batches in 465.68 sec.    training loss:\t\t4.07521856932\n",
      "Done 2630 batches in 467.44 sec.    training loss:\t\t4.07509084092\n",
      "Done 2640 batches in 468.92 sec.    training loss:\t\t4.07475107908\n",
      "Done 2650 batches in 471.17 sec.    training loss:\t\t4.07459345431\n",
      "Done 2660 batches in 473.30 sec.    training loss:\t\t4.07443681274\n",
      "Done 2670 batches in 474.81 sec.    training loss:\t\t4.07437219146\n",
      "Done 2680 batches in 476.57 sec.    training loss:\t\t4.07411745164\n",
      "Done 2690 batches in 478.35 sec.    training loss:\t\t4.07402098604\n",
      "Done 2700 batches in 480.03 sec.    training loss:\t\t4.07389230234\n",
      "Done 2710 batches in 481.56 sec.    training loss:\t\t4.07359610466\n",
      "Done 2720 batches in 483.77 sec.    training loss:\t\t4.0735604401\n",
      "Done 2730 batches in 485.91 sec.    training loss:\t\t4.07345771938\n",
      "Done 2740 batches in 487.84 sec.    training loss:\t\t4.07337067049\n",
      "Done 2750 batches in 489.74 sec.    training loss:\t\t4.07336841349\n",
      "Done 2760 batches in 491.32 sec.    training loss:\t\t4.07309262718\n",
      "Done 2770 batches in 493.38 sec.    training loss:\t\t4.07301146304\n",
      "Done 2780 batches in 494.87 sec.    training loss:\t\t4.07297721655\n",
      "Done 2790 batches in 496.87 sec.    training loss:\t\t4.07264828126\n",
      "Done 2800 batches in 498.98 sec.    training loss:\t\t4.07251600095\n",
      "Done 2810 batches in 500.88 sec.    training loss:\t\t4.07244678833\n",
      "Done 2820 batches in 502.53 sec.    training loss:\t\t4.07224707933\n",
      "Done 2830 batches in 504.02 sec.    training loss:\t\t4.07216554261\n",
      "Done 2840 batches in 505.93 sec.    training loss:\t\t4.07222038046\n",
      "Done 2850 batches in 507.47 sec.    training loss:\t\t4.07194325757\n",
      "Done 2860 batches in 508.94 sec.    training loss:\t\t4.07168032489\n",
      "Done 2870 batches in 510.60 sec.    training loss:\t\t4.07150074522\n",
      "Done 2880 batches in 512.15 sec.    training loss:\t\t4.07129172575\n",
      "Done 2890 batches in 513.78 sec.    training loss:\t\t4.0710908012\n",
      "Done 2900 batches in 515.38 sec.    training loss:\t\t4.07098959882\n",
      "Done 2910 batches in 517.29 sec.    training loss:\t\t4.07064526155\n",
      "Done 2920 batches in 519.26 sec.    training loss:\t\t4.07051666526\n",
      "Done 2930 batches in 520.60 sec.    training loss:\t\t4.07018086015\n",
      "Done 2940 batches in 522.42 sec.    training loss:\t\t4.06999472761\n",
      "Done 2950 batches in 524.24 sec.    training loss:\t\t4.0697660308\n",
      "Done 2960 batches in 525.75 sec.    training loss:\t\t4.06955379473\n",
      "Done 2970 batches in 527.50 sec.    training loss:\t\t4.06926524174\n",
      "Done 2980 batches in 529.05 sec.    training loss:\t\t4.06910988516\n",
      "Done 2990 batches in 530.56 sec.    training loss:\t\t4.06901614722\n",
      "Done 3000 batches in 532.60 sec.    training loss:\t\t4.06883050593\n",
      "Done 3010 batches in 534.65 sec.    training loss:\t\t4.06878763916\n",
      "Done 3020 batches in 536.10 sec.    training loss:\t\t4.06844927475\n",
      "Done 3030 batches in 537.69 sec.    training loss:\t\t4.06839467417\n",
      "Done 3040 batches in 539.97 sec.    training loss:\t\t4.06849648458\n",
      "Done 3050 batches in 541.80 sec.    training loss:\t\t4.0683459106\n",
      "Done 3060 batches in 543.55 sec.    training loss:\t\t4.06813298581\n",
      "Done 3070 batches in 545.51 sec.    training loss:\t\t4.06794939398\n",
      "Done 3080 batches in 547.03 sec.    training loss:\t\t4.06772227148\n",
      "Done 3090 batches in 548.60 sec.    training loss:\t\t4.06762319599\n",
      "Done 3100 batches in 550.28 sec.    training loss:\t\t4.06762104111\n",
      "Done 3110 batches in 551.99 sec.    training loss:\t\t4.06732628499\n",
      "Done 3120 batches in 554.43 sec.    training loss:\t\t4.0672760158\n",
      "Done 3130 batches in 556.61 sec.    training loss:\t\t4.06715311052\n",
      "Done 3140 batches in 558.70 sec.    training loss:\t\t4.0670415709\n",
      "Done 3150 batches in 560.51 sec.    training loss:\t\t4.06684415946\n",
      "Done 3160 batches in 562.44 sec.    training loss:\t\t4.06667961456\n",
      "Done 3170 batches in 563.89 sec.    training loss:\t\t4.06662573995\n",
      "Done 3180 batches in 565.58 sec.    training loss:\t\t4.06656719769\n",
      "Done 3190 batches in 567.86 sec.    training loss:\t\t4.0664666693\n",
      "Done 3200 batches in 569.60 sec.    training loss:\t\t4.06605314799\n",
      "Done 3210 batches in 571.38 sec.    training loss:\t\t4.06586496733\n",
      "Done 3220 batches in 573.02 sec.    training loss:\t\t4.06591463163\n",
      "Done 3230 batches in 574.78 sec.    training loss:\t\t4.06558083639\n",
      "Done 3240 batches in 576.99 sec.    training loss:\t\t4.06545034846\n",
      "Done 3250 batches in 578.86 sec.    training loss:\t\t4.06538782619\n",
      "Done 3260 batches in 580.67 sec.    training loss:\t\t4.06537331226\n",
      "Done 3270 batches in 582.91 sec.    training loss:\t\t4.06532017302\n",
      "Done 3280 batches in 584.74 sec.    training loss:\t\t4.06506900046\n",
      "Done 3290 batches in 586.74 sec.    training loss:\t\t4.06494198389\n",
      "Done 3300 batches in 588.80 sec.    training loss:\t\t4.06475726157\n",
      "Done 3310 batches in 590.68 sec.    training loss:\t\t4.0646235842\n",
      "Done 3320 batches in 592.24 sec.    training loss:\t\t4.06448517995\n",
      "Done 3330 batches in 593.94 sec.    training loss:\t\t4.06445525308\n",
      "Done 3340 batches in 595.44 sec.    training loss:\t\t4.06410301845\n",
      "Done 3350 batches in 597.08 sec.    training loss:\t\t4.0640583923\n",
      "Done 3360 batches in 598.95 sec.    training loss:\t\t4.06398613446\n",
      "Done 3370 batches in 600.74 sec.    training loss:\t\t4.06403664262\n",
      "Done 3380 batches in 602.42 sec.    training loss:\t\t4.06390398938\n",
      "Done 3390 batches in 604.18 sec.    training loss:\t\t4.06377371989\n",
      "Done 3400 batches in 605.85 sec.    training loss:\t\t4.06357934952\n",
      "Done 3410 batches in 607.72 sec.    training loss:\t\t4.06350954201\n",
      "Done 3420 batches in 609.41 sec.    training loss:\t\t4.06330648214\n",
      "Done 3430 batches in 610.87 sec.    training loss:\t\t4.06293849632\n",
      "Done 3440 batches in 612.33 sec.    training loss:\t\t4.06263319788\n",
      "Done 3450 batches in 614.24 sec.    training loss:\t\t4.06256555281\n",
      "Done 3460 batches in 616.14 sec.    training loss:\t\t4.0623953769\n",
      "Done 3470 batches in 617.99 sec.    training loss:\t\t4.06229920284\n",
      "Done 3480 batches in 619.50 sec.    training loss:\t\t4.06206429491\n",
      "Done 3490 batches in 621.01 sec.    training loss:\t\t4.06172103738\n",
      "Done 3500 batches in 622.73 sec.    training loss:\t\t4.06154569789\n",
      "Done 3510 batches in 624.52 sec.    training loss:\t\t4.06149157235\n",
      "Done 3520 batches in 626.60 sec.    training loss:\t\t4.06128238196\n",
      "Done 3530 batches in 629.05 sec.    training loss:\t\t4.06119112671\n",
      "Done 3540 batches in 630.85 sec.    training loss:\t\t4.06107190288\n",
      "Done 3550 batches in 632.30 sec.    training loss:\t\t4.06101226088\n",
      "Done 3560 batches in 634.03 sec.    training loss:\t\t4.06093697943\n",
      "Done 3570 batches in 635.69 sec.    training loss:\t\t4.06075044253\n",
      "Done 3580 batches in 637.30 sec.    training loss:\t\t4.06074110036\n",
      "Done 3590 batches in 638.96 sec.    training loss:\t\t4.06062977075\n",
      "Done 3600 batches in 640.58 sec.    training loss:\t\t4.06060210678\n",
      "Done 3610 batches in 642.38 sec.    training loss:\t\t4.06056377168\n",
      "Done 3620 batches in 644.42 sec.    training loss:\t\t4.0603863207\n",
      "Done 3630 batches in 646.03 sec.    training loss:\t\t4.06021627579\n",
      "Done 3640 batches in 647.60 sec.    training loss:\t\t4.06010648648\n",
      "Done 3650 batches in 649.16 sec.    training loss:\t\t4.05986732574\n",
      "Done 3660 batches in 650.98 sec.    training loss:\t\t4.05976072569\n",
      "Done 3670 batches in 653.13 sec.    training loss:\t\t4.05968693336\n",
      "Done 3680 batches in 654.88 sec.    training loss:\t\t4.05973232473\n",
      "Done 3690 batches in 656.47 sec.    training loss:\t\t4.0595609159\n",
      "Done 3700 batches in 657.83 sec.    training loss:\t\t4.05926860635\n",
      "Done 3710 batches in 659.35 sec.    training loss:\t\t4.05917257074\n",
      "Done 3720 batches in 661.26 sec.    training loss:\t\t4.05900238387\n",
      "Done 3730 batches in 662.72 sec.    training loss:\t\t4.05889839148\n",
      "Done 3740 batches in 664.41 sec.    training loss:\t\t4.05887085327\n",
      "Done 3750 batches in 666.30 sec.    training loss:\t\t4.05863665899\n",
      "Done 3760 batches in 667.71 sec.    training loss:\t\t4.05841576869\n",
      "Done 3770 batches in 669.43 sec.    training loss:\t\t4.05824437464\n",
      "Done 3780 batches in 670.97 sec.    training loss:\t\t4.05804258932\n",
      "Done 3790 batches in 673.43 sec.    training loss:\t\t4.05802282765\n",
      "Done 3800 batches in 675.11 sec.    training loss:\t\t4.05785603316\n",
      "Done 3810 batches in 676.66 sec.    training loss:\t\t4.05768211864\n",
      "Done 3820 batches in 678.40 sec.    training loss:\t\t4.05752638997\n",
      "Done 3830 batches in 679.79 sec.    training loss:\t\t4.05743348399\n",
      "Done 3840 batches in 681.77 sec.    training loss:\t\t4.05740882779\n",
      "Done 3850 batches in 683.40 sec.    training loss:\t\t4.05721816911\n",
      "Done 3860 batches in 685.00 sec.    training loss:\t\t4.05704820656\n",
      "Done 3870 batches in 687.10 sec.    training loss:\t\t4.05683601845\n",
      "Done 3880 batches in 688.76 sec.    training loss:\t\t4.05667027979\n",
      "Done 3890 batches in 690.40 sec.    training loss:\t\t4.0564565126\n",
      "Done 3900 batches in 691.92 sec.    training loss:\t\t4.05629818739\n",
      "Done 3910 batches in 693.80 sec.    training loss:\t\t4.05620781333\n",
      "Done 3920 batches in 695.56 sec.    training loss:\t\t4.05603569904\n",
      "Epoch 2 of 5 took 711.896s\n",
      "  training loss:\t\t4.055914\n",
      "  validation loss:\t\t4.035515\n",
      "Done 10 batches in 1.78 sec.    training loss:\t\t4.00996334553\n",
      "Done 20 batches in 3.50 sec.    training loss:\t\t4.0021427989\n",
      "Done 30 batches in 5.21 sec.    training loss:\t\t3.99170536995\n",
      "Done 40 batches in 7.21 sec.    training loss:\t\t3.99899285436\n",
      "Done 50 batches in 9.38 sec.    training loss:\t\t4.00372230053\n",
      "Done 60 batches in 11.30 sec.    training loss:\t\t3.99826642672\n",
      "Done 70 batches in 13.17 sec.    training loss:\t\t4.00084098407\n",
      "Done 80 batches in 15.38 sec.    training loss:\t\t3.99967732728\n",
      "Done 90 batches in 17.01 sec.    training loss:\t\t4.00088282426\n",
      "Done 100 batches in 18.76 sec.    training loss:\t\t4.00079968214\n",
      "Done 110 batches in 20.38 sec.    training loss:\t\t3.9991643949\n",
      "Done 120 batches in 22.25 sec.    training loss:\t\t3.99865522385\n",
      "Done 130 batches in 24.34 sec.    training loss:\t\t4.00131287025\n",
      "Done 140 batches in 25.89 sec.    training loss:\t\t4.00434480906\n",
      "Done 150 batches in 27.75 sec.    training loss:\t\t4.00178143819\n",
      "Done 160 batches in 29.36 sec.    training loss:\t\t4.00316269547\n",
      "Done 170 batches in 31.24 sec.    training loss:\t\t4.00154556527\n",
      "Done 180 batches in 33.26 sec.    training loss:\t\t4.0006336742\n",
      "Done 190 batches in 34.99 sec.    training loss:\t\t4.00170747104\n",
      "Done 200 batches in 36.39 sec.    training loss:\t\t3.99839406133\n",
      "Done 210 batches in 38.32 sec.    training loss:\t\t4.00005126681\n",
      "Done 220 batches in 40.04 sec.    training loss:\t\t4.00098402283\n",
      "Done 230 batches in 41.71 sec.    training loss:\t\t3.99924589344\n",
      "Done 240 batches in 43.90 sec.    training loss:\t\t4.00036555529\n",
      "Done 250 batches in 45.42 sec.    training loss:\t\t3.99997012043\n",
      "Done 260 batches in 47.22 sec.    training loss:\t\t4.00121105451\n",
      "Done 270 batches in 48.88 sec.    training loss:\t\t4.00020057714\n",
      "Done 280 batches in 50.58 sec.    training loss:\t\t4.00090032646\n",
      "Done 290 batches in 52.37 sec.    training loss:\t\t4.00031267363\n",
      "Done 300 batches in 54.34 sec.    training loss:\t\t4.00009273132\n",
      "Done 310 batches in 56.47 sec.    training loss:\t\t3.99980298858\n",
      "Done 320 batches in 58.37 sec.    training loss:\t\t4.00049676672\n",
      "Done 330 batches in 60.04 sec.    training loss:\t\t4.00072844751\n",
      "Done 340 batches in 61.73 sec.    training loss:\t\t4.0010055002\n",
      "Done 350 batches in 63.34 sec.    training loss:\t\t4.00010034834\n",
      "Done 360 batches in 64.89 sec.    training loss:\t\t3.99973881046\n",
      "Done 370 batches in 66.42 sec.    training loss:\t\t3.99932377661\n",
      "Done 380 batches in 67.83 sec.    training loss:\t\t3.99924289176\n",
      "Done 390 batches in 69.38 sec.    training loss:\t\t3.99939013995\n",
      "Done 400 batches in 71.17 sec.    training loss:\t\t4.00044790149\n",
      "Done 410 batches in 73.17 sec.    training loss:\t\t3.99882367471\n",
      "Done 420 batches in 74.95 sec.    training loss:\t\t3.99957367977\n",
      "Done 430 batches in 76.94 sec.    training loss:\t\t3.99948734405\n",
      "Done 440 batches in 78.47 sec.    training loss:\t\t3.99878192436\n",
      "Done 450 batches in 79.98 sec.    training loss:\t\t3.99837555144\n",
      "Done 460 batches in 81.40 sec.    training loss:\t\t3.99942935239\n",
      "Done 470 batches in 82.74 sec.    training loss:\t\t3.99886148635\n",
      "Done 480 batches in 84.20 sec.    training loss:\t\t3.99846229305\n",
      "Done 490 batches in 86.26 sec.    training loss:\t\t3.99888223872\n",
      "Done 500 batches in 88.03 sec.    training loss:\t\t3.99934871721\n",
      "Done 510 batches in 89.63 sec.    training loss:\t\t3.99951494301\n",
      "Done 520 batches in 91.17 sec.    training loss:\t\t3.99974792095\n",
      "Done 530 batches in 92.81 sec.    training loss:\t\t3.99976388868\n",
      "Done 540 batches in 94.56 sec.    training loss:\t\t3.99998022671\n",
      "Done 550 batches in 96.20 sec.    training loss:\t\t3.99956038215\n",
      "Done 560 batches in 98.01 sec.    training loss:\t\t4.00072561673\n",
      "Done 570 batches in 99.77 sec.    training loss:\t\t4.00010079375\n",
      "Done 580 batches in 102.40 sec.    training loss:\t\t4.00025818718\n",
      "Done 590 batches in 104.33 sec.    training loss:\t\t3.9996109914\n",
      "Done 600 batches in 106.08 sec.    training loss:\t\t3.99987361153\n",
      "Done 610 batches in 107.71 sec.    training loss:\t\t3.99917172174\n",
      "Done 620 batches in 109.58 sec.    training loss:\t\t3.99884450666\n",
      "Done 630 batches in 111.18 sec.    training loss:\t\t3.99935647003\n",
      "Done 640 batches in 112.92 sec.    training loss:\t\t3.99912861027\n",
      "Done 650 batches in 114.89 sec.    training loss:\t\t3.99903233271\n",
      "Done 660 batches in 116.50 sec.    training loss:\t\t3.99871300459\n",
      "Done 670 batches in 118.23 sec.    training loss:\t\t3.99885972365\n",
      "Done 680 batches in 120.08 sec.    training loss:\t\t3.99779082712\n",
      "Done 690 batches in 121.66 sec.    training loss:\t\t3.99790612511\n",
      "Done 700 batches in 123.49 sec.    training loss:\t\t3.9988394383\n",
      "Done 710 batches in 125.30 sec.    training loss:\t\t3.99943179715\n",
      "Done 720 batches in 127.16 sec.    training loss:\t\t3.99922983944\n",
      "Done 730 batches in 128.88 sec.    training loss:\t\t3.99894360288\n",
      "Done 740 batches in 130.57 sec.    training loss:\t\t3.99844696135\n",
      "Done 750 batches in 132.18 sec.    training loss:\t\t3.99753672504\n",
      "Done 760 batches in 133.76 sec.    training loss:\t\t3.99754767041\n",
      "Done 770 batches in 135.13 sec.    training loss:\t\t3.99724125181\n",
      "Done 780 batches in 136.76 sec.    training loss:\t\t3.99668564705\n",
      "Done 790 batches in 138.38 sec.    training loss:\t\t3.99616125384\n",
      "Done 800 batches in 140.15 sec.    training loss:\t\t3.99683446497\n",
      "Done 810 batches in 141.96 sec.    training loss:\t\t3.99739325871\n",
      "Done 820 batches in 143.79 sec.    training loss:\t\t3.99709479722\n",
      "Done 830 batches in 145.69 sec.    training loss:\t\t3.99671734988\n",
      "Done 840 batches in 147.24 sec.    training loss:\t\t3.99625477876\n",
      "Done 850 batches in 149.25 sec.    training loss:\t\t3.9964450365\n",
      "Done 860 batches in 151.08 sec.    training loss:\t\t3.99631213698\n",
      "Done 870 batches in 152.73 sec.    training loss:\t\t3.99551446821\n",
      "Done 880 batches in 154.44 sec.    training loss:\t\t3.99546995542\n",
      "Done 890 batches in 156.45 sec.    training loss:\t\t3.99546255021\n",
      "Done 900 batches in 158.28 sec.    training loss:\t\t3.99585483975\n",
      "Done 910 batches in 160.23 sec.    training loss:\t\t3.99629823009\n",
      "Done 920 batches in 161.86 sec.    training loss:\t\t3.99598128796\n",
      "Done 930 batches in 163.77 sec.    training loss:\t\t3.9956237893\n",
      "Done 940 batches in 165.44 sec.    training loss:\t\t3.99588294029\n",
      "Done 950 batches in 167.57 sec.    training loss:\t\t3.99605262455\n",
      "Done 960 batches in 169.07 sec.    training loss:\t\t3.99559519614\n",
      "Done 970 batches in 170.83 sec.    training loss:\t\t3.99525702\n",
      "Done 980 batches in 172.42 sec.    training loss:\t\t3.99530847\n",
      "Done 990 batches in 174.14 sec.    training loss:\t\t3.99508487624\n",
      "Done 1000 batches in 175.90 sec.    training loss:\t\t3.99455646157\n",
      "Done 1010 batches in 177.55 sec.    training loss:\t\t3.99410563625\n",
      "Done 1020 batches in 179.81 sec.    training loss:\t\t3.99413759685\n",
      "Done 1030 batches in 181.37 sec.    training loss:\t\t3.99394768858\n",
      "Done 1040 batches in 183.31 sec.    training loss:\t\t3.99447912161\n",
      "Done 1050 batches in 184.94 sec.    training loss:\t\t3.99412684872\n",
      "Done 1060 batches in 186.52 sec.    training loss:\t\t3.99416683867\n",
      "Done 1070 batches in 188.66 sec.    training loss:\t\t3.9940723593\n",
      "Done 1080 batches in 190.25 sec.    training loss:\t\t3.99382788539\n",
      "Done 1090 batches in 192.03 sec.    training loss:\t\t3.99390693126\n",
      "Done 1100 batches in 193.56 sec.    training loss:\t\t3.99377201427\n",
      "Done 1110 batches in 195.38 sec.    training loss:\t\t3.99380223579\n",
      "Done 1120 batches in 197.21 sec.    training loss:\t\t3.99368239407\n",
      "Done 1130 batches in 198.91 sec.    training loss:\t\t3.99373460301\n",
      "Done 1140 batches in 200.48 sec.    training loss:\t\t3.99349819568\n",
      "Done 1150 batches in 202.11 sec.    training loss:\t\t3.99387747495\n",
      "Done 1160 batches in 203.85 sec.    training loss:\t\t3.99381033737\n",
      "Done 1170 batches in 205.87 sec.    training loss:\t\t3.99360861676\n",
      "Done 1180 batches in 207.64 sec.    training loss:\t\t3.99345940917\n",
      "Done 1190 batches in 209.74 sec.    training loss:\t\t3.99336865189\n",
      "Done 1200 batches in 211.17 sec.    training loss:\t\t3.99310070256\n",
      "Done 1210 batches in 212.80 sec.    training loss:\t\t3.99300108724\n",
      "Done 1220 batches in 214.35 sec.    training loss:\t\t3.99278583878\n",
      "Done 1230 batches in 216.64 sec.    training loss:\t\t3.99331133831\n",
      "Done 1240 batches in 218.45 sec.    training loss:\t\t3.99336063535\n",
      "Done 1250 batches in 220.42 sec.    training loss:\t\t3.99304398117\n",
      "Done 1260 batches in 222.20 sec.    training loss:\t\t3.9930760569\n",
      "Done 1270 batches in 224.01 sec.    training loss:\t\t3.99327238132\n",
      "Done 1280 batches in 225.72 sec.    training loss:\t\t3.99321084488\n",
      "Done 1290 batches in 227.58 sec.    training loss:\t\t3.99295780603\n",
      "Done 1300 batches in 229.07 sec.    training loss:\t\t3.99285140111\n",
      "Done 1310 batches in 230.67 sec.    training loss:\t\t3.99267200277\n",
      "Done 1320 batches in 232.98 sec.    training loss:\t\t3.9926136947\n",
      "Done 1330 batches in 234.55 sec.    training loss:\t\t3.99269524481\n",
      "Done 1340 batches in 236.49 sec.    training loss:\t\t3.99239317125\n",
      "Done 1350 batches in 238.35 sec.    training loss:\t\t3.99233138455\n",
      "Done 1360 batches in 240.38 sec.    training loss:\t\t3.99252531774\n",
      "Done 1370 batches in 242.50 sec.    training loss:\t\t3.99294756259\n",
      "Done 1380 batches in 244.85 sec.    training loss:\t\t3.99303065055\n",
      "Done 1390 batches in 246.69 sec.    training loss:\t\t3.99271061438\n",
      "Done 1400 batches in 248.36 sec.    training loss:\t\t3.99245374169\n",
      "Done 1410 batches in 250.29 sec.    training loss:\t\t3.99241367435\n",
      "Done 1420 batches in 251.97 sec.    training loss:\t\t3.99256341004\n",
      "Done 1430 batches in 253.54 sec.    training loss:\t\t3.99225954926\n",
      "Done 1440 batches in 255.13 sec.    training loss:\t\t3.99211306009\n",
      "Done 1450 batches in 256.93 sec.    training loss:\t\t3.99225378892\n",
      "Done 1460 batches in 258.58 sec.    training loss:\t\t3.99221442628\n",
      "Done 1470 batches in 260.41 sec.    training loss:\t\t3.99216871472\n",
      "Done 1480 batches in 262.28 sec.    training loss:\t\t3.99211340853\n",
      "Done 1490 batches in 263.91 sec.    training loss:\t\t3.99237252898\n",
      "Done 1500 batches in 265.46 sec.    training loss:\t\t3.99266996447\n",
      "Done 1510 batches in 267.57 sec.    training loss:\t\t3.99273272982\n",
      "Done 1520 batches in 269.18 sec.    training loss:\t\t3.99298511289\n",
      "Done 1530 batches in 270.84 sec.    training loss:\t\t3.99285527959\n",
      "Done 1540 batches in 272.52 sec.    training loss:\t\t3.99257119135\n",
      "Done 1550 batches in 274.18 sec.    training loss:\t\t3.99255962972\n",
      "Done 1560 batches in 276.28 sec.    training loss:\t\t3.99253772512\n",
      "Done 1570 batches in 278.42 sec.    training loss:\t\t3.99231752423\n",
      "Done 1580 batches in 280.19 sec.    training loss:\t\t3.99218592387\n",
      "Done 1590 batches in 282.10 sec.    training loss:\t\t3.99226023386\n",
      "Done 1600 batches in 283.92 sec.    training loss:\t\t3.99208866\n",
      "Done 1610 batches in 285.84 sec.    training loss:\t\t3.99204732972\n",
      "Done 1620 batches in 287.64 sec.    training loss:\t\t3.99218876274\n",
      "Done 1630 batches in 289.31 sec.    training loss:\t\t3.99190685778\n",
      "Done 1640 batches in 290.91 sec.    training loss:\t\t3.99161450151\n",
      "Done 1650 batches in 292.66 sec.    training loss:\t\t3.99168307478\n",
      "Done 1660 batches in 294.51 sec.    training loss:\t\t3.99159105255\n",
      "Done 1670 batches in 296.28 sec.    training loss:\t\t3.99148121979\n",
      "Done 1680 batches in 298.70 sec.    training loss:\t\t3.99143058757\n",
      "Done 1690 batches in 300.23 sec.    training loss:\t\t3.99108914525\n",
      "Done 1700 batches in 302.31 sec.    training loss:\t\t3.99076005178\n",
      "Done 1710 batches in 304.04 sec.    training loss:\t\t3.99077355248\n",
      "Done 1720 batches in 306.04 sec.    training loss:\t\t3.99053625756\n",
      "Done 1730 batches in 307.67 sec.    training loss:\t\t3.99041343052\n",
      "Done 1740 batches in 309.17 sec.    training loss:\t\t3.99033371991\n",
      "Done 1750 batches in 310.87 sec.    training loss:\t\t3.99020550891\n",
      "Done 1760 batches in 312.70 sec.    training loss:\t\t3.99011056924\n",
      "Done 1770 batches in 314.17 sec.    training loss:\t\t3.98972093278\n",
      "Done 1780 batches in 315.65 sec.    training loss:\t\t3.98926494068\n",
      "Done 1790 batches in 317.25 sec.    training loss:\t\t3.98904796193\n",
      "Done 1800 batches in 318.97 sec.    training loss:\t\t3.98893794656\n",
      "Done 1810 batches in 320.87 sec.    training loss:\t\t3.98872870582\n",
      "Done 1820 batches in 322.60 sec.    training loss:\t\t3.98839822706\n",
      "Done 1830 batches in 324.19 sec.    training loss:\t\t3.98815914138\n",
      "Done 1840 batches in 326.40 sec.    training loss:\t\t3.98809768389\n",
      "Done 1850 batches in 327.83 sec.    training loss:\t\t3.98786625372\n",
      "Done 1860 batches in 329.52 sec.    training loss:\t\t3.9877856111\n",
      "Done 1870 batches in 331.28 sec.    training loss:\t\t3.98768188303\n",
      "Done 1880 batches in 332.87 sec.    training loss:\t\t3.98758425827\n",
      "Done 1890 batches in 334.88 sec.    training loss:\t\t3.98724373585\n",
      "Done 1900 batches in 336.72 sec.    training loss:\t\t3.98708252455\n",
      "Done 1910 batches in 338.79 sec.    training loss:\t\t3.98708278371\n",
      "Done 1920 batches in 340.94 sec.    training loss:\t\t3.98711860776\n",
      "Done 1930 batches in 342.69 sec.    training loss:\t\t3.98703480817\n",
      "Done 1940 batches in 344.53 sec.    training loss:\t\t3.9868873112\n",
      "Done 1950 batches in 345.83 sec.    training loss:\t\t3.98676033228\n",
      "Done 1960 batches in 347.87 sec.    training loss:\t\t3.98677471852\n",
      "Done 1970 batches in 349.60 sec.    training loss:\t\t3.98660945081\n",
      "Done 1980 batches in 351.62 sec.    training loss:\t\t3.98648301279\n",
      "Done 1990 batches in 353.20 sec.    training loss:\t\t3.98614959333\n",
      "Done 2000 batches in 354.86 sec.    training loss:\t\t3.98601695275\n",
      "Done 2010 batches in 356.59 sec.    training loss:\t\t3.98572633444\n",
      "Done 2020 batches in 358.62 sec.    training loss:\t\t3.98579955455\n",
      "Done 2030 batches in 360.67 sec.    training loss:\t\t3.98586459136\n",
      "Done 2040 batches in 362.41 sec.    training loss:\t\t3.98595021355\n",
      "Done 2050 batches in 364.88 sec.    training loss:\t\t3.98603272194\n",
      "Done 2060 batches in 366.47 sec.    training loss:\t\t3.98568482978\n",
      "Done 2070 batches in 367.90 sec.    training loss:\t\t3.98539770283\n",
      "Done 2080 batches in 369.80 sec.    training loss:\t\t3.98541905685\n",
      "Done 2090 batches in 371.84 sec.    training loss:\t\t3.98538744644\n",
      "Done 2100 batches in 373.76 sec.    training loss:\t\t3.98532126347\n",
      "Done 2110 batches in 375.75 sec.    training loss:\t\t3.98544140947\n",
      "Done 2120 batches in 377.61 sec.    training loss:\t\t3.9852944518\n",
      "Done 2130 batches in 379.22 sec.    training loss:\t\t3.98538519944\n",
      "Done 2140 batches in 380.99 sec.    training loss:\t\t3.98545974415\n",
      "Done 2150 batches in 382.67 sec.    training loss:\t\t3.98528540256\n",
      "Done 2160 batches in 384.33 sec.    training loss:\t\t3.98514983378\n",
      "Done 2170 batches in 385.99 sec.    training loss:\t\t3.98513238672\n",
      "Done 2180 batches in 387.67 sec.    training loss:\t\t3.98495420939\n",
      "Done 2190 batches in 389.93 sec.    training loss:\t\t3.98501437712\n",
      "Done 2200 batches in 391.75 sec.    training loss:\t\t3.98490143603\n",
      "Done 2210 batches in 393.76 sec.    training loss:\t\t3.98494193263\n",
      "Done 2220 batches in 395.57 sec.    training loss:\t\t3.98487541869\n",
      "Done 2230 batches in 397.07 sec.    training loss:\t\t3.98479253123\n",
      "Done 2240 batches in 398.86 sec.    training loss:\t\t3.98472243643\n",
      "Done 2250 batches in 400.52 sec.    training loss:\t\t3.98455344518\n",
      "Done 2260 batches in 402.14 sec.    training loss:\t\t3.98431240457\n",
      "Done 2270 batches in 403.73 sec.    training loss:\t\t3.98408503585\n",
      "Done 2280 batches in 405.41 sec.    training loss:\t\t3.98419072252\n",
      "Done 2290 batches in 407.12 sec.    training loss:\t\t3.98423459509\n",
      "Done 2300 batches in 408.73 sec.    training loss:\t\t3.98390241229\n",
      "Done 2310 batches in 410.60 sec.    training loss:\t\t3.9837896088\n",
      "Done 2320 batches in 411.98 sec.    training loss:\t\t3.98379594231\n",
      "Done 2330 batches in 414.10 sec.    training loss:\t\t3.9835165031\n",
      "Done 2340 batches in 415.61 sec.    training loss:\t\t3.98332282875\n",
      "Done 2350 batches in 418.00 sec.    training loss:\t\t3.98349948163\n",
      "Done 2360 batches in 419.70 sec.    training loss:\t\t3.98345877185\n",
      "Done 2370 batches in 421.27 sec.    training loss:\t\t3.98347790402\n",
      "Done 2380 batches in 422.74 sec.    training loss:\t\t3.98337081891\n",
      "Done 2390 batches in 424.79 sec.    training loss:\t\t3.98323874244\n",
      "Done 2400 batches in 426.61 sec.    training loss:\t\t3.98310976287\n",
      "Done 2410 batches in 428.25 sec.    training loss:\t\t3.98302561722\n",
      "Done 2420 batches in 430.01 sec.    training loss:\t\t3.98306771674\n",
      "Done 2430 batches in 432.15 sec.    training loss:\t\t3.98282237043\n",
      "Done 2440 batches in 433.99 sec.    training loss:\t\t3.98278557473\n",
      "Done 2450 batches in 435.47 sec.    training loss:\t\t3.98273092007\n",
      "Done 2460 batches in 437.31 sec.    training loss:\t\t3.98259064405\n",
      "Done 2470 batches in 439.29 sec.    training loss:\t\t3.98236821731\n",
      "Done 2480 batches in 440.98 sec.    training loss:\t\t3.98230449884\n",
      "Done 2490 batches in 443.09 sec.    training loss:\t\t3.98241785463\n",
      "Done 2500 batches in 444.80 sec.    training loss:\t\t3.98222145758\n",
      "Done 2510 batches in 446.43 sec.    training loss:\t\t3.98199962962\n",
      "Done 2520 batches in 447.97 sec.    training loss:\t\t3.98190208475\n",
      "Done 2530 batches in 449.53 sec.    training loss:\t\t3.9815680518\n",
      "Done 2540 batches in 451.46 sec.    training loss:\t\t3.98163603469\n",
      "Done 2550 batches in 453.08 sec.    training loss:\t\t3.98167738933\n",
      "Done 2560 batches in 455.26 sec.    training loss:\t\t3.98155525718\n",
      "Done 2570 batches in 457.17 sec.    training loss:\t\t3.98135774117\n",
      "Done 2580 batches in 459.06 sec.    training loss:\t\t3.98101537597\n",
      "Done 2590 batches in 460.73 sec.    training loss:\t\t3.98077205645\n",
      "Done 2600 batches in 462.14 sec.    training loss:\t\t3.98069655418\n",
      "Done 2610 batches in 463.99 sec.    training loss:\t\t3.98054946775\n",
      "Done 2620 batches in 466.25 sec.    training loss:\t\t3.9805205155\n",
      "Done 2630 batches in 468.01 sec.    training loss:\t\t3.98046129829\n",
      "Done 2640 batches in 469.50 sec.    training loss:\t\t3.98019800457\n",
      "Done 2650 batches in 471.74 sec.    training loss:\t\t3.98010960831\n",
      "Done 2660 batches in 473.87 sec.    training loss:\t\t3.98002404745\n",
      "Done 2670 batches in 475.38 sec.    training loss:\t\t3.98002845241\n",
      "Done 2680 batches in 477.14 sec.    training loss:\t\t3.97983351288\n",
      "Done 2690 batches in 478.92 sec.    training loss:\t\t3.97971576696\n",
      "Done 2700 batches in 480.61 sec.    training loss:\t\t3.97964996797\n",
      "Done 2710 batches in 482.13 sec.    training loss:\t\t3.97942153044\n",
      "Done 2720 batches in 484.34 sec.    training loss:\t\t3.97944672862\n",
      "Done 2730 batches in 486.48 sec.    training loss:\t\t3.97941508747\n",
      "Done 2740 batches in 488.41 sec.    training loss:\t\t3.9794085855\n",
      "Done 2750 batches in 490.31 sec.    training loss:\t\t3.9794739125\n",
      "Done 2760 batches in 491.89 sec.    training loss:\t\t3.97926968889\n",
      "Done 2770 batches in 493.95 sec.    training loss:\t\t3.97924753423\n",
      "Done 2780 batches in 495.44 sec.    training loss:\t\t3.97928130678\n",
      "Done 2790 batches in 497.44 sec.    training loss:\t\t3.97902411466\n",
      "Done 2800 batches in 499.56 sec.    training loss:\t\t3.9789549229\n",
      "Done 2810 batches in 501.46 sec.    training loss:\t\t3.97895340317\n",
      "Done 2820 batches in 503.11 sec.    training loss:\t\t3.97882170195\n",
      "Done 2830 batches in 504.60 sec.    training loss:\t\t3.97880851102\n",
      "Done 2840 batches in 506.52 sec.    training loss:\t\t3.97893746504\n",
      "Done 2850 batches in 508.06 sec.    training loss:\t\t3.97873160814\n",
      "Done 2860 batches in 509.53 sec.    training loss:\t\t3.97853346238\n",
      "Done 2870 batches in 511.19 sec.    training loss:\t\t3.97841786699\n",
      "Done 2880 batches in 512.74 sec.    training loss:\t\t3.97827512241\n",
      "Done 2890 batches in 514.36 sec.    training loss:\t\t3.97813810312\n",
      "Done 2900 batches in 515.97 sec.    training loss:\t\t3.97810324669\n",
      "Done 2910 batches in 517.88 sec.    training loss:\t\t3.97783723107\n",
      "Done 2920 batches in 519.85 sec.    training loss:\t\t3.97777743364\n",
      "Done 2930 batches in 521.19 sec.    training loss:\t\t3.97751142238\n",
      "Done 2940 batches in 523.01 sec.    training loss:\t\t3.97739151578\n",
      "Done 2950 batches in 524.83 sec.    training loss:\t\t3.97723316952\n",
      "Done 2960 batches in 526.34 sec.    training loss:\t\t3.97708900192\n",
      "Done 2970 batches in 528.10 sec.    training loss:\t\t3.97686688121\n",
      "Done 2980 batches in 529.65 sec.    training loss:\t\t3.97677187696\n",
      "Done 2990 batches in 531.16 sec.    training loss:\t\t3.9767510765\n",
      "Done 3000 batches in 533.20 sec.    training loss:\t\t3.976631332\n",
      "Done 3010 batches in 535.25 sec.    training loss:\t\t3.9766496907\n",
      "Done 3020 batches in 536.70 sec.    training loss:\t\t3.97637498355\n",
      "Done 3030 batches in 538.29 sec.    training loss:\t\t3.97638791717\n",
      "Done 3040 batches in 540.57 sec.    training loss:\t\t3.9765582814\n",
      "Done 3050 batches in 542.40 sec.    training loss:\t\t3.97647600471\n",
      "Done 3060 batches in 544.15 sec.    training loss:\t\t3.97632929422\n",
      "Done 3070 batches in 546.11 sec.    training loss:\t\t3.97620733739\n",
      "Done 3080 batches in 547.63 sec.    training loss:\t\t3.97605483973\n",
      "Done 3090 batches in 549.19 sec.    training loss:\t\t3.97601917137\n",
      "Done 3100 batches in 550.87 sec.    training loss:\t\t3.9760831886\n",
      "Done 3110 batches in 552.59 sec.    training loss:\t\t3.97585099157\n",
      "Done 3120 batches in 555.03 sec.    training loss:\t\t3.97586974326\n",
      "Done 3130 batches in 557.21 sec.    training loss:\t\t3.97581310661\n",
      "Done 3140 batches in 559.30 sec.    training loss:\t\t3.97576542295\n",
      "Done 3150 batches in 561.11 sec.    training loss:\t\t3.97563004978\n",
      "Done 3160 batches in 563.04 sec.    training loss:\t\t3.97553515087\n",
      "Done 3170 batches in 564.49 sec.    training loss:\t\t3.97553753853\n",
      "Done 3180 batches in 566.18 sec.    training loss:\t\t3.97554450717\n",
      "Done 3190 batches in 568.46 sec.    training loss:\t\t3.97550345095\n",
      "Done 3200 batches in 570.20 sec.    training loss:\t\t3.97515890062\n",
      "Done 3210 batches in 571.98 sec.    training loss:\t\t3.97502847505\n",
      "Done 3220 batches in 573.62 sec.    training loss:\t\t3.97513642918\n",
      "Done 3230 batches in 575.38 sec.    training loss:\t\t3.97486877168\n",
      "Done 3240 batches in 577.59 sec.    training loss:\t\t3.97479877273\n",
      "Done 3250 batches in 579.47 sec.    training loss:\t\t3.97480957457\n",
      "Done 3260 batches in 581.28 sec.    training loss:\t\t3.97485283211\n",
      "Done 3270 batches in 583.52 sec.    training loss:\t\t3.97486466275\n",
      "Done 3280 batches in 585.35 sec.    training loss:\t\t3.97467851079\n",
      "Done 3290 batches in 587.35 sec.    training loss:\t\t3.97461887529\n",
      "Done 3300 batches in 589.41 sec.    training loss:\t\t3.97449866317\n",
      "Done 3310 batches in 591.29 sec.    training loss:\t\t3.97442225164\n",
      "Done 3320 batches in 592.85 sec.    training loss:\t\t3.9743408155\n",
      "Done 3330 batches in 594.54 sec.    training loss:\t\t3.97437687923\n",
      "Done 3340 batches in 596.05 sec.    training loss:\t\t3.97408777104\n",
      "Done 3350 batches in 597.69 sec.    training loss:\t\t3.97410695268\n",
      "Done 3360 batches in 599.56 sec.    training loss:\t\t3.97409420049\n",
      "Done 3370 batches in 601.35 sec.    training loss:\t\t3.97420481426\n",
      "Done 3380 batches in 603.03 sec.    training loss:\t\t3.97412862037\n",
      "Done 3390 batches in 604.79 sec.    training loss:\t\t3.97406168005\n",
      "Done 3400 batches in 606.46 sec.    training loss:\t\t3.97392864178\n",
      "Done 3410 batches in 608.33 sec.    training loss:\t\t3.97392002068\n",
      "Done 3420 batches in 610.02 sec.    training loss:\t\t3.97378106103\n",
      "Done 3430 batches in 611.48 sec.    training loss:\t\t3.97347950456\n",
      "Done 3440 batches in 612.94 sec.    training loss:\t\t3.97324086095\n",
      "Done 3450 batches in 614.85 sec.    training loss:\t\t3.97322958193\n",
      "Done 3460 batches in 616.76 sec.    training loss:\t\t3.97312589229\n",
      "Done 3470 batches in 618.60 sec.    training loss:\t\t3.97309224963\n",
      "Done 3480 batches in 620.12 sec.    training loss:\t\t3.97292358834\n",
      "Done 3490 batches in 621.63 sec.    training loss:\t\t3.97264094435\n",
      "Done 3500 batches in 623.34 sec.    training loss:\t\t3.97253154285\n",
      "Done 3510 batches in 625.14 sec.    training loss:\t\t3.97253569612\n",
      "Done 3520 batches in 627.22 sec.    training loss:\t\t3.97238374007\n",
      "Done 3530 batches in 629.67 sec.    training loss:\t\t3.97234770448\n",
      "Done 3540 batches in 631.47 sec.    training loss:\t\t3.97228879619\n",
      "Done 3550 batches in 632.91 sec.    training loss:\t\t3.97228627742\n",
      "Done 3560 batches in 634.65 sec.    training loss:\t\t3.97227086847\n",
      "Done 3570 batches in 636.30 sec.    training loss:\t\t3.97214830109\n",
      "Done 3580 batches in 637.91 sec.    training loss:\t\t3.97219460317\n",
      "Done 3590 batches in 639.58 sec.    training loss:\t\t3.97214310827\n",
      "Done 3600 batches in 641.20 sec.    training loss:\t\t3.97217472547\n",
      "Done 3610 batches in 643.00 sec.    training loss:\t\t3.97219566718\n",
      "Done 3620 batches in 645.04 sec.    training loss:\t\t3.97208329657\n",
      "Done 3630 batches in 646.65 sec.    training loss:\t\t3.97196987063\n",
      "Done 3640 batches in 648.21 sec.    training loss:\t\t3.97191957831\n",
      "Done 3650 batches in 649.77 sec.    training loss:\t\t3.97173808679\n",
      "Done 3660 batches in 651.59 sec.    training loss:\t\t3.97169114716\n",
      "Done 3670 batches in 653.74 sec.    training loss:\t\t3.97167288568\n",
      "Done 3680 batches in 655.49 sec.    training loss:\t\t3.97177514201\n",
      "Done 3690 batches in 657.08 sec.    training loss:\t\t3.97166213776\n",
      "Done 3700 batches in 658.45 sec.    training loss:\t\t3.97143639017\n",
      "Done 3710 batches in 659.97 sec.    training loss:\t\t3.97139626398\n",
      "Done 3720 batches in 661.88 sec.    training loss:\t\t3.97128741767\n",
      "Done 3730 batches in 663.33 sec.    training loss:\t\t3.97124134906\n",
      "Done 3740 batches in 665.03 sec.    training loss:\t\t3.97127025733\n",
      "Done 3750 batches in 666.92 sec.    training loss:\t\t3.97109407641\n",
      "Done 3760 batches in 668.33 sec.    training loss:\t\t3.97093443699\n",
      "Done 3770 batches in 670.04 sec.    training loss:\t\t3.97082372736\n",
      "Done 3780 batches in 671.58 sec.    training loss:\t\t3.97067426519\n",
      "Done 3790 batches in 674.04 sec.    training loss:\t\t3.97071282958\n",
      "Done 3800 batches in 675.73 sec.    training loss:\t\t3.97060444029\n",
      "Done 3810 batches in 677.28 sec.    training loss:\t\t3.97048882199\n",
      "Done 3820 batches in 679.02 sec.    training loss:\t\t3.97038560744\n",
      "Done 3830 batches in 680.40 sec.    training loss:\t\t3.97034843615\n",
      "Done 3840 batches in 682.39 sec.    training loss:\t\t3.970377997\n",
      "Done 3850 batches in 684.01 sec.    training loss:\t\t3.9702418624\n",
      "Done 3860 batches in 685.62 sec.    training loss:\t\t3.97013579033\n",
      "Done 3870 batches in 687.72 sec.    training loss:\t\t3.96998032562\n",
      "Done 3880 batches in 689.38 sec.    training loss:\t\t3.96987521968\n",
      "Done 3890 batches in 691.02 sec.    training loss:\t\t3.96972339288\n",
      "Done 3900 batches in 692.54 sec.    training loss:\t\t3.96962594931\n",
      "Done 3910 batches in 694.42 sec.    training loss:\t\t3.96958823692\n",
      "Done 3920 batches in 696.18 sec.    training loss:\t\t3.96947151745\n",
      "Epoch 3 of 5 took 712.503s\n",
      "  training loss:\t\t3.969382\n",
      "  validation loss:\t\t3.985306\n",
      "Done 10 batches in 1.78 sec.    training loss:\t\t3.94646530151\n",
      "Done 20 batches in 3.50 sec.    training loss:\t\t3.93894754648\n",
      "Done 30 batches in 5.21 sec.    training loss:\t\t3.92807852427\n",
      "Done 40 batches in 7.21 sec.    training loss:\t\t3.93529916406\n",
      "Done 50 batches in 9.38 sec.    training loss:\t\t3.94005037308\n",
      "Done 60 batches in 11.29 sec.    training loss:\t\t3.93451761405\n",
      "Done 70 batches in 13.17 sec.    training loss:\t\t3.93698566301\n",
      "Done 80 batches in 15.37 sec.    training loss:\t\t3.9359092623\n",
      "Done 90 batches in 17.00 sec.    training loss:\t\t3.93706380791\n",
      "Done 100 batches in 18.75 sec.    training loss:\t\t3.9370241642\n",
      "Done 110 batches in 20.38 sec.    training loss:\t\t3.93549934301\n",
      "Done 120 batches in 22.25 sec.    training loss:\t\t3.9351558884\n",
      "Done 130 batches in 24.34 sec.    training loss:\t\t3.93777653804\n",
      "Done 140 batches in 25.88 sec.    training loss:\t\t3.94072229862\n",
      "Done 150 batches in 27.74 sec.    training loss:\t\t3.93828144073\n",
      "Done 160 batches in 29.36 sec.    training loss:\t\t3.93980669379\n",
      "Done 170 batches in 31.24 sec.    training loss:\t\t3.93825572939\n",
      "Done 180 batches in 33.25 sec.    training loss:\t\t3.93719965882\n",
      "Done 190 batches in 34.98 sec.    training loss:\t\t3.93819592752\n",
      "Done 200 batches in 36.38 sec.    training loss:\t\t3.93504564166\n",
      "Done 210 batches in 38.31 sec.    training loss:\t\t3.93671982856\n",
      "Done 220 batches in 40.04 sec.    training loss:\t\t3.93764262416\n",
      "Done 230 batches in 41.70 sec.    training loss:\t\t3.93595294745\n",
      "Done 240 batches in 43.90 sec.    training loss:\t\t3.93714614709\n",
      "Done 250 batches in 45.42 sec.    training loss:\t\t3.93679362965\n",
      "Done 260 batches in 47.21 sec.    training loss:\t\t3.93813681327\n",
      "Done 270 batches in 48.88 sec.    training loss:\t\t3.9371687324\n",
      "Done 280 batches in 50.58 sec.    training loss:\t\t3.93786716035\n",
      "Done 290 batches in 52.37 sec.    training loss:\t\t3.93734445819\n",
      "Done 300 batches in 54.34 sec.    training loss:\t\t3.93712897698\n",
      "Done 310 batches in 56.47 sec.    training loss:\t\t3.93693189467\n",
      "Done 320 batches in 58.36 sec.    training loss:\t\t3.93764032573\n",
      "Done 330 batches in 60.03 sec.    training loss:\t\t3.93787950458\n",
      "Done 340 batches in 61.72 sec.    training loss:\t\t3.93817269381\n",
      "Done 350 batches in 63.34 sec.    training loss:\t\t3.93729528768\n",
      "Done 360 batches in 64.89 sec.    training loss:\t\t3.93696788549\n",
      "Done 370 batches in 66.42 sec.    training loss:\t\t3.93654383389\n",
      "Done 380 batches in 67.83 sec.    training loss:\t\t3.93649427452\n",
      "Done 390 batches in 69.38 sec.    training loss:\t\t3.93667144592\n",
      "Done 400 batches in 71.17 sec.    training loss:\t\t3.93775305986\n",
      "Done 410 batches in 73.16 sec.    training loss:\t\t3.93621581706\n",
      "Done 420 batches in 74.95 sec.    training loss:\t\t3.93700095415\n",
      "Done 430 batches in 76.93 sec.    training loss:\t\t3.93695174372\n",
      "Done 440 batches in 78.46 sec.    training loss:\t\t3.93628725951\n",
      "Done 450 batches in 79.98 sec.    training loss:\t\t3.93590084553\n",
      "Done 460 batches in 81.39 sec.    training loss:\t\t3.93698462352\n",
      "Done 470 batches in 82.73 sec.    training loss:\t\t3.9364882038\n",
      "Done 480 batches in 84.19 sec.    training loss:\t\t3.93620053828\n",
      "Done 490 batches in 86.26 sec.    training loss:\t\t3.93662890415\n",
      "Done 500 batches in 88.03 sec.    training loss:\t\t3.93711945391\n",
      "Done 510 batches in 89.63 sec.    training loss:\t\t3.93736122449\n",
      "Done 520 batches in 91.17 sec.    training loss:\t\t3.93758165653\n",
      "Done 530 batches in 92.81 sec.    training loss:\t\t3.937633971\n",
      "Done 540 batches in 94.55 sec.    training loss:\t\t3.93786224259\n",
      "Done 550 batches in 96.20 sec.    training loss:\t\t3.9374527307\n",
      "Done 560 batches in 98.01 sec.    training loss:\t\t3.93866217903\n",
      "Done 570 batches in 99.76 sec.    training loss:\t\t3.93808970786\n",
      "Done 580 batches in 102.39 sec.    training loss:\t\t3.93827199114\n",
      "Done 590 batches in 104.33 sec.    training loss:\t\t3.9376552311\n",
      "Done 600 batches in 106.07 sec.    training loss:\t\t3.93793842554\n",
      "Done 610 batches in 107.71 sec.    training loss:\t\t3.93729128642\n",
      "Done 620 batches in 109.58 sec.    training loss:\t\t3.93700885042\n",
      "Done 630 batches in 111.18 sec.    training loss:\t\t3.93755513524\n",
      "Done 640 batches in 112.91 sec.    training loss:\t\t3.93735296428\n",
      "Done 650 batches in 114.89 sec.    training loss:\t\t3.93729039962\n",
      "Done 660 batches in 116.49 sec.    training loss:\t\t3.93701034098\n",
      "Done 670 batches in 118.23 sec.    training loss:\t\t3.93719831936\n",
      "Done 680 batches in 120.07 sec.    training loss:\t\t3.9361665014\n",
      "Done 690 batches in 121.66 sec.    training loss:\t\t3.93629687904\n",
      "Done 700 batches in 123.48 sec.    training loss:\t\t3.9372723014\n",
      "Done 710 batches in 125.30 sec.    training loss:\t\t3.93791205715\n",
      "Done 720 batches in 127.15 sec.    training loss:\t\t3.93774936166\n",
      "Done 730 batches in 128.88 sec.    training loss:\t\t3.93751712335\n",
      "Done 740 batches in 130.56 sec.    training loss:\t\t3.93707641363\n",
      "Done 750 batches in 132.18 sec.    training loss:\t\t3.93620331478\n",
      "Done 760 batches in 133.75 sec.    training loss:\t\t3.9362357218\n",
      "Done 770 batches in 135.12 sec.    training loss:\t\t3.93596272747\n",
      "Done 780 batches in 136.75 sec.    training loss:\t\t3.93545803932\n",
      "Done 790 batches in 138.37 sec.    training loss:\t\t3.93496134583\n",
      "Done 800 batches in 140.14 sec.    training loss:\t\t3.93565714449\n",
      "Done 810 batches in 141.95 sec.    training loss:\t\t3.93624093621\n",
      "Done 820 batches in 143.78 sec.    training loss:\t\t3.93599250113\n",
      "Done 830 batches in 145.68 sec.    training loss:\t\t3.93563435796\n",
      "Done 840 batches in 147.23 sec.    training loss:\t\t3.93521606496\n",
      "Done 850 batches in 149.24 sec.    training loss:\t\t3.93542527535\n",
      "Done 860 batches in 151.07 sec.    training loss:\t\t3.93532718725\n",
      "Done 870 batches in 152.72 sec.    training loss:\t\t3.93458013233\n",
      "Done 880 batches in 154.43 sec.    training loss:\t\t3.93456909629\n",
      "Done 890 batches in 156.44 sec.    training loss:\t\t3.93458916096\n",
      "Done 900 batches in 158.27 sec.    training loss:\t\t3.93503172106\n",
      "Done 910 batches in 160.22 sec.    training loss:\t\t3.93550546955\n",
      "Done 920 batches in 161.85 sec.    training loss:\t\t3.93522439469\n",
      "Done 930 batches in 163.76 sec.    training loss:\t\t3.93491701541\n",
      "Done 940 batches in 165.43 sec.    training loss:\t\t3.93520936433\n",
      "Done 950 batches in 167.56 sec.    training loss:\t\t3.93541921766\n",
      "Done 960 batches in 169.07 sec.    training loss:\t\t3.93500038137\n",
      "Done 970 batches in 170.83 sec.    training loss:\t\t3.93470634677\n",
      "Done 980 batches in 172.42 sec.    training loss:\t\t3.93479598542\n",
      "Done 990 batches in 174.13 sec.    training loss:\t\t3.93460811172\n",
      "Done 1000 batches in 175.89 sec.    training loss:\t\t3.93412336087\n",
      "Done 1010 batches in 177.55 sec.    training loss:\t\t3.93369739622\n",
      "Done 1020 batches in 179.80 sec.    training loss:\t\t3.93375402292\n",
      "Done 1030 batches in 181.36 sec.    training loss:\t\t3.93358823674\n",
      "Done 1040 batches in 183.31 sec.    training loss:\t\t3.93414597695\n",
      "Done 1050 batches in 184.94 sec.    training loss:\t\t3.93382199083\n",
      "Done 1060 batches in 186.51 sec.    training loss:\t\t3.93389691542\n",
      "Done 1070 batches in 188.66 sec.    training loss:\t\t3.93355544237\n",
      "Done 1080 batches in 190.25 sec.    training loss:\t\t3.93326211748\n",
      "Done 1090 batches in 192.03 sec.    training loss:\t\t3.93332789688\n",
      "Done 1100 batches in 193.56 sec.    training loss:\t\t3.93319496198\n",
      "Done 1110 batches in 195.38 sec.    training loss:\t\t3.93325085361\n",
      "Done 1120 batches in 197.21 sec.    training loss:\t\t3.93313733786\n",
      "Done 1130 batches in 198.91 sec.    training loss:\t\t3.9332192115\n",
      "Done 1140 batches in 200.48 sec.    training loss:\t\t3.93300556718\n",
      "Done 1150 batches in 202.10 sec.    training loss:\t\t3.93341539072\n",
      "Done 1160 batches in 203.85 sec.    training loss:\t\t3.93336156339\n",
      "Done 1170 batches in 205.87 sec.    training loss:\t\t3.93318529027\n",
      "Done 1180 batches in 207.63 sec.    training loss:\t\t3.93306286153\n",
      "Done 1190 batches in 209.73 sec.    training loss:\t\t3.93300213734\n",
      "Done 1200 batches in 211.17 sec.    training loss:\t\t3.93276342789\n",
      "Done 1210 batches in 212.80 sec.    training loss:\t\t3.93270888427\n",
      "Done 1220 batches in 214.34 sec.    training loss:\t\t3.93252100358\n",
      "Done 1230 batches in 216.63 sec.    training loss:\t\t3.93306998334\n",
      "Done 1240 batches in 218.45 sec.    training loss:\t\t3.93315285348\n",
      "Done 1250 batches in 220.41 sec.    training loss:\t\t3.93286287537\n",
      "Done 1260 batches in 222.20 sec.    training loss:\t\t3.93294307183\n",
      "Done 1270 batches in 224.01 sec.    training loss:\t\t3.93315667036\n",
      "Done 1280 batches in 225.72 sec.    training loss:\t\t3.93313282281\n",
      "Done 1290 batches in 227.58 sec.    training loss:\t\t3.93291793812\n",
      "Done 1300 batches in 229.07 sec.    training loss:\t\t3.93284490567\n",
      "Done 1310 batches in 230.67 sec.    training loss:\t\t3.93269694761\n",
      "Done 1320 batches in 232.98 sec.    training loss:\t\t3.93266997084\n",
      "Done 1330 batches in 234.55 sec.    training loss:\t\t3.93278501733\n",
      "Done 1340 batches in 236.49 sec.    training loss:\t\t3.93251581993\n",
      "Done 1350 batches in 238.35 sec.    training loss:\t\t3.93249224521\n",
      "Done 1360 batches in 240.38 sec.    training loss:\t\t3.93270699242\n",
      "Done 1370 batches in 242.49 sec.    training loss:\t\t3.93316214659\n",
      "Done 1380 batches in 244.84 sec.    training loss:\t\t3.93327493011\n",
      "Done 1390 batches in 246.69 sec.    training loss:\t\t3.93299161153\n",
      "Done 1400 batches in 248.36 sec.    training loss:\t\t3.93277136786\n",
      "Done 1410 batches in 250.29 sec.    training loss:\t\t3.93277220895\n",
      "Done 1420 batches in 251.97 sec.    training loss:\t\t3.9329612559\n",
      "Done 1430 batches in 253.54 sec.    training loss:\t\t3.93269145122\n",
      "Done 1440 batches in 255.12 sec.    training loss:\t\t3.93256011506\n",
      "Done 1450 batches in 256.93 sec.    training loss:\t\t3.93272962077\n",
      "Done 1460 batches in 258.57 sec.    training loss:\t\t3.93273438395\n",
      "Done 1470 batches in 260.41 sec.    training loss:\t\t3.9327285958\n",
      "Done 1480 batches in 262.28 sec.    training loss:\t\t3.9327191791\n",
      "Done 1490 batches in 263.91 sec.    training loss:\t\t3.933005969\n",
      "Done 1500 batches in 265.47 sec.    training loss:\t\t3.93334080108\n",
      "Done 1510 batches in 267.58 sec.    training loss:\t\t3.93341447297\n",
      "Done 1520 batches in 269.18 sec.    training loss:\t\t3.9337072443\n",
      "Done 1530 batches in 270.85 sec.    training loss:\t\t3.93361128234\n",
      "Done 1540 batches in 272.52 sec.    training loss:\t\t3.93336796203\n",
      "Done 1550 batches in 274.19 sec.    training loss:\t\t3.93340123469\n",
      "Done 1560 batches in 276.29 sec.    training loss:\t\t3.93341583396\n",
      "Done 1570 batches in 278.42 sec.    training loss:\t\t3.9332323861\n",
      "Done 1580 batches in 280.20 sec.    training loss:\t\t3.9331361588\n",
      "Done 1590 batches in 282.11 sec.    training loss:\t\t3.93325413488\n",
      "Done 1600 batches in 283.93 sec.    training loss:\t\t3.93312728405\n",
      "Done 1610 batches in 285.84 sec.    training loss:\t\t3.93311559517\n",
      "Done 1620 batches in 287.65 sec.    training loss:\t\t3.93330499361\n",
      "Done 1630 batches in 289.32 sec.    training loss:\t\t3.93306377973\n",
      "Done 1640 batches in 290.92 sec.    training loss:\t\t3.93281211126\n",
      "Done 1650 batches in 292.67 sec.    training loss:\t\t3.93291049784\n",
      "Done 1660 batches in 294.52 sec.    training loss:\t\t3.93284344328\n",
      "Done 1670 batches in 296.29 sec.    training loss:\t\t3.93276515578\n",
      "Done 1680 batches in 298.71 sec.    training loss:\t\t3.93273857548\n",
      "Done 1690 batches in 300.25 sec.    training loss:\t\t3.93243458398\n",
      "Done 1700 batches in 302.32 sec.    training loss:\t\t3.93215072786\n",
      "Done 1710 batches in 304.05 sec.    training loss:\t\t3.93220263378\n",
      "Done 1720 batches in 306.05 sec.    training loss:\t\t3.93199909038\n",
      "Done 1730 batches in 307.69 sec.    training loss:\t\t3.93190799862\n",
      "Done 1740 batches in 309.19 sec.    training loss:\t\t3.93186510897\n",
      "Done 1750 batches in 310.89 sec.    training loss:\t\t3.93176769979\n",
      "Done 1760 batches in 312.72 sec.    training loss:\t\t3.93169965026\n",
      "Done 1770 batches in 314.18 sec.    training loss:\t\t3.93135257953\n",
      "Done 1780 batches in 315.67 sec.    training loss:\t\t3.93092525153\n",
      "Done 1790 batches in 317.27 sec.    training loss:\t\t3.9307394113\n",
      "Done 1800 batches in 318.99 sec.    training loss:\t\t3.93066464967\n",
      "Done 1810 batches in 320.88 sec.    training loss:\t\t3.93049228152\n",
      "Done 1820 batches in 322.61 sec.    training loss:\t\t3.93019107017\n",
      "Done 1830 batches in 324.21 sec.    training loss:\t\t3.92998527412\n",
      "Done 1840 batches in 326.41 sec.    training loss:\t\t3.92996060291\n",
      "Done 1850 batches in 327.84 sec.    training loss:\t\t3.92975019133\n",
      "Done 1860 batches in 329.53 sec.    training loss:\t\t3.92970350801\n",
      "Done 1870 batches in 331.29 sec.    training loss:\t\t3.9296355041\n",
      "Done 1880 batches in 332.88 sec.    training loss:\t\t3.92957261027\n",
      "Done 1890 batches in 334.89 sec.    training loss:\t\t3.92927291898\n",
      "Done 1900 batches in 336.73 sec.    training loss:\t\t3.92914736258\n",
      "Done 1910 batches in 338.81 sec.    training loss:\t\t3.92917434458\n",
      "Done 1920 batches in 340.96 sec.    training loss:\t\t3.92923835653\n",
      "Done 1930 batches in 342.71 sec.    training loss:\t\t3.92919103506\n",
      "Done 1940 batches in 344.55 sec.    training loss:\t\t3.92907235672\n",
      "Done 1950 batches in 345.85 sec.    training loss:\t\t3.9289659461\n",
      "Done 1960 batches in 347.89 sec.    training loss:\t\t3.92901868845\n",
      "Done 1970 batches in 349.62 sec.    training loss:\t\t3.92888184695\n",
      "Done 1980 batches in 351.64 sec.    training loss:\t\t3.92879091284\n",
      "Done 1990 batches in 353.22 sec.    training loss:\t\t3.92848682475\n",
      "Done 2000 batches in 354.88 sec.    training loss:\t\t3.92839221323\n",
      "Done 2010 batches in 356.61 sec.    training loss:\t\t3.92813714416\n",
      "Done 2020 batches in 358.64 sec.    training loss:\t\t3.92823376101\n",
      "Done 2030 batches in 360.69 sec.    training loss:\t\t3.92832942655\n",
      "Done 2040 batches in 362.42 sec.    training loss:\t\t3.92844807295\n",
      "Done 2050 batches in 364.90 sec.    training loss:\t\t3.92855947669\n",
      "Done 2060 batches in 366.49 sec.    training loss:\t\t3.92824362898\n",
      "Done 2070 batches in 367.92 sec.    training loss:\t\t3.92798194413\n",
      "Done 2080 batches in 369.82 sec.    training loss:\t\t3.92803355318\n",
      "Done 2090 batches in 371.86 sec.    training loss:\t\t3.92802904325\n",
      "Done 2100 batches in 373.78 sec.    training loss:\t\t3.92799813952\n",
      "Done 2110 batches in 375.77 sec.    training loss:\t\t3.92814668321\n",
      "Done 2120 batches in 377.63 sec.    training loss:\t\t3.92803183108\n",
      "Done 2130 batches in 379.24 sec.    training loss:\t\t3.92814776338\n",
      "Done 2140 batches in 381.01 sec.    training loss:\t\t3.92825050889\n",
      "Done 2150 batches in 382.69 sec.    training loss:\t\t3.92811265291\n",
      "Done 2160 batches in 384.35 sec.    training loss:\t\t3.92800915749\n",
      "Done 2170 batches in 386.00 sec.    training loss:\t\t3.92802094556\n",
      "Done 2180 batches in 387.69 sec.    training loss:\t\t3.92787758199\n",
      "Done 2190 batches in 389.95 sec.    training loss:\t\t3.92797127608\n",
      "Done 2200 batches in 391.78 sec.    training loss:\t\t3.92788706324\n",
      "Done 2210 batches in 393.79 sec.    training loss:\t\t3.92795694414\n",
      "Done 2220 batches in 395.59 sec.    training loss:\t\t3.92791253554\n",
      "Done 2230 batches in 397.09 sec.    training loss:\t\t3.92785673783\n",
      "Done 2240 batches in 398.89 sec.    training loss:\t\t3.9278235924\n",
      "Done 2250 batches in 400.54 sec.    training loss:\t\t3.92768333181\n",
      "Done 2260 batches in 402.16 sec.    training loss:\t\t3.9274641901\n",
      "Done 2270 batches in 403.75 sec.    training loss:\t\t3.92726745574\n",
      "Done 2280 batches in 405.43 sec.    training loss:\t\t3.92740109061\n",
      "Done 2290 batches in 407.14 sec.    training loss:\t\t3.92747528261\n",
      "Done 2300 batches in 408.75 sec.    training loss:\t\t3.92717310076\n",
      "Done 2310 batches in 410.62 sec.    training loss:\t\t3.92708285402\n",
      "Done 2320 batches in 412.00 sec.    training loss:\t\t3.92712143516\n",
      "Done 2330 batches in 414.13 sec.    training loss:\t\t3.92686787347\n",
      "Done 2340 batches in 415.63 sec.    training loss:\t\t3.92670218639\n",
      "Done 2350 batches in 418.02 sec.    training loss:\t\t3.92690125638\n",
      "Done 2360 batches in 419.72 sec.    training loss:\t\t3.9268881034\n",
      "Done 2370 batches in 421.30 sec.    training loss:\t\t3.92693278347\n",
      "Done 2380 batches in 422.76 sec.    training loss:\t\t3.92685075307\n",
      "Done 2390 batches in 424.81 sec.    training loss:\t\t3.92675707221\n",
      "Done 2400 batches in 426.64 sec.    training loss:\t\t3.92665546676\n",
      "Done 2410 batches in 428.28 sec.    training loss:\t\t3.92659868066\n",
      "Done 2420 batches in 430.04 sec.    training loss:\t\t3.92666476581\n",
      "Done 2430 batches in 432.19 sec.    training loss:\t\t3.92644871253\n",
      "Done 2440 batches in 434.02 sec.    training loss:\t\t3.92644563904\n",
      "Done 2450 batches in 435.51 sec.    training loss:\t\t3.926415542\n",
      "Done 2460 batches in 437.34 sec.    training loss:\t\t3.92631026216\n",
      "Done 2470 batches in 439.32 sec.    training loss:\t\t3.92612391609\n",
      "Done 2480 batches in 441.01 sec.    training loss:\t\t3.92608629955\n",
      "Done 2490 batches in 443.12 sec.    training loss:\t\t3.92622683422\n",
      "Done 2500 batches in 444.83 sec.    training loss:\t\t3.92605820532\n",
      "Done 2510 batches in 446.46 sec.    training loss:\t\t3.9258631123\n",
      "Done 2520 batches in 448.00 sec.    training loss:\t\t3.92579666244\n",
      "Done 2530 batches in 449.56 sec.    training loss:\t\t3.92548300033\n",
      "Done 2540 batches in 451.49 sec.    training loss:\t\t3.92558287279\n",
      "Done 2550 batches in 453.11 sec.    training loss:\t\t3.92565220188\n",
      "Done 2560 batches in 455.28 sec.    training loss:\t\t3.92555937609\n",
      "Done 2570 batches in 457.20 sec.    training loss:\t\t3.92538999593\n",
      "Done 2580 batches in 459.09 sec.    training loss:\t\t3.92507852556\n",
      "Done 2590 batches in 460.75 sec.    training loss:\t\t3.92486084539\n",
      "Done 2600 batches in 462.16 sec.    training loss:\t\t3.92482158477\n",
      "Done 2610 batches in 464.02 sec.    training loss:\t\t3.92470479075\n",
      "Done 2620 batches in 466.28 sec.    training loss:\t\t3.9247007896\n",
      "Done 2630 batches in 468.04 sec.    training loss:\t\t3.92466537436\n",
      "Done 2640 batches in 469.52 sec.    training loss:\t\t3.92443367044\n",
      "Done 2650 batches in 471.77 sec.    training loss:\t\t3.92437086879\n",
      "Done 2660 batches in 473.90 sec.    training loss:\t\t3.92431329426\n",
      "Done 2670 batches in 475.41 sec.    training loss:\t\t3.92434407834\n",
      "Done 2680 batches in 477.18 sec.    training loss:\t\t3.92417327441\n",
      "Done 2690 batches in 478.95 sec.    training loss:\t\t3.92408806391\n",
      "Done 2700 batches in 480.64 sec.    training loss:\t\t3.92405144806\n",
      "Done 2710 batches in 482.17 sec.    training loss:\t\t3.92384721415\n",
      "Done 2720 batches in 484.38 sec.    training loss:\t\t3.92389221051\n",
      "Done 2730 batches in 486.51 sec.    training loss:\t\t3.92388614604\n",
      "Done 2740 batches in 488.45 sec.    training loss:\t\t3.92389535408\n",
      "Done 2750 batches in 490.34 sec.    training loss:\t\t3.92398585493\n",
      "Done 2760 batches in 491.92 sec.    training loss:\t\t3.92380969913\n",
      "Done 2770 batches in 493.98 sec.    training loss:\t\t3.92381289935\n",
      "Done 2780 batches in 495.47 sec.    training loss:\t\t3.9238713117\n",
      "Done 2790 batches in 497.47 sec.    training loss:\t\t3.92364625897\n",
      "Done 2800 batches in 499.59 sec.    training loss:\t\t3.9235977705\n",
      "Done 2810 batches in 501.49 sec.    training loss:\t\t3.92362220695\n",
      "Done 2820 batches in 503.13 sec.    training loss:\t\t3.92351658945\n",
      "Done 2830 batches in 504.62 sec.    training loss:\t\t3.9235296246\n",
      "Done 2840 batches in 506.54 sec.    training loss:\t\t3.9236852546\n",
      "Done 2850 batches in 508.08 sec.    training loss:\t\t3.92350626435\n",
      "Done 2860 batches in 509.55 sec.    training loss:\t\t3.92332848569\n",
      "Done 2870 batches in 511.21 sec.    training loss:\t\t3.92323537122\n",
      "Done 2880 batches in 512.76 sec.    training loss:\t\t3.9231179345\n",
      "Done 2890 batches in 514.38 sec.    training loss:\t\t3.9230002739\n",
      "Done 2900 batches in 515.99 sec.    training loss:\t\t3.92298851926\n",
      "Done 2910 batches in 517.90 sec.    training loss:\t\t3.92275214474\n",
      "Done 2920 batches in 519.87 sec.    training loss:\t\t3.92272032769\n",
      "Done 2930 batches in 521.21 sec.    training loss:\t\t3.92248102886\n",
      "Done 2940 batches in 523.02 sec.    training loss:\t\t3.92238706748\n",
      "Done 2950 batches in 524.85 sec.    training loss:\t\t3.92225657253\n",
      "Done 2960 batches in 526.36 sec.    training loss:\t\t3.92214241696\n",
      "Done 2970 batches in 528.11 sec.    training loss:\t\t3.92194794276\n",
      "Done 2980 batches in 529.66 sec.    training loss:\t\t3.92187714137\n",
      "Done 2990 batches in 531.17 sec.    training loss:\t\t3.92188536435\n",
      "Done 3000 batches in 533.21 sec.    training loss:\t\t3.92179192233\n",
      "Done 3010 batches in 535.26 sec.    training loss:\t\t3.92183142255\n",
      "Done 3020 batches in 536.71 sec.    training loss:\t\t3.9215790871\n",
      "Done 3030 batches in 538.30 sec.    training loss:\t\t3.92161948185\n",
      "Done 3040 batches in 540.57 sec.    training loss:\t\t3.92181847025\n",
      "Done 3050 batches in 542.40 sec.    training loss:\t\t3.92175945368\n",
      "Done 3060 batches in 544.16 sec.    training loss:\t\t3.92163945148\n",
      "Done 3070 batches in 546.11 sec.    training loss:\t\t3.92154211524\n",
      "Done 3080 batches in 547.63 sec.    training loss:\t\t3.9214219115\n",
      "Done 3090 batches in 549.20 sec.    training loss:\t\t3.92140990584\n",
      "Done 3100 batches in 550.88 sec.    training loss:\t\t3.92149781627\n",
      "Done 3110 batches in 552.59 sec.    training loss:\t\t3.92129100151\n",
      "Done 3120 batches in 555.04 sec.    training loss:\t\t3.9213489712\n",
      "Done 3130 batches in 557.22 sec.    training loss:\t\t3.92132519114\n",
      "Done 3140 batches in 559.30 sec.    training loss:\t\t3.92130282768\n",
      "Done 3150 batches in 561.11 sec.    training loss:\t\t3.92119387445\n",
      "Done 3160 batches in 563.04 sec.    training loss:\t\t3.92112339833\n",
      "Done 3170 batches in 564.49 sec.    training loss:\t\t3.92114803603\n",
      "Done 3180 batches in 566.18 sec.    training loss:\t\t3.92118191712\n",
      "Done 3190 batches in 568.46 sec.    training loss:\t\t3.92116357122\n",
      "Done 3200 batches in 570.20 sec.    training loss:\t\t3.92084471427\n",
      "Done 3210 batches in 571.98 sec.    training loss:\t\t3.9207378383\n",
      "Done 3220 batches in 573.62 sec.    training loss:\t\t3.92086754874\n",
      "Done 3230 batches in 575.38 sec.    training loss:\t\t3.9206258542\n",
      "Done 3240 batches in 577.59 sec.    training loss:\t\t3.92057929848\n",
      "Done 3250 batches in 579.46 sec.    training loss:\t\t3.92061806415\n",
      "Done 3260 batches in 581.27 sec.    training loss:\t\t3.92068169249\n",
      "Done 3270 batches in 583.51 sec.    training loss:\t\t3.92071484375\n",
      "Done 3280 batches in 585.34 sec.    training loss:\t\t3.92055605366\n",
      "Done 3290 batches in 587.34 sec.    training loss:\t\t3.92052388068\n",
      "Done 3300 batches in 589.41 sec.    training loss:\t\t3.9204303214\n",
      "Done 3310 batches in 591.28 sec.    training loss:\t\t3.92037237749\n",
      "Done 3320 batches in 592.84 sec.    training loss:\t\t3.92031107387\n",
      "Done 3330 batches in 594.54 sec.    training loss:\t\t3.92037233175\n",
      "Done 3340 batches in 596.04 sec.    training loss:\t\t3.92010833368\n",
      "Done 3350 batches in 597.68 sec.    training loss:\t\t3.92015286439\n",
      "Done 3360 batches in 599.55 sec.    training loss:\t\t3.92016126506\n",
      "Done 3370 batches in 601.34 sec.    training loss:\t\t3.92029442497\n",
      "Done 3380 batches in 603.02 sec.    training loss:\t\t3.92024170344\n",
      "Done 3390 batches in 604.77 sec.    training loss:\t\t3.92020054834\n",
      "Done 3400 batches in 606.45 sec.    training loss:\t\t3.92009270949\n",
      "Done 3410 batches in 608.31 sec.    training loss:\t\t3.9201089794\n",
      "Done 3420 batches in 610.00 sec.    training loss:\t\t3.91999261491\n",
      "Done 3430 batches in 611.46 sec.    training loss:\t\t3.91971860182\n",
      "Done 3440 batches in 612.92 sec.    training loss:\t\t3.91950700034\n",
      "Done 3450 batches in 614.83 sec.    training loss:\t\t3.91951931338\n",
      "Done 3460 batches in 616.74 sec.    training loss:\t\t3.91944374811\n",
      "Done 3470 batches in 618.58 sec.    training loss:\t\t3.91943593231\n",
      "Done 3480 batches in 620.09 sec.    training loss:\t\t3.91929449309\n",
      "Done 3490 batches in 621.60 sec.    training loss:\t\t3.91903496438\n",
      "Done 3500 batches in 623.31 sec.    training loss:\t\t3.91895048148\n",
      "Done 3510 batches in 625.11 sec.    training loss:\t\t3.91897677024\n",
      "Done 3520 batches in 627.19 sec.    training loss:\t\t3.91888944168\n",
      "Done 3530 batches in 629.64 sec.    training loss:\t\t3.91889599815\n",
      "Done 3540 batches in 631.44 sec.    training loss:\t\t3.91886674839\n",
      "Done 3550 batches in 632.89 sec.    training loss:\t\t3.9188918741\n",
      "Done 3560 batches in 634.62 sec.    training loss:\t\t3.91890997726\n",
      "Done 3570 batches in 636.28 sec.    training loss:\t\t3.91881861226\n",
      "Done 3580 batches in 637.89 sec.    training loss:\t\t3.91888971955\n",
      "Done 3590 batches in 639.55 sec.    training loss:\t\t3.91886616663\n",
      "Done 3600 batches in 641.17 sec.    training loss:\t\t3.91892544733\n",
      "Done 3610 batches in 642.97 sec.    training loss:\t\t3.91896863288\n",
      "Done 3620 batches in 645.01 sec.    training loss:\t\t3.91888661246\n",
      "Done 3630 batches in 646.63 sec.    training loss:\t\t3.91879725003\n",
      "Done 3640 batches in 648.19 sec.    training loss:\t\t3.91877312464\n",
      "Done 3650 batches in 649.75 sec.    training loss:\t\t3.91861628663\n",
      "Done 3660 batches in 651.57 sec.    training loss:\t\t3.91859429169\n",
      "Done 3670 batches in 653.72 sec.    training loss:\t\t3.91859743758\n",
      "Done 3680 batches in 655.47 sec.    training loss:\t\t3.91872362326\n",
      "Done 3690 batches in 657.06 sec.    training loss:\t\t3.91863508638\n",
      "Done 3700 batches in 658.43 sec.    training loss:\t\t3.91843697786\n",
      "Done 3710 batches in 659.94 sec.    training loss:\t\t3.91842056831\n",
      "Done 3720 batches in 661.85 sec.    training loss:\t\t3.91833842717\n",
      "Done 3730 batches in 663.31 sec.    training loss:\t\t3.91831695247\n",
      "Done 3740 batches in 665.01 sec.    training loss:\t\t3.91836970324\n",
      "Done 3750 batches in 666.90 sec.    training loss:\t\t3.91821990369\n",
      "Done 3760 batches in 668.31 sec.    training loss:\t\t3.91808607648\n",
      "Done 3770 batches in 670.02 sec.    training loss:\t\t3.91800303883\n",
      "Done 3780 batches in 671.56 sec.    training loss:\t\t3.91787316124\n",
      "Done 3790 batches in 674.02 sec.    training loss:\t\t3.91793658645\n",
      "Done 3800 batches in 675.70 sec.    training loss:\t\t3.91785139592\n",
      "Done 3810 batches in 677.25 sec.    training loss:\t\t3.91775957105\n",
      "Done 3820 batches in 678.99 sec.    training loss:\t\t3.91767737029\n",
      "Done 3830 batches in 680.38 sec.    training loss:\t\t3.91766273527\n",
      "Done 3840 batches in 682.36 sec.    training loss:\t\t3.91771672995\n",
      "Done 3850 batches in 683.99 sec.    training loss:\t\t3.9176043773\n",
      "Done 3860 batches in 685.59 sec.    training loss:\t\t3.91752539595\n",
      "Done 3870 batches in 687.69 sec.    training loss:\t\t3.91739421193\n",
      "Done 3880 batches in 689.35 sec.    training loss:\t\t3.91731452819\n",
      "Done 3890 batches in 690.99 sec.    training loss:\t\t3.91718988302\n",
      "Done 3900 batches in 692.51 sec.    training loss:\t\t3.91711806212\n",
      "Done 3910 batches in 694.39 sec.    training loss:\t\t3.91710216792\n",
      "Done 3920 batches in 696.15 sec.    training loss:\t\t3.91701033821\n",
      "Epoch 4 of 5 took 712.476s\n",
      "  training loss:\t\t3.916934\n",
      "  validation loss:\t\t3.953432\n",
      "Done 10 batches in 1.78 sec.    training loss:\t\t3.90386381149\n",
      "Done 20 batches in 3.50 sec.    training loss:\t\t3.89613980055\n",
      "Done 30 batches in 5.21 sec.    training loss:\t\t3.88505137761\n",
      "Done 40 batches in 7.22 sec.    training loss:\t\t3.89231674075\n",
      "Done 50 batches in 9.39 sec.    training loss:\t\t3.89701686382\n",
      "Done 60 batches in 11.30 sec.    training loss:\t\t3.89144481818\n",
      "Done 70 batches in 13.18 sec.    training loss:\t\t3.89384769031\n",
      "Done 80 batches in 15.39 sec.    training loss:\t\t3.89284842014\n",
      "Done 90 batches in 17.02 sec.    training loss:\t\t3.89396135012\n",
      "Done 100 batches in 18.77 sec.    training loss:\t\t3.89393590212\n",
      "Done 110 batches in 20.39 sec.    training loss:\t\t3.89248125553\n",
      "Done 120 batches in 22.26 sec.    training loss:\t\t3.89226826429\n",
      "Done 130 batches in 24.35 sec.    training loss:\t\t3.89483193067\n",
      "Done 140 batches in 25.90 sec.    training loss:\t\t3.89761531864\n",
      "Done 150 batches in 27.76 sec.    training loss:\t\t3.89523122629\n",
      "Done 160 batches in 29.37 sec.    training loss:\t\t3.89684676528\n",
      "Done 170 batches in 31.25 sec.    training loss:\t\t3.89535437472\n",
      "Done 180 batches in 33.26 sec.    training loss:\t\t3.89430581729\n",
      "Done 190 batches in 34.99 sec.    training loss:\t\t3.89530542775\n",
      "Done 200 batches in 36.39 sec.    training loss:\t\t3.8922236979\n",
      "Done 210 batches in 38.33 sec.    training loss:\t\t3.89388331459\n",
      "Done 220 batches in 40.05 sec.    training loss:\t\t3.89483478178\n",
      "Done 230 batches in 41.72 sec.    training loss:\t\t3.89319664292\n",
      "Done 240 batches in 43.92 sec.    training loss:\t\t3.89443948766\n",
      "Done 250 batches in 45.43 sec.    training loss:\t\t3.89407949162\n",
      "Done 260 batches in 47.23 sec.    training loss:\t\t3.89546777285\n",
      "Done 270 batches in 48.90 sec.    training loss:\t\t3.89449860343\n",
      "Done 280 batches in 50.60 sec.    training loss:\t\t3.89516391073\n",
      "Done 290 batches in 52.39 sec.    training loss:\t\t3.89467076926\n",
      "Done 300 batches in 54.36 sec.    training loss:\t\t3.89445852598\n",
      "Done 310 batches in 56.49 sec.    training loss:\t\t3.89431923051\n",
      "Done 320 batches in 58.38 sec.    training loss:\t\t3.89502789378\n",
      "Done 330 batches in 60.06 sec.    training loss:\t\t3.89525353909\n",
      "Done 340 batches in 61.75 sec.    training loss:\t\t3.89555475361\n",
      "Done 350 batches in 63.36 sec.    training loss:\t\t3.89469497749\n",
      "Done 360 batches in 64.92 sec.    training loss:\t\t3.89437503947\n",
      "Done 370 batches in 66.44 sec.    training loss:\t\t3.89394720052\n",
      "Done 380 batches in 67.85 sec.    training loss:\t\t3.89389088468\n",
      "Done 390 batches in 69.41 sec.    training loss:\t\t3.8940733121\n",
      "Done 400 batches in 71.19 sec.    training loss:\t\t3.89515754044\n",
      "Done 410 batches in 73.19 sec.    training loss:\t\t3.89369026917\n",
      "Done 420 batches in 74.97 sec.    training loss:\t\t3.89449078299\n",
      "Done 430 batches in 76.95 sec.    training loss:\t\t3.89444818996\n",
      "Done 440 batches in 78.48 sec.    training loss:\t\t3.89380152117\n",
      "Done 450 batches in 80.00 sec.    training loss:\t\t3.89340977033\n",
      "Done 460 batches in 81.41 sec.    training loss:\t\t3.89450973324\n",
      "Done 470 batches in 82.76 sec.    training loss:\t\t3.89406679032\n",
      "Done 480 batches in 84.22 sec.    training loss:\t\t3.89383424421\n",
      "Done 490 batches in 86.28 sec.    training loss:\t\t3.89426705643\n",
      "Done 500 batches in 88.05 sec.    training loss:\t\t3.89476514959\n",
      "Done 510 batches in 89.65 sec.    training loss:\t\t3.89503144423\n",
      "Done 520 batches in 91.19 sec.    training loss:\t\t3.89523819639\n",
      "Done 530 batches in 92.83 sec.    training loss:\t\t3.89530977978\n",
      "Done 540 batches in 94.57 sec.    training loss:\t\t3.89552733015\n",
      "Done 550 batches in 96.22 sec.    training loss:\t\t3.89510858839\n",
      "Done 560 batches in 98.03 sec.    training loss:\t\t3.89632925561\n",
      "Done 570 batches in 99.78 sec.    training loss:\t\t3.89578825675\n",
      "Done 580 batches in 102.41 sec.    training loss:\t\t3.89598642135\n",
      "Done 590 batches in 104.35 sec.    training loss:\t\t3.89538610669\n",
      "Done 600 batches in 106.09 sec.    training loss:\t\t3.89567415078\n",
      "Done 610 batches in 107.73 sec.    training loss:\t\t3.89504928276\n",
      "Done 620 batches in 109.60 sec.    training loss:\t\t3.89478494967\n",
      "Done 630 batches in 111.20 sec.    training loss:\t\t3.89533772355\n",
      "Done 640 batches in 112.93 sec.    training loss:\t\t3.8951468274\n",
      "Done 650 batches in 114.91 sec.    training loss:\t\t3.89509940367\n",
      "Done 660 batches in 116.51 sec.    training loss:\t\t3.89484504931\n",
      "Done 670 batches in 118.25 sec.    training loss:\t\t3.89505505028\n",
      "Done 680 batches in 120.09 sec.    training loss:\t\t3.89404096253\n",
      "Done 690 batches in 121.68 sec.    training loss:\t\t3.89418203485\n",
      "Done 700 batches in 123.50 sec.    training loss:\t\t3.89517013822\n",
      "Done 710 batches in 125.32 sec.    training loss:\t\t3.89582232186\n",
      "Done 720 batches in 127.17 sec.    training loss:\t\t3.89567750957\n",
      "Done 730 batches in 128.90 sec.    training loss:\t\t3.89547652251\n",
      "Done 740 batches in 130.58 sec.    training loss:\t\t3.89507464074\n",
      "Done 750 batches in 132.20 sec.    training loss:\t\t3.89422312737\n",
      "Done 760 batches in 133.78 sec.    training loss:\t\t3.89425311685\n",
      "Done 770 batches in 135.15 sec.    training loss:\t\t3.89401394702\n",
      "Done 780 batches in 136.78 sec.    training loss:\t\t3.89353373326\n",
      "Done 790 batches in 138.40 sec.    training loss:\t\t3.89305284778\n",
      "Done 800 batches in 140.17 sec.    training loss:\t\t3.89375443757\n",
      "Done 810 batches in 141.98 sec.    training loss:\t\t3.89433929802\n",
      "Done 820 batches in 143.80 sec.    training loss:\t\t3.89410860684\n",
      "Done 830 batches in 145.71 sec.    training loss:\t\t3.89375838975\n",
      "Done 840 batches in 147.26 sec.    training loss:\t\t3.89336304352\n",
      "Done 850 batches in 149.26 sec.    training loss:\t\t3.89357929566\n",
      "Done 860 batches in 151.10 sec.    training loss:\t\t3.89349702735\n",
      "Done 870 batches in 152.75 sec.    training loss:\t\t3.89277707489\n",
      "Done 880 batches in 154.45 sec.    training loss:\t\t3.89278216443\n",
      "Done 890 batches in 156.47 sec.    training loss:\t\t3.89281025067\n",
      "Done 900 batches in 158.29 sec.    training loss:\t\t3.8932744805\n",
      "Done 910 batches in 160.24 sec.    training loss:\t\t3.89376179019\n",
      "Done 920 batches in 161.87 sec.    training loss:\t\t3.8934953915\n",
      "Done 930 batches in 163.79 sec.    training loss:\t\t3.89320656587\n",
      "Done 940 batches in 165.46 sec.    training loss:\t\t3.89350883352\n",
      "Done 950 batches in 167.59 sec.    training loss:\t\t3.89374297569\n",
      "Done 960 batches in 169.09 sec.    training loss:\t\t3.89334344318\n",
      "Done 970 batches in 170.85 sec.    training loss:\t\t3.89306754599\n",
      "Done 980 batches in 172.44 sec.    training loss:\t\t3.89316854209\n",
      "Done 990 batches in 174.16 sec.    training loss:\t\t3.89300052758\n",
      "Done 1000 batches in 175.91 sec.    training loss:\t\t3.89254055166\n",
      "Done 1010 batches in 177.57 sec.    training loss:\t\t3.89212514811\n",
      "Done 1020 batches in 179.82 sec.    training loss:\t\t3.89218743362\n",
      "Done 1030 batches in 181.38 sec.    training loss:\t\t3.89202842898\n",
      "Done 1040 batches in 183.33 sec.    training loss:\t\t3.89258925984\n",
      "Done 1050 batches in 184.96 sec.    training loss:\t\t3.8922770423\n",
      "Done 1060 batches in 186.53 sec.    training loss:\t\t3.8923724793\n",
      "Done 1070 batches in 188.68 sec.    training loss:\t\t3.89205923103\n",
      "Done 1080 batches in 190.27 sec.    training loss:\t\t3.89178402887\n",
      "Done 1090 batches in 192.05 sec.    training loss:\t\t3.89186172551\n",
      "Done 1100 batches in 193.58 sec.    training loss:\t\t3.89174463684\n",
      "Done 1110 batches in 195.40 sec.    training loss:\t\t3.89182359592\n",
      "Done 1120 batches in 197.23 sec.    training loss:\t\t3.89173306078\n",
      "Done 1130 batches in 198.93 sec.    training loss:\t\t3.89183367269\n",
      "Done 1140 batches in 200.50 sec.    training loss:\t\t3.89163260146\n",
      "Done 1150 batches in 202.13 sec.    training loss:\t\t3.89206193053\n",
      "Done 1160 batches in 203.87 sec.    training loss:\t\t3.89201286805\n",
      "Done 1170 batches in 205.89 sec.    training loss:\t\t3.89185250763\n",
      "Done 1180 batches in 207.66 sec.    training loss:\t\t3.89174525212\n",
      "Done 1190 batches in 209.76 sec.    training loss:\t\t3.89170575503\n",
      "Done 1200 batches in 211.19 sec.    training loss:\t\t3.89148680071\n",
      "Done 1210 batches in 212.82 sec.    training loss:\t\t3.89145694212\n",
      "Done 1220 batches in 214.36 sec.    training loss:\t\t3.89127836677\n",
      "Done 1230 batches in 216.65 sec.    training loss:\t\t3.89184438988\n",
      "Done 1240 batches in 218.46 sec.    training loss:\t\t3.89194285677\n",
      "Done 1250 batches in 220.43 sec.    training loss:\t\t3.89166822414\n",
      "Done 1260 batches in 222.22 sec.    training loss:\t\t3.89177439061\n",
      "Done 1270 batches in 224.02 sec.    training loss:\t\t3.89199187023\n",
      "Done 1280 batches in 225.73 sec.    training loss:\t\t3.89198793322\n",
      "Done 1290 batches in 227.59 sec.    training loss:\t\t3.89179117218\n",
      "Done 1300 batches in 229.09 sec.    training loss:\t\t3.89172987296\n",
      "Done 1310 batches in 230.69 sec.    training loss:\t\t3.89158915618\n",
      "Done 1320 batches in 233.00 sec.    training loss:\t\t3.89157633384\n",
      "Done 1330 batches in 234.57 sec.    training loss:\t\t3.89170896182\n",
      "Done 1340 batches in 236.51 sec.    training loss:\t\t3.89145438885\n",
      "Done 1350 batches in 238.36 sec.    training loss:\t\t3.89145259221\n",
      "Done 1360 batches in 240.39 sec.    training loss:\t\t3.89167149505\n",
      "Done 1370 batches in 242.51 sec.    training loss:\t\t3.89214552754\n",
      "Done 1380 batches in 244.86 sec.    training loss:\t\t3.89227029603\n",
      "Done 1390 batches in 246.70 sec.    training loss:\t\t3.89200410363\n",
      "Done 1400 batches in 248.37 sec.    training loss:\t\t3.89180232644\n",
      "Done 1410 batches in 250.30 sec.    training loss:\t\t3.89182057431\n",
      "Done 1420 batches in 251.98 sec.    training loss:\t\t3.89202418613\n",
      "Done 1430 batches in 253.54 sec.    training loss:\t\t3.89177124033\n",
      "Done 1440 batches in 255.13 sec.    training loss:\t\t3.89164309965\n",
      "Done 1450 batches in 256.94 sec.    training loss:\t\t3.89182365516\n",
      "Done 1460 batches in 258.59 sec.    training loss:\t\t3.8918530167\n",
      "Done 1470 batches in 260.42 sec.    training loss:\t\t3.89186781231\n",
      "Done 1480 batches in 262.29 sec.    training loss:\t\t3.89188178391\n",
      "Done 1490 batches in 263.92 sec.    training loss:\t\t3.89217747698\n",
      "Done 1500 batches in 265.47 sec.    training loss:\t\t3.89251677195\n",
      "Done 1510 batches in 267.58 sec.    training loss:\t\t3.8925920638\n",
      "Done 1520 batches in 269.19 sec.    training loss:\t\t3.89290851511\n",
      "Done 1530 batches in 270.85 sec.    training loss:\t\t3.89282825476\n",
      "Done 1540 batches in 272.53 sec.    training loss:\t\t3.89260542919\n",
      "Done 1550 batches in 274.19 sec.    training loss:\t\t3.89266171425\n",
      "Done 1560 batches in 276.30 sec.    training loss:\t\t3.89269501643\n",
      "Done 1570 batches in 278.43 sec.    training loss:\t\t3.89253206056\n",
      "Done 1580 batches in 280.21 sec.    training loss:\t\t3.89245232842\n",
      "Done 1590 batches in 282.12 sec.    training loss:\t\t3.8925899888\n",
      "Done 1600 batches in 283.93 sec.    training loss:\t\t3.89248450086\n",
      "Done 1610 batches in 285.85 sec.    training loss:\t\t3.89248483374\n",
      "Done 1620 batches in 287.66 sec.    training loss:\t\t3.89269999781\n",
      "Done 1630 batches in 289.32 sec.    training loss:\t\t3.89247803732\n",
      "Done 1640 batches in 290.92 sec.    training loss:\t\t3.89224436923\n",
      "Done 1650 batches in 292.67 sec.    training loss:\t\t3.89235204798\n",
      "Done 1660 batches in 294.52 sec.    training loss:\t\t3.89229747861\n",
      "Done 1670 batches in 296.30 sec.    training loss:\t\t3.89223191495\n",
      "Done 1680 batches in 298.71 sec.    training loss:\t\t3.89221628691\n",
      "Done 1690 batches in 300.24 sec.    training loss:\t\t3.89193185702\n",
      "Done 1700 batches in 302.32 sec.    training loss:\t\t3.89167184647\n",
      "Done 1710 batches in 304.05 sec.    training loss:\t\t3.89174029465\n",
      "Done 1720 batches in 306.05 sec.    training loss:\t\t3.89155630178\n",
      "Done 1730 batches in 307.68 sec.    training loss:\t\t3.89147998101\n",
      "Done 1740 batches in 309.18 sec.    training loss:\t\t3.89145378587\n",
      "Done 1750 batches in 310.88 sec.    training loss:\t\t3.89136956596\n",
      "Done 1760 batches in 312.71 sec.    training loss:\t\t3.89131137106\n",
      "Done 1770 batches in 314.18 sec.    training loss:\t\t3.89098721359\n",
      "Done 1780 batches in 315.67 sec.    training loss:\t\t3.89057179113\n",
      "Done 1790 batches in 317.26 sec.    training loss:\t\t3.89040049015\n",
      "Done 1800 batches in 318.98 sec.    training loss:\t\t3.89034532388\n",
      "Done 1810 batches in 320.87 sec.    training loss:\t\t3.89018633682\n",
      "Done 1820 batches in 322.61 sec.    training loss:\t\t3.88990182523\n",
      "Done 1830 batches in 324.20 sec.    training loss:\t\t3.88971392105\n",
      "Done 1840 batches in 326.41 sec.    training loss:\t\t3.88970586683\n",
      "Done 1850 batches in 327.83 sec.    training loss:\t\t3.88950237377\n",
      "Done 1860 batches in 329.52 sec.    training loss:\t\t3.88947008284\n",
      "Done 1870 batches in 331.28 sec.    training loss:\t\t3.88942008235\n",
      "Done 1880 batches in 332.88 sec.    training loss:\t\t3.88937440312\n",
      "Done 1890 batches in 334.89 sec.    training loss:\t\t3.88909590143\n",
      "Done 1900 batches in 336.73 sec.    training loss:\t\t3.88898460526\n",
      "Done 1910 batches in 338.81 sec.    training loss:\t\t3.88902417577\n",
      "Done 1920 batches in 340.95 sec.    training loss:\t\t3.88910205315\n",
      "Done 1930 batches in 342.70 sec.    training loss:\t\t3.88907257584\n",
      "Done 1940 batches in 344.54 sec.    training loss:\t\t3.8889648172\n",
      "Done 1950 batches in 345.85 sec.    training loss:\t\t3.88886740049\n",
      "Done 1960 batches in 347.89 sec.    training loss:\t\t3.88893943393\n",
      "Done 1970 batches in 349.61 sec.    training loss:\t\t3.88881419407\n",
      "Done 1980 batches in 351.63 sec.    training loss:\t\t3.88873964548\n",
      "Done 1990 batches in 353.21 sec.    training loss:\t\t3.88845190475\n",
      "Done 2000 batches in 354.87 sec.    training loss:\t\t3.88837712538\n",
      "Done 2010 batches in 356.61 sec.    training loss:\t\t3.88814030132\n",
      "Done 2020 batches in 358.64 sec.    training loss:\t\t3.88824666786\n",
      "Done 2030 batches in 360.69 sec.    training loss:\t\t3.88835599787\n",
      "Done 2040 batches in 362.43 sec.    training loss:\t\t3.88849027531\n",
      "Done 2050 batches in 364.90 sec.    training loss:\t\t3.88861425016\n",
      "Done 2060 batches in 366.49 sec.    training loss:\t\t3.8883140314\n",
      "Done 2070 batches in 367.92 sec.    training loss:\t\t3.88806576948\n",
      "Done 2080 batches in 369.82 sec.    training loss:\t\t3.88813107724\n",
      "Done 2090 batches in 371.86 sec.    training loss:\t\t3.88813850458\n",
      "Done 2100 batches in 373.78 sec.    training loss:\t\t3.88812627134\n",
      "Done 2110 batches in 375.77 sec.    training loss:\t\t3.8882871977\n",
      "Done 2120 batches in 377.63 sec.    training loss:\t\t3.88819022808\n",
      "Done 2130 batches in 379.24 sec.    training loss:\t\t3.8883171433\n",
      "Done 2140 batches in 381.01 sec.    training loss:\t\t3.88843508703\n",
      "Done 2150 batches in 382.69 sec.    training loss:\t\t3.88831615437\n",
      "Done 2160 batches in 384.35 sec.    training loss:\t\t3.88822787768\n",
      "Done 2170 batches in 386.01 sec.    training loss:\t\t3.88825489167\n",
      "Done 2180 batches in 387.69 sec.    training loss:\t\t3.88813004975\n",
      "Done 2190 batches in 389.95 sec.    training loss:\t\t3.88824017897\n",
      "Done 2200 batches in 391.78 sec.    training loss:\t\t3.88817029086\n",
      "Done 2210 batches in 393.79 sec.    training loss:\t\t3.8882531877\n",
      "Done 2220 batches in 395.59 sec.    training loss:\t\t3.88822226073\n",
      "Done 2230 batches in 397.09 sec.    training loss:\t\t3.88817925624\n",
      "Done 2240 batches in 398.89 sec.    training loss:\t\t3.88816689753\n",
      "Done 2250 batches in 400.55 sec.    training loss:\t\t3.88803948042\n",
      "Done 2260 batches in 402.17 sec.    training loss:\t\t3.88783160448\n",
      "Done 2270 batches in 403.76 sec.    training loss:\t\t3.88764894492\n",
      "Done 2280 batches in 405.43 sec.    training loss:\t\t3.88779468139\n",
      "Done 2290 batches in 407.15 sec.    training loss:\t\t3.88788323808\n",
      "Done 2300 batches in 408.75 sec.    training loss:\t\t3.88759617826\n",
      "Done 2310 batches in 410.63 sec.    training loss:\t\t3.88751434537\n",
      "Done 2320 batches in 412.01 sec.    training loss:\t\t3.8875704234\n",
      "Done 2330 batches in 414.13 sec.    training loss:\t\t3.88732892894\n",
      "Done 2340 batches in 415.64 sec.    training loss:\t\t3.88717699784\n",
      "Done 2350 batches in 418.03 sec.    training loss:\t\t3.88738559297\n",
      "Done 2360 batches in 419.73 sec.    training loss:\t\t3.88738504143\n",
      "Done 2370 batches in 421.30 sec.    training loss:\t\t3.8874438481\n",
      "Done 2380 batches in 422.76 sec.    training loss:\t\t3.88737403214\n",
      "Done 2390 batches in 424.81 sec.    training loss:\t\t3.88730203218\n",
      "Done 2400 batches in 426.64 sec.    training loss:\t\t3.88721666743\n",
      "Done 2410 batches in 428.28 sec.    training loss:\t\t3.88717372952\n",
      "Done 2420 batches in 430.04 sec.    training loss:\t\t3.88725332416\n",
      "Done 2430 batches in 432.18 sec.    training loss:\t\t3.88705248136\n",
      "Done 2440 batches in 434.01 sec.    training loss:\t\t3.88706569877\n",
      "Done 2450 batches in 435.50 sec.    training loss:\t\t3.88704783459\n",
      "Done 2460 batches in 437.33 sec.    training loss:\t\t3.88695969213\n",
      "Done 2470 batches in 439.31 sec.    training loss:\t\t3.88679114595\n",
      "Done 2480 batches in 441.01 sec.    training loss:\t\t3.88676459357\n",
      "Done 2490 batches in 443.11 sec.    training loss:\t\t3.88691743014\n",
      "Done 2500 batches in 444.82 sec.    training loss:\t\t3.88676329575\n",
      "Done 2510 batches in 446.45 sec.    training loss:\t\t3.88658232584\n",
      "Done 2520 batches in 447.99 sec.    training loss:\t\t3.88653371571\n",
      "Done 2530 batches in 449.55 sec.    training loss:\t\t3.88623471599\n",
      "Done 2540 batches in 451.48 sec.    training loss:\t\t3.88634685766\n",
      "Done 2550 batches in 453.10 sec.    training loss:\t\t3.88642984185\n",
      "Done 2560 batches in 455.28 sec.    training loss:\t\t3.88635297986\n",
      "Done 2570 batches in 457.19 sec.    training loss:\t\t3.88619819365\n",
      "Done 2580 batches in 459.09 sec.    training loss:\t\t3.88590404516\n",
      "Done 2590 batches in 460.75 sec.    training loss:\t\t3.88569917099\n",
      "Done 2600 batches in 462.16 sec.    training loss:\t\t3.88567790096\n",
      "Done 2610 batches in 464.02 sec.    training loss:\t\t3.88557633806\n",
      "Done 2620 batches in 466.28 sec.    training loss:\t\t3.88558350437\n",
      "Done 2630 batches in 468.03 sec.    training loss:\t\t3.88555820995\n",
      "Done 2640 batches in 469.52 sec.    training loss:\t\t3.88534495659\n",
      "Done 2650 batches in 471.76 sec.    training loss:\t\t3.88529412611\n",
      "Done 2660 batches in 473.89 sec.    training loss:\t\t3.88525003007\n",
      "Done 2670 batches in 475.40 sec.    training loss:\t\t3.88529513097\n",
      "Done 2680 batches in 477.16 sec.    training loss:\t\t3.88513481528\n",
      "Done 2690 batches in 478.94 sec.    training loss:\t\t3.88506613948\n",
      "Done 2700 batches in 480.63 sec.    training loss:\t\t3.88504352322\n",
      "Done 2710 batches in 482.15 sec.    training loss:\t\t3.884849594\n",
      "Done 2720 batches in 484.36 sec.    training loss:\t\t3.88490020273\n",
      "Done 2730 batches in 486.50 sec.    training loss:\t\t3.88490610542\n",
      "Done 2740 batches in 488.43 sec.    training loss:\t\t3.884929683\n",
      "Done 2750 batches in 490.33 sec.    training loss:\t\t3.88503409481\n",
      "Done 2760 batches in 491.91 sec.    training loss:\t\t3.88487246373\n",
      "Done 2770 batches in 493.97 sec.    training loss:\t\t3.88488739139\n",
      "Done 2780 batches in 495.46 sec.    training loss:\t\t3.88495799749\n",
      "Done 2790 batches in 497.46 sec.    training loss:\t\t3.88474884093\n",
      "Done 2800 batches in 499.57 sec.    training loss:\t\t3.88470837636\n",
      "Done 2810 batches in 501.47 sec.    training loss:\t\t3.88474381097\n",
      "Done 2820 batches in 503.12 sec.    training loss:\t\t3.88464912599\n",
      "Done 2830 batches in 504.61 sec.    training loss:\t\t3.88467422012\n",
      "Done 2840 batches in 506.53 sec.    training loss:\t\t3.88484155544\n",
      "Done 2850 batches in 508.07 sec.    training loss:\t\t3.88467619586\n",
      "Done 2860 batches in 509.54 sec.    training loss:\t\t3.88450614889\n",
      "Done 2870 batches in 511.20 sec.    training loss:\t\t3.88442335694\n",
      "Done 2880 batches in 512.75 sec.    training loss:\t\t3.88431853023\n",
      "Done 2890 batches in 514.38 sec.    training loss:\t\t3.88420814519\n",
      "Done 2900 batches in 515.98 sec.    training loss:\t\t3.88420612426\n",
      "Done 2910 batches in 517.89 sec.    training loss:\t\t3.88398531122\n",
      "Done 2920 batches in 519.87 sec.    training loss:\t\t3.88396841704\n",
      "Done 2930 batches in 521.20 sec.    training loss:\t\t3.88374161655\n",
      "Done 2940 batches in 523.02 sec.    training loss:\t\t3.88366131012\n",
      "Done 2950 batches in 524.85 sec.    training loss:\t\t3.88354439881\n",
      "Done 2960 batches in 526.35 sec.    training loss:\t\t3.88344609278\n",
      "Done 2970 batches in 528.11 sec.    training loss:\t\t3.88326586481\n",
      "Done 2980 batches in 529.66 sec.    training loss:\t\t3.88320653775\n",
      "Done 2990 batches in 531.17 sec.    training loss:\t\t3.88322980205\n",
      "Done 3000 batches in 533.20 sec.    training loss:\t\t3.88315006804\n",
      "Done 3010 batches in 535.25 sec.    training loss:\t\t3.88319890008\n",
      "Done 3020 batches in 536.70 sec.    training loss:\t\t3.88295700187\n",
      "Done 3030 batches in 538.29 sec.    training loss:\t\t3.88301146951\n",
      "Done 3040 batches in 540.57 sec.    training loss:\t\t3.88322639787\n",
      "Done 3050 batches in 542.41 sec.    training loss:\t\t3.88317653898\n",
      "Done 3060 batches in 544.15 sec.    training loss:\t\t3.88307154506\n",
      "Done 3070 batches in 546.11 sec.    training loss:\t\t3.88298688711\n",
      "Done 3080 batches in 547.63 sec.    training loss:\t\t3.88288312423\n",
      "Done 3090 batches in 549.20 sec.    training loss:\t\t3.88288324118\n",
      "Done 3100 batches in 550.88 sec.    training loss:\t\t3.88298145471\n",
      "Done 3110 batches in 552.59 sec.    training loss:\t\t3.88278858094\n",
      "Done 3120 batches in 555.03 sec.    training loss:\t\t3.88284671727\n",
      "Done 3130 batches in 557.21 sec.    training loss:\t\t3.88283237619\n",
      "Done 3140 batches in 559.29 sec.    training loss:\t\t3.88281728333\n",
      "Done 3150 batches in 561.11 sec.    training loss:\t\t3.88272038112\n",
      "Done 3160 batches in 563.03 sec.    training loss:\t\t3.88266001306\n",
      "Done 3170 batches in 564.48 sec.    training loss:\t\t3.8826949349\n",
      "Done 3180 batches in 566.17 sec.    training loss:\t\t3.88274229975\n",
      "Done 3190 batches in 568.45 sec.    training loss:\t\t3.88273251363\n",
      "Done 3200 batches in 570.19 sec.    training loss:\t\t3.88242707767\n",
      "Done 3210 batches in 571.96 sec.    training loss:\t\t3.88233188305\n",
      "Done 3220 batches in 573.61 sec.    training loss:\t\t3.88247058221\n",
      "Done 3230 batches in 575.37 sec.    training loss:\t\t3.88224123004\n",
      "Done 3240 batches in 577.58 sec.    training loss:\t\t3.88220496766\n",
      "Done 3250 batches in 579.45 sec.    training loss:\t\t3.88225698038\n",
      "Done 3260 batches in 581.26 sec.    training loss:\t\t3.88233102626\n",
      "Done 3270 batches in 583.49 sec.    training loss:\t\t3.88237376585\n",
      "Done 3280 batches in 585.32 sec.    training loss:\t\t3.88222939299\n",
      "Done 3290 batches in 587.32 sec.    training loss:\t\t3.88221054744\n",
      "Done 3300 batches in 589.38 sec.    training loss:\t\t3.88213048516\n",
      "Done 3310 batches in 591.26 sec.    training loss:\t\t3.88208193548\n",
      "Done 3320 batches in 592.82 sec.    training loss:\t\t3.88202951989\n",
      "Done 3330 batches in 594.52 sec.    training loss:\t\t3.88210240219\n",
      "Done 3340 batches in 596.02 sec.    training loss:\t\t3.8818516056\n",
      "Done 3350 batches in 597.66 sec.    training loss:\t\t3.88190598687\n",
      "Done 3360 batches in 599.53 sec.    training loss:\t\t3.88192350417\n",
      "Done 3370 batches in 601.32 sec.    training loss:\t\t3.88206741244\n",
      "Done 3380 batches in 603.00 sec.    training loss:\t\t3.88202722983\n",
      "Done 3390 batches in 604.76 sec.    training loss:\t\t3.88199939679\n",
      "Done 3400 batches in 606.43 sec.    training loss:\t\t3.88190334825\n",
      "Done 3410 batches in 608.30 sec.    training loss:\t\t3.88193183099\n",
      "Done 3420 batches in 609.99 sec.    training loss:\t\t3.8818239272\n",
      "Done 3430 batches in 611.45 sec.    training loss:\t\t3.88156378624\n",
      "Done 3440 batches in 612.91 sec.    training loss:\t\t3.88136587489\n",
      "Done 3450 batches in 614.83 sec.    training loss:\t\t3.88138909347\n",
      "Done 3460 batches in 616.73 sec.    training loss:\t\t3.88132835627\n",
      "Done 3470 batches in 618.58 sec.    training loss:\t\t3.88133397645\n",
      "Done 3480 batches in 620.09 sec.    training loss:\t\t3.88120696545\n",
      "Done 3490 batches in 621.60 sec.    training loss:\t\t3.88095831243\n",
      "Done 3500 batches in 623.31 sec.    training loss:\t\t3.88088776159\n",
      "Done 3510 batches in 625.11 sec.    training loss:\t\t3.88092490594\n",
      "Done 3520 batches in 627.19 sec.    training loss:\t\t3.88080390218\n",
      "Done 3530 batches in 629.64 sec.    training loss:\t\t3.88079581321\n",
      "Done 3540 batches in 631.44 sec.    training loss:\t\t3.88077299945\n",
      "Done 3550 batches in 632.88 sec.    training loss:\t\t3.88080371252\n",
      "Done 3560 batches in 634.62 sec.    training loss:\t\t3.88082763057\n",
      "Done 3570 batches in 636.27 sec.    training loss:\t\t3.88074530533\n",
      "Done 3580 batches in 637.88 sec.    training loss:\t\t3.88082335701\n",
      "Done 3590 batches in 639.55 sec.    training loss:\t\t3.88080964082\n",
      "Done 3600 batches in 641.17 sec.    training loss:\t\t3.88087808563\n",
      "Done 3610 batches in 642.96 sec.    training loss:\t\t3.88093082588\n",
      "Done 3620 batches in 645.01 sec.    training loss:\t\t3.88086215949\n",
      "Done 3630 batches in 646.62 sec.    training loss:\t\t3.88078009918\n",
      "Done 3640 batches in 648.18 sec.    training loss:\t\t3.8807671691\n",
      "Done 3650 batches in 649.74 sec.    training loss:\t\t3.88062078267\n",
      "Done 3660 batches in 651.57 sec.    training loss:\t\t3.88060963975\n",
      "Done 3670 batches in 653.71 sec.    training loss:\t\t3.880621896\n",
      "Done 3680 batches in 655.46 sec.    training loss:\t\t3.8807570725\n",
      "Done 3690 batches in 657.05 sec.    training loss:\t\t3.88068170547\n",
      "Done 3700 batches in 658.42 sec.    training loss:\t\t3.8804963726\n",
      "Done 3710 batches in 659.93 sec.    training loss:\t\t3.88049154841\n",
      "Done 3720 batches in 661.84 sec.    training loss:\t\t3.88042311886\n",
      "Done 3730 batches in 663.30 sec.    training loss:\t\t3.88041364213\n",
      "Done 3740 batches in 665.00 sec.    training loss:\t\t3.88047584085\n",
      "Done 3750 batches in 666.89 sec.    training loss:\t\t3.88033724613\n",
      "Done 3760 batches in 668.30 sec.    training loss:\t\t3.88021456073\n",
      "Done 3770 batches in 670.01 sec.    training loss:\t\t3.88014507825\n",
      "Done 3780 batches in 671.55 sec.    training loss:\t\t3.88002335634\n",
      "Done 3790 batches in 674.00 sec.    training loss:\t\t3.8800968223\n",
      "Done 3800 batches in 675.69 sec.    training loss:\t\t3.8800238716\n",
      "Done 3810 batches in 677.24 sec.    training loss:\t\t3.87994321012\n",
      "Done 3820 batches in 678.98 sec.    training loss:\t\t3.87987244398\n",
      "Done 3830 batches in 680.36 sec.    training loss:\t\t3.87986732457\n",
      "Done 3840 batches in 682.35 sec.    training loss:\t\t3.87993359659\n",
      "Done 3850 batches in 683.98 sec.    training loss:\t\t3.87983358352\n",
      "Done 3860 batches in 685.58 sec.    training loss:\t\t3.87976653743\n",
      "Done 3870 batches in 687.68 sec.    training loss:\t\t3.87964712317\n",
      "Done 3880 batches in 689.34 sec.    training loss:\t\t3.87958049909\n",
      "Done 3890 batches in 690.98 sec.    training loss:\t\t3.87947005729\n",
      "Done 3900 batches in 692.50 sec.    training loss:\t\t3.87941127673\n",
      "Done 3910 batches in 694.38 sec.    training loss:\t\t3.87940684731\n",
      "Done 3920 batches in 696.14 sec.    training loss:\t\t3.87932800657\n",
      "Epoch 5 of 5 took 712.470s\n",
      "  training loss:\t\t3.879258\n",
      "  validation loss:\t\t3.930674\n"
     ]
    }
   ],
   "source": [
    "# training, taken from mnist.py in lasagne examples\n",
    "\n",
    "num_epochs = 5\n",
    "mt_batch_size = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    time_wasted = 0\n",
    "    training_time = 0\n",
    "    \n",
    "    for batch in iterate_minibatches(mt_train, mt_batch_size):\n",
    "        \n",
    "        inputs, targets, mask, t = batch\n",
    "        \n",
    "        batch_training_time = time.time()\n",
    "        batch_err, net_output = train_fn(inputs, targets, mask)\n",
    "        train_err += batch_err\n",
    "        #print net_output\n",
    "        training_time += time.time() - batch_training_time\n",
    "        train_batches += 1\n",
    "        \n",
    "        time_wasted += t\n",
    "        if not train_batches % 10:\n",
    "            print(\"Done {} batches in {:.2f} sec.    training loss:\\t\\t{}\").format(\n",
    "                train_batches, time.time() - start_time, train_err / train_batches)\n",
    "            #print net_output\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_err = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(mt_val, mt_batch_size):\n",
    "        inputs, targets, mask, t = batch\n",
    "        err = val_fn(inputs, targets, mask)\n",
    "        val_err += err\n",
    "        val_batches += 1\n",
    "\n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "    #print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "    #    val_acc / val_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savez('model_trained_singleLSTM.npz', *L.layers.get_all_param_values(net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_var = T.imatrix('inputs')\n",
    "gen_net = build_hsoft_rnnlm(input_var, None, None, voc_size, emb_size, rec_size)\n",
    "probs = L.layers.get_output(gen_net)[:,-1,:]\n",
    "get_probs = theano.function([input_var], probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with np.load('model_trained_singleLSTM.npz') as f:\n",
    "    param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "    L.layers.set_all_param_values(gen_net, param_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rnd_next_word(probs):\n",
    "    return np.random.choice(np.arange(len(probs[0])), p=probs[0])\n",
    "\n",
    "init_seq = ''\n",
    "utt = [1] + map(lambda w: mt_w_to_i.get(w, mt_w_to_i['<unk>']), init_seq.split())\n",
    "utt = np.asarray(utt, dtype=np.int32)[np.newaxis]\n",
    "\n",
    "i = 0\n",
    "while mt_i_to_w[utt[0,-1]] != '<utt_end>' and i < 50:\n",
    "    word_probs = get_probs(utt)\n",
    "    next_idx = rnd_next_word(word_probs)\n",
    "    utt = np.append(utt, next_idx)[np.newaxis].astype(np.int32)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"how many common has turning gotten knocked around hunting into <person> ? your technician can keep this wasn ' t some things for you ? i would have . i think she rains sounds never -- i trust me -- but though i come up you ' re very\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = map(lambda i: mt_i_to_w[i], list(utt[0]))\n",
    "' '.join(text[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

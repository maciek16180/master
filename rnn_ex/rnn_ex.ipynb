{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 780 (CNMeM is enabled with initial size: 30.0% of memory, cuDNN 4007)\n",
      "/home/i258346/.local/lib/python2.7/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import lasagne as L\n",
    "\n",
    "sys.path.insert(0, '../HSoftmaxLayerLasagne/')\n",
    "\n",
    "from HSoftmaxLayer import HierarchicalSoftmaxDenseLayer\n",
    "from SampledSoftmaxLayer import SampledSoftmaxDenseLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mt_path = \"/pio/data/data/mtriples/\"\n",
    "\n",
    "def get_mt_voc(path=mt_path):\n",
    "    word_list = np.load(mt_path + 'Training.dict.pkl')\n",
    "    word_list = map(lambda x: x[:2], word_list)\n",
    "    wc = len(word_list)\n",
    "    \n",
    "    w_to_idx = dict(word_list)\n",
    "    idx_to_w = {v : k for (k,v) in w_to_idx.items()}\n",
    "    \n",
    "    return idx_to_w, w_to_idx, wc\n",
    "\n",
    "idx_to_w, w_to_idx, voc_size = get_mt_voc()\n",
    "\n",
    "\n",
    "def load_mt(path=mt_path):\n",
    "    tr = np.load(mt_path + 'Training.triples.pkl')\n",
    "    vl = np.load(mt_path + 'Validation.triples.pkl')\n",
    "    ts = np.load(mt_path + 'Test.triples.pkl')\n",
    "    \n",
    "    return tr, vl, ts\n",
    "\n",
    "train, valid, test = load_mt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec_embs, word2vec_embs_mask = np.load(mt_path + 'Word2Vec_WordEmb.pkl')\n",
    "word2vec_embs = np.vstack([word2vec_embs, L.init.GlorotUniform()((1,300))]).astype(np.float32)\n",
    "word2vec_embs_mask = np.vstack([word2vec_embs_mask, np.zeros((1,300))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Similar to Lasagne mnist.py example, added input mask and different sequence lengths\n",
    "\n",
    "def iterate_minibatches(inputs, batchsize, pad=-1):\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):        \n",
    "        excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        inp = inputs[excerpt]\n",
    "        \n",
    "        inp_max_len = len(max(inp, key=len))\n",
    "        inp = map(lambda l: l + [pad]*(inp_max_len-len(l)), inp)\n",
    "        inp = np.asarray(inp, dtype=np.int32)\n",
    "        tar = np.hstack([inp[:,1:], np.zeros((batchsize,1), dtype=np.int32) + pad])\n",
    "        def not_pad(x):\n",
    "            return x != pad\n",
    "        v_not_pad = np.vectorize(not_pad, otypes=[np.float32])\n",
    "        mask = v_not_pad(inp) # there is no separate value for the end of an utterance right now, just pad\n",
    "        \n",
    "        yield inp, tar, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_simple_rnnlm(input_var, mask_input_var, voc_size, emb_size, rec_size, emb_init=None):\n",
    "    l_in = L.layers.InputLayer(shape=(None, None), input_var=input_var)  \n",
    "    batch_size, seq_len = l_in.input_var.shape\n",
    "    \n",
    "    l_mask = None\n",
    "    if mask_input_var is not None:\n",
    "        print 'setting up input mask...'\n",
    "        l_mask = L.layers.InputLayer(shape=(batch_size, seq_len), input_var=mask_input_var)\n",
    "    \n",
    "    if emb_init is None:\n",
    "        l_emb = L.layers.EmbeddingLayer(l_in,\n",
    "                                        input_size=voc_size+1, \n",
    "                                        output_size=emb_size)\n",
    "    else:\n",
    "        l_emb = L.layers.EmbeddingLayer(l_in,\n",
    "                                        input_size=voc_size+1, \n",
    "                                        output_size=emb_size,\n",
    "                                        W=emb_init)\n",
    "        l_emb.params[l_emb.W].remove('trainable')\n",
    "    \n",
    "    l_lstm1 = L.layers.LSTMLayer(l_emb,\n",
    "                                 num_units=rec_size,\n",
    "                                 nonlinearity=L.nonlinearities.tanh,\n",
    "                                 grad_clipping=100,\n",
    "                                 mask_input=l_mask)\n",
    "    \n",
    "    l_lstm2 = L.layers.LSTMLayer(l_lstm1,\n",
    "                                 num_units=rec_size,\n",
    "                                 nonlinearity=L.nonlinearities.tanh,\n",
    "                                 grad_clipping=100,\n",
    "                                 mask_input=l_mask)\n",
    "    \n",
    "    l_resh = L.layers.ReshapeLayer(l_lstm2, shape=(-1, rec_size))\n",
    "    \n",
    "    l_soft = L.layers.DenseLayer(l_resh,\n",
    "                                num_units=voc_size,\n",
    "                                nonlinearity=L.nonlinearities.softmax)\n",
    "    \n",
    "    l_out = L.layers.ReshapeLayer(l_soft, shape=(batch_size, seq_len, voc_size))\n",
    "    \n",
    "    return l_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_hsoft_rnnlm(input_var, target_var, mask_input_var, voc_size, emb_size, rec_size):\n",
    "    l_in = L.layers.InputLayer(shape=(None, None), input_var=input_var)    \n",
    "    batch_size, seq_len = l_in.input_var.shape\n",
    "    l_mask = None\n",
    "    if mask_input_var is not None:\n",
    "        print 'setting up input mask...'\n",
    "        l_mask = L.layers.InputLayer(shape=(batch_size, seq_len), input_var=mask_input_var)\n",
    "    \n",
    "    l_emb = L.layers.EmbeddingLayer(l_in,\n",
    "                                    input_size=voc_size+1, \n",
    "                                    output_size=emb_size)\n",
    "    \n",
    "    l_lstm1 = L.layers.LSTMLayer(l_emb,\n",
    "                                 num_units=rec_size,\n",
    "                                 nonlinearity=L.nonlinearities.tanh,\n",
    "                                 grad_clipping=100,\n",
    "                                 mask_input=l_mask)    \n",
    "    \n",
    "#     l_lstm2 = L.layers.LSTMLayer(l_lstm1,\n",
    "#                                  num_units=rec_size,\n",
    "#                                  nonlinearity=L.nonlinearities.tanh,\n",
    "#                                  grad_clipping=100,\n",
    "#                                  mask_input=l_mask)\n",
    "    \n",
    "    l_resh = L.layers.ReshapeLayer(l_lstm1, shape=(-1, rec_size))\n",
    "    \n",
    "    # hierarchical softmax\n",
    "    \n",
    "    l_resh_tar = None\n",
    "    if target_var is not None:\n",
    "        print 'setting up targets for hsoftmax...'\n",
    "        l_tar = L.layers.InputLayer(shape=(None, None), input_var=target_var)\n",
    "        l_resh_tar = L.layers.ReshapeLayer(l_tar, shape=(-1, 1))\n",
    "        \n",
    "    l_hsoft = HierarchicalSoftmaxDenseLayer(l_resh,\n",
    "                                            num_units=voc_size,\n",
    "                                            target=l_resh_tar)\n",
    "    l_out = None\n",
    "    if target_var is not None:\n",
    "        l_out = L.layers.ReshapeLayer(l_hsoft, shape=(batch_size, seq_len))\n",
    "    else:\n",
    "        l_out = L.layers.ReshapeLayer(l_hsoft, shape=(batch_size, seq_len, voc_size))\n",
    "    \n",
    "    return l_out\n",
    "\n",
    "# 1 epoch on gpu with hsoft took about 700s, batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_sampledsoft_rnnlm(input_var, mask_input_var, num_sampled, voc_size, \n",
    "                            emb_size, rec_size, target_var=None, use_all_words=False):\n",
    "    l_in = L.layers.InputLayer(shape=(None, None), input_var=input_var)    \n",
    "    batch_size, seq_len = l_in.input_var.shape\n",
    "    l_mask = None\n",
    "    if mask_input_var != None:\n",
    "        print 'setting up input mask...'\n",
    "        l_mask = L.layers.InputLayer(shape=(batch_size, seq_len), input_var=mask_input_var)\n",
    "    \n",
    "    l_emb = L.layers.EmbeddingLayer(l_in,\n",
    "                                    input_size=voc_size+1, \n",
    "                                    output_size=emb_size)\n",
    "    \n",
    "    l_lstm1 = L.layers.LSTMLayer(l_emb,\n",
    "                                 num_units=rec_size,\n",
    "                                 nonlinearity=L.nonlinearities.tanh,\n",
    "                                 grad_clipping=100,\n",
    "                                 mask_input=l_mask)\n",
    "    \n",
    "    l_lstm2 = L.layers.LSTMLayer(l_lstm1,\n",
    "                                 num_units=rec_size,\n",
    "                                 nonlinearity=L.nonlinearities.tanh,\n",
    "                                 grad_clipping=100,\n",
    "                                 mask_input=l_mask)\n",
    "      \n",
    "    l_resh = L.layers.ReshapeLayer(l_lstm2, shape=(-1, rec_size))\n",
    "    \n",
    "    if target_var is not None:\n",
    "        print 'setting up targets for sampled softmax...'\n",
    "        target_var = target_var.ravel()\n",
    "    \n",
    "    l_ssoft = SampledSoftmaxDenseLayer(l_resh, num_sampled, voc_size, \n",
    "                                       targets=target_var, \n",
    "                                       use_all_words=use_all_words)\n",
    "    \n",
    "    if target_var is not None:\n",
    "        l_out = L.layers.ReshapeLayer(l_ssoft, shape=(batch_size, seq_len))\n",
    "    else:\n",
    "        l_out = L.layers.ReshapeLayer(l_ssoft, shape=(batch_size, seq_len, voc_size))\n",
    "    \n",
    "    return l_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emb_size = 300\n",
    "rec_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clone_param_values(net_from, net_to):\n",
    "    L.layers.set_all_param_values(net_to, L.layers.get_all_param_values(net_from))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up input mask...\n"
     ]
    }
   ],
   "source": [
    "# full softmax test\n",
    "\n",
    "input_var = T.imatrix('inputs')\n",
    "targets = T.imatrix('targets') # these will be inputs shifted by 1\n",
    "mask_input_var = T.matrix('input_mask')\n",
    "voc_mask = T.ivector('voc_mask')\n",
    "\n",
    "net = build_simple_rnnlm(input_var, mask_input_var, voc_size, emb_size, rec_size, emb_init=word2vec_embs)\n",
    "out = L.layers.get_output(net)\n",
    "\n",
    "mask_idx = mask_input_var.nonzero()\n",
    "loss = L.objectives.categorical_crossentropy(out[mask_idx], targets[mask_idx])\n",
    "loss = loss.mean() # mean batch loss\n",
    "\n",
    "params = L.layers.get_all_params(net, trainable=True)\n",
    "updates = L.updates.adagrad(loss, params, learning_rate=.01)\n",
    "\n",
    "train_fn = theano.function([input_var, targets, mask_input_var], loss, updates=updates)\n",
    "\n",
    "### for validation\n",
    "\n",
    "test_net = net # this line is just for compatibility later\n",
    "\n",
    "test_out = L.layers.get_output(net, deterministic=True)\n",
    "test_loss = L.objectives.categorical_crossentropy(test_out[mask_idx], targets[mask_idx])\n",
    "test_loss = test_loss.mean()\n",
    "# test_acc = T.mean(T.eq(T.argmax(test_out, axis=1), targets), dtype=theano.config.floatX)\n",
    "\n",
    "val_fn = theano.function([input_var, targets, mask_input_var], test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up input mask...\n",
      "setting up targets for sampled softmax...\n",
      "setting up input mask...\n",
      "setting up targets for sampled softmax...\n"
     ]
    }
   ],
   "source": [
    "# sampled softmax test (with targets!)\n",
    "\n",
    "num_sampled = None\n",
    "\n",
    "input_var = T.imatrix('inputs')\n",
    "targets = T.imatrix('targets') # these will be inputs shifted by 1\n",
    "mask_input_var = T.matrix('input_mask')\n",
    "\n",
    "net = build_sampledsoft_rnnlm(input_var, mask_input_var, num_sampled, voc_size, \n",
    "                              emb_size, rec_size, target_var=targets)\n",
    "out = L.layers.get_output(net)\n",
    "\n",
    "mask_idx = mask_input_var.nonzero()\n",
    "loss = -T.sum(T.log(out[mask_idx])) / T.sum(mask_input_var)\n",
    "\n",
    "params = L.layers.get_all_params(net, trainable=True)\n",
    "updates = L.updates.adagrad(loss, params, learning_rate=.01)\n",
    "# updates = L.updates.rmsprop(loss, params, learning_rate=.001, rho=.9, epsilon=1e-06)\n",
    "\n",
    "train_fn = theano.function([input_var, targets, mask_input_var], loss, updates=updates)\n",
    "\n",
    "### for validation\n",
    "\n",
    "test_input_var = T.imatrix('inputs')\n",
    "test_targets = T.imatrix('targets')\n",
    "test_mask_input_var = T.matrix('input_mask')\n",
    "\n",
    "test_net = build_sampledsoft_rnnlm(test_input_var, test_mask_input_var, num_sampled, voc_size, \n",
    "                                   emb_size, rec_size, target_var=test_targets, use_all_words=True)\n",
    "\n",
    "test_mask_idx = test_mask_input_var.nonzero()\n",
    "\n",
    "test_out = L.layers.get_output(test_net, deterministic=True)\n",
    "test_loss = -T.sum(T.log(test_out[test_mask_idx])) / T.sum(test_mask_input_var)\n",
    "\n",
    "# test_acc = T.mean(T.eq(T.argmax(test_out, axis=1), targets), dtype=theano.config.floatX)\n",
    "\n",
    "val_fn = theano.function([test_input_var, test_targets, test_mask_input_var], test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up input mask...\n",
      "setting up targets for hsoftmax...\n",
      "setting up input mask...\n",
      "setting up targets for hsoftmax...\n"
     ]
    }
   ],
   "source": [
    "# hierarchical softmax test\n",
    "\n",
    "input_var = T.imatrix('inputs')\n",
    "targets = T.imatrix('targets') # these will be inputs shifted by 1\n",
    "mask_input_var = T.matrix('input_mask')\n",
    "\n",
    "net = build_hsoft_rnnlm(input_var, targets, mask_input_var, voc_size, emb_size, rec_size)\n",
    "out = L.layers.get_output(net)\n",
    "\n",
    "mask_idx = mask_input_var.nonzero()\n",
    "loss = -T.sum(T.log(out[mask_idx])) / T.sum(mask_input_var)\n",
    "\n",
    "params = L.layers.get_all_params(net, trainable=True)\n",
    "#updates = L.updates.rmsprop(loss, params, learning_rate=.001, rho=.9, epsilon=1e-06)\n",
    "updates = L.updates.adagrad(loss, params, learning_rate=.01)\n",
    "\n",
    "train_fn = theano.function([input_var, targets, mask_input_var], loss, updates=updates)\n",
    "\n",
    "#### for validation\n",
    "\n",
    "test_net = net # this line is just for compatibility later\n",
    "\n",
    "test_out = L.layers.get_output(net, deterministic=True)\n",
    "test_loss = -T.sum(T.log(test_out[mask_idx])) / T.sum(mask_input_var)\n",
    "\n",
    "#test_acc = T.mean(T.eq(T.argmax(test_out, axis=1), targets), dtype=theano.config.floatX)\n",
    "\n",
    "val_fn = theano.function([input_var, targets, mask_input_var], test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 10 batches in 1.51 sec.    training loss:\t\t8.43167209625\n",
      "Done 20 batches in 2.83 sec.    training loss:\t\t7.2098290205\n",
      "Done 30 batches in 4.47 sec.    training loss:\t\t6.80911650658\n",
      "Done 40 batches in 5.73 sec.    training loss:\t\t6.51092954874\n",
      "Done 50 batches in 7.50 sec.    training loss:\t\t6.4106634903\n",
      "Done 60 batches in 9.09 sec.    training loss:\t\t6.26619983514\n",
      "Done 70 batches in 10.56 sec.    training loss:\t\t6.16145815168\n",
      "Done 80 batches in 11.94 sec.    training loss:\t\t6.09250831604\n",
      "Done 90 batches in 13.37 sec.    training loss:\t\t6.03046529558\n",
      "Done 100 batches in 14.76 sec.    training loss:\t\t5.97374654293\n",
      "Done 110 batches in 16.47 sec.    training loss:\t\t5.9317473585\n",
      "Done 120 batches in 18.11 sec.    training loss:\t\t5.89309691588\n",
      "Done 130 batches in 19.50 sec.    training loss:\t\t5.85920016582\n",
      "Done 140 batches in 20.93 sec.    training loss:\t\t5.82559099197\n",
      "Done 150 batches in 22.43 sec.    training loss:\t\t5.79228741646\n",
      "Done 160 batches in 24.24 sec.    training loss:\t\t5.77439932823\n",
      "Done 170 batches in 25.64 sec.    training loss:\t\t5.75376809906\n",
      "Done 180 batches in 27.37 sec.    training loss:\t\t5.74241132736\n",
      "Done 190 batches in 28.74 sec.    training loss:\t\t5.72543793227\n",
      "Done 200 batches in 30.09 sec.    training loss:\t\t5.69992337704\n",
      "Done 210 batches in 31.37 sec.    training loss:\t\t5.68570503734\n",
      "Done 220 batches in 32.40 sec.    training loss:\t\t5.66819519346\n",
      "Done 230 batches in 33.95 sec.    training loss:\t\t5.66166653011\n",
      "Done 240 batches in 35.63 sec.    training loss:\t\t5.65602727532\n",
      "Done 250 batches in 36.77 sec.    training loss:\t\t5.64077597618\n",
      "Done 260 batches in 38.75 sec.    training loss:\t\t5.63532099907\n",
      "Done 270 batches in 40.19 sec.    training loss:\t\t5.62482058737\n",
      "Done 280 batches in 41.81 sec.    training loss:\t\t5.61674490997\n",
      "Done 290 batches in 43.20 sec.    training loss:\t\t5.60797794276\n",
      "Done 300 batches in 44.52 sec.    training loss:\t\t5.59678164641\n",
      "Done 310 batches in 46.06 sec.    training loss:\t\t5.58606273282\n",
      "Done 320 batches in 47.61 sec.    training loss:\t\t5.5766003564\n",
      "Done 330 batches in 48.75 sec.    training loss:\t\t5.55889301445\n",
      "Done 340 batches in 50.03 sec.    training loss:\t\t5.54379204301\n",
      "Done 350 batches in 51.55 sec.    training loss:\t\t5.53182162012\n",
      "Done 360 batches in 52.94 sec.    training loss:\t\t5.52238056395\n",
      "Done 370 batches in 54.17 sec.    training loss:\t\t5.50956055538\n",
      "Done 380 batches in 55.62 sec.    training loss:\t\t5.4950246836\n",
      "Done 390 batches in 56.81 sec.    training loss:\t\t5.48228798768\n",
      "Done 400 batches in 58.28 sec.    training loss:\t\t5.47080409646\n",
      "Done 410 batches in 59.79 sec.    training loss:\t\t5.4566345052\n",
      "Done 420 batches in 61.56 sec.    training loss:\t\t5.44789945285\n",
      "Done 430 batches in 62.69 sec.    training loss:\t\t5.43001877208\n",
      "Done 440 batches in 64.35 sec.    training loss:\t\t5.41582561298\n",
      "Done 450 batches in 65.87 sec.    training loss:\t\t5.40339264976\n",
      "Done 460 batches in 67.08 sec.    training loss:\t\t5.38751734029\n",
      "Done 470 batches in 68.61 sec.    training loss:\t\t5.37645206654\n",
      "Done 480 batches in 69.99 sec.    training loss:\t\t5.36504618128\n",
      "Done 490 batches in 71.72 sec.    training loss:\t\t5.35609140007\n",
      "Done 500 batches in 73.10 sec.    training loss:\t\t5.34268100357\n",
      "Done 510 batches in 74.53 sec.    training loss:\t\t5.33133210668\n",
      "Done 520 batches in 75.96 sec.    training loss:\t\t5.32104479991\n",
      "Done 530 batches in 77.29 sec.    training loss:\t\t5.3095070605\n",
      "Done 540 batches in 78.99 sec.    training loss:\t\t5.30122044616\n",
      "Done 550 batches in 80.24 sec.    training loss:\t\t5.28879330678\n",
      "Done 560 batches in 81.47 sec.    training loss:\t\t5.27874562272\n",
      "Done 570 batches in 83.02 sec.    training loss:\t\t5.26754318413\n",
      "Done 580 batches in 84.69 sec.    training loss:\t\t5.25898514575\n",
      "Done 590 batches in 86.45 sec.    training loss:\t\t5.24924984342\n",
      "Done 600 batches in 88.20 sec.    training loss:\t\t5.240482651\n",
      "Done 610 batches in 89.78 sec.    training loss:\t\t5.23167531881\n",
      "Done 620 batches in 91.05 sec.    training loss:\t\t5.22078376393\n",
      "Done 630 batches in 92.30 sec.    training loss:\t\t5.21135354345\n",
      "Done 640 batches in 93.87 sec.    training loss:\t\t5.20363413766\n",
      "Done 650 batches in 95.29 sec.    training loss:\t\t5.19451581515\n",
      "Done 660 batches in 97.08 sec.    training loss:\t\t5.18735614834\n",
      "Done 670 batches in 98.29 sec.    training loss:\t\t5.17809648158\n",
      "Done 680 batches in 99.89 sec.    training loss:\t\t5.16962288057\n",
      "Done 690 batches in 101.41 sec.    training loss:\t\t5.16112949675\n",
      "Done 700 batches in 102.80 sec.    training loss:\t\t5.15164027521\n",
      "Done 710 batches in 104.45 sec.    training loss:\t\t5.14437808218\n",
      "Done 720 batches in 105.80 sec.    training loss:\t\t5.13758091629\n",
      "Done 730 batches in 107.41 sec.    training loss:\t\t5.12911789254\n",
      "Done 740 batches in 108.71 sec.    training loss:\t\t5.1192463108\n",
      "Done 750 batches in 110.02 sec.    training loss:\t\t5.11038448461\n",
      "Done 760 batches in 111.31 sec.    training loss:\t\t5.10201024382\n",
      "Done 770 batches in 112.72 sec.    training loss:\t\t5.09289678165\n",
      "Done 780 batches in 113.96 sec.    training loss:\t\t5.0863782846\n",
      "Done 790 batches in 115.35 sec.    training loss:\t\t5.07908485056\n",
      "Done 800 batches in 116.36 sec.    training loss:\t\t5.06975994945\n",
      "Done 810 batches in 117.60 sec.    training loss:\t\t5.06240007024\n",
      "Done 820 batches in 119.18 sec.    training loss:\t\t5.0553190714\n",
      "Done 830 batches in 120.43 sec.    training loss:\t\t5.04762808214\n",
      "Done 840 batches in 122.00 sec.    training loss:\t\t5.04174022618\n",
      "Done 850 batches in 123.24 sec.    training loss:\t\t5.03257054245\n",
      "Done 860 batches in 124.93 sec.    training loss:\t\t5.02614202084\n",
      "Done 870 batches in 126.57 sec.    training loss:\t\t5.01895662258\n",
      "Done 880 batches in 127.92 sec.    training loss:\t\t5.01339380172\n",
      "Done 890 batches in 129.53 sec.    training loss:\t\t5.00673060337\n",
      "Done 900 batches in 131.00 sec.    training loss:\t\t5.00007989645\n",
      "Done 910 batches in 132.60 sec.    training loss:\t\t4.99348645682\n",
      "Done 920 batches in 134.05 sec.    training loss:\t\t4.98711539144\n",
      "Done 930 batches in 135.38 sec.    training loss:\t\t4.98241061652\n",
      "Done 940 batches in 136.76 sec.    training loss:\t\t4.97624325093\n",
      "Done 950 batches in 138.13 sec.    training loss:\t\t4.97014943198\n",
      "Done 960 batches in 139.59 sec.    training loss:\t\t4.96573123659\n",
      "Done 970 batches in 141.03 sec.    training loss:\t\t4.95844067421\n",
      "Done 980 batches in 142.59 sec.    training loss:\t\t4.95492645891\n",
      "Done 990 batches in 143.96 sec.    training loss:\t\t4.94819200328\n",
      "Done 1000 batches in 145.05 sec.    training loss:\t\t4.94099868631\n",
      "Done 1010 batches in 146.40 sec.    training loss:\t\t4.93446469283\n",
      "Done 1020 batches in 147.71 sec.    training loss:\t\t4.92856581234\n",
      "Done 1030 batches in 149.29 sec.    training loss:\t\t4.92467927724\n",
      "Done 1040 batches in 150.70 sec.    training loss:\t\t4.91897983413\n",
      "Done 1050 batches in 151.94 sec.    training loss:\t\t4.91341022128\n",
      "Done 1060 batches in 153.56 sec.    training loss:\t\t4.90886189848\n",
      "Done 1070 batches in 154.97 sec.    training loss:\t\t4.90444701743\n",
      "Done 1080 batches in 156.36 sec.    training loss:\t\t4.89983381894\n",
      "Done 1090 batches in 157.86 sec.    training loss:\t\t4.89380136761\n",
      "Done 1100 batches in 159.28 sec.    training loss:\t\t4.88886343978\n",
      "Done 1110 batches in 160.66 sec.    training loss:\t\t4.88395556888\n",
      "Done 1120 batches in 162.20 sec.    training loss:\t\t4.87976937294\n",
      "Done 1130 batches in 163.74 sec.    training loss:\t\t4.87455851715\n",
      "Done 1140 batches in 165.34 sec.    training loss:\t\t4.86919811462\n",
      "Done 1150 batches in 166.86 sec.    training loss:\t\t4.86433737651\n",
      "Done 1160 batches in 168.12 sec.    training loss:\t\t4.8599802566\n",
      "Done 1170 batches in 169.52 sec.    training loss:\t\t4.85563532002\n",
      "Done 1180 batches in 170.79 sec.    training loss:\t\t4.85068397542\n",
      "Done 1190 batches in 172.32 sec.    training loss:\t\t4.84483197517\n",
      "Done 1200 batches in 173.70 sec.    training loss:\t\t4.84088519732\n",
      "Done 1210 batches in 175.15 sec.    training loss:\t\t4.83643301735\n",
      "Done 1220 batches in 176.68 sec.    training loss:\t\t4.83177459084\n",
      "Done 1230 batches in 178.15 sec.    training loss:\t\t4.82797108549\n",
      "Done 1240 batches in 179.75 sec.    training loss:\t\t4.82293160635\n",
      "Done 1250 batches in 181.19 sec.    training loss:\t\t4.81803514633\n",
      "Done 1260 batches in 182.39 sec.    training loss:\t\t4.81194669141\n",
      "Done 1270 batches in 184.17 sec.    training loss:\t\t4.80851235315\n",
      "Done 1280 batches in 185.44 sec.    training loss:\t\t4.80409552641\n",
      "Done 1290 batches in 186.89 sec.    training loss:\t\t4.79844807137\n",
      "Done 1300 batches in 188.13 sec.    training loss:\t\t4.79307809921\n",
      "Done 1310 batches in 189.51 sec.    training loss:\t\t4.7887679444\n",
      "Done 1320 batches in 190.86 sec.    training loss:\t\t4.784968564\n",
      "Done 1330 batches in 192.50 sec.    training loss:\t\t4.78198151033\n",
      "Done 1340 batches in 193.89 sec.    training loss:\t\t4.77759942813\n",
      "Done 1350 batches in 195.52 sec.    training loss:\t\t4.77457688597\n",
      "Done 1360 batches in 196.92 sec.    training loss:\t\t4.77014979071\n",
      "Done 1370 batches in 198.23 sec.    training loss:\t\t4.76544593268\n",
      "Done 1380 batches in 199.76 sec.    training loss:\t\t4.76182668762\n",
      "Done 1390 batches in 201.06 sec.    training loss:\t\t4.75813822163\n",
      "Done 1400 batches in 202.56 sec.    training loss:\t\t4.75424803155\n",
      "Done 1410 batches in 204.30 sec.    training loss:\t\t4.75052361505\n",
      "Done 1420 batches in 205.92 sec.    training loss:\t\t4.74747484671\n",
      "Done 1430 batches in 207.23 sec.    training loss:\t\t4.74445557728\n",
      "Done 1440 batches in 209.23 sec.    training loss:\t\t4.74142069817\n",
      "Done 1450 batches in 210.57 sec.    training loss:\t\t4.73697376087\n",
      "Done 1460 batches in 212.52 sec.    training loss:\t\t4.73483462987\n",
      "Done 1470 batches in 213.71 sec.    training loss:\t\t4.73128877205\n",
      "Done 1480 batches in 215.02 sec.    training loss:\t\t4.72783237164\n",
      "Done 1490 batches in 216.31 sec.    training loss:\t\t4.72430071479\n",
      "Done 1500 batches in 217.48 sec.    training loss:\t\t4.71948061736\n",
      "Done 1510 batches in 218.75 sec.    training loss:\t\t4.7149036398\n",
      "Done 1520 batches in 220.06 sec.    training loss:\t\t4.71177773334\n",
      "Done 1530 batches in 221.39 sec.    training loss:\t\t4.70833730651\n",
      "Done 1540 batches in 222.73 sec.    training loss:\t\t4.70451738819\n",
      "Done 1550 batches in 224.55 sec.    training loss:\t\t4.70268423619\n",
      "Done 1560 batches in 225.72 sec.    training loss:\t\t4.69919448846\n",
      "Done 1570 batches in 227.13 sec.    training loss:\t\t4.69601393353\n",
      "Done 1580 batches in 228.37 sec.    training loss:\t\t4.69271722353\n",
      "Done 1590 batches in 229.45 sec.    training loss:\t\t4.68848862048\n",
      "Done 1600 batches in 230.89 sec.    training loss:\t\t4.68492221385\n",
      "Done 1610 batches in 232.28 sec.    training loss:\t\t4.68162429436\n",
      "Done 1620 batches in 233.77 sec.    training loss:\t\t4.67828457061\n",
      "Done 1630 batches in 235.48 sec.    training loss:\t\t4.67512697003\n",
      "Done 1640 batches in 237.17 sec.    training loss:\t\t4.67253631891\n",
      "Done 1650 batches in 238.54 sec.    training loss:\t\t4.67071702206\n",
      "Done 1660 batches in 239.96 sec.    training loss:\t\t4.66762206971\n",
      "Done 1670 batches in 241.33 sec.    training loss:\t\t4.66453163981\n",
      "Done 1680 batches in 242.73 sec.    training loss:\t\t4.66116335165\n",
      "Done 1690 batches in 244.04 sec.    training loss:\t\t4.65766694983\n",
      "Done 1700 batches in 245.47 sec.    training loss:\t\t4.65547289021\n",
      "Done 1710 batches in 246.49 sec.    training loss:\t\t4.65163504073\n",
      "Done 1720 batches in 247.81 sec.    training loss:\t\t4.64900523286\n",
      "Done 1730 batches in 249.15 sec.    training loss:\t\t4.6461469478\n",
      "Done 1740 batches in 250.51 sec.    training loss:\t\t4.64280137961\n",
      "Done 1750 batches in 251.61 sec.    training loss:\t\t4.63967271914\n",
      "Done 1760 batches in 253.15 sec.    training loss:\t\t4.63689504821\n",
      "Done 1770 batches in 254.44 sec.    training loss:\t\t4.63386148679\n",
      "Done 1780 batches in 255.89 sec.    training loss:\t\t4.63128709177\n",
      "Done 1790 batches in 257.25 sec.    training loss:\t\t4.62715235223\n",
      "Done 1800 batches in 258.60 sec.    training loss:\t\t4.62318105446\n",
      "Done 1810 batches in 260.22 sec.    training loss:\t\t4.62033490977\n",
      "Done 1820 batches in 261.94 sec.    training loss:\t\t4.61866089263\n",
      "Done 1830 batches in 263.14 sec.    training loss:\t\t4.61563916571\n",
      "Done 1840 batches in 264.68 sec.    training loss:\t\t4.61341593317\n",
      "Done 1850 batches in 266.16 sec.    training loss:\t\t4.61127292453\n",
      "Done 1860 batches in 267.63 sec.    training loss:\t\t4.60871079955\n",
      "Done 1870 batches in 269.12 sec.    training loss:\t\t4.60617840698\n",
      "Done 1880 batches in 270.47 sec.    training loss:\t\t4.60457699071\n",
      "Done 1890 batches in 271.85 sec.    training loss:\t\t4.60221199333\n",
      "Done 1900 batches in 273.25 sec.    training loss:\t\t4.60076997732\n",
      "Done 1910 batches in 274.98 sec.    training loss:\t\t4.59863523351\n",
      "Done 1920 batches in 276.63 sec.    training loss:\t\t4.59694704326\n",
      "Done 1930 batches in 278.08 sec.    training loss:\t\t4.59448198864\n",
      "Done 1940 batches in 279.33 sec.    training loss:\t\t4.59154689017\n",
      "Done 1950 batches in 280.74 sec.    training loss:\t\t4.58906369967\n",
      "Done 1960 batches in 282.17 sec.    training loss:\t\t4.58701759784\n",
      "Done 1970 batches in 283.67 sec.    training loss:\t\t4.58481969168\n",
      "Done 1980 batches in 284.96 sec.    training loss:\t\t4.58274445955\n",
      "Done 1990 batches in 286.00 sec.    training loss:\t\t4.57932826179\n",
      "Done 2000 batches in 287.65 sec.    training loss:\t\t4.57779159081\n",
      "Done 2010 batches in 289.02 sec.    training loss:\t\t4.57563979958\n",
      "Done 2020 batches in 290.61 sec.    training loss:\t\t4.57326883078\n",
      "Done 2030 batches in 291.79 sec.    training loss:\t\t4.57064014879\n",
      "Done 2040 batches in 293.24 sec.    training loss:\t\t4.56876869073\n",
      "Done 2050 batches in 294.71 sec.    training loss:\t\t4.56627694153\n",
      "Done 2060 batches in 295.88 sec.    training loss:\t\t4.5638338251\n",
      "Done 2070 batches in 297.49 sec.    training loss:\t\t4.56207790352\n",
      "Done 2080 batches in 298.79 sec.    training loss:\t\t4.55962495219\n",
      "Done 2090 batches in 300.37 sec.    training loss:\t\t4.55792700754\n",
      "Done 2100 batches in 301.98 sec.    training loss:\t\t4.55571927865\n",
      "Done 2110 batches in 303.31 sec.    training loss:\t\t4.55372455052\n",
      "Done 2120 batches in 304.87 sec.    training loss:\t\t4.55145658097\n",
      "Done 2130 batches in 306.21 sec.    training loss:\t\t4.54866097253\n",
      "Done 2140 batches in 307.76 sec.    training loss:\t\t4.54648758296\n",
      "Done 2150 batches in 309.48 sec.    training loss:\t\t4.54490670426\n",
      "Done 2160 batches in 311.11 sec.    training loss:\t\t4.54318605154\n",
      "Done 2170 batches in 312.48 sec.    training loss:\t\t4.54112012848\n",
      "Done 2180 batches in 313.76 sec.    training loss:\t\t4.53898358422\n",
      "Done 2190 batches in 315.37 sec.    training loss:\t\t4.53678378515\n",
      "Done 2200 batches in 316.60 sec.    training loss:\t\t4.5345451083\n",
      "Done 2210 batches in 317.95 sec.    training loss:\t\t4.53259074915\n",
      "Done 2220 batches in 319.28 sec.    training loss:\t\t4.53119829646\n",
      "Done 2230 batches in 320.57 sec.    training loss:\t\t4.52944711313\n",
      "Done 2240 batches in 321.85 sec.    training loss:\t\t4.52748645384\n",
      "Done 2250 batches in 323.50 sec.    training loss:\t\t4.52597456816\n",
      "Done 2260 batches in 325.06 sec.    training loss:\t\t4.52430812996\n",
      "Done 2270 batches in 326.27 sec.    training loss:\t\t4.52209933046\n",
      "Done 2280 batches in 327.69 sec.    training loss:\t\t4.52041808137\n",
      "Done 2290 batches in 328.82 sec.    training loss:\t\t4.5179235448\n",
      "Done 2300 batches in 330.32 sec.    training loss:\t\t4.5159664634\n",
      "Done 2310 batches in 331.52 sec.    training loss:\t\t4.51324983995\n",
      "Done 2320 batches in 333.10 sec.    training loss:\t\t4.51139812634\n",
      "Done 2330 batches in 334.38 sec.    training loss:\t\t4.50878146411\n",
      "Done 2340 batches in 335.49 sec.    training loss:\t\t4.50672046496\n",
      "Done 2350 batches in 337.03 sec.    training loss:\t\t4.50493579398\n",
      "Done 2360 batches in 338.45 sec.    training loss:\t\t4.50262354441\n",
      "Done 2370 batches in 339.79 sec.    training loss:\t\t4.50053016556\n",
      "Done 2380 batches in 341.19 sec.    training loss:\t\t4.49819790926\n",
      "Done 2390 batches in 342.68 sec.    training loss:\t\t4.49646961689\n",
      "Done 2400 batches in 344.06 sec.    training loss:\t\t4.49464274794\n",
      "Done 2410 batches in 345.26 sec.    training loss:\t\t4.49268471374\n",
      "Done 2420 batches in 346.46 sec.    training loss:\t\t4.49143570405\n",
      "Done 2430 batches in 347.89 sec.    training loss:\t\t4.49001918418\n",
      "Done 2440 batches in 349.43 sec.    training loss:\t\t4.48861259457\n",
      "Done 2450 batches in 351.05 sec.    training loss:\t\t4.48660625847\n",
      "Done 2460 batches in 352.84 sec.    training loss:\t\t4.48568508858\n",
      "Done 2470 batches in 354.13 sec.    training loss:\t\t4.48420424124\n",
      "Done 2480 batches in 355.76 sec.    training loss:\t\t4.48304093961\n",
      "Done 2490 batches in 357.41 sec.    training loss:\t\t4.4820263763\n",
      "Done 2500 batches in 358.75 sec.    training loss:\t\t4.48007048235\n",
      "Done 2510 batches in 359.94 sec.    training loss:\t\t4.47857229567\n",
      "Done 2520 batches in 361.62 sec.    training loss:\t\t4.47703700387\n",
      "Done 2530 batches in 363.47 sec.    training loss:\t\t4.47619965783\n",
      "Done 2540 batches in 364.91 sec.    training loss:\t\t4.47506235462\n",
      "Done 2550 batches in 366.23 sec.    training loss:\t\t4.47349132659\n",
      "Done 2560 batches in 367.65 sec.    training loss:\t\t4.47226891359\n",
      "Done 2570 batches in 369.11 sec.    training loss:\t\t4.47069903079\n",
      "Done 2580 batches in 370.43 sec.    training loss:\t\t4.46901529223\n",
      "Done 2590 batches in 372.16 sec.    training loss:\t\t4.46751986811\n",
      "Done 2600 batches in 373.45 sec.    training loss:\t\t4.4656329232\n",
      "Done 2610 batches in 374.86 sec.    training loss:\t\t4.46407663557\n",
      "Done 2620 batches in 376.06 sec.    training loss:\t\t4.46165608559\n",
      "Done 2630 batches in 377.45 sec.    training loss:\t\t4.46035217332\n",
      "Done 2640 batches in 378.87 sec.    training loss:\t\t4.45868205112\n",
      "Done 2650 batches in 380.79 sec.    training loss:\t\t4.45774250696\n",
      "Done 2660 batches in 382.55 sec.    training loss:\t\t4.45689710352\n",
      "Done 2670 batches in 384.64 sec.    training loss:\t\t4.45578710167\n",
      "Done 2680 batches in 386.37 sec.    training loss:\t\t4.45498642005\n",
      "Done 2690 batches in 387.62 sec.    training loss:\t\t4.45352953358\n",
      "Done 2700 batches in 389.22 sec.    training loss:\t\t4.4522416355\n",
      "Done 2710 batches in 390.62 sec.    training loss:\t\t4.45058908419\n",
      "Done 2720 batches in 391.99 sec.    training loss:\t\t4.44909626351\n",
      "Done 2730 batches in 393.49 sec.    training loss:\t\t4.44724993182\n",
      "Done 2740 batches in 394.92 sec.    training loss:\t\t4.44597075899\n",
      "Done 2750 batches in 396.49 sec.    training loss:\t\t4.44500240057\n",
      "Done 2760 batches in 397.67 sec.    training loss:\t\t4.44340095088\n",
      "Done 2770 batches in 399.09 sec.    training loss:\t\t4.44188753732\n",
      "Done 2780 batches in 400.45 sec.    training loss:\t\t4.44054764235\n",
      "Done 2790 batches in 401.69 sec.    training loss:\t\t4.43864667544\n",
      "Done 2800 batches in 402.81 sec.    training loss:\t\t4.43620415398\n",
      "Done 2810 batches in 404.42 sec.    training loss:\t\t4.43519375197\n",
      "Done 2820 batches in 406.01 sec.    training loss:\t\t4.43407621299\n",
      "Done 2830 batches in 407.47 sec.    training loss:\t\t4.43232536341\n",
      "Done 2840 batches in 408.95 sec.    training loss:\t\t4.43094402526\n",
      "Done 2850 batches in 410.42 sec.    training loss:\t\t4.42987007024\n",
      "Done 2860 batches in 411.80 sec.    training loss:\t\t4.4284713435\n",
      "Done 2870 batches in 413.09 sec.    training loss:\t\t4.42710976385\n",
      "Done 2880 batches in 414.42 sec.    training loss:\t\t4.42617786568\n",
      "Done 2890 batches in 415.73 sec.    training loss:\t\t4.42485028542\n",
      "Done 2900 batches in 417.31 sec.    training loss:\t\t4.42389987641\n",
      "Done 2910 batches in 419.04 sec.    training loss:\t\t4.42312203551\n",
      "Done 2920 batches in 420.36 sec.    training loss:\t\t4.42169166322\n",
      "Done 2930 batches in 421.94 sec.    training loss:\t\t4.42099298755\n",
      "Done 2940 batches in 423.46 sec.    training loss:\t\t4.41962029747\n",
      "Done 2950 batches in 424.67 sec.    training loss:\t\t4.41769257966\n",
      "Done 2960 batches in 425.82 sec.    training loss:\t\t4.41581053001\n",
      "Done 2970 batches in 427.54 sec.    training loss:\t\t4.41468205677\n",
      "Done 2980 batches in 428.83 sec.    training loss:\t\t4.41365966653\n",
      "Done 2990 batches in 430.00 sec.    training loss:\t\t4.41216476825\n",
      "Done 3000 batches in 431.31 sec.    training loss:\t\t4.40999615844\n",
      "Done 3010 batches in 432.75 sec.    training loss:\t\t4.40856796357\n",
      "Done 3020 batches in 434.28 sec.    training loss:\t\t4.40755504346\n",
      "Done 3030 batches in 435.71 sec.    training loss:\t\t4.40635523411\n",
      "Done 3040 batches in 437.10 sec.    training loss:\t\t4.40527463312\n",
      "Done 3050 batches in 438.50 sec.    training loss:\t\t4.40402773115\n",
      "Done 3060 batches in 439.88 sec.    training loss:\t\t4.40262220701\n",
      "Done 3070 batches in 441.38 sec.    training loss:\t\t4.40179297815\n",
      "Done 3080 batches in 443.23 sec.    training loss:\t\t4.40078397486\n",
      "Done 3090 batches in 444.59 sec.    training loss:\t\t4.39905987334\n",
      "Done 3100 batches in 446.03 sec.    training loss:\t\t4.39798719168\n",
      "Done 3110 batches in 447.63 sec.    training loss:\t\t4.39677050988\n",
      "Done 3120 batches in 448.83 sec.    training loss:\t\t4.39505893542\n",
      "Done 3130 batches in 450.25 sec.    training loss:\t\t4.39376160565\n",
      "Done 3140 batches in 451.97 sec.    training loss:\t\t4.39227431663\n",
      "Done 3150 batches in 453.49 sec.    training loss:\t\t4.39111653502\n",
      "Done 3160 batches in 454.96 sec.    training loss:\t\t4.38996524396\n",
      "Done 3170 batches in 456.32 sec.    training loss:\t\t4.38907161263\n",
      "Done 3180 batches in 457.51 sec.    training loss:\t\t4.38757017956\n",
      "Done 3190 batches in 458.82 sec.    training loss:\t\t4.38673318695\n",
      "Done 3200 batches in 460.02 sec.    training loss:\t\t4.38531508237\n",
      "Done 3210 batches in 461.35 sec.    training loss:\t\t4.3840798407\n",
      "Done 3220 batches in 463.29 sec.    training loss:\t\t4.38297917961\n",
      "Done 3230 batches in 464.85 sec.    training loss:\t\t4.38210338579\n",
      "Done 3240 batches in 466.37 sec.    training loss:\t\t4.38085933401\n",
      "Done 3250 batches in 467.87 sec.    training loss:\t\t4.38011162919\n",
      "Done 3260 batches in 469.41 sec.    training loss:\t\t4.37916222145\n",
      "Done 3270 batches in 470.61 sec.    training loss:\t\t4.37758054398\n",
      "Done 3280 batches in 471.84 sec.    training loss:\t\t4.37621383769\n",
      "Done 3290 batches in 473.31 sec.    training loss:\t\t4.3751847852\n",
      "Done 3300 batches in 474.83 sec.    training loss:\t\t4.37461312532\n",
      "Done 3310 batches in 476.21 sec.    training loss:\t\t4.37363879832\n",
      "Done 3320 batches in 477.55 sec.    training loss:\t\t4.37277189249\n",
      "Done 3330 batches in 479.04 sec.    training loss:\t\t4.37163785743\n",
      "Done 3340 batches in 480.52 sec.    training loss:\t\t4.37046787068\n",
      "Done 3350 batches in 481.95 sec.    training loss:\t\t4.36979307146\n",
      "Done 3360 batches in 483.86 sec.    training loss:\t\t4.36879367914\n",
      "Done 3370 batches in 485.44 sec.    training loss:\t\t4.36779893838\n",
      "Done 3380 batches in 487.11 sec.    training loss:\t\t4.36634925378\n",
      "Done 3390 batches in 488.22 sec.    training loss:\t\t4.3649048078\n",
      "Done 3400 batches in 489.66 sec.    training loss:\t\t4.3635433514\n",
      "Done 3410 batches in 490.96 sec.    training loss:\t\t4.36226563125\n",
      "Done 3420 batches in 492.08 sec.    training loss:\t\t4.36102839667\n",
      "Done 3430 batches in 493.45 sec.    training loss:\t\t4.35997928343\n",
      "Done 3440 batches in 494.82 sec.    training loss:\t\t4.3593080836\n",
      "Done 3450 batches in 496.29 sec.    training loss:\t\t4.3581058422\n",
      "Done 3460 batches in 497.74 sec.    training loss:\t\t4.35734466273\n",
      "Done 3470 batches in 499.03 sec.    training loss:\t\t4.35608685161\n",
      "Done 3480 batches in 500.35 sec.    training loss:\t\t4.35518841052\n",
      "Done 3490 batches in 501.87 sec.    training loss:\t\t4.35453313367\n",
      "Done 3500 batches in 503.39 sec.    training loss:\t\t4.35341992283\n",
      "Done 3510 batches in 504.90 sec.    training loss:\t\t4.35306206741\n",
      "Done 3520 batches in 506.16 sec.    training loss:\t\t4.3517379588\n",
      "Done 3530 batches in 507.48 sec.    training loss:\t\t4.35051055812\n",
      "Done 3540 batches in 509.02 sec.    training loss:\t\t4.34953272929\n",
      "Done 3550 batches in 510.44 sec.    training loss:\t\t4.34883749465\n",
      "Done 3560 batches in 511.99 sec.    training loss:\t\t4.34772859919\n",
      "Done 3570 batches in 513.42 sec.    training loss:\t\t4.34668618905\n",
      "Done 3580 batches in 514.87 sec.    training loss:\t\t4.34545955252\n",
      "Done 3590 batches in 516.59 sec.    training loss:\t\t4.34492654269\n",
      "Done 3600 batches in 517.99 sec.    training loss:\t\t4.34383798341\n",
      "Done 3610 batches in 519.53 sec.    training loss:\t\t4.34259924228\n",
      "Done 3620 batches in 520.70 sec.    training loss:\t\t4.34139285733\n",
      "Done 3630 batches in 522.08 sec.    training loss:\t\t4.34058179685\n",
      "Done 3640 batches in 523.59 sec.    training loss:\t\t4.33974799566\n",
      "Done 3650 batches in 524.75 sec.    training loss:\t\t4.33805870742\n",
      "Done 3660 batches in 526.08 sec.    training loss:\t\t4.33694121369\n",
      "Done 3670 batches in 527.70 sec.    training loss:\t\t4.33612937265\n",
      "Done 3680 batches in 529.14 sec.    training loss:\t\t4.33516647207\n",
      "Done 3690 batches in 530.67 sec.    training loss:\t\t4.33437061026\n",
      "Done 3700 batches in 532.09 sec.    training loss:\t\t4.33361261001\n",
      "Done 3710 batches in 533.56 sec.    training loss:\t\t4.33299906267\n",
      "Done 3720 batches in 534.96 sec.    training loss:\t\t4.33218978015\n",
      "Done 3730 batches in 536.12 sec.    training loss:\t\t4.33095796242\n",
      "Done 3740 batches in 537.54 sec.    training loss:\t\t4.32995344713\n",
      "Done 3750 batches in 538.97 sec.    training loss:\t\t4.32923567575\n",
      "Done 3760 batches in 540.13 sec.    training loss:\t\t4.32795164598\n",
      "Done 3770 batches in 541.81 sec.    training loss:\t\t4.32733970991\n",
      "Done 3780 batches in 543.18 sec.    training loss:\t\t4.32651028684\n",
      "Done 3790 batches in 544.43 sec.    training loss:\t\t4.32565057894\n",
      "Done 3800 batches in 545.79 sec.    training loss:\t\t4.32475512548\n",
      "Done 3810 batches in 547.16 sec.    training loss:\t\t4.32386211923\n",
      "Done 3820 batches in 548.46 sec.    training loss:\t\t4.32244213994\n",
      "Done 3830 batches in 550.29 sec.    training loss:\t\t4.32225868976\n",
      "Done 3840 batches in 551.61 sec.    training loss:\t\t4.32139988778\n",
      "Done 3850 batches in 552.75 sec.    training loss:\t\t4.32022551233\n",
      "Done 3860 batches in 554.35 sec.    training loss:\t\t4.31917733034\n",
      "Done 3870 batches in 555.65 sec.    training loss:\t\t4.31818999271\n",
      "Done 3880 batches in 557.03 sec.    training loss:\t\t4.31749902804\n",
      "Done 3890 batches in 558.40 sec.    training loss:\t\t4.31669040829\n",
      "Done 3900 batches in 559.74 sec.    training loss:\t\t4.31554449265\n",
      "Done 3910 batches in 561.30 sec.    training loss:\t\t4.31466586382\n",
      "Done 3920 batches in 563.00 sec.    training loss:\t\t4.31416080302\n",
      "Done 3930 batches in 564.68 sec.    training loss:\t\t4.31308995033\n",
      "Done 3940 batches in 566.05 sec.    training loss:\t\t4.31203061088\n",
      "Done 3950 batches in 567.28 sec.    training loss:\t\t4.31086020953\n",
      "Done 3960 batches in 568.68 sec.    training loss:\t\t4.31011626486\n",
      "Done 3970 batches in 569.95 sec.    training loss:\t\t4.30907626837\n",
      "Done 3980 batches in 571.18 sec.    training loss:\t\t4.30795044755\n",
      "Done 3990 batches in 572.54 sec.    training loss:\t\t4.30722092572\n",
      "Done 4000 batches in 573.84 sec.    training loss:\t\t4.30627732372\n",
      "Done 4010 batches in 575.40 sec.    training loss:\t\t4.30543674757\n",
      "Done 4020 batches in 577.32 sec.    training loss:\t\t4.30471295231\n",
      "Done 4030 batches in 578.65 sec.    training loss:\t\t4.30341684883\n",
      "Done 4040 batches in 580.93 sec.    training loss:\t\t4.30290282262\n",
      "Done 4050 batches in 582.30 sec.    training loss:\t\t4.302018084\n",
      "Done 4060 batches in 583.68 sec.    training loss:\t\t4.30094330129\n",
      "Done 4070 batches in 584.95 sec.    training loss:\t\t4.29971462638\n",
      "Done 4080 batches in 586.26 sec.    training loss:\t\t4.29860809299\n",
      "Done 4090 batches in 587.79 sec.    training loss:\t\t4.29787891092\n",
      "Done 4100 batches in 589.10 sec.    training loss:\t\t4.29704234338\n",
      "Done 4110 batches in 590.40 sec.    training loss:\t\t4.29634936442\n",
      "Done 4120 batches in 591.97 sec.    training loss:\t\t4.29549278316\n",
      "Done 4130 batches in 593.51 sec.    training loss:\t\t4.29447749233\n",
      "Done 4140 batches in 594.80 sec.    training loss:\t\t4.29347164239\n",
      "Done 4150 batches in 596.82 sec.    training loss:\t\t4.29276055589\n",
      "Done 4160 batches in 598.04 sec.    training loss:\t\t4.29125933401\n",
      "Done 4170 batches in 599.43 sec.    training loss:\t\t4.29010538243\n",
      "Done 4180 batches in 600.87 sec.    training loss:\t\t4.28919093632\n",
      "Done 4190 batches in 602.26 sec.    training loss:\t\t4.28871828047\n",
      "Done 4200 batches in 604.03 sec.    training loss:\t\t4.28826402091\n",
      "Done 4210 batches in 605.52 sec.    training loss:\t\t4.28755796828\n",
      "Done 4220 batches in 606.88 sec.    training loss:\t\t4.28648489612\n",
      "Done 4230 batches in 608.67 sec.    training loss:\t\t4.28601079827\n",
      "Done 4240 batches in 609.94 sec.    training loss:\t\t4.28502158939\n",
      "Done 4250 batches in 611.59 sec.    training loss:\t\t4.28437654865\n",
      "Done 4260 batches in 613.22 sec.    training loss:\t\t4.28370434527\n",
      "Done 4270 batches in 614.55 sec.    training loss:\t\t4.28289158333\n",
      "Done 4280 batches in 616.23 sec.    training loss:\t\t4.28215129866\n",
      "Done 4290 batches in 617.43 sec.    training loss:\t\t4.28123222853\n",
      "Done 4300 batches in 618.78 sec.    training loss:\t\t4.28065057777\n",
      "Done 4310 batches in 620.01 sec.    training loss:\t\t4.27981005539\n",
      "Done 4320 batches in 621.41 sec.    training loss:\t\t4.27893521874\n",
      "Done 4330 batches in 622.77 sec.    training loss:\t\t4.27783353059\n",
      "Done 4340 batches in 624.16 sec.    training loss:\t\t4.27687210152\n",
      "Done 4350 batches in 625.60 sec.    training loss:\t\t4.27594867542\n",
      "Done 4360 batches in 627.42 sec.    training loss:\t\t4.27505607036\n",
      "Done 4370 batches in 629.02 sec.    training loss:\t\t4.27420404787\n",
      "Done 4380 batches in 630.77 sec.    training loss:\t\t4.27340304639\n",
      "Done 4390 batches in 632.18 sec.    training loss:\t\t4.27225529268\n",
      "Done 4400 batches in 633.77 sec.    training loss:\t\t4.27145636699\n",
      "Done 4410 batches in 635.29 sec.    training loss:\t\t4.27073192056\n",
      "Done 4420 batches in 636.52 sec.    training loss:\t\t4.26945289853\n",
      "Done 4430 batches in 637.71 sec.    training loss:\t\t4.26862078208\n",
      "Done 4440 batches in 639.04 sec.    training loss:\t\t4.26772281962\n",
      "Done 4450 batches in 640.38 sec.    training loss:\t\t4.26677771831\n",
      "Done 4460 batches in 641.78 sec.    training loss:\t\t4.26561586237\n",
      "Done 4470 batches in 643.57 sec.    training loss:\t\t4.26482285098\n",
      "Done 4480 batches in 644.69 sec.    training loss:\t\t4.26375329361\n",
      "Done 4490 batches in 645.94 sec.    training loss:\t\t4.26279312609\n",
      "Done 4500 batches in 647.23 sec.    training loss:\t\t4.26182914692\n",
      "Done 4510 batches in 648.73 sec.    training loss:\t\t4.26122824501\n",
      "Done 4520 batches in 650.00 sec.    training loss:\t\t4.26014406206\n",
      "Done 4530 batches in 651.21 sec.    training loss:\t\t4.2589947611\n",
      "Done 4540 batches in 652.60 sec.    training loss:\t\t4.25831907079\n",
      "Done 4550 batches in 653.85 sec.    training loss:\t\t4.25735271637\n",
      "Done 4560 batches in 655.29 sec.    training loss:\t\t4.256478802\n",
      "Done 4570 batches in 656.77 sec.    training loss:\t\t4.25577754244\n",
      "Done 4580 batches in 658.33 sec.    training loss:\t\t4.25502532127\n",
      "Done 4590 batches in 659.55 sec.    training loss:\t\t4.25439636666\n",
      "Done 4600 batches in 660.86 sec.    training loss:\t\t4.25382416165\n",
      "Done 4610 batches in 662.29 sec.    training loss:\t\t4.25277344317\n",
      "Done 4620 batches in 663.43 sec.    training loss:\t\t4.25166998524\n",
      "Done 4630 batches in 665.22 sec.    training loss:\t\t4.25128314217\n",
      "Done 4640 batches in 666.63 sec.    training loss:\t\t4.25044725769\n",
      "Done 4650 batches in 668.06 sec.    training loss:\t\t4.24977124353\n",
      "Done 4660 batches in 669.69 sec.    training loss:\t\t4.24936271062\n",
      "Done 4670 batches in 671.11 sec.    training loss:\t\t4.24858648277\n",
      "Done 4680 batches in 672.54 sec.    training loss:\t\t4.24781865681\n",
      "Done 4690 batches in 674.15 sec.    training loss:\t\t4.24736896197\n",
      "Done 4700 batches in 675.81 sec.    training loss:\t\t4.24671351884\n",
      "Done 4710 batches in 677.82 sec.    training loss:\t\t4.2461663765\n",
      "Done 4720 batches in 679.22 sec.    training loss:\t\t4.24568544212\n",
      "Done 4730 batches in 680.74 sec.    training loss:\t\t4.24488309907\n",
      "Done 4740 batches in 682.09 sec.    training loss:\t\t4.24408433322\n",
      "Done 4750 batches in 683.43 sec.    training loss:\t\t4.24300451208\n",
      "Done 4760 batches in 685.04 sec.    training loss:\t\t4.24255156096\n",
      "Done 4770 batches in 686.34 sec.    training loss:\t\t4.24168508338\n",
      "Done 4780 batches in 687.84 sec.    training loss:\t\t4.24100253632\n",
      "Done 4790 batches in 689.39 sec.    training loss:\t\t4.24069568688\n",
      "Done 4800 batches in 690.72 sec.    training loss:\t\t4.23972629085\n",
      "Done 4810 batches in 692.09 sec.    training loss:\t\t4.2388146391\n",
      "Done 4820 batches in 693.76 sec.    training loss:\t\t4.23815390094\n",
      "Done 4830 batches in 695.22 sec.    training loss:\t\t4.23756975501\n",
      "Done 4840 batches in 696.61 sec.    training loss:\t\t4.23686600761\n",
      "Done 4850 batches in 697.92 sec.    training loss:\t\t4.23612162216\n",
      "Done 4860 batches in 699.16 sec.    training loss:\t\t4.23555294687\n",
      "Done 4870 batches in 700.70 sec.    training loss:\t\t4.23497952901\n",
      "Done 4880 batches in 702.06 sec.    training loss:\t\t4.23421323754\n",
      "Done 4890 batches in 703.58 sec.    training loss:\t\t4.23350299336\n",
      "Done 4900 batches in 704.98 sec.    training loss:\t\t4.23279034147\n",
      "Done 4910 batches in 706.19 sec.    training loss:\t\t4.23184811049\n",
      "Done 4920 batches in 707.53 sec.    training loss:\t\t4.231333931\n",
      "Done 4930 batches in 708.78 sec.    training loss:\t\t4.23040528699\n",
      "Done 4940 batches in 710.28 sec.    training loss:\t\t4.2295819382\n",
      "Done 4950 batches in 711.44 sec.    training loss:\t\t4.22885533988\n",
      "Done 4960 batches in 712.67 sec.    training loss:\t\t4.22770851038\n",
      "Done 4970 batches in 714.37 sec.    training loss:\t\t4.22735981687\n",
      "Done 4980 batches in 715.94 sec.    training loss:\t\t4.22656260619\n",
      "Done 4990 batches in 717.16 sec.    training loss:\t\t4.22547744883\n",
      "Done 5000 batches in 718.46 sec.    training loss:\t\t4.22469896603\n",
      "Done 5010 batches in 720.11 sec.    training loss:\t\t4.22416806664\n",
      "Done 5020 batches in 721.38 sec.    training loss:\t\t4.22338548001\n",
      "Done 5030 batches in 722.53 sec.    training loss:\t\t4.22222904766\n",
      "Done 5040 batches in 723.91 sec.    training loss:\t\t4.22128459144\n",
      "Done 5050 batches in 725.34 sec.    training loss:\t\t4.22045406884\n",
      "Done 5060 batches in 726.65 sec.    training loss:\t\t4.21972120595\n",
      "Done 5070 batches in 727.91 sec.    training loss:\t\t4.21907014461\n",
      "Done 5080 batches in 729.10 sec.    training loss:\t\t4.2182039381\n",
      "Done 5090 batches in 730.53 sec.    training loss:\t\t4.21788068904\n",
      "Done 5100 batches in 731.78 sec.    training loss:\t\t4.21697838811\n",
      "Done 5110 batches in 733.44 sec.    training loss:\t\t4.21648434916\n",
      "Done 5120 batches in 734.64 sec.    training loss:\t\t4.21551426542\n",
      "Done 5130 batches in 736.40 sec.    training loss:\t\t4.21474093531\n",
      "Done 5140 batches in 737.68 sec.    training loss:\t\t4.21390104934\n",
      "Done 5150 batches in 738.96 sec.    training loss:\t\t4.21314111325\n",
      "Done 5160 batches in 740.48 sec.    training loss:\t\t4.21251775983\n",
      "Done 5170 batches in 741.75 sec.    training loss:\t\t4.2118658889\n",
      "Done 5180 batches in 743.31 sec.    training loss:\t\t4.21146169408\n",
      "Done 5190 batches in 744.84 sec.    training loss:\t\t4.21101353426\n",
      "Done 5200 batches in 746.31 sec.    training loss:\t\t4.21035426254\n",
      "Done 5210 batches in 747.82 sec.    training loss:\t\t4.20991715131\n",
      "Done 5220 batches in 749.18 sec.    training loss:\t\t4.20917416277\n",
      "Done 5230 batches in 750.85 sec.    training loss:\t\t4.20881083308\n",
      "Done 5240 batches in 752.47 sec.    training loss:\t\t4.20826589188\n",
      "Done 5250 batches in 753.86 sec.    training loss:\t\t4.20798616832\n",
      "Done 5260 batches in 755.55 sec.    training loss:\t\t4.20699162628\n",
      "Done 5270 batches in 757.15 sec.    training loss:\t\t4.20661158684\n",
      "Done 5280 batches in 758.81 sec.    training loss:\t\t4.20610116797\n",
      "Done 5290 batches in 760.26 sec.    training loss:\t\t4.20542658754\n",
      "Done 5300 batches in 761.67 sec.    training loss:\t\t4.20482173767\n",
      "Done 5310 batches in 763.00 sec.    training loss:\t\t4.20407639632\n",
      "Done 5320 batches in 764.67 sec.    training loss:\t\t4.20355826648\n",
      "Done 5330 batches in 766.11 sec.    training loss:\t\t4.20318738189\n",
      "Done 5340 batches in 767.33 sec.    training loss:\t\t4.20246292112\n",
      "Done 5350 batches in 768.54 sec.    training loss:\t\t4.20167544984\n",
      "Done 5360 batches in 769.68 sec.    training loss:\t\t4.20093787255\n",
      "Done 5370 batches in 770.76 sec.    training loss:\t\t4.20012622377\n",
      "Done 5380 batches in 772.77 sec.    training loss:\t\t4.19990585621\n",
      "Done 5390 batches in 774.00 sec.    training loss:\t\t4.19924137252\n",
      "Done 5400 batches in 775.48 sec.    training loss:\t\t4.19866837621\n",
      "Done 5410 batches in 777.24 sec.    training loss:\t\t4.198318775\n",
      "Done 5420 batches in 778.45 sec.    training loss:\t\t4.19771083875\n",
      "Done 5430 batches in 779.99 sec.    training loss:\t\t4.19716810074\n",
      "Done 5440 batches in 781.48 sec.    training loss:\t\t4.19634054102\n",
      "Done 5450 batches in 782.80 sec.    training loss:\t\t4.19566107837\n",
      "Done 5460 batches in 784.09 sec.    training loss:\t\t4.19504449337\n",
      "Done 5470 batches in 785.66 sec.    training loss:\t\t4.19451486188\n",
      "Done 5480 batches in 787.20 sec.    training loss:\t\t4.19421441285\n",
      "Done 5490 batches in 788.46 sec.    training loss:\t\t4.19342503873\n",
      "Done 5500 batches in 789.81 sec.    training loss:\t\t4.19290076863\n",
      "Done 5510 batches in 791.03 sec.    training loss:\t\t4.19214762811\n",
      "Done 5520 batches in 792.28 sec.    training loss:\t\t4.19151137225\n",
      "Done 5530 batches in 793.58 sec.    training loss:\t\t4.19087618333\n",
      "Done 5540 batches in 794.86 sec.    training loss:\t\t4.19036269907\n",
      "Done 5550 batches in 796.28 sec.    training loss:\t\t4.18967841861\n",
      "Done 5560 batches in 797.56 sec.    training loss:\t\t4.18895871\n",
      "Done 5570 batches in 799.09 sec.    training loss:\t\t4.18862094931\n",
      "Done 5580 batches in 800.65 sec.    training loss:\t\t4.1886214599\n",
      "Done 5590 batches in 802.44 sec.    training loss:\t\t4.18819323981\n",
      "Done 5600 batches in 804.03 sec.    training loss:\t\t4.18767751613\n",
      "Done 5610 batches in 805.17 sec.    training loss:\t\t4.18703503536\n",
      "Done 5620 batches in 806.77 sec.    training loss:\t\t4.1864729074\n",
      "Done 5630 batches in 808.20 sec.    training loss:\t\t4.18576894963\n",
      "Done 5640 batches in 809.54 sec.    training loss:\t\t4.18525276231\n",
      "Done 5650 batches in 811.04 sec.    training loss:\t\t4.18476264814\n",
      "Done 5660 batches in 812.52 sec.    training loss:\t\t4.18419174272\n",
      "Done 5670 batches in 813.87 sec.    training loss:\t\t4.1833739152\n",
      "Done 5680 batches in 815.12 sec.    training loss:\t\t4.18268306704\n",
      "Done 5690 batches in 816.35 sec.    training loss:\t\t4.18208991333\n",
      "Done 5700 batches in 817.87 sec.    training loss:\t\t4.18153311504\n",
      "Done 5710 batches in 819.38 sec.    training loss:\t\t4.18080977525\n",
      "Done 5720 batches in 821.29 sec.    training loss:\t\t4.18065028307\n",
      "Done 5730 batches in 822.69 sec.    training loss:\t\t4.18035448829\n",
      "Done 5740 batches in 824.32 sec.    training loss:\t\t4.17994274686\n",
      "Done 5750 batches in 825.74 sec.    training loss:\t\t4.17939747288\n",
      "Done 5760 batches in 826.96 sec.    training loss:\t\t4.17896132775\n",
      "Done 5770 batches in 828.14 sec.    training loss:\t\t4.1783179824\n",
      "Done 5780 batches in 829.50 sec.    training loss:\t\t4.17761214452\n",
      "Done 5790 batches in 830.97 sec.    training loss:\t\t4.17728147482\n",
      "Done 5800 batches in 832.43 sec.    training loss:\t\t4.17690631131\n",
      "Done 5810 batches in 834.09 sec.    training loss:\t\t4.17646333351\n",
      "Done 5820 batches in 835.78 sec.    training loss:\t\t4.17635648246\n",
      "Done 5830 batches in 837.16 sec.    training loss:\t\t4.175792975\n",
      "Done 5840 batches in 838.75 sec.    training loss:\t\t4.17547442738\n",
      "Done 5850 batches in 840.13 sec.    training loss:\t\t4.17488966974\n",
      "Done 5860 batches in 841.50 sec.    training loss:\t\t4.17439212164\n",
      "Done 5870 batches in 842.75 sec.    training loss:\t\t4.17390080327\n",
      "Done 5880 batches in 844.36 sec.    training loss:\t\t4.1737754319\n",
      "Done 5890 batches in 846.13 sec.    training loss:\t\t4.17303533882\n",
      "Done 5900 batches in 847.75 sec.    training loss:\t\t4.17252381757\n",
      "Done 5910 batches in 849.06 sec.    training loss:\t\t4.17159319323\n",
      "Done 5920 batches in 850.73 sec.    training loss:\t\t4.17107656924\n",
      "Done 5930 batches in 851.76 sec.    training loss:\t\t4.17018294861\n",
      "Done 5940 batches in 853.04 sec.    training loss:\t\t4.16947140092\n",
      "Done 5950 batches in 854.45 sec.    training loss:\t\t4.16880200414\n",
      "Done 5960 batches in 855.64 sec.    training loss:\t\t4.16809639819\n",
      "Done 5970 batches in 857.21 sec.    training loss:\t\t4.1677406948\n",
      "Done 5980 batches in 858.84 sec.    training loss:\t\t4.16749015556\n",
      "Done 5990 batches in 860.18 sec.    training loss:\t\t4.16695630682\n",
      "Done 6000 batches in 861.62 sec.    training loss:\t\t4.16659875743\n",
      "Done 6010 batches in 862.98 sec.    training loss:\t\t4.16613170843\n",
      "Done 6020 batches in 864.41 sec.    training loss:\t\t4.16565897481\n",
      "Done 6030 batches in 865.91 sec.    training loss:\t\t4.16508949376\n",
      "Done 6040 batches in 867.57 sec.    training loss:\t\t4.16485292134\n",
      "Done 6050 batches in 868.67 sec.    training loss:\t\t4.16420117173\n",
      "Done 6060 batches in 870.00 sec.    training loss:\t\t4.16343933084\n",
      "Done 6070 batches in 871.42 sec.    training loss:\t\t4.16294542895\n",
      "Done 6080 batches in 873.03 sec.    training loss:\t\t4.16262634593\n",
      "Done 6090 batches in 874.41 sec.    training loss:\t\t4.16217020184\n",
      "Done 6100 batches in 875.63 sec.    training loss:\t\t4.16161522009\n",
      "Done 6110 batches in 876.93 sec.    training loss:\t\t4.16121578014\n",
      "Done 6120 batches in 878.44 sec.    training loss:\t\t4.16057904889\n",
      "Done 6130 batches in 879.95 sec.    training loss:\t\t4.1603396112\n",
      "Done 6140 batches in 881.44 sec.    training loss:\t\t4.15972408561\n",
      "Done 6150 batches in 882.58 sec.    training loss:\t\t4.15884050931\n",
      "Done 6160 batches in 883.93 sec.    training loss:\t\t4.15842206292\n",
      "Done 6170 batches in 885.38 sec.    training loss:\t\t4.15796376469\n",
      "Done 6180 batches in 886.95 sec.    training loss:\t\t4.15727255962\n",
      "Done 6190 batches in 888.33 sec.    training loss:\t\t4.15689791813\n",
      "Done 6200 batches in 889.84 sec.    training loss:\t\t4.15656962975\n",
      "Done 6210 batches in 891.37 sec.    training loss:\t\t4.15590188641\n",
      "Done 6220 batches in 892.77 sec.    training loss:\t\t4.15533950156\n",
      "Done 6230 batches in 894.31 sec.    training loss:\t\t4.15483524463\n",
      "Done 6240 batches in 895.74 sec.    training loss:\t\t4.15439962676\n",
      "Done 6250 batches in 897.03 sec.    training loss:\t\t4.15409182598\n",
      "Done 6260 batches in 898.04 sec.    training loss:\t\t4.15330319881\n",
      "Done 6270 batches in 899.65 sec.    training loss:\t\t4.15282977536\n",
      "Done 6280 batches in 901.01 sec.    training loss:\t\t4.15225229563\n",
      "Done 6290 batches in 902.49 sec.    training loss:\t\t4.15188478677\n",
      "Done 6300 batches in 903.74 sec.    training loss:\t\t4.1513188792\n",
      "Done 6310 batches in 905.41 sec.    training loss:\t\t4.15085476507\n",
      "Done 6320 batches in 906.87 sec.    training loss:\t\t4.15015188256\n",
      "Done 6330 batches in 908.25 sec.    training loss:\t\t4.14989381292\n",
      "Done 6340 batches in 909.97 sec.    training loss:\t\t4.14921851812\n",
      "Done 6350 batches in 911.21 sec.    training loss:\t\t4.14872277789\n",
      "Done 6360 batches in 912.65 sec.    training loss:\t\t4.14832571884\n",
      "Done 6370 batches in 914.51 sec.    training loss:\t\t4.14784722003\n",
      "Done 6380 batches in 915.82 sec.    training loss:\t\t4.14721096971\n",
      "Done 6390 batches in 917.10 sec.    training loss:\t\t4.1468870704\n",
      "Done 6400 batches in 918.48 sec.    training loss:\t\t4.14648119882\n",
      "Done 6410 batches in 920.20 sec.    training loss:\t\t4.14626313357\n",
      "Done 6420 batches in 921.65 sec.    training loss:\t\t4.14585547016\n",
      "Done 6430 batches in 922.94 sec.    training loss:\t\t4.14496750461\n",
      "Done 6440 batches in 924.24 sec.    training loss:\t\t4.14415988478\n",
      "Done 6450 batches in 926.01 sec.    training loss:\t\t4.14396516837\n",
      "Done 6460 batches in 927.57 sec.    training loss:\t\t4.14349490054\n",
      "Done 6470 batches in 928.75 sec.    training loss:\t\t4.14281822695\n",
      "Done 6480 batches in 929.94 sec.    training loss:\t\t4.14190577574\n",
      "Done 6490 batches in 931.44 sec.    training loss:\t\t4.1417191022\n",
      "Done 6500 batches in 932.72 sec.    training loss:\t\t4.14108078164\n",
      "Done 6510 batches in 934.21 sec.    training loss:\t\t4.14055920885\n",
      "Done 6520 batches in 935.51 sec.    training loss:\t\t4.14010077031\n",
      "Done 6530 batches in 937.08 sec.    training loss:\t\t4.1395178283\n",
      "Done 6540 batches in 938.38 sec.    training loss:\t\t4.13890713956\n",
      "Done 6550 batches in 940.01 sec.    training loss:\t\t4.13839626411\n",
      "Done 6560 batches in 941.63 sec.    training loss:\t\t4.13818667662\n",
      "Done 6570 batches in 943.34 sec.    training loss:\t\t4.13793328552\n",
      "Done 6580 batches in 944.85 sec.    training loss:\t\t4.1375510161\n",
      "Done 6590 batches in 946.58 sec.    training loss:\t\t4.13703576093\n",
      "Done 6600 batches in 947.86 sec.    training loss:\t\t4.13647071156\n",
      "Done 6610 batches in 949.07 sec.    training loss:\t\t4.13576171423\n",
      "Done 6620 batches in 950.36 sec.    training loss:\t\t4.13506265191\n",
      "Done 6630 batches in 952.51 sec.    training loss:\t\t4.13483538804\n",
      "Done 6640 batches in 953.78 sec.    training loss:\t\t4.1344596577\n",
      "Done 6650 batches in 954.96 sec.    training loss:\t\t4.13408694726\n",
      "Done 6660 batches in 956.60 sec.    training loss:\t\t4.13393249759\n",
      "Done 6670 batches in 958.29 sec.    training loss:\t\t4.13364937942\n",
      "Done 6680 batches in 959.61 sec.    training loss:\t\t4.13322437955\n",
      "Done 6690 batches in 960.94 sec.    training loss:\t\t4.13278645632\n",
      "Done 6700 batches in 962.34 sec.    training loss:\t\t4.13238365049\n",
      "Done 6710 batches in 963.87 sec.    training loss:\t\t4.13216351727\n",
      "Done 6720 batches in 965.04 sec.    training loss:\t\t4.13156131666\n",
      "Done 6730 batches in 966.38 sec.    training loss:\t\t4.13125722833\n",
      "Done 6740 batches in 967.90 sec.    training loss:\t\t4.13070727898\n",
      "Done 6750 batches in 969.22 sec.    training loss:\t\t4.13024036683\n",
      "Done 6760 batches in 970.40 sec.    training loss:\t\t4.12968137695\n",
      "Done 6770 batches in 971.95 sec.    training loss:\t\t4.12935514091\n",
      "Done 6780 batches in 973.28 sec.    training loss:\t\t4.12887911016\n",
      "Done 6790 batches in 974.55 sec.    training loss:\t\t4.12839526923\n",
      "Done 6800 batches in 976.29 sec.    training loss:\t\t4.12821923936\n",
      "Done 6810 batches in 977.96 sec.    training loss:\t\t4.1278761034\n",
      "Done 6820 batches in 979.11 sec.    training loss:\t\t4.12740669285\n",
      "Done 6830 batches in 980.50 sec.    training loss:\t\t4.12704188873\n",
      "Done 6840 batches in 981.72 sec.    training loss:\t\t4.12643960564\n",
      "Done 6850 batches in 983.17 sec.    training loss:\t\t4.12598069605\n",
      "Done 6860 batches in 984.88 sec.    training loss:\t\t4.12577991965\n",
      "Done 6870 batches in 986.23 sec.    training loss:\t\t4.12528466373\n",
      "Done 6880 batches in 987.59 sec.    training loss:\t\t4.12484692619\n",
      "Done 6890 batches in 989.14 sec.    training loss:\t\t4.12452138415\n",
      "Done 6900 batches in 990.47 sec.    training loss:\t\t4.12402309753\n",
      "Done 6910 batches in 991.71 sec.    training loss:\t\t4.12351298398\n",
      "Done 6920 batches in 993.32 sec.    training loss:\t\t4.12315681377\n",
      "Done 6930 batches in 995.57 sec.    training loss:\t\t4.12296946241\n",
      "Done 6940 batches in 997.08 sec.    training loss:\t\t4.1224929734\n",
      "Done 6950 batches in 998.40 sec.    training loss:\t\t4.12202807241\n",
      "Done 6960 batches in 999.38 sec.    training loss:\t\t4.12120042101\n",
      "Done 6970 batches in 1001.25 sec.    training loss:\t\t4.12110005246\n",
      "Done 6980 batches in 1002.72 sec.    training loss:\t\t4.12031292929\n",
      "Done 6990 batches in 1004.38 sec.    training loss:\t\t4.11997434646\n",
      "Done 7000 batches in 1005.66 sec.    training loss:\t\t4.11930891882\n",
      "Done 7010 batches in 1007.02 sec.    training loss:\t\t4.11893672933\n",
      "Done 7020 batches in 1008.49 sec.    training loss:\t\t4.11839015192\n",
      "Done 7030 batches in 1009.76 sec.    training loss:\t\t4.11765673361\n",
      "Done 7040 batches in 1011.32 sec.    training loss:\t\t4.11713328527\n",
      "Done 7050 batches in 1012.40 sec.    training loss:\t\t4.11642808008\n",
      "Done 7060 batches in 1014.47 sec.    training loss:\t\t4.1164772675\n",
      "Done 7070 batches in 1016.21 sec.    training loss:\t\t4.11611201315\n",
      "Done 7080 batches in 1017.61 sec.    training loss:\t\t4.11586157478\n",
      "Done 7090 batches in 1018.92 sec.    training loss:\t\t4.11547549077\n",
      "Done 7100 batches in 1020.30 sec.    training loss:\t\t4.11531100542\n",
      "Done 7110 batches in 1021.78 sec.    training loss:\t\t4.1149564001\n",
      "Done 7120 batches in 1023.48 sec.    training loss:\t\t4.11461672063\n",
      "Done 7130 batches in 1024.69 sec.    training loss:\t\t4.1142104388\n",
      "Done 7140 batches in 1026.40 sec.    training loss:\t\t4.11409826683\n",
      "Done 7150 batches in 1027.59 sec.    training loss:\t\t4.11354616832\n",
      "Done 7160 batches in 1029.37 sec.    training loss:\t\t4.11346200348\n",
      "Done 7170 batches in 1030.70 sec.    training loss:\t\t4.11293521706\n",
      "Done 7180 batches in 1031.86 sec.    training loss:\t\t4.11242781814\n",
      "Done 7190 batches in 1033.22 sec.    training loss:\t\t4.11186101636\n",
      "Done 7200 batches in 1034.69 sec.    training loss:\t\t4.11164093183\n",
      "Done 7210 batches in 1035.92 sec.    training loss:\t\t4.11108543139\n",
      "Done 7220 batches in 1037.78 sec.    training loss:\t\t4.11075758617\n",
      "Done 7230 batches in 1039.39 sec.    training loss:\t\t4.11024639465\n",
      "Done 7240 batches in 1040.51 sec.    training loss:\t\t4.10967848923\n",
      "Done 7250 batches in 1042.20 sec.    training loss:\t\t4.10931931111\n",
      "Done 7260 batches in 1043.70 sec.    training loss:\t\t4.10900228533\n",
      "Done 7270 batches in 1045.27 sec.    training loss:\t\t4.10850064118\n",
      "Done 7280 batches in 1046.57 sec.    training loss:\t\t4.10816548146\n",
      "Done 7290 batches in 1047.94 sec.    training loss:\t\t4.10752455534\n",
      "Done 7300 batches in 1049.27 sec.    training loss:\t\t4.10710281032\n",
      "Done 7310 batches in 1050.85 sec.    training loss:\t\t4.10676573393\n",
      "Done 7320 batches in 1052.48 sec.    training loss:\t\t4.10632136549\n",
      "Done 7330 batches in 1053.75 sec.    training loss:\t\t4.1057388722\n",
      "Done 7340 batches in 1055.07 sec.    training loss:\t\t4.10507799718\n",
      "Done 7350 batches in 1056.21 sec.    training loss:\t\t4.1044812172\n",
      "Done 7360 batches in 1057.47 sec.    training loss:\t\t4.10394007959\n",
      "Done 7370 batches in 1058.76 sec.    training loss:\t\t4.1034450982\n",
      "Done 7380 batches in 1060.36 sec.    training loss:\t\t4.10301186153\n",
      "Done 7390 batches in 1061.96 sec.    training loss:\t\t4.10274083837\n",
      "Done 7400 batches in 1063.47 sec.    training loss:\t\t4.10241560285\n",
      "Done 7410 batches in 1064.92 sec.    training loss:\t\t4.10209343131\n",
      "Done 7420 batches in 1066.16 sec.    training loss:\t\t4.10156548267\n",
      "Done 7430 batches in 1067.92 sec.    training loss:\t\t4.10116499443\n",
      "Done 7440 batches in 1069.63 sec.    training loss:\t\t4.1009396696\n",
      "Done 7450 batches in 1070.85 sec.    training loss:\t\t4.1005259303\n",
      "Done 7460 batches in 1073.12 sec.    training loss:\t\t4.10018044496\n",
      "Done 7470 batches in 1074.66 sec.    training loss:\t\t4.09981864316\n",
      "Done 7480 batches in 1075.87 sec.    training loss:\t\t4.09927788912\n",
      "Done 7490 batches in 1077.32 sec.    training loss:\t\t4.09887658276\n",
      "Done 7500 batches in 1078.66 sec.    training loss:\t\t4.09854389582\n",
      "Done 7510 batches in 1080.06 sec.    training loss:\t\t4.0981565865\n",
      "Done 7520 batches in 1081.49 sec.    training loss:\t\t4.0979144492\n",
      "Done 7530 batches in 1082.90 sec.    training loss:\t\t4.09771410311\n",
      "Done 7540 batches in 1084.41 sec.    training loss:\t\t4.09732703537\n",
      "Done 7550 batches in 1086.04 sec.    training loss:\t\t4.097004172\n",
      "Done 7560 batches in 1087.89 sec.    training loss:\t\t4.09681082418\n",
      "Done 7570 batches in 1089.36 sec.    training loss:\t\t4.09641882129\n",
      "Done 7580 batches in 1090.54 sec.    training loss:\t\t4.09625897845\n",
      "Done 7590 batches in 1092.01 sec.    training loss:\t\t4.09591115762\n",
      "Done 7600 batches in 1093.37 sec.    training loss:\t\t4.0953965109\n",
      "Done 7610 batches in 1094.50 sec.    training loss:\t\t4.09501788287\n",
      "Done 7620 batches in 1095.72 sec.    training loss:\t\t4.09459469541\n",
      "Done 7630 batches in 1097.40 sec.    training loss:\t\t4.09454495119\n",
      "Done 7640 batches in 1098.83 sec.    training loss:\t\t4.09434831607\n",
      "Done 7650 batches in 1100.25 sec.    training loss:\t\t4.09390291454\n",
      "Done 7660 batches in 1101.63 sec.    training loss:\t\t4.09356786729\n",
      "Done 7670 batches in 1103.34 sec.    training loss:\t\t4.09328934447\n",
      "Done 7680 batches in 1104.86 sec.    training loss:\t\t4.09291806972\n",
      "Done 7690 batches in 1106.52 sec.    training loss:\t\t4.09261137667\n",
      "Done 7700 batches in 1108.28 sec.    training loss:\t\t4.09253912495\n",
      "Done 7710 batches in 1109.83 sec.    training loss:\t\t4.09225273754\n",
      "Done 7720 batches in 1111.33 sec.    training loss:\t\t4.09189446371\n",
      "Done 7730 batches in 1112.66 sec.    training loss:\t\t4.09148720304\n",
      "Done 7740 batches in 1114.14 sec.    training loss:\t\t4.09102311424\n",
      "Done 7750 batches in 1115.35 sec.    training loss:\t\t4.0903064781\n",
      "Done 7760 batches in 1116.83 sec.    training loss:\t\t4.08989156887\n",
      "Done 7770 batches in 1118.15 sec.    training loss:\t\t4.08930571113\n",
      "Done 7780 batches in 1119.44 sec.    training loss:\t\t4.08872139288\n",
      "Done 7790 batches in 1120.96 sec.    training loss:\t\t4.088626215\n",
      "Done 7800 batches in 1122.51 sec.    training loss:\t\t4.08824462594\n",
      "Done 7810 batches in 1124.01 sec.    training loss:\t\t4.08784639881\n",
      "Done 7820 batches in 1125.50 sec.    training loss:\t\t4.08753858424\n",
      "Done 7830 batches in 1126.91 sec.    training loss:\t\t4.08715068968\n",
      "Done 7840 batches in 1128.15 sec.    training loss:\t\t4.08669207309\n",
      "Done 7850 batches in 1129.48 sec.    training loss:\t\t4.08638503746\n",
      "Done 7860 batches in 1130.86 sec.    training loss:\t\t4.0858107784\n",
      "Done 7870 batches in 1132.48 sec.    training loss:\t\t4.08539514536\n",
      "Done 7880 batches in 1133.67 sec.    training loss:\t\t4.08509328892\n",
      "Done 7890 batches in 1135.11 sec.    training loss:\t\t4.08462998628\n",
      "Done 7900 batches in 1136.39 sec.    training loss:\t\t4.08443300739\n",
      "Done 7910 batches in 1138.16 sec.    training loss:\t\t4.08400658214\n",
      "Done 7920 batches in 1139.98 sec.    training loss:\t\t4.08387127874\n",
      "Done 7930 batches in 1141.29 sec.    training loss:\t\t4.08359312076\n",
      "Done 7940 batches in 1142.35 sec.    training loss:\t\t4.082859598\n",
      "Done 7950 batches in 1143.99 sec.    training loss:\t\t4.08264256513\n",
      "Done 7960 batches in 1145.53 sec.    training loss:\t\t4.08231270936\n",
      "Done 7970 batches in 1146.88 sec.    training loss:\t\t4.08189140356\n",
      "Done 7980 batches in 1147.99 sec.    training loss:\t\t4.08147188368\n",
      "Done 7990 batches in 1149.70 sec.    training loss:\t\t4.08115954166\n",
      "Done 8000 batches in 1151.26 sec.    training loss:\t\t4.08074349174\n",
      "Done 8010 batches in 1152.58 sec.    training loss:\t\t4.08038815756\n",
      "Done 8020 batches in 1153.99 sec.    training loss:\t\t4.08020579532\n",
      "Done 8030 batches in 1155.31 sec.    training loss:\t\t4.07984364413\n",
      "Done 8040 batches in 1157.48 sec.    training loss:\t\t4.07960572806\n",
      "Done 8050 batches in 1158.89 sec.    training loss:\t\t4.07902403606\n",
      "Done 8060 batches in 1160.17 sec.    training loss:\t\t4.0787032787\n",
      "Done 8070 batches in 1161.63 sec.    training loss:\t\t4.07834927149\n",
      "Done 8080 batches in 1163.26 sec.    training loss:\t\t4.07800905884\n",
      "Done 8090 batches in 1164.53 sec.    training loss:\t\t4.07752217639\n",
      "Done 8100 batches in 1165.86 sec.    training loss:\t\t4.0772209589\n",
      "Done 8110 batches in 1167.02 sec.    training loss:\t\t4.07673587238\n",
      "Done 8120 batches in 1168.57 sec.    training loss:\t\t4.07650629611\n",
      "Done 8130 batches in 1170.87 sec.    training loss:\t\t4.07638138945\n",
      "Done 8140 batches in 1172.16 sec.    training loss:\t\t4.07611205036\n",
      "Done 8150 batches in 1173.57 sec.    training loss:\t\t4.07563874294\n",
      "Done 8160 batches in 1175.07 sec.    training loss:\t\t4.07535914153\n",
      "Done 8170 batches in 1176.32 sec.    training loss:\t\t4.07506974362\n",
      "Done 8180 batches in 1177.47 sec.    training loss:\t\t4.07457731318\n",
      "Done 8190 batches in 1178.62 sec.    training loss:\t\t4.07395396992\n",
      "Done 8200 batches in 1179.88 sec.    training loss:\t\t4.07362319475\n",
      "Done 8210 batches in 1181.19 sec.    training loss:\t\t4.07312722026\n",
      "Done 8220 batches in 1182.48 sec.    training loss:\t\t4.07281107723\n",
      "Done 8230 batches in 1184.07 sec.    training loss:\t\t4.07251100453\n",
      "Done 8240 batches in 1185.40 sec.    training loss:\t\t4.07206496842\n",
      "Done 8250 batches in 1187.02 sec.    training loss:\t\t4.0717266535\n",
      "Done 8260 batches in 1188.76 sec.    training loss:\t\t4.07149758936\n",
      "Done 8270 batches in 1190.31 sec.    training loss:\t\t4.07124594293\n",
      "Done 8280 batches in 1192.17 sec.    training loss:\t\t4.07108881802\n",
      "Done 8290 batches in 1193.78 sec.    training loss:\t\t4.07106447165\n",
      "Done 8300 batches in 1195.35 sec.    training loss:\t\t4.07087476495\n",
      "Done 8310 batches in 1196.72 sec.    training loss:\t\t4.07055277348\n",
      "Done 8320 batches in 1198.10 sec.    training loss:\t\t4.07030839837\n",
      "Done 8330 batches in 1199.38 sec.    training loss:\t\t4.06987609935\n",
      "Done 8340 batches in 1200.56 sec.    training loss:\t\t4.0694524994\n",
      "Done 8350 batches in 1201.80 sec.    training loss:\t\t4.0692236257\n",
      "Done 8360 batches in 1203.11 sec.    training loss:\t\t4.06885214551\n",
      "Done 8370 batches in 1204.51 sec.    training loss:\t\t4.06855541811\n",
      "Done 8380 batches in 1205.88 sec.    training loss:\t\t4.06828342027\n",
      "Done 8390 batches in 1207.07 sec.    training loss:\t\t4.06784756795\n",
      "Done 8400 batches in 1208.47 sec.    training loss:\t\t4.0675553399\n",
      "Done 8410 batches in 1210.20 sec.    training loss:\t\t4.06742135699\n",
      "Done 8420 batches in 1211.78 sec.    training loss:\t\t4.06719333452\n",
      "Done 8430 batches in 1213.26 sec.    training loss:\t\t4.06691349577\n",
      "Done 8440 batches in 1214.68 sec.    training loss:\t\t4.06648927636\n",
      "Done 8450 batches in 1215.98 sec.    training loss:\t\t4.06609358813\n",
      "Done 8460 batches in 1217.29 sec.    training loss:\t\t4.06580529514\n",
      "Done 8470 batches in 1218.50 sec.    training loss:\t\t4.06535307052\n",
      "Done 8480 batches in 1220.01 sec.    training loss:\t\t4.06496741409\n",
      "Done 8490 batches in 1221.33 sec.    training loss:\t\t4.06455677713\n",
      "Done 8500 batches in 1222.82 sec.    training loss:\t\t4.06442052592\n",
      "Done 8510 batches in 1224.02 sec.    training loss:\t\t4.06407515322\n",
      "Done 8520 batches in 1225.32 sec.    training loss:\t\t4.0635912914\n",
      "Done 8530 batches in 1226.84 sec.    training loss:\t\t4.06337895768\n",
      "Done 8540 batches in 1228.31 sec.    training loss:\t\t4.06306866352\n",
      "Done 8550 batches in 1229.73 sec.    training loss:\t\t4.06268483987\n",
      "Done 8560 batches in 1231.09 sec.    training loss:\t\t4.06240440713\n",
      "Done 8570 batches in 1232.89 sec.    training loss:\t\t4.06232067086\n",
      "Done 8580 batches in 1234.28 sec.    training loss:\t\t4.06191875509\n",
      "Done 8590 batches in 1235.86 sec.    training loss:\t\t4.06174579991\n",
      "Done 8600 batches in 1237.32 sec.    training loss:\t\t4.06130665627\n",
      "Done 8610 batches in 1238.75 sec.    training loss:\t\t4.06103180986\n",
      "Done 8620 batches in 1240.32 sec.    training loss:\t\t4.06068167001\n",
      "Done 8630 batches in 1241.43 sec.    training loss:\t\t4.06039676791\n",
      "Done 8640 batches in 1242.76 sec.    training loss:\t\t4.05999645754\n",
      "Done 8650 batches in 1244.86 sec.    training loss:\t\t4.05972700885\n",
      "Done 8660 batches in 1246.46 sec.    training loss:\t\t4.05947020728\n",
      "Done 8670 batches in 1247.84 sec.    training loss:\t\t4.05910204441\n",
      "Done 8680 batches in 1249.07 sec.    training loss:\t\t4.05867356246\n",
      "Done 8690 batches in 1250.21 sec.    training loss:\t\t4.05841560169\n",
      "Done 8700 batches in 1251.45 sec.    training loss:\t\t4.05814932061\n",
      "Done 8710 batches in 1253.00 sec.    training loss:\t\t4.05774554555\n",
      "Done 8720 batches in 1254.46 sec.    training loss:\t\t4.05736750908\n",
      "Done 8730 batches in 1256.20 sec.    training loss:\t\t4.05738012141\n",
      "Done 8740 batches in 1257.64 sec.    training loss:\t\t4.05705884714\n",
      "Done 8750 batches in 1259.13 sec.    training loss:\t\t4.05668778945\n",
      "Done 8760 batches in 1260.32 sec.    training loss:\t\t4.05608684429\n",
      "Done 8770 batches in 1261.85 sec.    training loss:\t\t4.0558320254\n",
      "Done 8780 batches in 1263.29 sec.    training loss:\t\t4.05569279683\n",
      "Done 8790 batches in 1264.60 sec.    training loss:\t\t4.05532984997\n",
      "Done 8800 batches in 1265.83 sec.    training loss:\t\t4.0548284102\n",
      "Done 8810 batches in 1267.02 sec.    training loss:\t\t4.05457905692\n",
      "Done 8820 batches in 1268.43 sec.    training loss:\t\t4.05441677543\n",
      "Done 8830 batches in 1270.02 sec.    training loss:\t\t4.05412053101\n",
      "Done 8840 batches in 1271.65 sec.    training loss:\t\t4.05382501785\n",
      "Done 8850 batches in 1273.23 sec.    training loss:\t\t4.05346038778\n",
      "Done 8860 batches in 1274.45 sec.    training loss:\t\t4.05302631451\n",
      "Done 8870 batches in 1276.00 sec.    training loss:\t\t4.05288274544\n",
      "Done 8880 batches in 1277.24 sec.    training loss:\t\t4.05257611699\n",
      "Done 8890 batches in 1278.87 sec.    training loss:\t\t4.05235480291\n",
      "Done 8900 batches in 1280.60 sec.    training loss:\t\t4.05209296677\n",
      "Done 8910 batches in 1281.85 sec.    training loss:\t\t4.05181164701\n",
      "Done 8920 batches in 1283.19 sec.    training loss:\t\t4.05137337542\n",
      "Done 8930 batches in 1284.79 sec.    training loss:\t\t4.05132550072\n",
      "Done 8940 batches in 1286.06 sec.    training loss:\t\t4.05094303242\n",
      "Done 8950 batches in 1287.42 sec.    training loss:\t\t4.05076600448\n",
      "Done 8960 batches in 1289.93 sec.    training loss:\t\t4.05069983604\n",
      "Done 8970 batches in 1291.46 sec.    training loss:\t\t4.05041864684\n",
      "Done 8980 batches in 1292.77 sec.    training loss:\t\t4.05000589786\n",
      "Done 8990 batches in 1294.11 sec.    training loss:\t\t4.04977795932\n",
      "Done 9000 batches in 1295.43 sec.    training loss:\t\t4.04939745357\n",
      "Done 9010 batches in 1296.92 sec.    training loss:\t\t4.04916074025\n",
      "Done 9020 batches in 1298.97 sec.    training loss:\t\t4.04904297413\n",
      "Done 9030 batches in 1300.40 sec.    training loss:\t\t4.04888389332\n",
      "Done 9040 batches in 1301.90 sec.    training loss:\t\t4.04867707657\n",
      "Done 9050 batches in 1303.25 sec.    training loss:\t\t4.04815983356\n",
      "Done 9060 batches in 1304.69 sec.    training loss:\t\t4.04807106667\n",
      "Done 9070 batches in 1306.01 sec.    training loss:\t\t4.04779093697\n",
      "Done 9080 batches in 1307.15 sec.    training loss:\t\t4.04740130497\n",
      "Done 9090 batches in 1308.57 sec.    training loss:\t\t4.04698999621\n",
      "Done 9100 batches in 1309.83 sec.    training loss:\t\t4.04653505522\n",
      "Done 9110 batches in 1311.11 sec.    training loss:\t\t4.04606188634\n",
      "Done 9120 batches in 1312.35 sec.    training loss:\t\t4.04576653663\n",
      "Done 9130 batches in 1313.93 sec.    training loss:\t\t4.04550385177\n",
      "Done 9140 batches in 1315.04 sec.    training loss:\t\t4.04507327346\n",
      "Done 9150 batches in 1316.51 sec.    training loss:\t\t4.04475257405\n",
      "Done 9160 batches in 1317.83 sec.    training loss:\t\t4.04441668227\n",
      "Done 9170 batches in 1319.16 sec.    training loss:\t\t4.0439930349\n",
      "Done 9180 batches in 1320.77 sec.    training loss:\t\t4.04372687223\n",
      "Done 9190 batches in 1322.21 sec.    training loss:\t\t4.0434838095\n",
      "Done 9200 batches in 1323.70 sec.    training loss:\t\t4.04332150265\n",
      "Done 9210 batches in 1325.12 sec.    training loss:\t\t4.04299340445\n",
      "Done 9220 batches in 1326.82 sec.    training loss:\t\t4.04290882039\n",
      "Done 9230 batches in 1328.51 sec.    training loss:\t\t4.04293654887\n",
      "Done 9240 batches in 1329.72 sec.    training loss:\t\t4.04256008665\n",
      "Done 9250 batches in 1331.39 sec.    training loss:\t\t4.04238007118\n",
      "Done 9260 batches in 1332.77 sec.    training loss:\t\t4.04204476001\n",
      "Done 9270 batches in 1333.84 sec.    training loss:\t\t4.04165763891\n",
      "Done 9280 batches in 1335.89 sec.    training loss:\t\t4.04135856441\n",
      "Done 9290 batches in 1337.52 sec.    training loss:\t\t4.04111980573\n",
      "Done 9300 batches in 1339.19 sec.    training loss:\t\t4.04086487601\n",
      "Done 9310 batches in 1340.46 sec.    training loss:\t\t4.04056501191\n",
      "Done 9320 batches in 1341.86 sec.    training loss:\t\t4.04027359179\n",
      "Done 9330 batches in 1343.08 sec.    training loss:\t\t4.03989650153\n",
      "Done 9340 batches in 1344.62 sec.    training loss:\t\t4.03963356309\n",
      "Done 9350 batches in 1345.94 sec.    training loss:\t\t4.03932268834\n",
      "Done 9360 batches in 1347.41 sec.    training loss:\t\t4.03902849914\n",
      "Done 9370 batches in 1348.66 sec.    training loss:\t\t4.03882807647\n",
      "Done 9380 batches in 1349.92 sec.    training loss:\t\t4.03842890446\n",
      "Done 9390 batches in 1351.16 sec.    training loss:\t\t4.03823263145\n",
      "Done 9400 batches in 1352.37 sec.    training loss:\t\t4.03780309332\n",
      "Done 9410 batches in 1353.68 sec.    training loss:\t\t4.03743647817\n",
      "Done 9420 batches in 1355.15 sec.    training loss:\t\t4.03719767122\n",
      "Done 9430 batches in 1356.38 sec.    training loss:\t\t4.03683873649\n",
      "Done 9440 batches in 1357.65 sec.    training loss:\t\t4.03653025438\n",
      "Done 9450 batches in 1359.04 sec.    training loss:\t\t4.03638680486\n",
      "Done 9460 batches in 1360.74 sec.    training loss:\t\t4.03614876164\n",
      "Done 9470 batches in 1362.04 sec.    training loss:\t\t4.0357961237\n",
      "Done 9480 batches in 1363.93 sec.    training loss:\t\t4.03577817045\n",
      "Done 9490 batches in 1365.55 sec.    training loss:\t\t4.03554961495\n",
      "Done 9500 batches in 1366.78 sec.    training loss:\t\t4.03507297827\n",
      "Done 9510 batches in 1368.15 sec.    training loss:\t\t4.03483244874\n",
      "Done 9520 batches in 1369.63 sec.    training loss:\t\t4.0344977067\n",
      "Done 9530 batches in 1371.00 sec.    training loss:\t\t4.03430528978\n",
      "Done 9540 batches in 1372.52 sec.    training loss:\t\t4.03413986877\n",
      "Done 9550 batches in 1373.84 sec.    training loss:\t\t4.03391433202\n",
      "Done 9560 batches in 1375.17 sec.    training loss:\t\t4.03352901185\n",
      "Done 9570 batches in 1376.40 sec.    training loss:\t\t4.03310935769\n",
      "Done 9580 batches in 1377.70 sec.    training loss:\t\t4.03293560158\n",
      "Done 9590 batches in 1378.79 sec.    training loss:\t\t4.03252433418\n",
      "Done 9600 batches in 1380.48 sec.    training loss:\t\t4.03213773804\n",
      "Done 9610 batches in 1381.66 sec.    training loss:\t\t4.03172031967\n",
      "Done 9620 batches in 1382.84 sec.    training loss:\t\t4.0311982856\n",
      "Done 9630 batches in 1384.77 sec.    training loss:\t\t4.03094858536\n",
      "Done 9640 batches in 1386.09 sec.    training loss:\t\t4.03067816627\n",
      "Done 9650 batches in 1387.41 sec.    training loss:\t\t4.03034104542\n",
      "Done 9660 batches in 1388.81 sec.    training loss:\t\t4.0301866853\n",
      "Done 9670 batches in 1390.21 sec.    training loss:\t\t4.03004834635\n",
      "Done 9680 batches in 1391.74 sec.    training loss:\t\t4.02998597464\n",
      "Done 9690 batches in 1393.18 sec.    training loss:\t\t4.02970268434\n",
      "Done 9700 batches in 1394.52 sec.    training loss:\t\t4.0294437743\n",
      "Done 9710 batches in 1395.81 sec.    training loss:\t\t4.02914018958\n",
      "Done 9720 batches in 1396.93 sec.    training loss:\t\t4.02876364312\n",
      "Done 9730 batches in 1398.16 sec.    training loss:\t\t4.02820477466\n",
      "Done 9740 batches in 1399.59 sec.    training loss:\t\t4.02786987469\n",
      "Done 9750 batches in 1400.99 sec.    training loss:\t\t4.02778291807\n",
      "Done 9760 batches in 1402.41 sec.    training loss:\t\t4.02743153201\n",
      "Done 9770 batches in 1403.89 sec.    training loss:\t\t4.02718870508\n",
      "Done 9780 batches in 1405.17 sec.    training loss:\t\t4.02678446343\n",
      "Done 9790 batches in 1406.57 sec.    training loss:\t\t4.02645536667\n",
      "Done 9800 batches in 1408.00 sec.    training loss:\t\t4.02612681737\n",
      "Done 9810 batches in 1409.38 sec.    training loss:\t\t4.02596224681\n",
      "Done 9820 batches in 1410.40 sec.    training loss:\t\t4.02564330113\n",
      "Done 9830 batches in 1412.10 sec.    training loss:\t\t4.0253826381\n",
      "Done 9840 batches in 1413.53 sec.    training loss:\t\t4.02505254026\n",
      "Done 9850 batches in 1415.61 sec.    training loss:\t\t4.02485949715\n",
      "Done 9860 batches in 1416.82 sec.    training loss:\t\t4.02445045461\n",
      "Done 9870 batches in 1418.26 sec.    training loss:\t\t4.02432474742\n",
      "Done 9880 batches in 1420.28 sec.    training loss:\t\t4.02415623136\n",
      "Done 9890 batches in 1421.55 sec.    training loss:\t\t4.02385182007\n",
      "Done 9900 batches in 1422.98 sec.    training loss:\t\t4.02365195587\n",
      "Done 9910 batches in 1424.47 sec.    training loss:\t\t4.02334031667\n",
      "Done 9920 batches in 1426.07 sec.    training loss:\t\t4.02303690826\n",
      "Done 9930 batches in 1427.19 sec.    training loss:\t\t4.02270646117\n",
      "Done 9940 batches in 1428.32 sec.    training loss:\t\t4.02239347598\n",
      "Done 9950 batches in 1429.66 sec.    training loss:\t\t4.0222017813\n",
      "Done 9960 batches in 1430.79 sec.    training loss:\t\t4.02176816638\n",
      "Done 9970 batches in 1431.88 sec.    training loss:\t\t4.02137865109\n",
      "Done 9980 batches in 1433.10 sec.    training loss:\t\t4.02109009457\n",
      "Done 9990 batches in 1434.52 sec.    training loss:\t\t4.02086771336\n",
      "Done 10000 batches in 1436.01 sec.    training loss:\t\t4.02065668514\n",
      "Done 10010 batches in 1437.50 sec.    training loss:\t\t4.02045271766\n",
      "Done 10020 batches in 1439.09 sec.    training loss:\t\t4.02021161108\n",
      "Done 10030 batches in 1440.26 sec.    training loss:\t\t4.01975996744\n",
      "Done 10040 batches in 1441.69 sec.    training loss:\t\t4.01954206276\n",
      "Done 10050 batches in 1442.79 sec.    training loss:\t\t4.01914352797\n",
      "Done 10060 batches in 1444.04 sec.    training loss:\t\t4.01882952999\n",
      "Done 10070 batches in 1445.57 sec.    training loss:\t\t4.01855056565\n",
      "Done 10080 batches in 1446.83 sec.    training loss:\t\t4.01825382087\n",
      "Done 10090 batches in 1448.60 sec.    training loss:\t\t4.01812544176\n",
      "Done 10100 batches in 1450.16 sec.    training loss:\t\t4.01783840621\n",
      "Done 10110 batches in 1451.74 sec.    training loss:\t\t4.01761716391\n",
      "Done 10120 batches in 1452.91 sec.    training loss:\t\t4.01722767928\n",
      "Done 10130 batches in 1454.48 sec.    training loss:\t\t4.01701575081\n",
      "Done 10140 batches in 1456.03 sec.    training loss:\t\t4.01671488012\n",
      "Done 10150 batches in 1457.59 sec.    training loss:\t\t4.01643514652\n",
      "Done 10160 batches in 1459.11 sec.    training loss:\t\t4.01624769766\n",
      "Done 10170 batches in 1460.61 sec.    training loss:\t\t4.01604214805\n",
      "Done 10180 batches in 1461.89 sec.    training loss:\t\t4.01563675591\n",
      "Done 10190 batches in 1463.13 sec.    training loss:\t\t4.0153058571\n",
      "Done 10200 batches in 1464.35 sec.    training loss:\t\t4.01504908351\n",
      "Done 10210 batches in 1465.73 sec.    training loss:\t\t4.01470658282\n",
      "Done 10220 batches in 1466.89 sec.    training loss:\t\t4.01436131929\n",
      "Done 10230 batches in 1468.49 sec.    training loss:\t\t4.01414744798\n",
      "Done 10240 batches in 1470.20 sec.    training loss:\t\t4.0137149269\n",
      "Done 10250 batches in 1471.48 sec.    training loss:\t\t4.01324021891\n",
      "Done 10260 batches in 1472.81 sec.    training loss:\t\t4.01278760675\n",
      "Done 10270 batches in 1474.16 sec.    training loss:\t\t4.01252417388\n",
      "Done 10280 batches in 1475.61 sec.    training loss:\t\t4.01234179188\n",
      "Done 10290 batches in 1477.19 sec.    training loss:\t\t4.01214313183\n",
      "Done 10300 batches in 1478.47 sec.    training loss:\t\t4.01175405748\n",
      "Done 10310 batches in 1479.72 sec.    training loss:\t\t4.01150126665\n",
      "Done 10320 batches in 1481.19 sec.    training loss:\t\t4.01121801859\n",
      "Done 10330 batches in 1482.64 sec.    training loss:\t\t4.01123385723\n",
      "Done 10340 batches in 1484.07 sec.    training loss:\t\t4.01097152182\n",
      "Done 10350 batches in 1485.83 sec.    training loss:\t\t4.01078432009\n",
      "Done 10360 batches in 1487.17 sec.    training loss:\t\t4.01045053054\n",
      "Done 10370 batches in 1488.74 sec.    training loss:\t\t4.0102352123\n",
      "Done 10380 batches in 1490.27 sec.    training loss:\t\t4.01008103737\n",
      "Done 10390 batches in 1491.67 sec.    training loss:\t\t4.00976597428\n",
      "Done 10400 batches in 1492.91 sec.    training loss:\t\t4.00942928938\n",
      "Done 10410 batches in 1494.59 sec.    training loss:\t\t4.00934807874\n",
      "Done 10420 batches in 1495.84 sec.    training loss:\t\t4.00912119523\n",
      "Done 10430 batches in 1497.43 sec.    training loss:\t\t4.00885397725\n",
      "Done 10440 batches in 1498.45 sec.    training loss:\t\t4.00839880447\n",
      "Done 10450 batches in 1499.74 sec.    training loss:\t\t4.00819336998\n",
      "Done 10460 batches in 1501.31 sec.    training loss:\t\t4.00797104186\n",
      "Done 10470 batches in 1502.76 sec.    training loss:\t\t4.00777758552\n",
      "Done 10480 batches in 1504.04 sec.    training loss:\t\t4.00754550802\n",
      "Done 10490 batches in 1505.48 sec.    training loss:\t\t4.00718529954\n",
      "Done 10500 batches in 1506.94 sec.    training loss:\t\t4.00698624361\n",
      "Done 10510 batches in 1508.25 sec.    training loss:\t\t4.00660501924\n",
      "Done 10520 batches in 1509.75 sec.    training loss:\t\t4.00647982731\n",
      "Done 10530 batches in 1511.12 sec.    training loss:\t\t4.00618950578\n",
      "Done 10540 batches in 1512.26 sec.    training loss:\t\t4.00585681686\n",
      "Done 10550 batches in 1513.61 sec.    training loss:\t\t4.00575745442\n",
      "Done 10560 batches in 1515.02 sec.    training loss:\t\t4.00538136564\n",
      "Done 10570 batches in 1516.42 sec.    training loss:\t\t4.00513183421\n",
      "Done 10580 batches in 1518.18 sec.    training loss:\t\t4.00485227464\n",
      "Done 10590 batches in 1519.49 sec.    training loss:\t\t4.00470027196\n",
      "Done 10600 batches in 1520.65 sec.    training loss:\t\t4.00435133513\n",
      "Done 10610 batches in 1522.25 sec.    training loss:\t\t4.00407713024\n",
      "Done 10620 batches in 1523.68 sec.    training loss:\t\t4.00388009384\n",
      "Done 10630 batches in 1525.03 sec.    training loss:\t\t4.0035883208\n",
      "Done 10640 batches in 1526.12 sec.    training loss:\t\t4.00330590081\n",
      "Done 10650 batches in 1527.80 sec.    training loss:\t\t4.00317241111\n",
      "Done 10660 batches in 1529.08 sec.    training loss:\t\t4.0029560557\n",
      "Done 10670 batches in 1530.40 sec.    training loss:\t\t4.0026727715\n",
      "Done 10680 batches in 1531.70 sec.    training loss:\t\t4.00244852316\n",
      "Done 10690 batches in 1533.19 sec.    training loss:\t\t4.00221683711\n",
      "Done 10700 batches in 1534.77 sec.    training loss:\t\t4.00201130564\n",
      "Done 10710 batches in 1535.93 sec.    training loss:\t\t4.0016939884\n",
      "Done 10720 batches in 1537.38 sec.    training loss:\t\t4.00143206435\n",
      "Done 10730 batches in 1538.72 sec.    training loss:\t\t4.00105935143\n",
      "Done 10740 batches in 1540.08 sec.    training loss:\t\t4.00079433421\n",
      "Done 10750 batches in 1541.45 sec.    training loss:\t\t4.00051149692\n",
      "Done 10760 batches in 1542.63 sec.    training loss:\t\t4.00031175633\n",
      "Done 10770 batches in 1544.01 sec.    training loss:\t\t4.00005867523\n",
      "Done 10780 batches in 1545.36 sec.    training loss:\t\t3.99973276371\n",
      "Done 10790 batches in 1546.47 sec.    training loss:\t\t3.9994707378\n",
      "Done 10800 batches in 1548.04 sec.    training loss:\t\t3.99926976175\n",
      "Done 10810 batches in 1549.36 sec.    training loss:\t\t3.9989825642\n",
      "Done 10820 batches in 1550.69 sec.    training loss:\t\t3.99850116763\n",
      "Done 10830 batches in 1552.07 sec.    training loss:\t\t3.99845370824\n",
      "Done 10840 batches in 1553.41 sec.    training loss:\t\t3.99817159099\n",
      "Done 10850 batches in 1554.69 sec.    training loss:\t\t3.99789987208\n",
      "Done 10860 batches in 1556.09 sec.    training loss:\t\t3.99766998245\n",
      "Done 10870 batches in 1557.65 sec.    training loss:\t\t3.99746921604\n",
      "Done 10880 batches in 1559.20 sec.    training loss:\t\t3.99723051219\n",
      "Done 10890 batches in 1560.52 sec.    training loss:\t\t3.99709135974\n",
      "Done 10900 batches in 1561.78 sec.    training loss:\t\t3.99692771588\n",
      "Done 10910 batches in 1563.20 sec.    training loss:\t\t3.99672908127\n",
      "Done 10920 batches in 1564.52 sec.    training loss:\t\t3.99653337008\n",
      "Done 10930 batches in 1566.44 sec.    training loss:\t\t3.9963246192\n",
      "Done 10940 batches in 1567.93 sec.    training loss:\t\t3.99612925541\n",
      "Done 10950 batches in 1569.49 sec.    training loss:\t\t3.99601609705\n",
      "Done 10960 batches in 1570.78 sec.    training loss:\t\t3.99574459585\n",
      "Done 10970 batches in 1572.27 sec.    training loss:\t\t3.99552158281\n",
      "Done 10980 batches in 1573.88 sec.    training loss:\t\t3.99534409485\n",
      "Done 10990 batches in 1575.09 sec.    training loss:\t\t3.99507622222\n",
      "Done 11000 batches in 1576.49 sec.    training loss:\t\t3.99498356667\n",
      "Done 11010 batches in 1577.68 sec.    training loss:\t\t3.99469942761\n",
      "Done 11020 batches in 1579.11 sec.    training loss:\t\t3.99445864626\n",
      "Done 11030 batches in 1580.76 sec.    training loss:\t\t3.99422863533\n",
      "Done 11040 batches in 1582.19 sec.    training loss:\t\t3.99417846378\n",
      "Done 11050 batches in 1583.40 sec.    training loss:\t\t3.99392721778\n",
      "Done 11060 batches in 1584.54 sec.    training loss:\t\t3.99356808835\n",
      "Done 11070 batches in 1585.99 sec.    training loss:\t\t3.99327458935\n",
      "Done 11080 batches in 1587.62 sec.    training loss:\t\t3.992970399\n",
      "Done 11090 batches in 1588.96 sec.    training loss:\t\t3.99271621769\n",
      "Done 11100 batches in 1590.41 sec.    training loss:\t\t3.99250832712\n",
      "Done 11110 batches in 1591.64 sec.    training loss:\t\t3.99217327261\n",
      "Done 11120 batches in 1593.12 sec.    training loss:\t\t3.99193844469\n",
      "Done 11130 batches in 1594.66 sec.    training loss:\t\t3.99181658954\n",
      "Done 11140 batches in 1595.74 sec.    training loss:\t\t3.991535259\n",
      "Done 11150 batches in 1597.25 sec.    training loss:\t\t3.99146737203\n",
      "Done 11160 batches in 1598.45 sec.    training loss:\t\t3.99117232523\n",
      "Done 11170 batches in 1600.28 sec.    training loss:\t\t3.99103034362\n",
      "Done 11180 batches in 1601.79 sec.    training loss:\t\t3.99095423522\n",
      "Done 11190 batches in 1603.10 sec.    training loss:\t\t3.99070144416\n",
      "Done 11200 batches in 1604.70 sec.    training loss:\t\t3.99051258387\n",
      "Done 11210 batches in 1606.22 sec.    training loss:\t\t3.99033484574\n",
      "Done 11220 batches in 1607.75 sec.    training loss:\t\t3.99024353639\n",
      "Done 11230 batches in 1609.11 sec.    training loss:\t\t3.99006165256\n",
      "Done 11240 batches in 1610.43 sec.    training loss:\t\t3.98972381383\n",
      "Done 11250 batches in 1611.87 sec.    training loss:\t\t3.98949856792\n",
      "Done 11260 batches in 1613.11 sec.    training loss:\t\t3.98918640114\n",
      "Done 11270 batches in 1614.71 sec.    training loss:\t\t3.9889209071\n",
      "Done 11280 batches in 1616.09 sec.    training loss:\t\t3.98874619639\n",
      "Done 11290 batches in 1617.53 sec.    training loss:\t\t3.98859687063\n",
      "Done 11300 batches in 1618.76 sec.    training loss:\t\t3.98830477449\n",
      "Done 11310 batches in 1620.18 sec.    training loss:\t\t3.98801702778\n",
      "Done 11320 batches in 1621.51 sec.    training loss:\t\t3.98772623107\n",
      "Done 11330 batches in 1622.69 sec.    training loss:\t\t3.98738753585\n",
      "Done 11340 batches in 1624.06 sec.    training loss:\t\t3.98714231421\n",
      "Done 11350 batches in 1625.44 sec.    training loss:\t\t3.98702133202\n",
      "Done 11360 batches in 1627.31 sec.    training loss:\t\t3.98688636345\n",
      "Done 11370 batches in 1629.96 sec.    training loss:\t\t3.98680906371\n",
      "Done 11380 batches in 1631.42 sec.    training loss:\t\t3.9866737134\n",
      "Done 11390 batches in 1633.10 sec.    training loss:\t\t3.98651100982\n",
      "Done 11400 batches in 1634.23 sec.    training loss:\t\t3.98614973116\n",
      "Done 11410 batches in 1635.61 sec.    training loss:\t\t3.98582760615\n",
      "Done 11420 batches in 1636.97 sec.    training loss:\t\t3.98561513779\n",
      "Done 11430 batches in 1638.32 sec.    training loss:\t\t3.98533539513\n",
      "Done 11440 batches in 1639.84 sec.    training loss:\t\t3.98506194286\n",
      "Done 11450 batches in 1641.25 sec.    training loss:\t\t3.98481557642\n",
      "Done 11460 batches in 1642.74 sec.    training loss:\t\t3.98457686776\n",
      "Done 11470 batches in 1644.03 sec.    training loss:\t\t3.98430095964\n",
      "Done 11480 batches in 1645.61 sec.    training loss:\t\t3.98411416943\n",
      "Done 11490 batches in 1646.84 sec.    training loss:\t\t3.98385410274\n",
      "Done 11500 batches in 1648.25 sec.    training loss:\t\t3.98364477527\n",
      "Done 11510 batches in 1649.58 sec.    training loss:\t\t3.98340765912\n",
      "Done 11520 batches in 1650.91 sec.    training loss:\t\t3.98333681056\n",
      "Done 11530 batches in 1652.28 sec.    training loss:\t\t3.98308423835\n",
      "Done 11540 batches in 1653.84 sec.    training loss:\t\t3.98305839385\n",
      "Done 11550 batches in 1655.03 sec.    training loss:\t\t3.98284143167\n",
      "Done 11560 batches in 1656.72 sec.    training loss:\t\t3.98274417487\n",
      "Done 11570 batches in 1658.28 sec.    training loss:\t\t3.98263208075\n",
      "Done 11580 batches in 1660.01 sec.    training loss:\t\t3.98245896025\n",
      "Done 11590 batches in 1661.46 sec.    training loss:\t\t3.98227894378\n",
      "Done 11600 batches in 1663.03 sec.    training loss:\t\t3.98218663152\n",
      "Done 11610 batches in 1664.72 sec.    training loss:\t\t3.98195139497\n",
      "Done 11620 batches in 1666.50 sec.    training loss:\t\t3.98189023038\n",
      "Done 11630 batches in 1667.72 sec.    training loss:\t\t3.98157859254\n",
      "Done 11640 batches in 1668.94 sec.    training loss:\t\t3.9813488353\n",
      "Done 11650 batches in 1670.30 sec.    training loss:\t\t3.98105273101\n",
      "Done 11660 batches in 1671.78 sec.    training loss:\t\t3.98074871588\n",
      "Done 11670 batches in 1673.07 sec.    training loss:\t\t3.98048721968\n",
      "Done 11680 batches in 1674.20 sec.    training loss:\t\t3.98030628078\n",
      "Done 11690 batches in 1675.55 sec.    training loss:\t\t3.97995414703\n",
      "Done 11700 batches in 1677.06 sec.    training loss:\t\t3.97984483299\n",
      "Done 11710 batches in 1678.61 sec.    training loss:\t\t3.97971928004\n",
      "Done 11720 batches in 1679.97 sec.    training loss:\t\t3.97959275118\n",
      "Done 11730 batches in 1681.24 sec.    training loss:\t\t3.97924490125\n",
      "Done 11740 batches in 1682.45 sec.    training loss:\t\t3.97884439468\n",
      "Done 11750 batches in 1683.82 sec.    training loss:\t\t3.97864385191\n",
      "Done 11760 batches in 1685.23 sec.    training loss:\t\t3.97839482455\n",
      "Done 11770 batches in 1686.55 sec.    training loss:\t\t3.97810476693\n",
      "Done 11780 batches in 1688.08 sec.    training loss:\t\t3.97799429806\n",
      "Done 11790 batches in 1689.66 sec.    training loss:\t\t3.97779164696\n",
      "Done 11800 batches in 1691.28 sec.    training loss:\t\t3.97757623499\n",
      "Done 11810 batches in 1692.48 sec.    training loss:\t\t3.97737602402\n",
      "Done 11820 batches in 1693.87 sec.    training loss:\t\t3.97720825503\n",
      "Done 11830 batches in 1695.46 sec.    training loss:\t\t3.97721619737\n",
      "Done 11840 batches in 1697.05 sec.    training loss:\t\t3.97715273642\n",
      "Done 11850 batches in 1698.83 sec.    training loss:\t\t3.97705001272\n",
      "Done 11860 batches in 1700.27 sec.    training loss:\t\t3.97691891213\n",
      "Done 11870 batches in 1701.48 sec.    training loss:\t\t3.97666902094\n",
      "Done 11880 batches in 1702.89 sec.    training loss:\t\t3.9764377538\n",
      "Done 11890 batches in 1704.58 sec.    training loss:\t\t3.97623702547\n",
      "Done 11900 batches in 1705.93 sec.    training loss:\t\t3.97604853131\n",
      "Done 11910 batches in 1707.88 sec.    training loss:\t\t3.97598628351\n",
      "Done 11920 batches in 1709.61 sec.    training loss:\t\t3.97584796806\n",
      "Done 11930 batches in 1711.11 sec.    training loss:\t\t3.97566400891\n",
      "Done 11940 batches in 1712.82 sec.    training loss:\t\t3.97551180282\n",
      "Done 11950 batches in 1714.25 sec.    training loss:\t\t3.97530058075\n",
      "Done 11960 batches in 1715.73 sec.    training loss:\t\t3.97504068501\n",
      "Done 11970 batches in 1717.73 sec.    training loss:\t\t3.97502041373\n",
      "Done 11980 batches in 1719.07 sec.    training loss:\t\t3.97484334518\n",
      "Done 11990 batches in 1720.64 sec.    training loss:\t\t3.97467078232\n",
      "Done 12000 batches in 1722.03 sec.    training loss:\t\t3.97445557435\n",
      "Done 12010 batches in 1723.39 sec.    training loss:\t\t3.97426400592\n",
      "Done 12020 batches in 1725.32 sec.    training loss:\t\t3.97423271742\n",
      "Done 12030 batches in 1726.75 sec.    training loss:\t\t3.97395360151\n",
      "Done 12040 batches in 1728.10 sec.    training loss:\t\t3.97364480299\n",
      "Done 12050 batches in 1729.41 sec.    training loss:\t\t3.97333688162\n",
      "Done 12060 batches in 1730.89 sec.    training loss:\t\t3.97316371254\n",
      "Done 12070 batches in 1732.83 sec.    training loss:\t\t3.9729392361\n",
      "Done 12080 batches in 1734.16 sec.    training loss:\t\t3.9727193806\n",
      "Done 12090 batches in 1735.31 sec.    training loss:\t\t3.97248864046\n",
      "Done 12100 batches in 1737.46 sec.    training loss:\t\t3.97237359019\n",
      "Done 12110 batches in 1738.77 sec.    training loss:\t\t3.97216563567\n",
      "Done 12120 batches in 1740.03 sec.    training loss:\t\t3.97202323892\n",
      "Done 12130 batches in 1741.75 sec.    training loss:\t\t3.97188347701\n",
      "Done 12140 batches in 1743.46 sec.    training loss:\t\t3.97174153966\n",
      "Done 12150 batches in 1744.82 sec.    training loss:\t\t3.97144806401\n",
      "Done 12160 batches in 1746.38 sec.    training loss:\t\t3.97136115671\n",
      "Done 12170 batches in 1747.89 sec.    training loss:\t\t3.97104272613\n",
      "Done 12180 batches in 1749.30 sec.    training loss:\t\t3.97081111638\n",
      "Done 12190 batches in 1750.67 sec.    training loss:\t\t3.97055574985\n",
      "Done 12200 batches in 1752.03 sec.    training loss:\t\t3.97037211129\n",
      "Done 12210 batches in 1753.44 sec.    training loss:\t\t3.97009314167\n",
      "Done 12220 batches in 1754.94 sec.    training loss:\t\t3.96996352325\n",
      "Done 12230 batches in 1756.31 sec.    training loss:\t\t3.96966860922\n",
      "Done 12240 batches in 1757.71 sec.    training loss:\t\t3.96959525647\n",
      "Done 12250 batches in 1758.96 sec.    training loss:\t\t3.96933661093\n",
      "Done 12260 batches in 1760.60 sec.    training loss:\t\t3.96917451729\n",
      "Done 12270 batches in 1761.70 sec.    training loss:\t\t3.96882205303\n",
      "Done 12280 batches in 1763.06 sec.    training loss:\t\t3.9686572815\n",
      "Done 12290 batches in 1765.00 sec.    training loss:\t\t3.96872465956\n",
      "Done 12300 batches in 1766.78 sec.    training loss:\t\t3.96871383838\n",
      "Done 12310 batches in 1768.27 sec.    training loss:\t\t3.96845765635\n",
      "Done 12320 batches in 1769.48 sec.    training loss:\t\t3.96816167026\n",
      "Done 12330 batches in 1771.08 sec.    training loss:\t\t3.96788409139\n",
      "Done 12340 batches in 1772.47 sec.    training loss:\t\t3.96773945733\n",
      "Done 12350 batches in 1773.77 sec.    training loss:\t\t3.96747215115\n",
      "Done 12360 batches in 1775.24 sec.    training loss:\t\t3.96734532722\n",
      "Done 12370 batches in 1776.56 sec.    training loss:\t\t3.96706784961\n",
      "Done 12380 batches in 1778.08 sec.    training loss:\t\t3.96686900203\n",
      "Done 12390 batches in 1779.61 sec.    training loss:\t\t3.96661139695\n",
      "Done 12400 batches in 1780.83 sec.    training loss:\t\t3.96638730074\n",
      "Done 12410 batches in 1782.43 sec.    training loss:\t\t3.96616594628\n",
      "Done 12420 batches in 1783.76 sec.    training loss:\t\t3.96590821031\n",
      "Done 12430 batches in 1785.36 sec.    training loss:\t\t3.96573092767\n",
      "Done 12440 batches in 1786.70 sec.    training loss:\t\t3.96555696843\n",
      "Done 12450 batches in 1787.79 sec.    training loss:\t\t3.96528182221\n",
      "Done 12460 batches in 1789.43 sec.    training loss:\t\t3.96510397146\n",
      "Done 12470 batches in 1791.16 sec.    training loss:\t\t3.96497160767\n",
      "Done 12480 batches in 1792.36 sec.    training loss:\t\t3.96473766135\n",
      "Done 12490 batches in 1794.03 sec.    training loss:\t\t3.96445052126\n",
      "Done 12500 batches in 1795.28 sec.    training loss:\t\t3.96423189362\n",
      "Done 12510 batches in 1796.54 sec.    training loss:\t\t3.96399498788\n",
      "Done 12520 batches in 1798.03 sec.    training loss:\t\t3.96386625371\n",
      "Done 12530 batches in 1799.62 sec.    training loss:\t\t3.9636324396\n",
      "Done 12540 batches in 1801.20 sec.    training loss:\t\t3.96354564864\n",
      "Done 12550 batches in 1802.73 sec.    training loss:\t\t3.96333079099\n",
      "Done 12560 batches in 1804.36 sec.    training loss:\t\t3.96315916583\n",
      "Done 12570 batches in 1805.70 sec.    training loss:\t\t3.96304040881\n",
      "Done 12580 batches in 1807.09 sec.    training loss:\t\t3.96299273107\n",
      "Done 12590 batches in 1808.76 sec.    training loss:\t\t3.96283868376\n",
      "Done 12600 batches in 1810.16 sec.    training loss:\t\t3.96266040391\n",
      "Done 12610 batches in 1811.33 sec.    training loss:\t\t3.96243612804\n",
      "Done 12620 batches in 1812.98 sec.    training loss:\t\t3.96234396755\n",
      "Done 12630 batches in 1814.43 sec.    training loss:\t\t3.96223792054\n",
      "Done 12640 batches in 1815.85 sec.    training loss:\t\t3.96194404262\n",
      "Done 12650 batches in 1817.31 sec.    training loss:\t\t3.96173097499\n",
      "Done 12660 batches in 1818.55 sec.    training loss:\t\t3.96155166532\n",
      "Done 12670 batches in 1819.94 sec.    training loss:\t\t3.96136735618\n",
      "Done 12680 batches in 1821.93 sec.    training loss:\t\t3.96130019214\n",
      "Done 12690 batches in 1823.27 sec.    training loss:\t\t3.96114317159\n",
      "Done 12700 batches in 1824.87 sec.    training loss:\t\t3.96097406611\n",
      "Done 12710 batches in 1826.13 sec.    training loss:\t\t3.96063637214\n",
      "Done 12720 batches in 1827.54 sec.    training loss:\t\t3.96040962029\n",
      "Done 12730 batches in 1829.05 sec.    training loss:\t\t3.96033204469\n",
      "Done 12740 batches in 1830.55 sec.    training loss:\t\t3.96022825323\n",
      "Done 12750 batches in 1832.16 sec.    training loss:\t\t3.9600760157\n",
      "Done 12760 batches in 1833.49 sec.    training loss:\t\t3.95988199421\n",
      "Done 12770 batches in 1835.05 sec.    training loss:\t\t3.95976072478\n",
      "Done 12780 batches in 1836.56 sec.    training loss:\t\t3.9595429413\n",
      "Done 12790 batches in 1838.55 sec.    training loss:\t\t3.95942434202\n",
      "Done 12800 batches in 1840.78 sec.    training loss:\t\t3.95940638835\n",
      "Done 12810 batches in 1842.62 sec.    training loss:\t\t3.95937727175\n",
      "Done 12820 batches in 1843.83 sec.    training loss:\t\t3.95909375997\n",
      "Done 12830 batches in 1845.15 sec.    training loss:\t\t3.95888769783\n",
      "Done 12840 batches in 1846.33 sec.    training loss:\t\t3.95858597523\n",
      "Done 12850 batches in 1847.68 sec.    training loss:\t\t3.95834466344\n",
      "Done 12860 batches in 1849.28 sec.    training loss:\t\t3.95816282318\n",
      "Done 12870 batches in 1850.66 sec.    training loss:\t\t3.95798395115\n",
      "Done 12880 batches in 1851.97 sec.    training loss:\t\t3.95787729643\n",
      "Done 12890 batches in 1853.42 sec.    training loss:\t\t3.95768367815\n",
      "Done 12900 batches in 1854.59 sec.    training loss:\t\t3.95747979044\n",
      "Done 12910 batches in 1856.08 sec.    training loss:\t\t3.95726944835\n",
      "Done 12920 batches in 1857.44 sec.    training loss:\t\t3.95707824876\n",
      "Done 12930 batches in 1858.62 sec.    training loss:\t\t3.95689718817\n",
      "Done 12940 batches in 1859.90 sec.    training loss:\t\t3.95658898846\n",
      "Done 12950 batches in 1861.23 sec.    training loss:\t\t3.95623430727\n",
      "Done 12960 batches in 1862.65 sec.    training loss:\t\t3.95602321527\n",
      "Done 12970 batches in 1864.39 sec.    training loss:\t\t3.95581896719\n",
      "Done 12980 batches in 1866.10 sec.    training loss:\t\t3.95576348463\n",
      "Done 12990 batches in 1867.63 sec.    training loss:\t\t3.95564754813\n",
      "Done 13000 batches in 1868.97 sec.    training loss:\t\t3.95548338657\n",
      "Done 13010 batches in 1870.54 sec.    training loss:\t\t3.95540542987\n",
      "Done 13020 batches in 1871.81 sec.    training loss:\t\t3.95510625352\n",
      "Done 13030 batches in 1873.32 sec.    training loss:\t\t3.95499355202\n",
      "Done 13040 batches in 1874.86 sec.    training loss:\t\t3.95486639791\n",
      "Done 13050 batches in 1875.99 sec.    training loss:\t\t3.95448917636\n",
      "Done 13060 batches in 1877.38 sec.    training loss:\t\t3.95429926528\n",
      "Done 13070 batches in 1879.02 sec.    training loss:\t\t3.95403891708\n",
      "Done 13080 batches in 1880.28 sec.    training loss:\t\t3.95374171185\n",
      "Done 13090 batches in 1882.10 sec.    training loss:\t\t3.95367493965\n",
      "Done 13100 batches in 1883.33 sec.    training loss:\t\t3.9534112707\n",
      "Done 13110 batches in 1884.84 sec.    training loss:\t\t3.95321066881\n",
      "Done 13120 batches in 1886.17 sec.    training loss:\t\t3.95302917349\n",
      "Done 13130 batches in 1887.69 sec.    training loss:\t\t3.95293918135\n",
      "Done 13140 batches in 1889.21 sec.    training loss:\t\t3.95283709888\n",
      "Done 13150 batches in 1891.18 sec.    training loss:\t\t3.95282145324\n",
      "Done 13160 batches in 1893.19 sec.    training loss:\t\t3.95277379398\n",
      "Done 13170 batches in 1894.90 sec.    training loss:\t\t3.95267513971\n",
      "Done 13180 batches in 1896.18 sec.    training loss:\t\t3.95236018533\n",
      "Done 13190 batches in 1897.39 sec.    training loss:\t\t3.95215277692\n",
      "Done 13200 batches in 1898.80 sec.    training loss:\t\t3.95191497615\n",
      "Done 13210 batches in 1900.09 sec.    training loss:\t\t3.95165139338\n",
      "Done 13220 batches in 1901.49 sec.    training loss:\t\t3.95156068701\n",
      "Done 13230 batches in 1902.83 sec.    training loss:\t\t3.95137684261\n",
      "Done 13240 batches in 1904.61 sec.    training loss:\t\t3.95121593663\n",
      "Done 13250 batches in 1905.92 sec.    training loss:\t\t3.95112032198\n",
      "Done 13260 batches in 1907.22 sec.    training loss:\t\t3.95078839121\n",
      "Done 13270 batches in 1908.95 sec.    training loss:\t\t3.95075045489\n",
      "Done 13280 batches in 1910.38 sec.    training loss:\t\t3.95060411905\n",
      "Done 13290 batches in 1911.67 sec.    training loss:\t\t3.95044251418\n",
      "Done 13300 batches in 1912.83 sec.    training loss:\t\t3.95021812785\n",
      "Done 13310 batches in 1914.21 sec.    training loss:\t\t3.95014733138\n",
      "Done 13320 batches in 1915.25 sec.    training loss:\t\t3.94983835188\n",
      "Done 13330 batches in 1916.51 sec.    training loss:\t\t3.9497301236\n",
      "Done 13340 batches in 1917.73 sec.    training loss:\t\t3.94950484656\n",
      "Done 13350 batches in 1919.39 sec.    training loss:\t\t3.94926581036\n",
      "Done 13360 batches in 1920.44 sec.    training loss:\t\t3.94893551967\n",
      "Done 13370 batches in 1921.81 sec.    training loss:\t\t3.9487156233\n",
      "Done 13380 batches in 1923.23 sec.    training loss:\t\t3.94857821486\n",
      "Done 13390 batches in 1924.46 sec.    training loss:\t\t3.94833465776\n",
      "Done 13400 batches in 1925.84 sec.    training loss:\t\t3.94803198814\n",
      "Done 13410 batches in 1927.53 sec.    training loss:\t\t3.94785376036\n",
      "Done 13420 batches in 1928.90 sec.    training loss:\t\t3.94763400532\n",
      "Done 13430 batches in 1930.06 sec.    training loss:\t\t3.94739315618\n",
      "Done 13440 batches in 1931.19 sec.    training loss:\t\t3.94713322043\n",
      "Done 13450 batches in 1932.90 sec.    training loss:\t\t3.94695393842\n",
      "Done 13460 batches in 1934.23 sec.    training loss:\t\t3.9466955558\n",
      "Done 13470 batches in 1935.52 sec.    training loss:\t\t3.94655285864\n",
      "Done 13480 batches in 1936.92 sec.    training loss:\t\t3.94635345962\n",
      "Done 13490 batches in 1938.37 sec.    training loss:\t\t3.94613500506\n",
      "Done 13500 batches in 1940.13 sec.    training loss:\t\t3.94597219614\n",
      "Done 13510 batches in 1941.57 sec.    training loss:\t\t3.94573075424\n",
      "Done 13520 batches in 1942.91 sec.    training loss:\t\t3.94550752254\n",
      "Done 13530 batches in 1944.49 sec.    training loss:\t\t3.94530233491\n",
      "Done 13540 batches in 1945.87 sec.    training loss:\t\t3.94499427989\n",
      "Done 13550 batches in 1946.94 sec.    training loss:\t\t3.9446537331\n",
      "Done 13560 batches in 1948.83 sec.    training loss:\t\t3.94451509319\n",
      "Done 13570 batches in 1950.12 sec.    training loss:\t\t3.94425495442\n",
      "Done 13580 batches in 1951.54 sec.    training loss:\t\t3.94405274065\n",
      "Done 13590 batches in 1952.69 sec.    training loss:\t\t3.94383145408\n",
      "Done 13600 batches in 1954.47 sec.    training loss:\t\t3.94378816456\n",
      "Done 13610 batches in 1955.98 sec.    training loss:\t\t3.94370752891\n",
      "Done 13620 batches in 1957.63 sec.    training loss:\t\t3.94356144885\n",
      "Done 13630 batches in 1959.26 sec.    training loss:\t\t3.94339084874\n",
      "Done 13640 batches in 1961.00 sec.    training loss:\t\t3.94320276689\n",
      "Done 13650 batches in 1962.73 sec.    training loss:\t\t3.94304442993\n",
      "Done 13660 batches in 1963.79 sec.    training loss:\t\t3.94280424198\n",
      "Done 13670 batches in 1965.17 sec.    training loss:\t\t3.94263049714\n",
      "Done 13680 batches in 1966.81 sec.    training loss:\t\t3.9424652323\n",
      "Done 13690 batches in 1968.24 sec.    training loss:\t\t3.94221704492\n",
      "Done 13700 batches in 1969.59 sec.    training loss:\t\t3.94217916177\n",
      "Done 13710 batches in 1971.35 sec.    training loss:\t\t3.94205832313\n",
      "Done 13720 batches in 1972.79 sec.    training loss:\t\t3.94185846184\n",
      "Done 13730 batches in 1974.65 sec.    training loss:\t\t3.94162798619\n",
      "Done 13740 batches in 1976.08 sec.    training loss:\t\t3.94148341822\n",
      "Done 13750 batches in 1977.36 sec.    training loss:\t\t3.94130357427\n",
      "Done 13760 batches in 1978.83 sec.    training loss:\t\t3.9411469322\n",
      "Done 13770 batches in 1980.22 sec.    training loss:\t\t3.94095844526\n",
      "Done 13780 batches in 1981.57 sec.    training loss:\t\t3.94070942078\n",
      "Done 13790 batches in 1983.01 sec.    training loss:\t\t3.9404878867\n",
      "Done 13800 batches in 1984.32 sec.    training loss:\t\t3.94035656547\n",
      "Done 13810 batches in 1985.70 sec.    training loss:\t\t3.94018742931\n",
      "Done 13820 batches in 1987.17 sec.    training loss:\t\t3.94013007108\n",
      "Done 13830 batches in 1988.65 sec.    training loss:\t\t3.9399860033\n",
      "Done 13840 batches in 1990.16 sec.    training loss:\t\t3.93983713337\n",
      "Done 13850 batches in 1991.48 sec.    training loss:\t\t3.93967929713\n",
      "Done 13860 batches in 1992.99 sec.    training loss:\t\t3.93961589665\n",
      "Done 13870 batches in 1994.89 sec.    training loss:\t\t3.93956321247\n",
      "Done 13880 batches in 1996.14 sec.    training loss:\t\t3.93944617967\n",
      "Done 13890 batches in 1997.26 sec.    training loss:\t\t3.93921528127\n",
      "Done 13900 batches in 1999.06 sec.    training loss:\t\t3.93907030212\n",
      "Done 13910 batches in 2000.62 sec.    training loss:\t\t3.9389919397\n",
      "Done 13920 batches in 2001.80 sec.    training loss:\t\t3.93878791697\n",
      "Done 13930 batches in 2003.56 sec.    training loss:\t\t3.93871048484\n",
      "Done 13940 batches in 2004.90 sec.    training loss:\t\t3.93843863169\n",
      "Done 13950 batches in 2006.77 sec.    training loss:\t\t3.93837426828\n",
      "Done 13960 batches in 2008.36 sec.    training loss:\t\t3.9382765365\n",
      "Done 13970 batches in 2009.73 sec.    training loss:\t\t3.93802243366\n",
      "Done 13980 batches in 2011.28 sec.    training loss:\t\t3.93786719603\n",
      "Done 13990 batches in 2012.75 sec.    training loss:\t\t3.93768813854\n",
      "Done 14000 batches in 2014.07 sec.    training loss:\t\t3.93754805762\n",
      "Done 14010 batches in 2015.23 sec.    training loss:\t\t3.93732929979\n",
      "Done 14020 batches in 2016.78 sec.    training loss:\t\t3.93715213956\n",
      "Done 14030 batches in 2018.17 sec.    training loss:\t\t3.93694554179\n",
      "Done 14040 batches in 2019.54 sec.    training loss:\t\t3.93670809262\n",
      "Done 14050 batches in 2021.00 sec.    training loss:\t\t3.93648468539\n",
      "Done 14060 batches in 2022.36 sec.    training loss:\t\t3.9363094118\n",
      "Done 14070 batches in 2023.95 sec.    training loss:\t\t3.93607412837\n",
      "Done 14080 batches in 2025.60 sec.    training loss:\t\t3.9359882538\n",
      "Done 14090 batches in 2026.79 sec.    training loss:\t\t3.93575038463\n",
      "Done 14100 batches in 2028.23 sec.    training loss:\t\t3.93551686747\n",
      "Done 14110 batches in 2029.59 sec.    training loss:\t\t3.93530909993\n",
      "Done 14120 batches in 2030.96 sec.    training loss:\t\t3.93515494329\n",
      "Done 14130 batches in 2032.42 sec.    training loss:\t\t3.93506882243\n",
      "Done 14140 batches in 2033.96 sec.    training loss:\t\t3.93503455753\n",
      "Done 14150 batches in 2035.48 sec.    training loss:\t\t3.93490242892\n",
      "Done 14160 batches in 2036.77 sec.    training loss:\t\t3.93473860121\n",
      "Done 14170 batches in 2037.98 sec.    training loss:\t\t3.93449203882\n",
      "Done 14180 batches in 2039.15 sec.    training loss:\t\t3.93428107011\n",
      "Done 14190 batches in 2040.74 sec.    training loss:\t\t3.9340702776\n",
      "Done 14200 batches in 2042.33 sec.    training loss:\t\t3.9339571731\n",
      "Done 14210 batches in 2043.77 sec.    training loss:\t\t3.93381810586\n",
      "Done 14220 batches in 2045.19 sec.    training loss:\t\t3.93362385744\n",
      "Done 14230 batches in 2046.56 sec.    training loss:\t\t3.9334632988\n",
      "Done 14240 batches in 2048.10 sec.    training loss:\t\t3.93333607718\n",
      "Done 14250 batches in 2049.52 sec.    training loss:\t\t3.93320290964\n",
      "Done 14260 batches in 2050.68 sec.    training loss:\t\t3.93296476058\n",
      "Done 14270 batches in 2051.98 sec.    training loss:\t\t3.93279111312\n",
      "Done 14280 batches in 2053.06 sec.    training loss:\t\t3.93255138028\n",
      "Done 14290 batches in 2054.49 sec.    training loss:\t\t3.93243625631\n",
      "Done 14300 batches in 2055.75 sec.    training loss:\t\t3.93217904378\n",
      "Done 14310 batches in 2057.53 sec.    training loss:\t\t3.93200134833\n",
      "Done 14320 batches in 2058.92 sec.    training loss:\t\t3.93180377445\n",
      "Done 14330 batches in 2060.65 sec.    training loss:\t\t3.93169517961\n",
      "Done 14340 batches in 2061.99 sec.    training loss:\t\t3.93155372262\n",
      "Done 14350 batches in 2063.08 sec.    training loss:\t\t3.93126578999\n",
      "Done 14360 batches in 2064.33 sec.    training loss:\t\t3.93102569494\n",
      "Done 14370 batches in 2065.46 sec.    training loss:\t\t3.93079260441\n",
      "Done 14380 batches in 2067.40 sec.    training loss:\t\t3.93076456268\n",
      "Done 14390 batches in 2068.85 sec.    training loss:\t\t3.93063085131\n",
      "Done 14400 batches in 2070.55 sec.    training loss:\t\t3.93059715697\n",
      "Done 14410 batches in 2072.07 sec.    training loss:\t\t3.93040261414\n",
      "Done 14420 batches in 2073.85 sec.    training loss:\t\t3.9303361229\n",
      "Done 14430 batches in 2075.46 sec.    training loss:\t\t3.93022000653\n",
      "Done 14440 batches in 2076.93 sec.    training loss:\t\t3.93000169971\n",
      "Done 14450 batches in 2078.32 sec.    training loss:\t\t3.92994583948\n",
      "Done 14460 batches in 2079.84 sec.    training loss:\t\t3.92987365723\n",
      "Done 14470 batches in 2081.06 sec.    training loss:\t\t3.92974558534\n",
      "Done 14480 batches in 2082.29 sec.    training loss:\t\t3.92951939835\n",
      "Done 14490 batches in 2084.06 sec.    training loss:\t\t3.9295353968\n",
      "Done 14500 batches in 2085.39 sec.    training loss:\t\t3.92934453676\n",
      "Done 14510 batches in 2086.93 sec.    training loss:\t\t3.92926411777\n",
      "Done 14520 batches in 2088.26 sec.    training loss:\t\t3.92900583884\n",
      "Done 14530 batches in 2089.68 sec.    training loss:\t\t3.928872656\n",
      "Done 14540 batches in 2091.19 sec.    training loss:\t\t3.92877111351\n",
      "Done 14550 batches in 2092.42 sec.    training loss:\t\t3.92852039085\n",
      "Done 14560 batches in 2093.88 sec.    training loss:\t\t3.9284041256\n",
      "Done 14570 batches in 2095.73 sec.    training loss:\t\t3.92830127135\n",
      "Done 14580 batches in 2097.16 sec.    training loss:\t\t3.92822830312\n",
      "Done 14590 batches in 2098.63 sec.    training loss:\t\t3.92805012011\n",
      "Done 14600 batches in 2100.22 sec.    training loss:\t\t3.92788353255\n",
      "Done 14610 batches in 2101.54 sec.    training loss:\t\t3.92765527506\n",
      "Done 14620 batches in 2102.78 sec.    training loss:\t\t3.92742106133\n",
      "Done 14630 batches in 2103.91 sec.    training loss:\t\t3.92712835179\n",
      "Done 14640 batches in 2105.67 sec.    training loss:\t\t3.92702405283\n",
      "Done 14650 batches in 2107.96 sec.    training loss:\t\t3.92692254273\n",
      "Done 14660 batches in 2109.42 sec.    training loss:\t\t3.92674553017\n",
      "Done 14670 batches in 2110.66 sec.    training loss:\t\t3.92654753851\n",
      "Done 14680 batches in 2112.13 sec.    training loss:\t\t3.92628006771\n",
      "Done 14690 batches in 2113.82 sec.    training loss:\t\t3.92621006823\n",
      "Done 14700 batches in 2115.34 sec.    training loss:\t\t3.92599588634\n",
      "Done 14710 batches in 2116.89 sec.    training loss:\t\t3.92590194197\n",
      "Done 14720 batches in 2118.17 sec.    training loss:\t\t3.92571038081\n",
      "Done 14730 batches in 2119.40 sec.    training loss:\t\t3.925538032\n",
      "Done 14740 batches in 2120.55 sec.    training loss:\t\t3.92530832748\n",
      "Done 14750 batches in 2121.82 sec.    training loss:\t\t3.92508951662\n",
      "Done 14760 batches in 2123.65 sec.    training loss:\t\t3.92501126562\n",
      "Done 14770 batches in 2125.44 sec.    training loss:\t\t3.92488130414\n",
      "Done 14780 batches in 2126.69 sec.    training loss:\t\t3.92474244131\n",
      "Done 14790 batches in 2128.09 sec.    training loss:\t\t3.92453717636\n",
      "Done 14800 batches in 2129.37 sec.    training loss:\t\t3.9243144364\n",
      "Done 14810 batches in 2130.80 sec.    training loss:\t\t3.92419142121\n",
      "Done 14820 batches in 2132.05 sec.    training loss:\t\t3.92400328735\n",
      "Done 14830 batches in 2133.63 sec.    training loss:\t\t3.92395501812\n",
      "Done 14840 batches in 2134.93 sec.    training loss:\t\t3.92382148369\n",
      "Done 14850 batches in 2136.25 sec.    training loss:\t\t3.92366476046\n",
      "Done 14860 batches in 2137.54 sec.    training loss:\t\t3.92346106094\n",
      "Done 14870 batches in 2139.07 sec.    training loss:\t\t3.92331154154\n",
      "Done 14880 batches in 2140.55 sec.    training loss:\t\t3.92323807489\n",
      "Done 14890 batches in 2141.82 sec.    training loss:\t\t3.92291181144\n",
      "Done 14900 batches in 2143.88 sec.    training loss:\t\t3.92281542709\n",
      "Done 14910 batches in 2145.27 sec.    training loss:\t\t3.92273660538\n",
      "Done 14920 batches in 2146.72 sec.    training loss:\t\t3.92269579302\n",
      "Done 14930 batches in 2148.16 sec.    training loss:\t\t3.92253380954\n",
      "Done 14940 batches in 2149.66 sec.    training loss:\t\t3.92235660424\n",
      "Done 14950 batches in 2150.81 sec.    training loss:\t\t3.92216350394\n",
      "Done 14960 batches in 2152.33 sec.    training loss:\t\t3.92202385956\n",
      "Done 14970 batches in 2153.85 sec.    training loss:\t\t3.92172278175\n",
      "Done 14980 batches in 2155.63 sec.    training loss:\t\t3.92163848228\n",
      "Done 14990 batches in 2157.31 sec.    training loss:\t\t3.92143750432\n",
      "Done 15000 batches in 2158.66 sec.    training loss:\t\t3.92124150139\n",
      "Done 15010 batches in 2160.07 sec.    training loss:\t\t3.92103595746\n",
      "Done 15020 batches in 2161.27 sec.    training loss:\t\t3.92082769737\n",
      "Done 15030 batches in 2162.59 sec.    training loss:\t\t3.92070497432\n",
      "Done 15040 batches in 2164.03 sec.    training loss:\t\t3.92052957696\n",
      "Done 15050 batches in 2165.56 sec.    training loss:\t\t3.92041245198\n",
      "Done 15060 batches in 2167.14 sec.    training loss:\t\t3.92022140093\n",
      "Done 15070 batches in 2168.60 sec.    training loss:\t\t3.92004256685\n",
      "Done 15080 batches in 2170.11 sec.    training loss:\t\t3.91992181641\n",
      "Done 15090 batches in 2171.43 sec.    training loss:\t\t3.91980503242\n",
      "Done 15100 batches in 2173.02 sec.    training loss:\t\t3.91966356993\n",
      "Done 15110 batches in 2174.40 sec.    training loss:\t\t3.91944243297\n",
      "Done 15120 batches in 2175.88 sec.    training loss:\t\t3.91939376039\n",
      "Done 15130 batches in 2177.38 sec.    training loss:\t\t3.91925729166\n",
      "Done 15140 batches in 2179.03 sec.    training loss:\t\t3.91905834445\n",
      "Done 15150 batches in 2180.49 sec.    training loss:\t\t3.91903065889\n",
      "Done 15160 batches in 2181.70 sec.    training loss:\t\t3.91891026533\n",
      "Done 15170 batches in 2183.33 sec.    training loss:\t\t3.91882872831\n",
      "Done 15180 batches in 2184.70 sec.    training loss:\t\t3.91866991644\n",
      "Done 15190 batches in 2186.07 sec.    training loss:\t\t3.91846205199\n",
      "Done 15200 batches in 2187.97 sec.    training loss:\t\t3.91844357511\n",
      "Done 15210 batches in 2189.33 sec.    training loss:\t\t3.91814037871\n",
      "Done 15220 batches in 2190.81 sec.    training loss:\t\t3.91812071211\n",
      "Done 15230 batches in 2192.01 sec.    training loss:\t\t3.91787793586\n",
      "Done 15240 batches in 2193.67 sec.    training loss:\t\t3.91775680094\n",
      "Done 15250 batches in 2194.79 sec.    training loss:\t\t3.91756713742\n",
      "Done 15260 batches in 2196.35 sec.    training loss:\t\t3.91745868928\n",
      "Done 15270 batches in 2197.45 sec.    training loss:\t\t3.91720066511\n",
      "Done 15280 batches in 2198.86 sec.    training loss:\t\t3.91705600821\n",
      "Done 15290 batches in 2200.30 sec.    training loss:\t\t3.91689467878\n",
      "Done 15300 batches in 2201.92 sec.    training loss:\t\t3.91676851695\n",
      "Done 15310 batches in 2203.31 sec.    training loss:\t\t3.91650926723\n",
      "Done 15320 batches in 2204.76 sec.    training loss:\t\t3.9163659301\n",
      "Done 15330 batches in 2206.37 sec.    training loss:\t\t3.91617478145\n",
      "Done 15340 batches in 2208.04 sec.    training loss:\t\t3.91609995521\n",
      "Done 15350 batches in 2209.45 sec.    training loss:\t\t3.91599447132\n",
      "Done 15360 batches in 2210.67 sec.    training loss:\t\t3.91585563392\n",
      "Done 15370 batches in 2212.16 sec.    training loss:\t\t3.91571425858\n",
      "Done 15380 batches in 2213.66 sec.    training loss:\t\t3.91557350828\n",
      "Done 15390 batches in 2215.15 sec.    training loss:\t\t3.91549314023\n",
      "Done 15400 batches in 2216.46 sec.    training loss:\t\t3.91530113392\n",
      "Done 15410 batches in 2217.74 sec.    training loss:\t\t3.91514258137\n",
      "Done 15420 batches in 2219.03 sec.    training loss:\t\t3.91494249848\n",
      "Done 15430 batches in 2220.87 sec.    training loss:\t\t3.91477477079\n",
      "Done 15440 batches in 2221.98 sec.    training loss:\t\t3.91458146799\n",
      "Done 15450 batches in 2223.30 sec.    training loss:\t\t3.91440325276\n",
      "Done 15460 batches in 2224.67 sec.    training loss:\t\t3.91428591775\n",
      "Done 15470 batches in 2226.04 sec.    training loss:\t\t3.91415845101\n",
      "Done 15480 batches in 2227.40 sec.    training loss:\t\t3.91406538191\n",
      "Done 15490 batches in 2228.73 sec.    training loss:\t\t3.91401633023\n",
      "Done 15500 batches in 2230.16 sec.    training loss:\t\t3.91385194895\n",
      "Done 15510 batches in 2231.38 sec.    training loss:\t\t3.91360843306\n",
      "Done 15520 batches in 2232.65 sec.    training loss:\t\t3.91349370722\n",
      "Done 15530 batches in 2233.94 sec.    training loss:\t\t3.91332061126\n",
      "Done 15540 batches in 2235.19 sec.    training loss:\t\t3.91306541928\n",
      "Done 15550 batches in 2236.54 sec.    training loss:\t\t3.91290879883\n",
      "Done 15560 batches in 2237.82 sec.    training loss:\t\t3.91272751425\n",
      "Done 15570 batches in 2239.22 sec.    training loss:\t\t3.91253854192\n",
      "Done 15580 batches in 2240.60 sec.    training loss:\t\t3.91237832588\n",
      "Done 15590 batches in 2242.11 sec.    training loss:\t\t3.91227299025\n",
      "Done 15600 batches in 2243.30 sec.    training loss:\t\t3.91211646575\n",
      "Done 15610 batches in 2244.68 sec.    training loss:\t\t3.91203916412\n",
      "Done 15620 batches in 2246.25 sec.    training loss:\t\t3.91195275223\n",
      "Done 15630 batches in 2247.64 sec.    training loss:\t\t3.91184892125\n",
      "Done 15640 batches in 2249.32 sec.    training loss:\t\t3.91175595932\n",
      "Done 15650 batches in 2250.80 sec.    training loss:\t\t3.9116549085\n",
      "Done 15660 batches in 2252.12 sec.    training loss:\t\t3.91149927175\n",
      "Done 15670 batches in 2253.63 sec.    training loss:\t\t3.91133292611\n",
      "Done 15680 batches in 2255.24 sec.    training loss:\t\t3.9112229488\n",
      "Done 15690 batches in 2256.47 sec.    training loss:\t\t3.91103655263\n",
      "Done 15700 batches in 2257.61 sec.    training loss:\t\t3.91086123547\n",
      "Done 15710 batches in 2259.01 sec.    training loss:\t\t3.91071307981\n",
      "Done 15720 batches in 2260.19 sec.    training loss:\t\t3.9104813725\n",
      "Done 15730 batches in 2261.53 sec.    training loss:\t\t3.91031424191\n",
      "Done 15740 batches in 2262.90 sec.    training loss:\t\t3.91016520534\n",
      "Done 15750 batches in 2264.60 sec.    training loss:\t\t3.91007333058\n",
      "Done 15760 batches in 2265.95 sec.    training loss:\t\t3.90992873257\n",
      "Done 15770 batches in 2267.93 sec.    training loss:\t\t3.90992500431\n",
      "Done 15780 batches in 2269.42 sec.    training loss:\t\t3.90979520125\n",
      "Done 15790 batches in 2271.33 sec.    training loss:\t\t3.90980046597\n",
      "Done 15800 batches in 2272.70 sec.    training loss:\t\t3.90964242584\n",
      "Done 15810 batches in 2274.12 sec.    training loss:\t\t3.9095705072\n",
      "Done 15820 batches in 2275.39 sec.    training loss:\t\t3.90932339844\n",
      "Done 15830 batches in 2276.70 sec.    training loss:\t\t3.90916900505\n",
      "Done 15840 batches in 2278.00 sec.    training loss:\t\t3.90912275007\n",
      "Done 15850 batches in 2279.87 sec.    training loss:\t\t3.90908388491\n",
      "Done 15860 batches in 2281.58 sec.    training loss:\t\t3.90896343767\n",
      "Done 15870 batches in 2283.04 sec.    training loss:\t\t3.90890917991\n",
      "Done 15880 batches in 2284.26 sec.    training loss:\t\t3.90866008429\n",
      "Done 15890 batches in 2285.43 sec.    training loss:\t\t3.90842165705\n",
      "Done 15900 batches in 2286.92 sec.    training loss:\t\t3.90827915607\n",
      "Done 15910 batches in 2288.39 sec.    training loss:\t\t3.90817015886\n",
      "Done 15920 batches in 2289.66 sec.    training loss:\t\t3.90797681804\n",
      "Done 15930 batches in 2291.01 sec.    training loss:\t\t3.90781426042\n",
      "Done 15940 batches in 2292.16 sec.    training loss:\t\t3.90759464981\n",
      "Done 15950 batches in 2293.65 sec.    training loss:\t\t3.90754096175\n",
      "Done 15960 batches in 2294.84 sec.    training loss:\t\t3.90739862695\n",
      "Done 15970 batches in 2296.10 sec.    training loss:\t\t3.90726351778\n",
      "Done 15980 batches in 2297.43 sec.    training loss:\t\t3.90698956323\n",
      "Done 15990 batches in 2299.08 sec.    training loss:\t\t3.90687244923\n",
      "Done 16000 batches in 2300.73 sec.    training loss:\t\t3.90675385299\n",
      "Done 16010 batches in 2302.51 sec.    training loss:\t\t3.90665925337\n",
      "Done 16020 batches in 2304.09 sec.    training loss:\t\t3.90648202912\n",
      "Done 16030 batches in 2305.49 sec.    training loss:\t\t3.90630740352\n",
      "Done 16040 batches in 2306.74 sec.    training loss:\t\t3.90612550198\n",
      "Done 16050 batches in 2308.80 sec.    training loss:\t\t3.90601901623\n",
      "Done 16060 batches in 2310.19 sec.    training loss:\t\t3.90577224885\n",
      "Done 16070 batches in 2311.85 sec.    training loss:\t\t3.90563149707\n",
      "Done 16080 batches in 2313.40 sec.    training loss:\t\t3.90555055134\n",
      "Done 16090 batches in 2315.06 sec.    training loss:\t\t3.90555187716\n",
      "Done 16100 batches in 2316.59 sec.    training loss:\t\t3.90547467287\n",
      "Done 16110 batches in 2317.79 sec.    training loss:\t\t3.90530187434\n",
      "Done 16120 batches in 2319.03 sec.    training loss:\t\t3.90510082274\n",
      "Done 16130 batches in 2320.38 sec.    training loss:\t\t3.90504906346\n",
      "Done 16140 batches in 2321.72 sec.    training loss:\t\t3.90492734933\n",
      "Done 16150 batches in 2322.85 sec.    training loss:\t\t3.90476454603\n",
      "Done 16160 batches in 2324.24 sec.    training loss:\t\t3.90462224649\n",
      "Done 16170 batches in 2325.55 sec.    training loss:\t\t3.90445633362\n",
      "Done 16180 batches in 2327.04 sec.    training loss:\t\t3.90424272134\n",
      "Done 16190 batches in 2328.22 sec.    training loss:\t\t3.90404596151\n",
      "Done 16200 batches in 2329.67 sec.    training loss:\t\t3.90393399964\n",
      "Done 16210 batches in 2330.98 sec.    training loss:\t\t3.90366857546\n",
      "Done 16220 batches in 2332.62 sec.    training loss:\t\t3.90352380483\n",
      "Done 16230 batches in 2333.95 sec.    training loss:\t\t3.90329487829\n",
      "Done 16240 batches in 2335.31 sec.    training loss:\t\t3.90327081821\n",
      "Done 16250 batches in 2336.67 sec.    training loss:\t\t3.90316236054\n",
      "Done 16260 batches in 2338.40 sec.    training loss:\t\t3.90309058912\n",
      "Done 16270 batches in 2339.71 sec.    training loss:\t\t3.90299464688\n",
      "Done 16280 batches in 2341.14 sec.    training loss:\t\t3.90292515124\n",
      "Done 16290 batches in 2342.61 sec.    training loss:\t\t3.90272392495\n",
      "Done 16300 batches in 2344.01 sec.    training loss:\t\t3.9025863038\n",
      "Done 16310 batches in 2345.42 sec.    training loss:\t\t3.90243135009\n",
      "Done 16320 batches in 2346.77 sec.    training loss:\t\t3.90225701348\n",
      "Done 16330 batches in 2347.93 sec.    training loss:\t\t3.90200456544\n",
      "Done 16340 batches in 2349.39 sec.    training loss:\t\t3.9018301209\n",
      "Done 16350 batches in 2350.68 sec.    training loss:\t\t3.90162141349\n",
      "Done 16360 batches in 2352.03 sec.    training loss:\t\t3.90145783089\n",
      "Done 16370 batches in 2353.53 sec.    training loss:\t\t3.90144999831\n",
      "Done 16380 batches in 2354.70 sec.    training loss:\t\t3.90126643913\n",
      "Done 16390 batches in 2355.98 sec.    training loss:\t\t3.90105756314\n",
      "Done 16400 batches in 2357.42 sec.    training loss:\t\t3.90094138785\n",
      "Done 16410 batches in 2358.78 sec.    training loss:\t\t3.90092253284\n",
      "Done 16420 batches in 2360.45 sec.    training loss:\t\t3.90080868062\n",
      "Done 16430 batches in 2361.91 sec.    training loss:\t\t3.90067835887\n",
      "Done 16440 batches in 2363.30 sec.    training loss:\t\t3.90056791935\n",
      "Done 16450 batches in 2364.89 sec.    training loss:\t\t3.90050572462\n",
      "Done 16460 batches in 2366.31 sec.    training loss:\t\t3.90032321559\n",
      "Done 16470 batches in 2367.94 sec.    training loss:\t\t3.90027129741\n",
      "Done 16480 batches in 2369.15 sec.    training loss:\t\t3.90007302745\n",
      "Done 16490 batches in 2370.70 sec.    training loss:\t\t3.89999961328\n",
      "Done 16500 batches in 2372.22 sec.    training loss:\t\t3.89996791744\n",
      "Done 16510 batches in 2376.58 sec.    training loss:\t\tnan\n",
      "Done 16520 batches in 2377.83 sec.    training loss:\t\tnan\n",
      "Done 16530 batches in 2379.34 sec.    training loss:\t\tnan\n",
      "Done 16540 batches in 2380.82 sec.    training loss:\t\tnan\n",
      "Done 16550 batches in 2382.34 sec.    training loss:\t\tnan\n",
      "Done 16560 batches in 2383.75 sec.    training loss:\t\tnan\n",
      "Done 16570 batches in 2385.12 sec.    training loss:\t\tnan\n",
      "Done 16580 batches in 2386.59 sec.    training loss:\t\tnan\n",
      "Done 16590 batches in 2388.00 sec.    training loss:\t\tnan\n",
      "Done 16600 batches in 2389.12 sec.    training loss:\t\tnan\n",
      "Done 16610 batches in 2390.38 sec.    training loss:\t\tnan\n",
      "Done 16620 batches in 2391.63 sec.    training loss:\t\tnan\n",
      "Done 16630 batches in 2393.05 sec.    training loss:\t\tnan\n",
      "Done 16640 batches in 2394.98 sec.    training loss:\t\tnan\n",
      "Done 16650 batches in 2396.43 sec.    training loss:\t\tnan\n",
      "Done 16660 batches in 2398.22 sec.    training loss:\t\tnan\n",
      "Done 16670 batches in 2399.36 sec.    training loss:\t\tnan\n",
      "Done 16680 batches in 2401.16 sec.    training loss:\t\tnan\n",
      "Done 16690 batches in 2402.35 sec.    training loss:\t\tnan\n",
      "Done 16700 batches in 2403.57 sec.    training loss:\t\tnan\n",
      "Done 16710 batches in 2404.98 sec.    training loss:\t\tnan\n",
      "Done 16720 batches in 2406.34 sec.    training loss:\t\tnan\n",
      "Done 16730 batches in 2408.09 sec.    training loss:\t\tnan\n",
      "Done 16740 batches in 2409.55 sec.    training loss:\t\tnan\n",
      "Done 16750 batches in 2410.78 sec.    training loss:\t\tnan\n",
      "Done 16760 batches in 2412.01 sec.    training loss:\t\tnan\n",
      "Done 16770 batches in 2413.56 sec.    training loss:\t\tnan\n",
      "Done 16780 batches in 2415.10 sec.    training loss:\t\tnan\n",
      "Done 16790 batches in 2416.36 sec.    training loss:\t\tnan\n",
      "Done 16800 batches in 2417.55 sec.    training loss:\t\tnan\n",
      "Done 16810 batches in 2419.41 sec.    training loss:\t\tnan\n",
      "Done 16820 batches in 2420.55 sec.    training loss:\t\tnan\n",
      "Done 16830 batches in 2421.95 sec.    training loss:\t\tnan\n",
      "Done 16840 batches in 2423.41 sec.    training loss:\t\tnan\n",
      "Done 16850 batches in 2424.86 sec.    training loss:\t\tnan\n",
      "Done 16860 batches in 2426.08 sec.    training loss:\t\tnan\n",
      "Done 16870 batches in 2427.37 sec.    training loss:\t\tnan\n",
      "Done 16880 batches in 2428.88 sec.    training loss:\t\tnan\n",
      "Done 16890 batches in 2430.32 sec.    training loss:\t\tnan\n",
      "Done 16900 batches in 2431.75 sec.    training loss:\t\tnan\n",
      "Done 16910 batches in 2433.67 sec.    training loss:\t\tnan\n",
      "Done 16920 batches in 2435.05 sec.    training loss:\t\tnan\n",
      "Done 16930 batches in 2436.86 sec.    training loss:\t\tnan\n",
      "Done 16940 batches in 2438.31 sec.    training loss:\t\tnan\n",
      "Done 16950 batches in 2439.57 sec.    training loss:\t\tnan\n",
      "Done 16960 batches in 2441.19 sec.    training loss:\t\tnan\n",
      "Done 16970 batches in 2442.78 sec.    training loss:\t\tnan\n",
      "Done 16980 batches in 2443.97 sec.    training loss:\t\tnan\n",
      "Done 16990 batches in 2445.21 sec.    training loss:\t\tnan\n",
      "Done 17000 batches in 2446.47 sec.    training loss:\t\tnan\n",
      "Done 17010 batches in 2447.95 sec.    training loss:\t\tnan\n",
      "Done 17020 batches in 2449.66 sec.    training loss:\t\tnan\n",
      "Done 17030 batches in 2450.89 sec.    training loss:\t\tnan\n",
      "Done 17040 batches in 2452.14 sec.    training loss:\t\tnan\n",
      "Done 17050 batches in 2453.33 sec.    training loss:\t\tnan\n",
      "Done 17060 batches in 2454.56 sec.    training loss:\t\tnan\n",
      "Done 17070 batches in 2455.86 sec.    training loss:\t\tnan\n",
      "Done 17080 batches in 2458.10 sec.    training loss:\t\tnan\n",
      "Done 17090 batches in 2459.66 sec.    training loss:\t\tnan\n",
      "Done 17100 batches in 2461.21 sec.    training loss:\t\tnan\n",
      "Done 17110 batches in 2462.45 sec.    training loss:\t\tnan\n",
      "Done 17120 batches in 2463.94 sec.    training loss:\t\tnan\n",
      "Done 17130 batches in 2465.46 sec.    training loss:\t\tnan\n",
      "Done 17140 batches in 2466.68 sec.    training loss:\t\tnan\n",
      "Done 17150 batches in 2467.97 sec.    training loss:\t\tnan\n",
      "Done 17160 batches in 2469.24 sec.    training loss:\t\tnan\n",
      "Done 17170 batches in 2470.79 sec.    training loss:\t\tnan\n",
      "Done 17180 batches in 2472.35 sec.    training loss:\t\tnan\n",
      "Done 17190 batches in 2473.75 sec.    training loss:\t\tnan\n",
      "Done 17200 batches in 2475.66 sec.    training loss:\t\tnan\n",
      "Done 17210 batches in 2477.11 sec.    training loss:\t\tnan\n",
      "Done 17220 batches in 2478.58 sec.    training loss:\t\tnan\n",
      "Done 17230 batches in 2480.18 sec.    training loss:\t\tnan\n",
      "Done 17240 batches in 2481.51 sec.    training loss:\t\tnan\n",
      "Done 17250 batches in 2483.06 sec.    training loss:\t\tnan\n",
      "Done 17260 batches in 2484.30 sec.    training loss:\t\tnan\n",
      "Done 17270 batches in 2485.67 sec.    training loss:\t\tnan\n",
      "Done 17280 batches in 2486.96 sec.    training loss:\t\tnan\n",
      "Done 17290 batches in 2488.37 sec.    training loss:\t\tnan\n",
      "Done 17300 batches in 2489.89 sec.    training loss:\t\tnan\n",
      "Done 17310 batches in 2491.20 sec.    training loss:\t\tnan\n",
      "Done 17320 batches in 2492.58 sec.    training loss:\t\tnan\n",
      "Done 17330 batches in 2494.19 sec.    training loss:\t\tnan\n",
      "Done 17340 batches in 2495.74 sec.    training loss:\t\tnan\n",
      "Done 17350 batches in 2497.74 sec.    training loss:\t\tnan\n",
      "Done 17360 batches in 2499.68 sec.    training loss:\t\tnan\n",
      "Done 17370 batches in 2501.82 sec.    training loss:\t\tnan\n",
      "Done 17380 batches in 2503.06 sec.    training loss:\t\tnan\n",
      "Done 17390 batches in 2504.46 sec.    training loss:\t\tnan\n",
      "Done 17400 batches in 2505.91 sec.    training loss:\t\tnan\n",
      "Done 17410 batches in 2507.49 sec.    training loss:\t\tnan\n",
      "Done 17420 batches in 2508.79 sec.    training loss:\t\tnan\n",
      "Done 17430 batches in 2510.31 sec.    training loss:\t\tnan\n",
      "Done 17440 batches in 2511.76 sec.    training loss:\t\tnan\n",
      "Done 17450 batches in 2513.21 sec.    training loss:\t\tnan\n",
      "Done 17460 batches in 2514.50 sec.    training loss:\t\tnan\n",
      "Done 17470 batches in 2515.72 sec.    training loss:\t\tnan\n",
      "Done 17480 batches in 2516.70 sec.    training loss:\t\tnan\n",
      "Done 17490 batches in 2517.73 sec.    training loss:\t\tnan\n",
      "Done 17500 batches in 2519.15 sec.    training loss:\t\tnan\n",
      "Done 17510 batches in 2520.91 sec.    training loss:\t\tnan\n",
      "Done 17520 batches in 2522.70 sec.    training loss:\t\tnan\n",
      "Done 17530 batches in 2523.91 sec.    training loss:\t\tnan\n",
      "Done 17540 batches in 2525.18 sec.    training loss:\t\tnan\n",
      "Done 17550 batches in 2526.49 sec.    training loss:\t\tnan\n",
      "Done 17560 batches in 2528.17 sec.    training loss:\t\tnan\n",
      "Done 17570 batches in 2529.64 sec.    training loss:\t\tnan\n",
      "Done 17580 batches in 2531.27 sec.    training loss:\t\tnan\n",
      "Done 17590 batches in 2532.52 sec.    training loss:\t\tnan\n",
      "Done 17600 batches in 2533.94 sec.    training loss:\t\tnan\n",
      "Done 17610 batches in 2535.09 sec.    training loss:\t\tnan\n",
      "Done 17620 batches in 2536.80 sec.    training loss:\t\tnan\n",
      "Done 17630 batches in 2538.37 sec.    training loss:\t\tnan\n",
      "Done 17640 batches in 2539.71 sec.    training loss:\t\tnan\n",
      "Done 17650 batches in 2541.37 sec.    training loss:\t\tnan\n",
      "Done 17660 batches in 2542.84 sec.    training loss:\t\tnan\n",
      "Done 17670 batches in 2544.30 sec.    training loss:\t\tnan\n",
      "Done 17680 batches in 2545.75 sec.    training loss:\t\tnan\n",
      "Done 17690 batches in 2547.02 sec.    training loss:\t\tnan\n",
      "Done 17700 batches in 2548.30 sec.    training loss:\t\tnan\n",
      "Done 17710 batches in 2549.57 sec.    training loss:\t\tnan\n",
      "Done 17720 batches in 2551.28 sec.    training loss:\t\tnan\n",
      "Done 17730 batches in 2552.57 sec.    training loss:\t\tnan\n",
      "Done 17740 batches in 2554.31 sec.    training loss:\t\tnan\n",
      "Done 17750 batches in 2555.46 sec.    training loss:\t\tnan\n",
      "Done 17760 batches in 2556.97 sec.    training loss:\t\tnan\n",
      "Done 17770 batches in 2558.13 sec.    training loss:\t\tnan\n",
      "Done 17780 batches in 2559.53 sec.    training loss:\t\tnan\n",
      "Done 17790 batches in 2560.68 sec.    training loss:\t\tnan\n",
      "Done 17800 batches in 2562.00 sec.    training loss:\t\tnan\n",
      "Done 17810 batches in 2563.43 sec.    training loss:\t\tnan\n",
      "Done 17820 batches in 2564.94 sec.    training loss:\t\tnan\n",
      "Done 17830 batches in 2566.34 sec.    training loss:\t\tnan\n",
      "Done 17840 batches in 2567.64 sec.    training loss:\t\tnan\n",
      "Done 17850 batches in 2569.19 sec.    training loss:\t\tnan\n",
      "Done 17860 batches in 2570.72 sec.    training loss:\t\tnan\n",
      "Done 17870 batches in 2572.23 sec.    training loss:\t\tnan\n",
      "Done 17880 batches in 2573.53 sec.    training loss:\t\tnan\n",
      "Done 17890 batches in 2574.89 sec.    training loss:\t\tnan\n",
      "Done 17900 batches in 2576.74 sec.    training loss:\t\tnan\n",
      "Done 17910 batches in 2578.30 sec.    training loss:\t\tnan\n",
      "Done 17920 batches in 2579.64 sec.    training loss:\t\tnan\n",
      "Done 17930 batches in 2581.33 sec.    training loss:\t\tnan\n",
      "Done 17940 batches in 2582.60 sec.    training loss:\t\tnan\n",
      "Done 17950 batches in 2584.13 sec.    training loss:\t\tnan\n",
      "Done 17960 batches in 2585.45 sec.    training loss:\t\tnan\n",
      "Done 17970 batches in 2586.90 sec.    training loss:\t\tnan\n",
      "Done 17980 batches in 2588.06 sec.    training loss:\t\tnan\n",
      "Done 17990 batches in 2589.65 sec.    training loss:\t\tnan\n",
      "Done 18000 batches in 2591.02 sec.    training loss:\t\tnan\n",
      "Done 18010 batches in 2592.54 sec.    training loss:\t\tnan\n",
      "Done 18020 batches in 2593.97 sec.    training loss:\t\tnan\n",
      "Done 18030 batches in 2595.33 sec.    training loss:\t\tnan\n",
      "Done 18040 batches in 2596.70 sec.    training loss:\t\tnan\n",
      "Done 18050 batches in 2598.03 sec.    training loss:\t\tnan\n",
      "Done 18060 batches in 2599.36 sec.    training loss:\t\tnan\n",
      "Done 18070 batches in 2600.81 sec.    training loss:\t\tnan\n",
      "Done 18080 batches in 2602.36 sec.    training loss:\t\tnan\n",
      "Done 18090 batches in 2603.70 sec.    training loss:\t\tnan\n",
      "Done 18100 batches in 2605.05 sec.    training loss:\t\tnan\n",
      "Done 18110 batches in 2606.45 sec.    training loss:\t\tnan\n",
      "Done 18120 batches in 2607.76 sec.    training loss:\t\tnan\n",
      "Done 18130 batches in 2609.37 sec.    training loss:\t\tnan\n",
      "Done 18140 batches in 2610.72 sec.    training loss:\t\tnan\n",
      "Done 18150 batches in 2612.26 sec.    training loss:\t\tnan\n",
      "Done 18160 batches in 2613.64 sec.    training loss:\t\tnan\n",
      "Done 18170 batches in 2615.05 sec.    training loss:\t\tnan\n",
      "Done 18180 batches in 2616.71 sec.    training loss:\t\tnan\n",
      "Done 18190 batches in 2618.99 sec.    training loss:\t\tnan\n",
      "Done 18200 batches in 2620.30 sec.    training loss:\t\tnan\n",
      "Done 18210 batches in 2621.73 sec.    training loss:\t\tnan\n",
      "Done 18220 batches in 2623.08 sec.    training loss:\t\tnan\n",
      "Done 18230 batches in 2624.25 sec.    training loss:\t\tnan\n",
      "Done 18240 batches in 2625.93 sec.    training loss:\t\tnan\n",
      "Done 18250 batches in 2627.46 sec.    training loss:\t\tnan\n",
      "Done 18260 batches in 2628.82 sec.    training loss:\t\tnan\n",
      "Done 18270 batches in 2630.21 sec.    training loss:\t\tnan\n",
      "Done 18280 batches in 2631.86 sec.    training loss:\t\tnan\n",
      "Done 18290 batches in 2633.33 sec.    training loss:\t\tnan\n",
      "Done 18300 batches in 2635.02 sec.    training loss:\t\tnan\n",
      "Done 18310 batches in 2636.41 sec.    training loss:\t\tnan\n",
      "Done 18320 batches in 2637.96 sec.    training loss:\t\tnan\n",
      "Done 18330 batches in 2639.70 sec.    training loss:\t\tnan\n",
      "Done 18340 batches in 2641.25 sec.    training loss:\t\tnan\n",
      "Done 18350 batches in 2642.69 sec.    training loss:\t\tnan\n",
      "Done 18360 batches in 2644.17 sec.    training loss:\t\tnan\n",
      "Done 18370 batches in 2645.80 sec.    training loss:\t\tnan\n",
      "Done 18380 batches in 2647.62 sec.    training loss:\t\tnan\n",
      "Done 18390 batches in 2649.22 sec.    training loss:\t\tnan\n",
      "Done 18400 batches in 2650.77 sec.    training loss:\t\tnan\n",
      "Done 18410 batches in 2651.88 sec.    training loss:\t\tnan\n",
      "Done 18420 batches in 2653.54 sec.    training loss:\t\tnan\n",
      "Done 18430 batches in 2654.94 sec.    training loss:\t\tnan\n",
      "Done 18440 batches in 2656.76 sec.    training loss:\t\tnan\n",
      "Done 18450 batches in 2657.92 sec.    training loss:\t\tnan\n",
      "Done 18460 batches in 2659.34 sec.    training loss:\t\tnan\n",
      "Done 18470 batches in 2660.64 sec.    training loss:\t\tnan\n",
      "Done 18480 batches in 2662.18 sec.    training loss:\t\tnan\n",
      "Done 18490 batches in 2663.56 sec.    training loss:\t\tnan\n",
      "Done 18500 batches in 2665.44 sec.    training loss:\t\tnan\n",
      "Done 18510 batches in 2666.89 sec.    training loss:\t\tnan\n",
      "Done 18520 batches in 2668.54 sec.    training loss:\t\tnan\n",
      "Done 18530 batches in 2669.96 sec.    training loss:\t\tnan\n",
      "Done 18540 batches in 2671.48 sec.    training loss:\t\tnan\n",
      "Done 18550 batches in 2672.98 sec.    training loss:\t\tnan\n",
      "Done 18560 batches in 2674.57 sec.    training loss:\t\tnan\n",
      "Done 18570 batches in 2675.68 sec.    training loss:\t\tnan\n",
      "Done 18580 batches in 2677.22 sec.    training loss:\t\tnan\n",
      "Done 18590 batches in 2678.77 sec.    training loss:\t\tnan\n",
      "Done 18600 batches in 2680.24 sec.    training loss:\t\tnan\n",
      "Done 18610 batches in 2681.39 sec.    training loss:\t\tnan\n",
      "Done 18620 batches in 2682.62 sec.    training loss:\t\tnan\n",
      "Done 18630 batches in 2684.15 sec.    training loss:\t\tnan\n",
      "Done 18640 batches in 2685.44 sec.    training loss:\t\tnan\n",
      "Done 18650 batches in 2686.59 sec.    training loss:\t\tnan\n",
      "Done 18660 batches in 2687.83 sec.    training loss:\t\tnan\n",
      "Done 18670 batches in 2688.90 sec.    training loss:\t\tnan\n",
      "Done 18680 batches in 2690.17 sec.    training loss:\t\tnan\n",
      "Done 18690 batches in 2691.71 sec.    training loss:\t\tnan\n",
      "Done 18700 batches in 2692.98 sec.    training loss:\t\tnan\n",
      "Done 18710 batches in 2694.16 sec.    training loss:\t\tnan\n",
      "Done 18720 batches in 2695.18 sec.    training loss:\t\tnan\n",
      "Done 18730 batches in 2696.80 sec.    training loss:\t\tnan\n",
      "Done 18740 batches in 2698.45 sec.    training loss:\t\tnan\n",
      "Done 18750 batches in 2699.71 sec.    training loss:\t\tnan\n",
      "Done 18760 batches in 2701.07 sec.    training loss:\t\tnan\n",
      "Done 18770 batches in 2702.81 sec.    training loss:\t\tnan\n",
      "Done 18780 batches in 2705.06 sec.    training loss:\t\tnan\n",
      "Done 18790 batches in 2706.33 sec.    training loss:\t\tnan\n",
      "Done 18800 batches in 2707.64 sec.    training loss:\t\tnan\n",
      "Done 18810 batches in 2708.93 sec.    training loss:\t\tnan\n",
      "Done 18820 batches in 2710.12 sec.    training loss:\t\tnan\n",
      "Done 18830 batches in 2711.59 sec.    training loss:\t\tnan\n",
      "Done 18840 batches in 2712.98 sec.    training loss:\t\tnan\n",
      "Done 18850 batches in 2714.19 sec.    training loss:\t\tnan\n",
      "Done 18860 batches in 2715.95 sec.    training loss:\t\tnan\n",
      "Done 18870 batches in 2717.12 sec.    training loss:\t\tnan\n",
      "Done 18880 batches in 2718.34 sec.    training loss:\t\tnan\n",
      "Done 18890 batches in 2719.86 sec.    training loss:\t\tnan\n",
      "Done 18900 batches in 2721.23 sec.    training loss:\t\tnan\n",
      "Done 18910 batches in 2722.72 sec.    training loss:\t\tnan\n",
      "Done 18920 batches in 2724.28 sec.    training loss:\t\tnan\n",
      "Done 18930 batches in 2725.65 sec.    training loss:\t\tnan\n",
      "Done 18940 batches in 2726.89 sec.    training loss:\t\tnan\n",
      "Done 18950 batches in 2728.25 sec.    training loss:\t\tnan\n",
      "Done 18960 batches in 2729.88 sec.    training loss:\t\tnan\n",
      "Done 18970 batches in 2730.88 sec.    training loss:\t\tnan\n",
      "Done 18980 batches in 2732.65 sec.    training loss:\t\tnan\n",
      "Done 18990 batches in 2734.27 sec.    training loss:\t\tnan\n",
      "Done 19000 batches in 2735.79 sec.    training loss:\t\tnan\n",
      "Done 19010 batches in 2737.29 sec.    training loss:\t\tnan\n",
      "Done 19020 batches in 2738.94 sec.    training loss:\t\tnan\n",
      "Done 19030 batches in 2740.24 sec.    training loss:\t\tnan\n",
      "Done 19040 batches in 2741.67 sec.    training loss:\t\tnan\n",
      "Done 19050 batches in 2743.09 sec.    training loss:\t\tnan\n",
      "Done 19060 batches in 2744.46 sec.    training loss:\t\tnan\n",
      "Done 19070 batches in 2746.09 sec.    training loss:\t\tnan\n",
      "Done 19080 batches in 2747.25 sec.    training loss:\t\tnan\n",
      "Done 19090 batches in 2748.60 sec.    training loss:\t\tnan\n",
      "Done 19100 batches in 2750.21 sec.    training loss:\t\tnan\n",
      "Done 19110 batches in 2751.78 sec.    training loss:\t\tnan\n",
      "Done 19120 batches in 2753.14 sec.    training loss:\t\tnan\n",
      "Done 19130 batches in 2754.80 sec.    training loss:\t\tnan\n",
      "Done 19140 batches in 2756.20 sec.    training loss:\t\tnan\n",
      "Done 19150 batches in 2757.45 sec.    training loss:\t\tnan\n",
      "Done 19160 batches in 2758.67 sec.    training loss:\t\tnan\n",
      "Done 19170 batches in 2760.16 sec.    training loss:\t\tnan\n",
      "Done 19180 batches in 2762.16 sec.    training loss:\t\tnan\n",
      "Done 19190 batches in 2763.43 sec.    training loss:\t\tnan\n",
      "Done 19200 batches in 2764.82 sec.    training loss:\t\tnan\n",
      "Done 19210 batches in 2766.27 sec.    training loss:\t\tnan\n",
      "Done 19220 batches in 2767.36 sec.    training loss:\t\tnan\n",
      "Done 19230 batches in 2768.83 sec.    training loss:\t\tnan\n",
      "Done 19240 batches in 2770.11 sec.    training loss:\t\tnan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-aa6d5ffef9f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mbatch_err\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mtrain_err\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbatch_err\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/i258346/.local/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/i258346/.local/lib/python2.7/site-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n)\u001b[0m\n\u001b[0;32m    910\u001b[0m             \u001b[1;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 912\u001b[1;33m                 \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    913\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m                     \u001b[0mcompute_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/i258346/.local/lib/python2.7/site-packages/theano/tensor/subtensor.pyc\u001b[0m in \u001b[0;36mperform\u001b[1;34m(self, node, inputs, out_)\u001b[0m\n\u001b[0;32m   2286\u001b[0m             \u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2287\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0minplace_increment\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2288\u001b[1;33m             \u001b[0minplace_increment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2289\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mallow_legacy_perform\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2290\u001b[0m             \u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training, taken from mnist.py in lasagne examples\n",
    "\n",
    "num_epochs = 1\n",
    "batch_size = 5\n",
    "val_batch_size = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch in iterate_minibatches(train, batch_size):\n",
    "        \n",
    "        inputs, targets, mask = batch\n",
    "        batch_err = train_fn(inputs, targets, mask)\n",
    "\n",
    "        train_err += batch_err\n",
    "        train_batches += 1\n",
    "        \n",
    "        if not train_batches % 10:\n",
    "            print \"Done {} batches in {:.2f} sec.    training loss:\\t\\t{}\".format(\n",
    "                train_batches, time.time() - start_time, train_err / train_batches)\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_err = 0\n",
    "    val_batches = 0\n",
    "    start_time2 = time.time()\n",
    "    \n",
    "    clone_param_values(net_from=net, net_to=test_net)\n",
    "    \n",
    "    for batch in iterate_minibatches(valid, val_batch_size):\n",
    "        inputs, targets, mask = batch\n",
    "        \n",
    "        err = val_fn(inputs, targets, mask)\n",
    "        val_err += err\n",
    "        val_batches += 1\n",
    "        if not val_batches % 100:\n",
    "            print \"Done {} batches in {:.2f} sec.\".format(\n",
    "                val_batches, time.time() - start_time2)\n",
    "\n",
    "    # Then we print the results for this epoch:\n",
    "    print \"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time)\n",
    "    print \"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches)\n",
    "    print \"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches)\n",
    "    #print \"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "    #    val_acc / val_batches * 100)\n",
    "    \n",
    "# np.savez('test_1ep_params_20_sampled_unique_bs50.npz', *L.layers.get_all_param_values(net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savez('test_1ep_params_20_sampled_unique_bs50.npz', *L.layers.get_all_param_values(net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with np.load('test_1ep_params.npz') as f:\n",
    "    param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "    L.layers.set_all_param_values(test_net, param_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savez('fsoft_trained_singleLSTM.npz', *L.layers.get_all_param_values(net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_var = T.imatrix('inputs')\n",
    "gen_net = build_hsoft_rnnlm(input_var, None, None, voc_size, emb_size, rec_size)\n",
    "probs = L.layers.get_output(gen_net)[:,-1,:]\n",
    "get_probs = theano.function([input_var], probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_var = T.imatrix('inputs')\n",
    "gen_net = build_sampledsoft_rnnlm(input_var, None, -1, voc_size, emb_size, rec_size)\n",
    "probs = L.layers.get_output(gen_net)[:,-1,:]\n",
    "get_probs = theano.function([input_var], probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_var = T.imatrix('inputs')\n",
    "gen_net = build_simple_rnnlm(input_var, None, voc_size, emb_size, rec_size)\n",
    "probs = L.layers.get_output(gen_net)[:,-1,:]\n",
    "get_probs = theano.function([input_var], probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with np.load('test_1ep_params_1000_sampled_unique_bs40.npz') as f:\n",
    "    param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "    L.layers.set_all_param_values(gen_net, param_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnd_next_word(probs, size=1):\n",
    "    return np.random.choice(np.arange(probs.shape[0], dtype=np.int32), size=size, p=probs)\n",
    "\n",
    "def beam_search(get_probs_fun, beam=10, init_seq='', mode='rr'):\n",
    "    utt = [1] + map(lambda w: w_to_idx.get(w, w_to_idx['<unk>']), init_seq.split())\n",
    "    utt = np.asarray(utt, dtype=np.int32)[np.newaxis]\n",
    "    \n",
    "    if mode[0] == 's':\n",
    "        words = get_probs_fun(utt)[0].argpartition(-beam)[-beam:].astype(np.int32)\n",
    "    elif mode[0] == 'r':\n",
    "        words = rnd_next_word(get_probs_fun(utt)[0], beam)\n",
    "    \n",
    "    candidates = utt.repeat(beam, axis=0)\n",
    "    candidates = np.hstack([candidates, words[np.newaxis].T])\n",
    "    scores = np.zeros(beam)\n",
    "    \n",
    "#     print candidates\n",
    "    \n",
    "    while 0 not in candidates[:,-1] and candidates.shape[1] < 100:\n",
    "        \n",
    "        if mode[1] == 's':\n",
    "            log_probs = np.log(get_probs_fun(candidates))\n",
    "            tot_scores = log_probs + scores[np.newaxis].T\n",
    "\n",
    "            idx = tot_scores.ravel().argpartition(-beam)[-beam:]\n",
    "            i,j = idx / tot_scores.shape[1], (idx % tot_scores.shape[1]).astype(np.int32)\n",
    "\n",
    "            scores = tot_scores[i,j]\n",
    "\n",
    "            candidates = np.hstack([candidates[i], j[np.newaxis].T])\n",
    "            \n",
    "        elif mode[1] == 'r':\n",
    "            probs = get_probs_fun(candidates)\n",
    "            words = []\n",
    "            for k in xrange(beam):\n",
    "                words.append(rnd_next_word(probs[k], beam)) # this doesn't have to be exactly 'beam'\n",
    "            words = np.array(words)\n",
    "            idx = np.indices((beam, words.shape[1]))[0]\n",
    "            tot_scores = scores[np.newaxis].T + np.log(probs)[idx, words]\n",
    "                \n",
    "            idx = tot_scores.ravel().argpartition(-beam)[-beam:]\n",
    "            i,j = idx / tot_scores.shape[1], (idx % tot_scores.shape[1])\n",
    "\n",
    "            scores = tot_scores[i,j]\n",
    "\n",
    "            candidates = np.hstack([candidates[i], words[i,j][np.newaxis].T])\n",
    "            \n",
    "#     print candidates[:,:10]\n",
    "#     print scores[:10]\n",
    "        \n",
    "    cands = candidates[candidates[:,-1] == 0]\n",
    "    if cands.size > 0:\n",
    "        return candidates[candidates[:,-1] == 0][0]\n",
    "    return candidates[scores.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"well , i ' m not going to be . i ' m not going to be . i ' m not going to be here .\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utt = beam_search(get_probs, init_seq='', beam=10, mode='rr')\n",
    "\n",
    "text = map(lambda i: idx_to_w[i], list(utt))\n",
    "' '.join(text[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'and then far , you know ? <unk> , put that one little hot . i mean . then not !'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rnd_next_word(probs):\n",
    "    return np.random.choice(np.arange(len(probs[0])), p=probs[0])\n",
    "\n",
    "init_seq = ''\n",
    "utt = [1] + map(lambda w: w_to_idx.get(w, w_to_idx['<unk>']), init_seq.split())\n",
    "utt = np.asarray(utt, dtype=np.int32)[np.newaxis]\n",
    "\n",
    "i = 0\n",
    "while idx_to_w[utt[0,-1]] != '<utt_end>' and i < 50:\n",
    "    word_probs = get_probs(utt)\n",
    "    next_idx = rnd_next_word(word_probs)\n",
    "    utt = np.append(utt, next_idx)[np.newaxis].astype(np.int32)\n",
    "    i += 1\n",
    "    \n",
    "text = map(lambda i: idx_to_w[i], list(utt[0]))\n",
    "' '.join(text[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

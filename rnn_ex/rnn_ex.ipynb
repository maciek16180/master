{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 780 (CNMeM is enabled with initial size: 30.0% of memory, cuDNN 4007)\n",
      "/home/i258346/.local/lib/python2.7/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import lasagne as L\n",
    "\n",
    "sys.path.insert(0, '../HSoftmaxLayerLasagne/')\n",
    "\n",
    "from HSoftmaxLayer import HierarchicalSoftmaxDenseLayer\n",
    "from SampledSoftmaxLayer import SampledSoftmaxDenseLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remember, now the pad value is the same as the <utt_end> token\n",
    "\n",
    "pad_value = -1 # <utt_end>'s vector is the last one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mt_path = \"/pio/data/data/mtriples/\"\n",
    "\n",
    "def load_mt(path=mt_path):\n",
    "    tr = np.load(mt_path + 'Training.triples.pkl')\n",
    "    vl = np.load(mt_path + 'Validation.triples.pkl')\n",
    "    ts = np.load(mt_path + 'Test.triples.pkl')\n",
    "    \n",
    "    return tr, vl, ts\n",
    "\n",
    "train, valid, test = load_mt()\n",
    "\n",
    "train = [utt for utt in train if len(utt) < 200]\n",
    "valid = [utt for utt in valid if len(utt) < 200]\n",
    "test  = [utt for utt in test  if len(utt) < 200]\n",
    "\n",
    "\n",
    "def get_mt_voc(mt_path=mt_path, train_len=len(train)):\n",
    "    word_list = np.load(mt_path + 'Training.dict.pkl')\n",
    "    word_list.sort(key=lambda x: x[1])\n",
    "    freqs = np.array(map(lambda x: x[2], word_list) + [train_len])\n",
    "    total_count = float(sum(freqs))\n",
    "    \n",
    "    words = map(lambda x: x[:2], word_list)\n",
    "    \n",
    "    w_to_idx = dict(words)\n",
    "    w_to_idx['<utt_end>'] = pad_value\n",
    "    idx_to_w = {v : k for (k,v) in w_to_idx.items()}\n",
    "    \n",
    "    return idx_to_w, w_to_idx, len(w_to_idx), freqs / total_count\n",
    "\n",
    "idx_to_w, w_to_idx, voc_size, freqs = get_mt_voc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec_embs, word2vec_embs_mask = np.load(mt_path + 'Word2Vec_WordEmb.pkl')\n",
    "word2vec_embs = np.vstack([word2vec_embs, L.init.GlorotUniform()((1,300))]).astype(np.float32)\n",
    "word2vec_embs_mask = np.vstack([word2vec_embs_mask, np.ones((1,300))])\n",
    "\n",
    "w2v_train_mask = np.where(word2vec_embs_mask[:,0] == 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Similar to Lasagne mnist.py example, added input mask and different sequence lengths\n",
    "\n",
    "def iterate_minibatches(inputs, batchsize, pad=pad_value):\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):        \n",
    "        excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        inp = inputs[excerpt]\n",
    "        \n",
    "        inp_max_len = len(max(inp, key=len))\n",
    "        inp = map(lambda l: l + [pad]*(inp_max_len-len(l)), inp)\n",
    "        inp = np.asarray(inp, dtype=np.int32)\n",
    "        tar = np.hstack([inp[:,1:], np.zeros((batchsize,1), dtype=np.int32) + pad])\n",
    "        def not_pad(x):\n",
    "            return x != pad\n",
    "        v_not_pad = np.vectorize(not_pad, otypes=[np.float32])\n",
    "        mask = v_not_pad(inp) # there is no separate value for the end of an utterance right now, just pad\n",
    "        \n",
    "        yield inp, tar, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_simple_rnnlm(input_var, mask_input_var, voc_size, emb_size, rec_size, \n",
    "                       emb_init=None, train_emb=True):\n",
    "    l_in = L.layers.InputLayer(shape=(None, None), input_var=input_var)  \n",
    "    batch_size, seq_len = l_in.input_var.shape\n",
    "    \n",
    "    l_mask = None\n",
    "    if mask_input_var is not None:\n",
    "        print 'setting up input mask...'\n",
    "        l_mask = L.layers.InputLayer(shape=(batch_size, seq_len), input_var=mask_input_var)\n",
    "    \n",
    "    if emb_init is None:\n",
    "        l_emb = L.layers.EmbeddingLayer(l_in,\n",
    "                                        input_size=voc_size, # not voc_size+1, because pad_value = <utt_end>\n",
    "                                        output_size=emb_size)\n",
    "    else:\n",
    "        l_emb = L.layers.EmbeddingLayer(l_in,\n",
    "                                        input_size=voc_size, \n",
    "                                        output_size=emb_size,\n",
    "                                        W=emb_init)\n",
    "        if not train_emb:\n",
    "            l_emb.params[l_emb.W].remove('trainable')\n",
    "    \n",
    "    l_lstm1 = L.layers.LSTMLayer(l_emb,\n",
    "                                 num_units=rec_size,\n",
    "                                 nonlinearity=L.nonlinearities.tanh,\n",
    "                                 grad_clipping=100,\n",
    "                                 mask_input=l_mask)\n",
    "    \n",
    "    l_lstm2 = L.layers.LSTMLayer(l_lstm1,\n",
    "                                 num_units=rec_size,\n",
    "                                 nonlinearity=L.nonlinearities.tanh,\n",
    "                                 grad_clipping=100,\n",
    "                                 mask_input=l_mask)\n",
    "    \n",
    "    l_resh = L.layers.ReshapeLayer(l_lstm2, shape=(-1, rec_size))\n",
    "    \n",
    "    l_soft = L.layers.DenseLayer(l_resh,\n",
    "                                 num_units=voc_size,\n",
    "                                 nonlinearity=L.nonlinearities.softmax)\n",
    "    \n",
    "    l_out = L.layers.ReshapeLayer(l_soft, shape=(batch_size, seq_len, voc_size))\n",
    "    \n",
    "    return l_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_hsoft_rnnlm(input_var, target_var, mask_input_var, voc_size, emb_size, rec_size):\n",
    "    l_in = L.layers.InputLayer(shape=(None, None), input_var=input_var)    \n",
    "    batch_size, seq_len = l_in.input_var.shape\n",
    "    l_mask = None\n",
    "    if mask_input_var is not None:\n",
    "        print 'setting up input mask...'\n",
    "        l_mask = L.layers.InputLayer(shape=(batch_size, seq_len), input_var=mask_input_var)\n",
    "    \n",
    "    l_emb = L.layers.EmbeddingLayer(l_in,\n",
    "                                    input_size=voc_size+1, \n",
    "                                    output_size=emb_size)\n",
    "    \n",
    "    l_lstm1 = L.layers.LSTMLayer(l_emb,\n",
    "                                 num_units=rec_size,\n",
    "                                 nonlinearity=L.nonlinearities.tanh,\n",
    "                                 grad_clipping=100,\n",
    "                                 mask_input=l_mask)    \n",
    "    \n",
    "#     l_lstm2 = L.layers.LSTMLayer(l_lstm1,\n",
    "#                                  num_units=rec_size,\n",
    "#                                  nonlinearity=L.nonlinearities.tanh,\n",
    "#                                  grad_clipping=100,\n",
    "#                                  mask_input=l_mask)\n",
    "    \n",
    "    l_resh = L.layers.ReshapeLayer(l_lstm1, shape=(-1, rec_size))\n",
    "    \n",
    "    # hierarchical softmax\n",
    "    \n",
    "    l_resh_tar = None\n",
    "    if target_var is not None:\n",
    "        print 'setting up targets for hsoftmax...'\n",
    "        l_tar = L.layers.InputLayer(shape=(None, None), input_var=target_var)\n",
    "        l_resh_tar = L.layers.ReshapeLayer(l_tar, shape=(-1, 1))\n",
    "        \n",
    "    l_hsoft = HierarchicalSoftmaxDenseLayer(l_resh,\n",
    "                                            num_units=voc_size,\n",
    "                                            target=l_resh_tar)\n",
    "    l_out = None\n",
    "    if target_var is not None:\n",
    "        l_out = L.layers.ReshapeLayer(l_hsoft, shape=(batch_size, seq_len))\n",
    "    else:\n",
    "        l_out = L.layers.ReshapeLayer(l_hsoft, shape=(batch_size, seq_len, voc_size))\n",
    "    \n",
    "    return l_out\n",
    "\n",
    "# 1 epoch on gpu with hsoft took about 700s, batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_sampledsoft_rnnlm(input_var, mask_input_var, num_sampled, voc_size, \n",
    "                            emb_size, rec_size, target_var=None, emb_init=None, \n",
    "                            train_emb=True, ssoft_probs=None, sample_unique=False):\n",
    "    l_in = L.layers.InputLayer(shape=(None, None), input_var=input_var)    \n",
    "    batch_size, seq_len = l_in.input_var.shape\n",
    "    l_mask = None\n",
    "    if mask_input_var != None:\n",
    "        print 'setting up input mask...'\n",
    "        l_mask = L.layers.InputLayer(shape=(batch_size, seq_len), input_var=mask_input_var)\n",
    "    \n",
    "    if emb_init is None:\n",
    "        l_emb = L.layers.EmbeddingLayer(l_in,\n",
    "                                        input_size=voc_size, # not voc_size+1, because pad_value = <utt_end>\n",
    "                                        output_size=emb_size)\n",
    "    else:\n",
    "        l_emb = L.layers.EmbeddingLayer(l_in,\n",
    "                                        input_size=voc_size, \n",
    "                                        output_size=emb_size,\n",
    "                                        W=emb_init)\n",
    "        if not train_emb:\n",
    "            l_emb.params[l_emb.W].remove('trainable')\n",
    "    \n",
    "    l_lstm1 = L.layers.LSTMLayer(l_emb,\n",
    "                                 num_units=rec_size,\n",
    "                                 nonlinearity=L.nonlinearities.tanh,\n",
    "                                 grad_clipping=100,\n",
    "                                 mask_input=l_mask)\n",
    "    \n",
    "    l_lstm2 = L.layers.LSTMLayer(l_lstm1,\n",
    "                                 num_units=rec_size,\n",
    "                                 nonlinearity=L.nonlinearities.tanh,\n",
    "                                 grad_clipping=100,\n",
    "                                 mask_input=l_mask)\n",
    "      \n",
    "    l_resh = L.layers.ReshapeLayer(l_lstm2, shape=(-1, rec_size))\n",
    "    \n",
    "    if target_var is not None:\n",
    "        print 'setting up targets for sampled softmax...'\n",
    "        target_var = target_var.ravel()\n",
    "    \n",
    "    l_ssoft = SampledSoftmaxDenseLayer(l_resh, num_sampled, voc_size, \n",
    "                                       targets=target_var,\n",
    "                                       probs=ssoft_probs,\n",
    "                                       sample_unique=sample_unique)\n",
    "    \n",
    "    if target_var is not None:\n",
    "        l_out = L.layers.ReshapeLayer(l_ssoft, shape=(batch_size, seq_len))\n",
    "    else:\n",
    "        l_out = L.layers.ReshapeLayer(l_ssoft, shape=(batch_size, seq_len, voc_size))\n",
    "    \n",
    "    return l_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emb_size = 300\n",
    "rec_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clone_param_values(net_from, net_to):\n",
    "    L.layers.set_all_param_values(net_to, L.layers.get_all_param_values(net_from))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up input mask...\n"
     ]
    }
   ],
   "source": [
    "# full softmax test\n",
    "\n",
    "input_var = T.imatrix('inputs')\n",
    "targets = T.imatrix('targets') # these will be inputs shifted by 1\n",
    "mask_input_var = T.matrix('input_mask')\n",
    "\n",
    "net = build_simple_rnnlm(input_var, mask_input_var, voc_size, emb_size, rec_size, \n",
    "                         emb_init=word2vec_embs, train_emb=False)\n",
    "out = L.layers.get_output(net)\n",
    "\n",
    "mask_idx = mask_input_var.nonzero()\n",
    "loss = L.objectives.categorical_crossentropy(out[mask_idx], targets[mask_idx])\n",
    "loss = loss.mean() # mean batch loss\n",
    "\n",
    "params = L.layers.get_all_params(net, trainable=True)\n",
    "updates = L.updates.adagrad(loss, params, learning_rate=.01)\n",
    "\n",
    "# update modification to train only randomly initialized embeddings\n",
    "# updates[params[0]] = T.set_subtensor(params[0][w2v_train_mask], updates[params[0]][w2v_train_mask])\n",
    "\n",
    "train_fn = theano.function([input_var, targets, mask_input_var], loss, updates=updates)\n",
    "\n",
    "### for validation\n",
    "\n",
    "test_out = L.layers.get_output(net, deterministic=True)\n",
    "test_loss = L.objectives.categorical_crossentropy(test_out[mask_idx], targets[mask_idx])\n",
    "test_loss = test_loss.mean()\n",
    "# test_acc = T.mean(T.eq(T.argmax(test_out, axis=1), targets), dtype=theano.config.floatX)\n",
    "\n",
    "val_fn = theano.function([input_var, targets, mask_input_var], test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up input mask...\n",
      "setting up targets for sampled softmax...\n"
     ]
    }
   ],
   "source": [
    "# sampled softmax test (with targets!)\n",
    "\n",
    "num_sampled = 200\n",
    "\n",
    "input_var = T.imatrix('inputs')\n",
    "targets = T.imatrix('targets') # these will be inputs shifted by 1\n",
    "mask_input_var = T.matrix('input_mask')\n",
    "\n",
    "net = build_sampledsoft_rnnlm(input_var, mask_input_var, num_sampled, voc_size, \n",
    "                              emb_size, rec_size, target_var=targets, emb_init=word2vec_embs,\n",
    "                              ssoft_probs=freqs)\n",
    "out = L.layers.get_output(net)\n",
    "\n",
    "mask_idx = mask_input_var.nonzero()\n",
    "loss = -T.sum(T.log(out[mask_idx])) / T.sum(mask_input_var)\n",
    "\n",
    "params = L.layers.get_all_params(net, trainable=True)\n",
    "updates = L.updates.adagrad(loss, params, learning_rate=.01)\n",
    "# updates = L.updates.rmsprop(loss, params, learning_rate=.001, rho=.9, epsilon=1e-06)\n",
    "\n",
    "train_fn = theano.function([input_var, targets, mask_input_var], loss, updates=updates)\n",
    "\n",
    "### for validation\n",
    "\n",
    "test_out = L.layers.get_output(net, deterministic=True, use_all_words=True)\n",
    "test_loss = -T.sum(T.log(test_out[mask_idx])) / T.sum(mask_input_var)\n",
    "\n",
    "val_fn = theano.function([input_var, targets, mask_input_var], test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up input mask...\n",
      "setting up targets for hsoftmax...\n",
      "setting up input mask...\n",
      "setting up targets for hsoftmax...\n"
     ]
    }
   ],
   "source": [
    "# hierarchical softmax test\n",
    "\n",
    "input_var = T.imatrix('inputs')\n",
    "targets = T.imatrix('targets') # these will be inputs shifted by 1\n",
    "mask_input_var = T.matrix('input_mask')\n",
    "\n",
    "net = build_hsoft_rnnlm(input_var, targets, mask_input_var, voc_size, emb_size, rec_size)\n",
    "out = L.layers.get_output(net)\n",
    "\n",
    "mask_idx = mask_input_var.nonzero()\n",
    "loss = -T.sum(T.log(out[mask_idx])) / T.sum(mask_input_var)\n",
    "\n",
    "params = L.layers.get_all_params(net, trainable=True)\n",
    "#updates = L.updates.rmsprop(loss, params, learning_rate=.001, rho=.9, epsilon=1e-06)\n",
    "updates = L.updates.adagrad(loss, params, learning_rate=.01)\n",
    "\n",
    "train_fn = theano.function([input_var, targets, mask_input_var], loss, updates=updates)\n",
    "\n",
    "#### for validation\n",
    "\n",
    "test_out = L.layers.get_output(net, deterministic=True)\n",
    "test_loss = -T.sum(T.log(test_out[mask_idx])) / T.sum(mask_input_var)\n",
    "\n",
    "#test_acc = T.mean(T.eq(T.argmax(test_out, axis=1), targets), dtype=theano.config.floatX)\n",
    "\n",
    "val_fn = theano.function([input_var, targets, mask_input_var], test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 10 batches in 2.45 sec.    training loss:\t\t7.44353927834\n",
      "Done 20 batches in 4.93 sec.    training loss:\t\t6.66934120304\n",
      "Done 30 batches in 7.24 sec.    training loss:\t\t6.36463218175\n",
      "Done 40 batches in 9.64 sec.    training loss:\t\t6.17626197709\n",
      "Done 50 batches in 12.34 sec.    training loss:\t\t6.04152103334\n",
      "Done 60 batches in 14.69 sec.    training loss:\t\t5.9128881931\n",
      "Done 70 batches in 17.10 sec.    training loss:\t\t5.82142742549\n",
      "Done 80 batches in 19.44 sec.    training loss:\t\t5.73142549026\n",
      "Done 90 batches in 22.07 sec.    training loss:\t\t5.64433819338\n",
      "Done 100 batches in 24.37 sec.    training loss:\t\t5.56514480491\n",
      "Done 110 batches in 26.87 sec.    training loss:\t\t5.49900743206\n",
      "Done 120 batches in 29.45 sec.    training loss:\t\t5.43617925437\n",
      "Done 130 batches in 31.88 sec.    training loss:\t\t5.37734211024\n",
      "Done 140 batches in 34.38 sec.    training loss:\t\t5.32326670707\n",
      "Done 150 batches in 36.63 sec.    training loss:\t\t5.27438875182\n",
      "Done 160 batches in 38.97 sec.    training loss:\t\t5.22660741064\n",
      "Done 170 batches in 41.35 sec.    training loss:\t\t5.18140344147\n",
      "Done 180 batches in 43.84 sec.    training loss:\t\t5.14077446073\n",
      "Done 190 batches in 46.21 sec.    training loss:\t\t5.10163708403\n",
      "Done 200 batches in 49.03 sec.    training loss:\t\t5.06540308519\n",
      "Done 210 batches in 51.53 sec.    training loss:\t\t5.02992965762\n",
      "Done 220 batches in 53.94 sec.    training loss:\t\t4.99911724536\n",
      "Done 230 batches in 56.30 sec.    training loss:\t\t4.96817711909\n",
      "Done 240 batches in 58.56 sec.    training loss:\t\t4.93783678426\n",
      "Done 250 batches in 61.20 sec.    training loss:\t\t4.91209749273\n",
      "Done 260 batches in 63.63 sec.    training loss:\t\t4.8867854439\n",
      "Done 270 batches in 65.98 sec.    training loss:\t\t4.86261310249\n",
      "Done 280 batches in 68.37 sec.    training loss:\t\t4.83919479021\n",
      "Done 290 batches in 70.82 sec.    training loss:\t\t4.81982949122\n",
      "Done 300 batches in 73.12 sec.    training loss:\t\t4.79760674476\n",
      "Done 310 batches in 75.39 sec.    training loss:\t\t4.77720350997\n",
      "Done 320 batches in 77.79 sec.    training loss:\t\t4.75790508724\n",
      "Done 330 batches in 80.27 sec.    training loss:\t\t4.74018989705\n",
      "Done 340 batches in 82.71 sec.    training loss:\t\t4.72229907973\n",
      "Done 350 batches in 85.35 sec.    training loss:\t\t4.70658283307\n",
      "Done 360 batches in 87.70 sec.    training loss:\t\t4.6915407926\n",
      "Done 370 batches in 90.15 sec.    training loss:\t\t4.676641527\n",
      "Done 380 batches in 92.73 sec.    training loss:\t\t4.66271959146\n",
      "Done 390 batches in 95.19 sec.    training loss:\t\t4.64815337402\n",
      "Done 400 batches in 97.54 sec.    training loss:\t\t4.63458052681\n",
      "Done 410 batches in 99.97 sec.    training loss:\t\t4.62114336774\n",
      "Done 420 batches in 102.59 sec.    training loss:\t\t4.60785643539\n",
      "Done 430 batches in 105.10 sec.    training loss:\t\t4.5946617254\n",
      "Done 440 batches in 107.69 sec.    training loss:\t\t4.58218949332\n",
      "Done 450 batches in 110.21 sec.    training loss:\t\t4.56997779607\n",
      "Done 460 batches in 112.61 sec.    training loss:\t\t4.55862455205\n",
      "Done 470 batches in 115.24 sec.    training loss:\t\t4.54834280326\n",
      "Done 480 batches in 117.64 sec.    training loss:\t\t4.53882020475\n",
      "Done 490 batches in 119.91 sec.    training loss:\t\t4.52857607444\n",
      "Done 500 batches in 122.32 sec.    training loss:\t\t4.51798889438\n",
      "Done 510 batches in 124.72 sec.    training loss:\t\t4.50771073372\n",
      "Done 520 batches in 127.26 sec.    training loss:\t\t4.49795192462\n",
      "Done 530 batches in 129.60 sec.    training loss:\t\t4.4887031543\n",
      "Done 540 batches in 131.99 sec.    training loss:\t\t4.47947522086\n",
      "Done 550 batches in 134.42 sec.    training loss:\t\t4.47083700944\n",
      "Done 560 batches in 136.93 sec.    training loss:\t\t4.46217010043\n",
      "Done 570 batches in 139.46 sec.    training loss:\t\t4.45323634934\n",
      "Done 580 batches in 141.64 sec.    training loss:\t\t4.44526933888\n",
      "Done 590 batches in 144.05 sec.    training loss:\t\t4.43706221096\n",
      "Done 600 batches in 146.54 sec.    training loss:\t\t4.42994405155\n",
      "Done 610 batches in 148.93 sec.    training loss:\t\t4.4219622207\n",
      "Done 620 batches in 151.62 sec.    training loss:\t\t4.41496912869\n",
      "Done 630 batches in 153.82 sec.    training loss:\t\t4.40735999211\n",
      "Done 640 batches in 156.14 sec.    training loss:\t\t4.40015144985\n",
      "Done 650 batches in 158.65 sec.    training loss:\t\t4.39282313292\n",
      "Done 660 batches in 160.94 sec.    training loss:\t\t4.38603401536\n",
      "Done 670 batches in 163.63 sec.    training loss:\t\t4.37973175831\n",
      "Done 680 batches in 165.98 sec.    training loss:\t\t4.37313182111\n",
      "Done 690 batches in 168.39 sec.    training loss:\t\t4.36724088215\n",
      "Done 700 batches in 170.62 sec.    training loss:\t\t4.36038094004\n",
      "Done 710 batches in 173.17 sec.    training loss:\t\t4.35437154536\n",
      "Done 720 batches in 175.78 sec.    training loss:\t\t4.34850128259\n",
      "Done 730 batches in 178.04 sec.    training loss:\t\t4.34243150917\n",
      "Done 740 batches in 180.49 sec.    training loss:\t\t4.33692834687\n",
      "Done 750 batches in 182.94 sec.    training loss:\t\t4.33167329386\n",
      "Done 760 batches in 185.35 sec.    training loss:\t\t4.32606001667\n",
      "Done 770 batches in 187.83 sec.    training loss:\t\t4.32081488163\n",
      "Done 780 batches in 190.22 sec.    training loss:\t\t4.3152633644\n",
      "Done 790 batches in 192.70 sec.    training loss:\t\t4.3099646842\n",
      "Done 800 batches in 195.24 sec.    training loss:\t\t4.3049991035\n",
      "Done 810 batches in 197.56 sec.    training loss:\t\t4.29953802934\n",
      "Done 820 batches in 199.95 sec.    training loss:\t\t4.29422340232\n",
      "Done 830 batches in 202.17 sec.    training loss:\t\t4.28978468445\n",
      "Done 840 batches in 204.54 sec.    training loss:\t\t4.28541079219\n",
      "Done 850 batches in 206.81 sec.    training loss:\t\t4.2807029364\n",
      "Done 860 batches in 209.22 sec.    training loss:\t\t4.27674521511\n",
      "Done 870 batches in 211.62 sec.    training loss:\t\t4.2723226805\n",
      "Done 880 batches in 213.92 sec.    training loss:\t\t4.26744916338\n",
      "Done 890 batches in 216.41 sec.    training loss:\t\t4.26340519415\n",
      "Done 900 batches in 218.75 sec.    training loss:\t\t4.25939201248\n",
      "Done 910 batches in 220.93 sec.    training loss:\t\t4.25471708775\n",
      "Done 920 batches in 223.21 sec.    training loss:\t\t4.25054162699\n",
      "Done 930 batches in 225.72 sec.    training loss:\t\t4.24680162375\n",
      "Done 940 batches in 228.15 sec.    training loss:\t\t4.2425531199\n",
      "Done 950 batches in 230.61 sec.    training loss:\t\t4.2386501108\n",
      "Done 960 batches in 233.12 sec.    training loss:\t\t4.23479925506\n",
      "Done 970 batches in 235.41 sec.    training loss:\t\t4.23075648765\n",
      "Done 980 batches in 237.83 sec.    training loss:\t\t4.22661569871\n",
      "Done 990 batches in 240.10 sec.    training loss:\t\t4.22289959593\n",
      "Done 1000 batches in 242.51 sec.    training loss:\t\t4.21939059306\n",
      "Done 1010 batches in 244.99 sec.    training loss:\t\t4.21566207782\n",
      "Done 1020 batches in 247.14 sec.    training loss:\t\t4.21166121103\n",
      "Done 1030 batches in 249.72 sec.    training loss:\t\t4.20820176489\n",
      "Done 1040 batches in 252.34 sec.    training loss:\t\t4.20460999944\n",
      "Done 1050 batches in 254.76 sec.    training loss:\t\t4.20117675536\n",
      "Done 1060 batches in 257.17 sec.    training loss:\t\t4.19776093687\n",
      "Done 1070 batches in 259.53 sec.    training loss:\t\t4.19406044638\n",
      "Done 1080 batches in 261.89 sec.    training loss:\t\t4.19065790129\n",
      "Done 1090 batches in 264.53 sec.    training loss:\t\t4.18767643851\n",
      "Done 1100 batches in 266.95 sec.    training loss:\t\t4.18440206205\n",
      "Done 1110 batches in 269.39 sec.    training loss:\t\t4.18112248301\n",
      "Done 1120 batches in 271.94 sec.    training loss:\t\t4.17810725058\n",
      "Done 1130 batches in 274.37 sec.    training loss:\t\t4.17520294004\n",
      "Done 1140 batches in 276.87 sec.    training loss:\t\t4.17221528122\n",
      "Done 1150 batches in 279.17 sec.    training loss:\t\t4.16943980018\n",
      "Done 1160 batches in 281.68 sec.    training loss:\t\t4.16623308608\n",
      "Done 1170 batches in 284.29 sec.    training loss:\t\t4.16301880338\n",
      "Done 1180 batches in 286.73 sec.    training loss:\t\t4.16005859196\n",
      "Done 1190 batches in 289.37 sec.    training loss:\t\t4.1572173687\n",
      "Done 1200 batches in 291.90 sec.    training loss:\t\t4.15416659947\n",
      "Done 1210 batches in 294.46 sec.    training loss:\t\t4.15110796737\n",
      "Done 1220 batches in 296.81 sec.    training loss:\t\t4.14825809686\n",
      "Done 1230 batches in 299.25 sec.    training loss:\t\t4.14547223051\n",
      "Done 1240 batches in 301.78 sec.    training loss:\t\t4.14273554434\n",
      "Done 1250 batches in 304.44 sec.    training loss:\t\t4.13994793226\n",
      "Done 1260 batches in 306.88 sec.    training loss:\t\t4.13713352096\n",
      "Done 1270 batches in 309.26 sec.    training loss:\t\t4.13432220174\n",
      "Done 1280 batches in 311.73 sec.    training loss:\t\t4.13171434526\n",
      "Done 1290 batches in 314.15 sec.    training loss:\t\t4.1289132501\n",
      "Done 1300 batches in 316.66 sec.    training loss:\t\t4.12635883647\n",
      "Done 1310 batches in 319.23 sec.    training loss:\t\t4.12384888821\n",
      "Done 1320 batches in 321.44 sec.    training loss:\t\t4.12137626097\n",
      "Done 1330 batches in 323.87 sec.    training loss:\t\t4.11895221134\n",
      "Done 1340 batches in 326.24 sec.    training loss:\t\t4.11610555079\n",
      "Done 1350 batches in 328.85 sec.    training loss:\t\t4.11356984389\n",
      "Done 1360 batches in 331.47 sec.    training loss:\t\t4.11107258722\n",
      "Done 1370 batches in 333.97 sec.    training loss:\t\t4.10853694822\n",
      "Done 1380 batches in 336.30 sec.    training loss:\t\t4.10606667417\n",
      "Done 1390 batches in 338.89 sec.    training loss:\t\t4.10361773004\n",
      "Done 1400 batches in 341.40 sec.    training loss:\t\t4.10126118265\n",
      "Done 1410 batches in 343.91 sec.    training loss:\t\t4.09879596196\n",
      "Done 1420 batches in 346.11 sec.    training loss:\t\t4.09649674113\n",
      "Done 1430 batches in 348.48 sec.    training loss:\t\t4.09429470892\n",
      "Done 1440 batches in 350.97 sec.    training loss:\t\t4.09221427101\n",
      "Done 1450 batches in 353.58 sec.    training loss:\t\t4.08999579676\n",
      "Done 1460 batches in 355.93 sec.    training loss:\t\t4.08775091245\n",
      "Done 1470 batches in 358.22 sec.    training loss:\t\t4.085463596\n",
      "Done 1480 batches in 360.87 sec.    training loss:\t\t4.08340131229\n",
      "Done 1490 batches in 363.45 sec.    training loss:\t\t4.08114733064\n",
      "Done 1500 batches in 365.99 sec.    training loss:\t\t4.07910488691\n",
      "Done 1510 batches in 368.50 sec.    training loss:\t\t4.07708371837\n",
      "Done 1520 batches in 370.97 sec.    training loss:\t\t4.07500740194\n",
      "Done 1530 batches in 373.50 sec.    training loss:\t\t4.07300100319\n",
      "Done 1540 batches in 375.99 sec.    training loss:\t\t4.07075962922\n",
      "Done 1550 batches in 378.37 sec.    training loss:\t\t4.0686076646\n",
      "Done 1560 batches in 380.91 sec.    training loss:\t\t4.06709425074\n",
      "Done 1570 batches in 383.39 sec.    training loss:\t\t4.06515959881\n",
      "Done 1580 batches in 385.65 sec.    training loss:\t\t4.06326865809\n",
      "Done 1590 batches in 388.40 sec.    training loss:\t\t4.06123512789\n",
      "Done 1600 batches in 390.72 sec.    training loss:\t\t4.05922520781\n",
      "Done 1610 batches in 392.87 sec.    training loss:\t\t4.05729077432\n",
      "Done 1620 batches in 395.48 sec.    training loss:\t\t4.05526304416\n",
      "Done 1630 batches in 397.93 sec.    training loss:\t\t4.05321775983\n",
      "Done 1640 batches in 400.69 sec.    training loss:\t\t4.05143607332\n",
      "Done 1650 batches in 403.36 sec.    training loss:\t\t4.04953954017\n",
      "Done 1660 batches in 405.79 sec.    training loss:\t\t4.04771329191\n",
      "Done 1670 batches in 408.36 sec.    training loss:\t\t4.04567766423\n",
      "Done 1680 batches in 410.98 sec.    training loss:\t\t4.04388672416\n",
      "Done 1690 batches in 413.60 sec.    training loss:\t\t4.04228945899\n",
      "Done 1700 batches in 415.84 sec.    training loss:\t\t4.04042590514\n",
      "Done 1710 batches in 418.29 sec.    training loss:\t\t4.03869132208\n",
      "Done 1720 batches in 420.64 sec.    training loss:\t\t4.03705043727\n",
      "Done 1730 batches in 423.29 sec.    training loss:\t\t4.03530380897\n",
      "Done 1740 batches in 425.65 sec.    training loss:\t\t4.03346902701\n",
      "Done 1750 batches in 428.14 sec.    training loss:\t\t4.03188932986\n",
      "Done 1760 batches in 430.68 sec.    training loss:\t\t4.03020724835\n",
      "Done 1770 batches in 432.94 sec.    training loss:\t\t4.02866171661\n",
      "Done 1780 batches in 435.57 sec.    training loss:\t\t4.02700402781\n",
      "Done 1790 batches in 438.02 sec.    training loss:\t\t4.02543776874\n",
      "Done 1800 batches in 440.48 sec.    training loss:\t\t4.02375753031\n",
      "Done 1810 batches in 443.00 sec.    training loss:\t\t4.02207538135\n",
      "Done 1820 batches in 445.72 sec.    training loss:\t\t4.0203259214\n",
      "Done 1830 batches in 448.18 sec.    training loss:\t\t4.01884063909\n",
      "Done 1840 batches in 450.46 sec.    training loss:\t\t4.01738689347\n",
      "Done 1850 batches in 452.76 sec.    training loss:\t\t4.01590086828\n",
      "Done 1860 batches in 454.92 sec.    training loss:\t\t4.01410384308\n",
      "Done 1870 batches in 457.42 sec.    training loss:\t\t4.01254671219\n",
      "Done 1880 batches in 459.68 sec.    training loss:\t\t4.01091714582\n",
      "Done 1890 batches in 462.29 sec.    training loss:\t\t4.00949369356\n",
      "Done 1900 batches in 464.72 sec.    training loss:\t\t4.0081197296\n",
      "Done 1910 batches in 467.23 sec.    training loss:\t\t4.00659715213\n",
      "Done 1920 batches in 469.62 sec.    training loss:\t\t4.00515640912\n",
      "Done 1930 batches in 472.06 sec.    training loss:\t\t4.00361299216\n",
      "Done 1940 batches in 474.72 sec.    training loss:\t\t4.00215135578\n",
      "Done 1950 batches in 477.09 sec.    training loss:\t\t4.0006036678\n",
      "Done 1960 batches in 479.62 sec.    training loss:\t\t3.9989931897\n",
      "Done 1970 batches in 481.73 sec.    training loss:\t\t3.997520917\n",
      "Done 1980 batches in 484.05 sec.    training loss:\t\t3.99598155423\n",
      "Done 1990 batches in 486.46 sec.    training loss:\t\t3.99457064915\n",
      "Done 2000 batches in 489.09 sec.    training loss:\t\t3.99314384028\n",
      "Done 2010 batches in 491.77 sec.    training loss:\t\t3.99165635118\n",
      "Done 2020 batches in 494.21 sec.    training loss:\t\t3.99001073242\n",
      "Done 2030 batches in 496.69 sec.    training loss:\t\t3.98854149505\n",
      "Done 2040 batches in 499.05 sec.    training loss:\t\t3.98712839588\n",
      "Done 2050 batches in 501.30 sec.    training loss:\t\t3.985751026\n",
      "Done 2060 batches in 503.85 sec.    training loss:\t\t3.98441578361\n",
      "Done 2070 batches in 506.25 sec.    training loss:\t\t3.98297747486\n",
      "Done 2080 batches in 508.72 sec.    training loss:\t\t3.98160391571\n",
      "Done 2090 batches in 511.13 sec.    training loss:\t\t3.98018339442\n",
      "Done 2100 batches in 513.52 sec.    training loss:\t\t3.97877836988\n",
      "Done 2110 batches in 515.77 sec.    training loss:\t\t3.97748001805\n",
      "Done 2120 batches in 518.17 sec.    training loss:\t\t3.97630698359\n",
      "Done 2130 batches in 520.38 sec.    training loss:\t\t3.97504532126\n",
      "Done 2140 batches in 522.71 sec.    training loss:\t\t3.97370262843\n",
      "Done 2150 batches in 525.24 sec.    training loss:\t\t3.97227776031\n",
      "Done 2160 batches in 527.72 sec.    training loss:\t\t3.9710415076\n",
      "Done 2170 batches in 530.31 sec.    training loss:\t\t3.96999429087\n",
      "Done 2180 batches in 532.60 sec.    training loss:\t\t3.96870999518\n",
      "Done 2190 batches in 534.79 sec.    training loss:\t\t3.96746098997\n",
      "Done 2200 batches in 537.25 sec.    training loss:\t\t3.96618536848\n",
      "Done 2210 batches in 539.74 sec.    training loss:\t\t3.96493312287\n",
      "Done 2220 batches in 542.11 sec.    training loss:\t\t3.96371241999\n",
      "Done 2230 batches in 544.35 sec.    training loss:\t\t3.96247532991\n",
      "Done 2240 batches in 546.89 sec.    training loss:\t\t3.96122770287\n",
      "Done 2250 batches in 549.39 sec.    training loss:\t\t3.96002843335\n",
      "Done 2260 batches in 551.70 sec.    training loss:\t\t3.95880118001\n",
      "Done 2270 batches in 554.13 sec.    training loss:\t\t3.95765680934\n",
      "Done 2280 batches in 556.62 sec.    training loss:\t\t3.95640457541\n",
      "Done 2290 batches in 559.29 sec.    training loss:\t\t3.95519783247\n",
      "Done 2300 batches in 561.82 sec.    training loss:\t\t3.95412056908\n",
      "Done 2310 batches in 564.39 sec.    training loss:\t\t3.95293777915\n",
      "Done 2320 batches in 566.83 sec.    training loss:\t\t3.9517958575\n",
      "Done 2330 batches in 569.53 sec.    training loss:\t\t3.9507428773\n",
      "Done 2340 batches in 571.95 sec.    training loss:\t\t3.94950615011\n",
      "Done 2350 batches in 574.41 sec.    training loss:\t\t3.94842330308\n",
      "Done 2360 batches in 576.76 sec.    training loss:\t\t3.94729623674\n",
      "Done 2370 batches in 579.18 sec.    training loss:\t\t3.94627305381\n",
      "Done 2380 batches in 581.51 sec.    training loss:\t\t3.94515070215\n",
      "Done 2390 batches in 583.89 sec.    training loss:\t\t3.94406782667\n",
      "Done 2400 batches in 586.33 sec.    training loss:\t\t3.94305818485\n",
      "Done 2410 batches in 588.90 sec.    training loss:\t\t3.9419645738\n",
      "Done 2420 batches in 591.53 sec.    training loss:\t\t3.94125071584\n",
      "Done 2430 batches in 593.98 sec.    training loss:\t\t3.94020388174\n",
      "Done 2440 batches in 596.32 sec.    training loss:\t\t3.9391154768\n",
      "Done 2450 batches in 598.75 sec.    training loss:\t\t3.93806708909\n",
      "Done 2460 batches in 601.15 sec.    training loss:\t\t3.93705316568\n",
      "Done 2470 batches in 603.64 sec.    training loss:\t\t3.93607291137\n",
      "Done 2480 batches in 606.19 sec.    training loss:\t\t3.93511080885\n",
      "Done 2490 batches in 608.81 sec.    training loss:\t\t3.93402820764\n",
      "Done 2500 batches in 611.29 sec.    training loss:\t\t3.93303620008\n",
      "Done 2510 batches in 613.66 sec.    training loss:\t\t3.93194559973\n",
      "Done 2520 batches in 616.04 sec.    training loss:\t\t3.93083451166\n",
      "Done 2530 batches in 618.81 sec.    training loss:\t\t3.92978254364\n",
      "Done 2540 batches in 621.22 sec.    training loss:\t\t3.92870368048\n",
      "Done 2550 batches in 623.80 sec.    training loss:\t\t3.92775222816\n",
      "Done 2560 batches in 626.28 sec.    training loss:\t\t3.92684637987\n",
      "Done 2570 batches in 628.77 sec.    training loss:\t\t3.92578548279\n",
      "Done 2580 batches in 631.48 sec.    training loss:\t\t3.92477364099\n",
      "Done 2590 batches in 633.83 sec.    training loss:\t\t3.92381263869\n",
      "Done 2600 batches in 636.29 sec.    training loss:\t\t3.92281087136\n",
      "Done 2610 batches in 638.88 sec.    training loss:\t\t3.92185760213\n",
      "Done 2620 batches in 641.49 sec.    training loss:\t\t3.92083141439\n",
      "Done 2630 batches in 643.93 sec.    training loss:\t\t3.91981890952\n",
      "Done 2640 batches in 646.61 sec.    training loss:\t\t3.91886850215\n",
      "Done 2650 batches in 649.06 sec.    training loss:\t\t3.91793401252\n",
      "Done 2660 batches in 651.40 sec.    training loss:\t\t3.91695299988\n",
      "Done 2670 batches in 653.66 sec.    training loss:\t\t3.91597495564\n",
      "Done 2680 batches in 656.31 sec.    training loss:\t\t3.91508246152\n",
      "Done 2690 batches in 658.79 sec.    training loss:\t\t3.91417004736\n",
      "Done 2700 batches in 661.31 sec.    training loss:\t\t3.91327950396\n",
      "Done 2710 batches in 663.54 sec.    training loss:\t\t3.9123522433\n",
      "Done 2720 batches in 665.90 sec.    training loss:\t\t3.9114721901\n",
      "Done 2730 batches in 668.25 sec.    training loss:\t\t3.91050235278\n",
      "Done 2740 batches in 670.61 sec.    training loss:\t\t3.90953104975\n",
      "Done 2750 batches in 673.19 sec.    training loss:\t\t3.9085804825\n",
      "Done 2760 batches in 675.66 sec.    training loss:\t\t3.90790486142\n",
      "Done 2770 batches in 678.36 sec.    training loss:\t\t3.90707026458\n",
      "Done 2780 batches in 680.47 sec.    training loss:\t\t3.90615579647\n",
      "Done 2790 batches in 682.98 sec.    training loss:\t\t3.90519905464\n",
      "Done 2800 batches in 685.32 sec.    training loss:\t\t3.90440166096\n",
      "Done 2810 batches in 687.69 sec.    training loss:\t\t3.90356022181\n",
      "Done 2820 batches in 690.06 sec.    training loss:\t\t3.90263140027\n",
      "Done 2830 batches in 692.88 sec.    training loss:\t\t3.90186600983\n",
      "Done 2840 batches in 695.35 sec.    training loss:\t\t3.90087327553\n",
      "Done 2850 batches in 697.74 sec.    training loss:\t\t3.8998994542\n",
      "Done 2860 batches in 700.49 sec.    training loss:\t\t3.89900454554\n",
      "Done 2870 batches in 703.06 sec.    training loss:\t\t3.89829047218\n",
      "Done 2880 batches in 705.34 sec.    training loss:\t\t3.89748896108\n",
      "Done 2890 batches in 707.89 sec.    training loss:\t\t3.89659967737\n",
      "Done 2900 batches in 710.42 sec.    training loss:\t\t3.89568010218\n",
      "Done 2910 batches in 712.56 sec.    training loss:\t\t3.8948263996\n",
      "Done 2920 batches in 715.00 sec.    training loss:\t\t3.89397014747\n",
      "Done 2930 batches in 717.34 sec.    training loss:\t\t3.89319740025\n",
      "Done 2940 batches in 719.84 sec.    training loss:\t\t3.89238103808\n",
      "Done 2950 batches in 722.38 sec.    training loss:\t\t3.89151361224\n",
      "Done 2960 batches in 724.95 sec.    training loss:\t\t3.89083214406\n",
      "Done 2970 batches in 727.24 sec.    training loss:\t\t3.8900658743\n",
      "Done 2980 batches in 729.61 sec.    training loss:\t\t3.88933676658\n",
      "Done 2990 batches in 732.08 sec.    training loss:\t\t3.88845248271\n",
      "Done 3000 batches in 734.65 sec.    training loss:\t\t3.88757592948\n",
      "Done 3010 batches in 737.02 sec.    training loss:\t\t3.88679767988\n",
      "Done 3020 batches in 739.23 sec.    training loss:\t\t3.88604242429\n",
      "Done 3030 batches in 741.71 sec.    training loss:\t\t3.88517889823\n",
      "Done 3040 batches in 744.29 sec.    training loss:\t\t3.88437474707\n",
      "Done 3050 batches in 746.91 sec.    training loss:\t\t3.88362791527\n",
      "Done 3060 batches in 749.34 sec.    training loss:\t\t3.88286491659\n",
      "Done 3070 batches in 751.80 sec.    training loss:\t\t3.88204434285\n",
      "Done 3080 batches in 754.23 sec.    training loss:\t\t3.8812550184\n",
      "Done 3090 batches in 756.70 sec.    training loss:\t\t3.88040796307\n",
      "Done 3100 batches in 759.00 sec.    training loss:\t\t3.87955901941\n",
      "Done 3110 batches in 761.45 sec.    training loss:\t\t3.87892086393\n",
      "Done 3120 batches in 763.96 sec.    training loss:\t\t3.87806935689\n",
      "Done 3130 batches in 766.56 sec.    training loss:\t\t3.87729338976\n",
      "Done 3140 batches in 768.86 sec.    training loss:\t\t3.87650808113\n",
      "Done 3150 batches in 770.97 sec.    training loss:\t\t3.87573772286\n",
      "Done 3160 batches in 773.46 sec.    training loss:\t\t3.87501568094\n",
      "Done 3170 batches in 775.90 sec.    training loss:\t\t3.87416095461\n",
      "Done 3180 batches in 778.44 sec.    training loss:\t\t3.87341801663\n",
      "Done 3190 batches in 780.87 sec.    training loss:\t\t3.87267903536\n",
      "Done 3200 batches in 783.30 sec.    training loss:\t\t3.87192753005\n",
      "Done 3210 batches in 785.71 sec.    training loss:\t\t3.87132920039\n",
      "Done 3220 batches in 788.20 sec.    training loss:\t\t3.87062199798\n",
      "Done 3230 batches in 790.77 sec.    training loss:\t\t3.86979473247\n",
      "Done 3240 batches in 793.26 sec.    training loss:\t\t3.86909410071\n",
      "Done 3250 batches in 795.72 sec.    training loss:\t\t3.86836729383\n",
      "Done 3260 batches in 798.07 sec.    training loss:\t\t3.86765363004\n",
      "Done 3270 batches in 800.61 sec.    training loss:\t\t3.86688409886\n",
      "Done 3280 batches in 803.26 sec.    training loss:\t\t3.86613880871\n",
      "Done 3290 batches in 805.52 sec.    training loss:\t\t3.86536796581\n",
      "Done 3300 batches in 808.07 sec.    training loss:\t\t3.86464484821\n",
      "Done 3310 batches in 810.39 sec.    training loss:\t\t3.86407585979\n",
      "Done 3320 batches in 812.66 sec.    training loss:\t\t3.86336138818\n",
      "Done 3330 batches in 815.25 sec.    training loss:\t\t3.86278776741\n",
      "Done 3340 batches in 817.70 sec.    training loss:\t\t3.86205192336\n",
      "Done 3350 batches in 820.18 sec.    training loss:\t\t3.86150402191\n",
      "Done 3360 batches in 822.58 sec.    training loss:\t\t3.86079239989\n",
      "Done 3370 batches in 825.04 sec.    training loss:\t\t3.8601281188\n",
      "Done 3380 batches in 827.46 sec.    training loss:\t\t3.859439794\n",
      "Done 3390 batches in 829.78 sec.    training loss:\t\t3.85880372145\n",
      "Done 3400 batches in 832.25 sec.    training loss:\t\t3.85811500416\n",
      "Done 3410 batches in 834.75 sec.    training loss:\t\t3.85742555193\n",
      "Done 3420 batches in 836.94 sec.    training loss:\t\t3.85681114274\n",
      "Done 3430 batches in 839.22 sec.    training loss:\t\t3.85609505444\n",
      "Done 3440 batches in 841.65 sec.    training loss:\t\t3.85543334006\n",
      "Done 3450 batches in 843.92 sec.    training loss:\t\t3.85486774885\n",
      "Done 3460 batches in 846.48 sec.    training loss:\t\t3.85412498705\n",
      "Done 3470 batches in 848.90 sec.    training loss:\t\t3.85344617222\n",
      "Done 3480 batches in 851.39 sec.    training loss:\t\t3.85275924717\n",
      "Done 3490 batches in 853.89 sec.    training loss:\t\t3.85212188519\n",
      "Done 3500 batches in 856.47 sec.    training loss:\t\t3.85147517595\n",
      "Done 3510 batches in 858.90 sec.    training loss:\t\t3.85089254622\n",
      "Done 3520 batches in 861.31 sec.    training loss:\t\t3.85028258876\n",
      "Done 3530 batches in 863.60 sec.    training loss:\t\t3.84963469851\n",
      "Done 3540 batches in 866.05 sec.    training loss:\t\t3.84898925419\n",
      "Done 3550 batches in 868.30 sec.    training loss:\t\t3.8482602546\n",
      "Done 3560 batches in 870.95 sec.    training loss:\t\t3.84760041231\n",
      "Done 3570 batches in 873.47 sec.    training loss:\t\t3.84697935566\n",
      "Done 3580 batches in 875.97 sec.    training loss:\t\t3.84636636754\n",
      "Done 3590 batches in 878.54 sec.    training loss:\t\t3.84572323052\n",
      "Done 3600 batches in 881.25 sec.    training loss:\t\t3.8450703339\n",
      "Done 3610 batches in 883.83 sec.    training loss:\t\t3.84447548328\n",
      "Done 3620 batches in 886.04 sec.    training loss:\t\t3.8438170545\n",
      "Done 3630 batches in 888.30 sec.    training loss:\t\t3.84330410451\n",
      "Done 3640 batches in 890.83 sec.    training loss:\t\t3.84267634654\n",
      "Done 3650 batches in 893.32 sec.    training loss:\t\t3.84200051882\n",
      "Done 3660 batches in 895.64 sec.    training loss:\t\t3.84139655862\n",
      "Done 3670 batches in 897.93 sec.    training loss:\t\t3.84072566225\n",
      "Done 3680 batches in 900.23 sec.    training loss:\t\t3.84003862878\n",
      "Done 3690 batches in 902.54 sec.    training loss:\t\t3.83945142888\n",
      "Done 3700 batches in 905.04 sec.    training loss:\t\t3.83887792847\n",
      "Done 3710 batches in 907.32 sec.    training loss:\t\t3.83827537396\n",
      "Done 3720 batches in 909.69 sec.    training loss:\t\t3.83767422187\n",
      "Done 3730 batches in 912.02 sec.    training loss:\t\t3.83703960011\n",
      "Done 3740 batches in 914.49 sec.    training loss:\t\t3.83636663365\n",
      "Done 3750 batches in 916.87 sec.    training loss:\t\t3.83584752055\n",
      "Done 3760 batches in 919.45 sec.    training loss:\t\t3.83520009815\n",
      "Done 3770 batches in 921.79 sec.    training loss:\t\t3.83464042703\n",
      "Done 3780 batches in 924.15 sec.    training loss:\t\t3.83408480446\n",
      "Done 3790 batches in 926.64 sec.    training loss:\t\t3.83359256334\n",
      "Done 3800 batches in 929.16 sec.    training loss:\t\t3.83299024733\n",
      "Done 3810 batches in 931.70 sec.    training loss:\t\t3.83243248615\n",
      "Done 3820 batches in 934.30 sec.    training loss:\t\t3.83190408645\n",
      "Done 3830 batches in 936.59 sec.    training loss:\t\t3.83129086737\n",
      "Done 3840 batches in 939.14 sec.    training loss:\t\t3.83079267548\n",
      "Done 3850 batches in 941.89 sec.    training loss:\t\t3.83020110018\n",
      "Done 3860 batches in 944.45 sec.    training loss:\t\t3.82978001307\n",
      "Done 3870 batches in 946.95 sec.    training loss:\t\t3.82920322364\n",
      "Done 3880 batches in 949.40 sec.    training loss:\t\t3.82863596377\n",
      "Done 3890 batches in 951.67 sec.    training loss:\t\t3.82804352329\n",
      "Done 100 batches in 7.27 sec.\n",
      "Done 200 batches in 14.42 sec.\n",
      "Done 300 batches in 21.54 sec.\n",
      "Done 400 batches in 28.71 sec.\n",
      "Done 500 batches in 36.29 sec.\n",
      "Done 600 batches in 43.28 sec.\n",
      "Done 700 batches in 50.64 sec.\n",
      "Done 800 batches in 57.88 sec.\n",
      "Done 900 batches in 65.39 sec.\n",
      "Epoch 1 of 5 took 1025.557s\n",
      "  training loss:\t\t3.827679\n",
      "  validation loss:\t\t3.660724\n",
      "Done 10 batches in 2.42 sec.    training loss:\t\t3.58363664176\n",
      "Done 20 batches in 4.89 sec.    training loss:\t\t3.58712980557\n",
      "Done 30 batches in 7.19 sec.    training loss:\t\t3.59339564929\n",
      "Done 40 batches in 9.58 sec.    training loss:\t\t3.59732279815\n",
      "Done 50 batches in 12.27 sec.    training loss:\t\t3.60146521512\n",
      "Done 60 batches in 14.61 sec.    training loss:\t\t3.59684907555\n",
      "Done 70 batches in 17.02 sec.    training loss:\t\t3.5976627878\n",
      "Done 80 batches in 19.37 sec.    training loss:\t\t3.59810416931\n",
      "Done 90 batches in 22.03 sec.    training loss:\t\t3.5974476366\n",
      "Done 100 batches in 24.32 sec.    training loss:\t\t3.59792663369\n",
      "Done 110 batches in 26.81 sec.    training loss:\t\t3.5978644871\n",
      "Done 120 batches in 29.40 sec.    training loss:\t\t3.59867880849\n",
      "Done 130 batches in 31.84 sec.    training loss:\t\t3.59650134748\n",
      "Done 140 batches in 34.33 sec.    training loss:\t\t3.59580885531\n",
      "Done 150 batches in 36.57 sec.    training loss:\t\t3.59458065675\n",
      "Done 160 batches in 38.90 sec.    training loss:\t\t3.593238874\n",
      "Done 170 batches in 41.25 sec.    training loss:\t\t3.59463093304\n",
      "Done 180 batches in 43.72 sec.    training loss:\t\t3.59256971172\n",
      "Done 190 batches in 46.09 sec.    training loss:\t\t3.59295258509\n",
      "Done 200 batches in 48.90 sec.    training loss:\t\t3.5928722323\n",
      "Done 210 batches in 51.38 sec.    training loss:\t\t3.59299831464\n",
      "Done 220 batches in 53.79 sec.    training loss:\t\t3.59298847827\n",
      "Done 230 batches in 56.15 sec.    training loss:\t\t3.59267605794\n",
      "Done 240 batches in 58.40 sec.    training loss:\t\t3.59186394629\n",
      "Done 250 batches in 61.03 sec.    training loss:\t\t3.59310248826\n",
      "Done 260 batches in 63.46 sec.    training loss:\t\t3.59258591673\n",
      "Done 270 batches in 65.82 sec.    training loss:\t\t3.59286000798\n",
      "Done 280 batches in 68.23 sec.    training loss:\t\t3.59222372619\n",
      "Done 290 batches in 70.66 sec.    training loss:\t\t3.59204266754\n",
      "Done 300 batches in 72.97 sec.    training loss:\t\t3.59127428874\n",
      "Done 310 batches in 75.25 sec.    training loss:\t\t3.59097750371\n",
      "Done 320 batches in 77.66 sec.    training loss:\t\t3.59145903136\n",
      "Done 330 batches in 80.15 sec.    training loss:\t\t3.5916528214\n",
      "Done 340 batches in 82.58 sec.    training loss:\t\t3.5918478787\n",
      "Done 350 batches in 85.21 sec.    training loss:\t\t3.59159870891\n",
      "Done 360 batches in 87.55 sec.    training loss:\t\t3.59172909701\n",
      "Done 370 batches in 90.01 sec.    training loss:\t\t3.59259419342\n",
      "Done 380 batches in 92.60 sec.    training loss:\t\t3.59250028542\n",
      "Done 390 batches in 95.06 sec.    training loss:\t\t3.59320917055\n",
      "Done 400 batches in 97.42 sec.    training loss:\t\t3.59256126719\n",
      "Done 410 batches in 99.85 sec.    training loss:\t\t3.59220927492\n",
      "Done 420 batches in 102.48 sec.    training loss:\t\t3.591633811\n",
      "Done 430 batches in 105.00 sec.    training loss:\t\t3.59176901072\n",
      "Done 440 batches in 107.60 sec.    training loss:\t\t3.59097683153\n",
      "Done 450 batches in 110.13 sec.    training loss:\t\t3.59007401759\n",
      "Done 460 batches in 112.54 sec.    training loss:\t\t3.59080234173\n",
      "Done 470 batches in 115.18 sec.    training loss:\t\t3.59113949507\n",
      "Done 480 batches in 117.57 sec.    training loss:\t\t3.59077527234\n",
      "Done 490 batches in 119.85 sec.    training loss:\t\t3.59059693093\n",
      "Done 500 batches in 122.26 sec.    training loss:\t\t3.5900006531\n",
      "Done 510 batches in 124.66 sec.    training loss:\t\t3.58980119179\n",
      "Done 520 batches in 127.20 sec.    training loss:\t\t3.59003844865\n",
      "Done 530 batches in 129.54 sec.    training loss:\t\t3.59030778269\n",
      "Done 540 batches in 131.94 sec.    training loss:\t\t3.58989738818\n",
      "Done 550 batches in 134.36 sec.    training loss:\t\t3.59043236649\n",
      "Done 560 batches in 136.89 sec.    training loss:\t\t3.59016309713\n",
      "Done 570 batches in 139.44 sec.    training loss:\t\t3.58973011729\n",
      "Done 580 batches in 141.61 sec.    training loss:\t\t3.58929419733\n",
      "Done 590 batches in 144.00 sec.    training loss:\t\t3.58883550464\n",
      "Done 600 batches in 146.48 sec.    training loss:\t\t3.58900972138\n",
      "Done 610 batches in 148.87 sec.    training loss:\t\t3.58848373605\n",
      "Done 620 batches in 151.56 sec.    training loss:\t\t3.58834120782\n",
      "Done 630 batches in 153.76 sec.    training loss:\t\t3.58777732874\n",
      "Done 640 batches in 156.08 sec.    training loss:\t\t3.58754567749\n",
      "Done 650 batches in 158.58 sec.    training loss:\t\t3.58694206284\n",
      "Done 660 batches in 160.87 sec.    training loss:\t\t3.5867803249\n",
      "Done 670 batches in 163.54 sec.    training loss:\t\t3.58738159617\n",
      "Done 680 batches in 165.88 sec.    training loss:\t\t3.58726487496\n",
      "Done 690 batches in 168.29 sec.    training loss:\t\t3.58730530524\n",
      "Done 700 batches in 170.54 sec.    training loss:\t\t3.58661902357\n",
      "Done 710 batches in 173.08 sec.    training loss:\t\t3.58674063991\n",
      "Done 720 batches in 175.67 sec.    training loss:\t\t3.5863123355\n",
      "Done 730 batches in 177.93 sec.    training loss:\t\t3.58576362009\n",
      "Done 740 batches in 180.39 sec.    training loss:\t\t3.58568074047\n",
      "Done 750 batches in 182.82 sec.    training loss:\t\t3.5856142214\n",
      "Done 760 batches in 185.24 sec.    training loss:\t\t3.58600821432\n",
      "Done 770 batches in 187.72 sec.    training loss:\t\t3.58618234934\n",
      "Done 780 batches in 190.11 sec.    training loss:\t\t3.58574002159\n",
      "Done 790 batches in 192.56 sec.    training loss:\t\t3.58539093994\n",
      "Done 800 batches in 195.09 sec.    training loss:\t\t3.58493435231\n",
      "Done 810 batches in 197.42 sec.    training loss:\t\t3.5846455834\n",
      "Done 820 batches in 199.82 sec.    training loss:\t\t3.58397406849\n",
      "Done 830 batches in 202.03 sec.    training loss:\t\t3.58391261767\n",
      "Done 840 batches in 204.40 sec.    training loss:\t\t3.58366865548\n",
      "Done 850 batches in 206.67 sec.    training loss:\t\t3.58331875487\n",
      "Done 860 batches in 209.07 sec.    training loss:\t\t3.58285393325\n",
      "Done 870 batches in 211.49 sec.    training loss:\t\t3.58267799769\n",
      "Done 880 batches in 213.78 sec.    training loss:\t\t3.58273973335\n",
      "Done 890 batches in 216.27 sec.    training loss:\t\t3.58311171095\n",
      "Done 900 batches in 218.60 sec.    training loss:\t\t3.58296522806\n",
      "Done 910 batches in 220.75 sec.    training loss:\t\t3.5825579599\n",
      "Done 920 batches in 223.03 sec.    training loss:\t\t3.58243691497\n",
      "Done 930 batches in 225.53 sec.    training loss:\t\t3.58229751374\n",
      "Done 940 batches in 227.93 sec.    training loss:\t\t3.58225505683\n",
      "Done 950 batches in 230.39 sec.    training loss:\t\t3.58208604261\n",
      "Done 960 batches in 232.90 sec.    training loss:\t\t3.58207010365\n",
      "Done 970 batches in 235.20 sec.    training loss:\t\t3.58186555526\n",
      "Done 980 batches in 237.62 sec.    training loss:\t\t3.58161693814\n",
      "Done 990 batches in 239.88 sec.    training loss:\t\t3.58105697641\n",
      "Done 1000 batches in 242.27 sec.    training loss:\t\t3.58073647439\n",
      "Done 1010 batches in 244.74 sec.    training loss:\t\t3.58050377316\n",
      "Done 1020 batches in 246.89 sec.    training loss:\t\t3.5799612446\n",
      "Done 1030 batches in 249.47 sec.    training loss:\t\t3.5797269712\n",
      "Done 1040 batches in 252.10 sec.    training loss:\t\t3.57972999584\n",
      "Done 1050 batches in 254.54 sec.    training loss:\t\t3.57948048749\n",
      "Done 1060 batches in 256.96 sec.    training loss:\t\t3.57922932096\n",
      "Done 1070 batches in 259.32 sec.    training loss:\t\t3.57899830793\n",
      "Done 1080 batches in 261.67 sec.    training loss:\t\t3.57863183884\n",
      "Done 1090 batches in 264.30 sec.    training loss:\t\t3.57892059119\n",
      "Done 1100 batches in 266.73 sec.    training loss:\t\t3.57892129944\n",
      "Done 1110 batches in 269.15 sec.    training loss:\t\t3.57887195411\n",
      "Done 1120 batches in 271.70 sec.    training loss:\t\t3.57885189887\n",
      "Done 1130 batches in 274.13 sec.    training loss:\t\t3.57866289962\n",
      "Done 1140 batches in 276.63 sec.    training loss:\t\t3.57819482022\n",
      "Done 1150 batches in 278.95 sec.    training loss:\t\t3.5782171616\n",
      "Done 1160 batches in 281.46 sec.    training loss:\t\t3.57791523941\n",
      "Done 1170 batches in 284.08 sec.    training loss:\t\t3.57766493159\n",
      "Done 1180 batches in 286.51 sec.    training loss:\t\t3.57750790484\n",
      "Done 1190 batches in 289.15 sec.    training loss:\t\t3.57732942174\n",
      "Done 1200 batches in 291.69 sec.    training loss:\t\t3.577114032\n",
      "Done 1210 batches in 294.25 sec.    training loss:\t\t3.57710317884\n",
      "Done 1220 batches in 296.61 sec.    training loss:\t\t3.57699144247\n",
      "Done 1230 batches in 299.05 sec.    training loss:\t\t3.57667161478\n",
      "Done 1240 batches in 301.56 sec.    training loss:\t\t3.57668243944\n",
      "Done 1250 batches in 304.20 sec.    training loss:\t\t3.57659000891\n",
      "Done 1260 batches in 306.64 sec.    training loss:\t\t3.57653256073\n",
      "Done 1270 batches in 309.02 sec.    training loss:\t\t3.57622263207\n",
      "Done 1280 batches in 311.48 sec.    training loss:\t\t3.57607745225\n",
      "Done 1290 batches in 313.90 sec.    training loss:\t\t3.57625305296\n",
      "Done 1300 batches in 316.37 sec.    training loss:\t\t3.57606134948\n",
      "Done 1310 batches in 318.98 sec.    training loss:\t\t3.57591644056\n",
      "Done 1320 batches in 321.20 sec.    training loss:\t\t3.57582579156\n",
      "Done 1330 batches in 323.61 sec.    training loss:\t\t3.57568168294\n",
      "Done 1340 batches in 325.96 sec.    training loss:\t\t3.57526753046\n",
      "Done 1350 batches in 328.54 sec.    training loss:\t\t3.57511130121\n",
      "Done 1360 batches in 331.15 sec.    training loss:\t\t3.57507211104\n",
      "Done 1370 batches in 333.64 sec.    training loss:\t\t3.57471142891\n",
      "Done 1380 batches in 335.94 sec.    training loss:\t\t3.57464378449\n",
      "Done 1390 batches in 338.52 sec.    training loss:\t\t3.5745868609\n",
      "Done 1400 batches in 341.01 sec.    training loss:\t\t3.57440895215\n",
      "Done 1410 batches in 343.53 sec.    training loss:\t\t3.57409704837\n",
      "Done 1420 batches in 345.74 sec.    training loss:\t\t3.5739375471\n",
      "Done 1430 batches in 348.10 sec.    training loss:\t\t3.5738095199\n",
      "Done 1440 batches in 350.58 sec.    training loss:\t\t3.57367449015\n",
      "Done 1450 batches in 353.18 sec.    training loss:\t\t3.57345578946\n",
      "Done 1460 batches in 355.55 sec.    training loss:\t\t3.57313407621\n",
      "Done 1470 batches in 357.83 sec.    training loss:\t\t3.57288710591\n",
      "Done 1480 batches in 360.48 sec.    training loss:\t\t3.57272806147\n",
      "Done 1490 batches in 363.07 sec.    training loss:\t\t3.57251486716\n",
      "Done 1500 batches in 365.63 sec.    training loss:\t\t3.57236622455\n",
      "Done 1510 batches in 368.14 sec.    training loss:\t\t3.5722548839\n",
      "Done 1520 batches in 370.60 sec.    training loss:\t\t3.57208070401\n",
      "Done 1530 batches in 373.13 sec.    training loss:\t\t3.57196143255\n",
      "Done 1540 batches in 375.61 sec.    training loss:\t\t3.57161711735\n",
      "Done 1550 batches in 377.96 sec.    training loss:\t\t3.57136537726\n",
      "Done 1560 batches in 380.49 sec.    training loss:\t\t3.57118565258\n",
      "Done 1570 batches in 382.96 sec.    training loss:\t\t3.57117132948\n",
      "Done 1580 batches in 385.21 sec.    training loss:\t\t3.57109359898\n",
      "Done 1590 batches in 387.97 sec.    training loss:\t\t3.57100645728\n",
      "Done 1600 batches in 390.29 sec.    training loss:\t\t3.57088179328\n",
      "Done 1610 batches in 392.42 sec.    training loss:\t\t3.57083461667\n",
      "Done 1620 batches in 395.04 sec.    training loss:\t\t3.57070570173\n",
      "Done 1630 batches in 397.47 sec.    training loss:\t\t3.57049388896\n",
      "Done 1640 batches in 400.24 sec.    training loss:\t\t3.57053152858\n",
      "Done 1650 batches in 402.90 sec.    training loss:\t\t3.57025085187\n",
      "Done 1660 batches in 405.34 sec.    training loss:\t\t3.57010396552\n",
      "Done 1670 batches in 407.91 sec.    training loss:\t\t3.57009749861\n",
      "Done 1680 batches in 410.54 sec.    training loss:\t\t3.57007863153\n",
      "Done 1690 batches in 413.17 sec.    training loss:\t\t3.57001881595\n",
      "Done 1700 batches in 415.40 sec.    training loss:\t\t3.56975268261\n",
      "Done 1710 batches in 417.84 sec.    training loss:\t\t3.56953481173\n",
      "Done 1720 batches in 420.20 sec.    training loss:\t\t3.56932543025\n",
      "Done 1730 batches in 422.85 sec.    training loss:\t\t3.56914326179\n",
      "Done 1740 batches in 425.20 sec.    training loss:\t\t3.56886452039\n",
      "Done 1750 batches in 427.69 sec.    training loss:\t\t3.56891829355\n",
      "Done 1760 batches in 430.23 sec.    training loss:\t\t3.5687800433\n",
      "Done 1770 batches in 432.49 sec.    training loss:\t\t3.56870041313\n",
      "Done 1780 batches in 435.11 sec.    training loss:\t\t3.56853907994\n",
      "Done 1790 batches in 437.57 sec.    training loss:\t\t3.56848835162\n",
      "Done 1800 batches in 440.05 sec.    training loss:\t\t3.56828590535\n",
      "Done 1810 batches in 442.56 sec.    training loss:\t\t3.56811400075\n",
      "Done 1820 batches in 445.27 sec.    training loss:\t\t3.56787599792\n",
      "Done 1830 batches in 447.72 sec.    training loss:\t\t3.56786451061\n",
      "Done 1840 batches in 450.01 sec.    training loss:\t\t3.56774141342\n",
      "Done 1850 batches in 452.33 sec.    training loss:\t\t3.56775274069\n",
      "Done 1860 batches in 454.50 sec.    training loss:\t\t3.56749365427\n",
      "Done 1870 batches in 456.98 sec.    training loss:\t\t3.5673776538\n",
      "Done 1880 batches in 459.24 sec.    training loss:\t\t3.56708561996\n",
      "Done 1890 batches in 461.86 sec.    training loss:\t\t3.567111486\n",
      "Done 1900 batches in 464.29 sec.    training loss:\t\t3.5669511673\n",
      "Done 1910 batches in 466.79 sec.    training loss:\t\t3.56684188688\n",
      "Done 1920 batches in 469.16 sec.    training loss:\t\t3.56660747691\n",
      "Done 1930 batches in 471.59 sec.    training loss:\t\t3.56666578426\n",
      "Done 1940 batches in 474.25 sec.    training loss:\t\t3.56665597166\n",
      "Done 1950 batches in 476.62 sec.    training loss:\t\t3.56645816035\n",
      "Done 1960 batches in 479.13 sec.    training loss:\t\t3.56622981115\n",
      "Done 1970 batches in 481.23 sec.    training loss:\t\t3.56607263724\n",
      "Done 1980 batches in 483.52 sec.    training loss:\t\t3.56583689666\n",
      "Done 1990 batches in 485.93 sec.    training loss:\t\t3.56574630932\n",
      "Done 2000 batches in 488.56 sec.    training loss:\t\t3.56551115539\n",
      "Done 2010 batches in 491.22 sec.    training loss:\t\t3.5654272162\n",
      "Done 2020 batches in 493.65 sec.    training loss:\t\t3.56508137553\n",
      "Done 2030 batches in 496.14 sec.    training loss:\t\t3.56486152041\n",
      "Done 2040 batches in 498.51 sec.    training loss:\t\t3.56481417137\n",
      "Done 2050 batches in 500.76 sec.    training loss:\t\t3.56468910446\n",
      "Done 2060 batches in 503.31 sec.    training loss:\t\t3.56459587209\n",
      "Done 2070 batches in 505.74 sec.    training loss:\t\t3.56437283489\n",
      "Done 2080 batches in 508.21 sec.    training loss:\t\t3.56420574995\n",
      "Done 2090 batches in 510.62 sec.    training loss:\t\t3.56400425067\n",
      "Done 2100 batches in 512.99 sec.    training loss:\t\t3.56384322653\n",
      "Done 2110 batches in 515.23 sec.    training loss:\t\t3.56363075705\n",
      "Done 2120 batches in 517.61 sec.    training loss:\t\t3.56352044765\n",
      "Done 2130 batches in 519.81 sec.    training loss:\t\t3.56332702299\n",
      "Done 2140 batches in 522.14 sec.    training loss:\t\t3.56318612702\n",
      "Done 2150 batches in 524.68 sec.    training loss:\t\t3.56292575446\n",
      "Done 2160 batches in 527.17 sec.    training loss:\t\t3.56280237924\n",
      "Done 2170 batches in 529.76 sec.    training loss:\t\t3.5626906785\n",
      "Done 2180 batches in 532.04 sec.    training loss:\t\t3.56267277264\n",
      "Done 2190 batches in 534.25 sec.    training loss:\t\t3.56260122769\n",
      "Done 2200 batches in 536.71 sec.    training loss:\t\t3.56239632735\n",
      "Done 2210 batches in 539.20 sec.    training loss:\t\t3.56217005326\n",
      "Done 2220 batches in 541.56 sec.    training loss:\t\t3.56201210788\n",
      "Done 2230 batches in 543.77 sec.    training loss:\t\t3.56181124109\n",
      "Done 2240 batches in 546.31 sec.    training loss:\t\t3.5616442794\n",
      "Done 2250 batches in 548.81 sec.    training loss:\t\t3.56156998372\n",
      "Done 2260 batches in 551.14 sec.    training loss:\t\t3.56147999679\n",
      "Done 2270 batches in 553.56 sec.    training loss:\t\t3.56140732992\n",
      "Done 2280 batches in 556.06 sec.    training loss:\t\t3.5612943038\n",
      "Done 2290 batches in 558.73 sec.    training loss:\t\t3.56120421648\n",
      "Done 2300 batches in 561.27 sec.    training loss:\t\t3.56114690552\n",
      "Done 2310 batches in 563.85 sec.    training loss:\t\t3.56097760494\n",
      "Done 2320 batches in 566.29 sec.    training loss:\t\t3.56079651428\n",
      "Done 2330 batches in 568.97 sec.    training loss:\t\t3.56068094542\n",
      "Done 2340 batches in 571.37 sec.    training loss:\t\t3.56052182963\n",
      "Done 2350 batches in 573.84 sec.    training loss:\t\t3.56033812803\n",
      "Done 2360 batches in 576.16 sec.    training loss:\t\t3.56031840246\n",
      "Done 2370 batches in 578.60 sec.    training loss:\t\t3.56021930074\n",
      "Done 2380 batches in 580.94 sec.    training loss:\t\t3.56008942746\n",
      "Done 2390 batches in 583.29 sec.    training loss:\t\t3.55995572294\n",
      "Done 2400 batches in 585.74 sec.    training loss:\t\t3.55987395172\n",
      "Done 2410 batches in 588.29 sec.    training loss:\t\t3.55984731604\n",
      "Done 2420 batches in 590.93 sec.    training loss:\t\t3.55987933313\n",
      "Done 2430 batches in 593.38 sec.    training loss:\t\t3.55975520015\n",
      "Done 2440 batches in 595.72 sec.    training loss:\t\t3.55961582535\n",
      "Done 2450 batches in 598.13 sec.    training loss:\t\t3.55941466556\n",
      "Done 2460 batches in 600.53 sec.    training loss:\t\t3.55931620878\n",
      "Done 2470 batches in 603.01 sec.    training loss:\t\t3.5592739623\n",
      "Done 2480 batches in 605.57 sec.    training loss:\t\t3.55910849009\n",
      "Done 2490 batches in 608.20 sec.    training loss:\t\t3.55899603182\n",
      "Done 2500 batches in 610.69 sec.    training loss:\t\t3.55888554688\n",
      "Done 2510 batches in 613.05 sec.    training loss:\t\t3.55875522768\n",
      "Done 2520 batches in 615.42 sec.    training loss:\t\t3.55856590388\n",
      "Done 2530 batches in 618.15 sec.    training loss:\t\t3.55842987122\n",
      "Done 2540 batches in 620.55 sec.    training loss:\t\t3.55825863319\n",
      "Done 2550 batches in 623.12 sec.    training loss:\t\t3.5581823331\n",
      "Done 2560 batches in 625.62 sec.    training loss:\t\t3.55799901718\n",
      "Done 2570 batches in 628.09 sec.    training loss:\t\t3.5577847145\n",
      "Done 2580 batches in 630.79 sec.    training loss:\t\t3.55766423675\n",
      "Done 2590 batches in 633.16 sec.    training loss:\t\t3.55752087493\n",
      "Done 2600 batches in 635.64 sec.    training loss:\t\t3.5574215294\n",
      "Done 2610 batches in 638.22 sec.    training loss:\t\t3.55739288492\n",
      "Done 2620 batches in 640.84 sec.    training loss:\t\t3.55720941211\n",
      "Done 2630 batches in 643.27 sec.    training loss:\t\t3.55713747766\n",
      "Done 2640 batches in 645.97 sec.    training loss:\t\t3.55704831656\n",
      "Done 2650 batches in 648.42 sec.    training loss:\t\t3.55692236692\n",
      "Done 2660 batches in 650.78 sec.    training loss:\t\t3.55681644564\n",
      "Done 2670 batches in 653.04 sec.    training loss:\t\t3.55661511208\n",
      "Done 2680 batches in 655.70 sec.    training loss:\t\t3.55649265304\n",
      "Done 2690 batches in 658.16 sec.    training loss:\t\t3.55636730949\n",
      "Done 2700 batches in 660.66 sec.    training loss:\t\t3.55626377669\n",
      "Done 2710 batches in 662.89 sec.    training loss:\t\t3.55612829965\n",
      "Done 2720 batches in 665.25 sec.    training loss:\t\t3.55607549344\n",
      "Done 2730 batches in 667.58 sec.    training loss:\t\t3.55609267747\n",
      "Done 2740 batches in 669.93 sec.    training loss:\t\t3.5559436248\n",
      "Done 2750 batches in 672.51 sec.    training loss:\t\t3.55587592613\n",
      "Done 2760 batches in 675.00 sec.    training loss:\t\t3.55589046419\n",
      "Done 2770 batches in 677.72 sec.    training loss:\t\t3.55579407574\n",
      "Done 2780 batches in 679.85 sec.    training loss:\t\t3.55567008331\n",
      "Done 2790 batches in 682.35 sec.    training loss:\t\t3.55544797105\n",
      "Done 2800 batches in 684.67 sec.    training loss:\t\t3.55538888843\n",
      "Done 2810 batches in 687.04 sec.    training loss:\t\t3.55533399325\n",
      "Done 2820 batches in 689.41 sec.    training loss:\t\t3.55522210669\n",
      "Done 2830 batches in 692.22 sec.    training loss:\t\t3.55510035908\n",
      "Done 2840 batches in 694.70 sec.    training loss:\t\t3.5549126136\n",
      "Done 2850 batches in 697.09 sec.    training loss:\t\t3.5547693719\n",
      "Done 2860 batches in 699.82 sec.    training loss:\t\t3.55461563338\n",
      "Done 2870 batches in 702.39 sec.    training loss:\t\t3.55449610063\n",
      "Done 2880 batches in 704.68 sec.    training loss:\t\t3.55440695622\n",
      "Done 2890 batches in 707.23 sec.    training loss:\t\t3.55428662524\n",
      "Done 2900 batches in 709.76 sec.    training loss:\t\t3.55405393982\n",
      "Done 2910 batches in 711.92 sec.    training loss:\t\t3.55396674137\n",
      "Done 2920 batches in 714.36 sec.    training loss:\t\t3.55382687214\n",
      "Done 2930 batches in 716.71 sec.    training loss:\t\t3.55368719353\n",
      "Done 2940 batches in 719.17 sec.    training loss:\t\t3.55351957253\n",
      "Done 2950 batches in 721.70 sec.    training loss:\t\t3.55336086769\n",
      "Done 2960 batches in 724.27 sec.    training loss:\t\t3.55329496425\n",
      "Done 2970 batches in 726.56 sec.    training loss:\t\t3.55318637393\n",
      "Done 2980 batches in 728.93 sec.    training loss:\t\t3.5531306284\n",
      "Done 2990 batches in 731.39 sec.    training loss:\t\t3.55295569501\n",
      "Done 3000 batches in 733.97 sec.    training loss:\t\t3.55283676724\n",
      "Done 3010 batches in 736.31 sec.    training loss:\t\t3.55273657284\n",
      "Done 3020 batches in 738.51 sec.    training loss:\t\t3.55273825025\n",
      "Done 3030 batches in 741.00 sec.    training loss:\t\t3.5525998617\n",
      "Done 3040 batches in 743.57 sec.    training loss:\t\t3.55247577099\n",
      "Done 3050 batches in 746.19 sec.    training loss:\t\t3.55230976948\n",
      "Done 3060 batches in 748.60 sec.    training loss:\t\t3.5521817432\n",
      "Done 3070 batches in 751.08 sec.    training loss:\t\t3.55203313528\n",
      "Done 3080 batches in 753.51 sec.    training loss:\t\t3.55191896948\n",
      "Done 3090 batches in 755.97 sec.    training loss:\t\t3.55176572415\n",
      "Done 3100 batches in 758.29 sec.    training loss:\t\t3.5516809885\n",
      "Done 3110 batches in 760.73 sec.    training loss:\t\t3.55169284633\n",
      "Done 3120 batches in 763.23 sec.    training loss:\t\t3.55152015497\n",
      "Done 3130 batches in 765.83 sec.    training loss:\t\t3.55136129529\n",
      "Done 3140 batches in 768.12 sec.    training loss:\t\t3.55125685689\n",
      "Done 3150 batches in 770.22 sec.    training loss:\t\t3.55116090993\n",
      "Done 3160 batches in 772.70 sec.    training loss:\t\t3.55108420355\n",
      "Done 3170 batches in 775.12 sec.    training loss:\t\t3.55082573511\n",
      "Done 3180 batches in 777.67 sec.    training loss:\t\t3.55071965734\n",
      "Done 3190 batches in 780.09 sec.    training loss:\t\t3.55063028968\n",
      "Done 3200 batches in 782.51 sec.    training loss:\t\t3.55051560019\n",
      "Done 3210 batches in 784.91 sec.    training loss:\t\t3.55050840109\n",
      "Done 3220 batches in 787.40 sec.    training loss:\t\t3.55040132971\n",
      "Done 3230 batches in 789.97 sec.    training loss:\t\t3.55023147306\n",
      "Done 3240 batches in 792.45 sec.    training loss:\t\t3.55021666485\n",
      "Done 3250 batches in 794.89 sec.    training loss:\t\t3.55011632039\n",
      "Done 3260 batches in 797.22 sec.    training loss:\t\t3.5499927582\n",
      "Done 3270 batches in 799.76 sec.    training loss:\t\t3.54981720675\n",
      "Done 3280 batches in 802.43 sec.    training loss:\t\t3.54966417091\n",
      "Done 3290 batches in 804.70 sec.    training loss:\t\t3.54946283335\n",
      "Done 3300 batches in 807.25 sec.    training loss:\t\t3.54932425752\n",
      "Done 3310 batches in 809.57 sec.    training loss:\t\t3.54920688473\n",
      "Done 3320 batches in 811.83 sec.    training loss:\t\t3.54916819774\n",
      "Done 3330 batches in 814.41 sec.    training loss:\t\t3.54902906942\n",
      "Done 3340 batches in 816.85 sec.    training loss:\t\t3.54892251344\n",
      "Done 3350 batches in 819.34 sec.    training loss:\t\t3.54889574108\n",
      "Done 3360 batches in 821.75 sec.    training loss:\t\t3.548776851\n",
      "Done 3370 batches in 824.22 sec.    training loss:\t\t3.54863111165\n",
      "Done 3380 batches in 826.63 sec.    training loss:\t\t3.54849348061\n",
      "Done 3390 batches in 828.95 sec.    training loss:\t\t3.54837616143\n",
      "Done 3400 batches in 831.43 sec.    training loss:\t\t3.54821203416\n",
      "Done 3410 batches in 833.92 sec.    training loss:\t\t3.54809714561\n",
      "Done 3420 batches in 836.12 sec.    training loss:\t\t3.54801162559\n",
      "Done 3430 batches in 838.42 sec.    training loss:\t\t3.5479015151\n",
      "Done 3440 batches in 840.84 sec.    training loss:\t\t3.5478474837\n",
      "Done 3450 batches in 843.12 sec.    training loss:\t\t3.54777106126\n",
      "Done 3460 batches in 845.70 sec.    training loss:\t\t3.54756414362\n",
      "Done 3470 batches in 848.13 sec.    training loss:\t\t3.54744816439\n",
      "Done 3480 batches in 850.60 sec.    training loss:\t\t3.54733429129\n",
      "Done 3490 batches in 853.11 sec.    training loss:\t\t3.54724870821\n",
      "Done 3500 batches in 855.70 sec.    training loss:\t\t3.5471378377\n",
      "Done 3510 batches in 858.14 sec.    training loss:\t\t3.54704759179\n",
      "Done 3520 batches in 860.55 sec.    training loss:\t\t3.54698814124\n",
      "Done 3530 batches in 862.83 sec.    training loss:\t\t3.54689011132\n",
      "Done 3540 batches in 865.29 sec.    training loss:\t\t3.54674966999\n",
      "Done 3550 batches in 867.55 sec.    training loss:\t\t3.54653549888\n",
      "Done 3560 batches in 870.20 sec.    training loss:\t\t3.54642449359\n",
      "Done 3570 batches in 872.74 sec.    training loss:\t\t3.54638650814\n",
      "Done 3580 batches in 875.23 sec.    training loss:\t\t3.54621064265\n",
      "Done 3590 batches in 877.80 sec.    training loss:\t\t3.54609867672\n",
      "Done 3600 batches in 880.48 sec.    training loss:\t\t3.54605593864\n",
      "Done 3610 batches in 883.06 sec.    training loss:\t\t3.54602812077\n",
      "Done 3620 batches in 885.27 sec.    training loss:\t\t3.5458171221\n",
      "Done 3630 batches in 887.53 sec.    training loss:\t\t3.5456945911\n",
      "Done 3640 batches in 890.03 sec.    training loss:\t\t3.5456068877\n",
      "Done 3650 batches in 892.51 sec.    training loss:\t\t3.54546550468\n",
      "Done 3660 batches in 894.84 sec.    training loss:\t\t3.54538110324\n",
      "Done 3670 batches in 897.13 sec.    training loss:\t\t3.54525304819\n",
      "Done 3680 batches in 899.43 sec.    training loss:\t\t3.54508658658\n",
      "Done 3690 batches in 901.76 sec.    training loss:\t\t3.54495054689\n",
      "Done 3700 batches in 904.27 sec.    training loss:\t\t3.54486955919\n",
      "Done 3710 batches in 906.55 sec.    training loss:\t\t3.54472414641\n",
      "Done 3720 batches in 908.93 sec.    training loss:\t\t3.54465870589\n",
      "Done 3730 batches in 911.26 sec.    training loss:\t\t3.54451129207\n",
      "Done 3740 batches in 913.74 sec.    training loss:\t\t3.54439163888\n",
      "Done 3750 batches in 916.12 sec.    training loss:\t\t3.54431258832\n",
      "Done 3760 batches in 918.68 sec.    training loss:\t\t3.54421182318\n",
      "Done 3770 batches in 921.02 sec.    training loss:\t\t3.54417540544\n",
      "Done 3780 batches in 923.39 sec.    training loss:\t\t3.54405599982\n",
      "Done 3790 batches in 925.91 sec.    training loss:\t\t3.5439824396\n",
      "Done 3800 batches in 928.41 sec.    training loss:\t\t3.54387286157\n",
      "Done 3810 batches in 930.92 sec.    training loss:\t\t3.5437410619\n",
      "Done 3820 batches in 933.49 sec.    training loss:\t\t3.54367130152\n",
      "Done 3830 batches in 935.78 sec.    training loss:\t\t3.54347732986\n",
      "Done 3840 batches in 938.31 sec.    training loss:\t\t3.54333305561\n",
      "Done 3850 batches in 941.06 sec.    training loss:\t\t3.54322995401\n",
      "Done 3860 batches in 943.60 sec.    training loss:\t\t3.54315775003\n",
      "Done 3870 batches in 946.12 sec.    training loss:\t\t3.54303605925\n",
      "Done 3880 batches in 948.57 sec.    training loss:\t\t3.5429331397\n",
      "Done 3890 batches in 950.85 sec.    training loss:\t\t3.54292012716\n",
      "Done 100 batches in 7.29 sec.\n",
      "Done 200 batches in 14.50 sec.\n",
      "Done 300 batches in 21.56 sec.\n",
      "Done 400 batches in 28.68 sec.\n",
      "Done 500 batches in 36.27 sec.\n",
      "Done 600 batches in 43.33 sec.\n",
      "Done 700 batches in 50.74 sec.\n",
      "Done 800 batches in 58.00 sec.\n",
      "Done 900 batches in 65.46 sec.\n",
      "Epoch 2 of 5 took 1024.766s\n",
      "  training loss:\t\t3.542927\n",
      "  validation loss:\t\t3.572682\n",
      "Done 10 batches in 2.42 sec.    training loss:\t\t3.48007408215\n",
      "Done 20 batches in 4.91 sec.    training loss:\t\t3.49030861087\n",
      "Done 30 batches in 7.21 sec.    training loss:\t\t3.4963619919\n",
      "Done 40 batches in 9.59 sec.    training loss:\t\t3.50210883483\n",
      "Done 50 batches in 12.30 sec.    training loss:\t\t3.50602126868\n",
      "Done 60 batches in 14.65 sec.    training loss:\t\t3.50309290106\n",
      "Done 70 batches in 17.07 sec.    training loss:\t\t3.50452577274\n",
      "Done 80 batches in 19.41 sec.    training loss:\t\t3.51303777911\n",
      "Done 90 batches in 22.07 sec.    training loss:\t\t3.50860222014\n",
      "Done 100 batches in 24.36 sec.    training loss:\t\t3.50737657082\n",
      "Done 110 batches in 26.86 sec.    training loss:\t\t3.50579571041\n",
      "Done 120 batches in 29.45 sec.    training loss:\t\t3.50569833171\n",
      "Done 130 batches in 31.87 sec.    training loss:\t\t3.5032060394\n",
      "Done 140 batches in 34.36 sec.    training loss:\t\t3.501418286\n",
      "Done 150 batches in 36.61 sec.    training loss:\t\t3.49974791822\n",
      "Done 160 batches in 38.97 sec.    training loss:\t\t3.49999517907\n",
      "Done 170 batches in 41.36 sec.    training loss:\t\t3.50105902923\n",
      "Done 180 batches in 43.85 sec.    training loss:\t\t3.4995241995\n",
      "Done 190 batches in 46.25 sec.    training loss:\t\t3.4998035459\n",
      "Done 200 batches in 49.08 sec.    training loss:\t\t3.49924335208\n",
      "Done 210 batches in 51.58 sec.    training loss:\t\t3.49868596405\n",
      "Done 220 batches in 54.02 sec.    training loss:\t\t3.4988407908\n",
      "Done 230 batches in 56.41 sec.    training loss:\t\t3.49829268665\n",
      "Done 240 batches in 58.67 sec.    training loss:\t\t3.49751696142\n",
      "Done 250 batches in 61.32 sec.    training loss:\t\t3.49967656864\n",
      "Done 260 batches in 63.75 sec.    training loss:\t\t3.49888770669\n",
      "Done 270 batches in 66.10 sec.    training loss:\t\t3.49923698557\n",
      "Done 280 batches in 68.52 sec.    training loss:\t\t3.49855977716\n",
      "Done 290 batches in 70.96 sec.    training loss:\t\t3.49938862706\n",
      "Done 300 batches in 73.25 sec.    training loss:\t\t3.49834092916\n",
      "Done 310 batches in 75.52 sec.    training loss:\t\t3.49731554236\n",
      "Done 320 batches in 77.93 sec.    training loss:\t\t3.49695294349\n",
      "Done 330 batches in 80.43 sec.    training loss:\t\t3.49716884905\n",
      "Done 340 batches in 82.90 sec.    training loss:\t\t3.49717627078\n",
      "Done 350 batches in 85.56 sec.    training loss:\t\t3.49744758842\n",
      "Done 360 batches in 87.92 sec.    training loss:\t\t3.49750047687\n",
      "Done 370 batches in 90.40 sec.    training loss:\t\t3.4974537724\n",
      "Done 380 batches in 92.97 sec.    training loss:\t\t3.49743800987\n",
      "Done 390 batches in 95.42 sec.    training loss:\t\t3.49734866394\n",
      "Done 400 batches in 97.77 sec.    training loss:\t\t3.49753911343\n",
      "Done 410 batches in 100.19 sec.    training loss:\t\t3.49738029865\n",
      "Done 420 batches in 102.80 sec.    training loss:\t\t3.49659587544\n",
      "Done 430 batches in 105.31 sec.    training loss:\t\t3.49659042789\n",
      "Done 440 batches in 107.91 sec.    training loss:\t\t3.4957470682\n",
      "Done 450 batches in 110.44 sec.    training loss:\t\t3.49517986934\n",
      "Done 460 batches in 112.84 sec.    training loss:\t\t3.49568047476\n",
      "Done 470 batches in 115.48 sec.    training loss:\t\t3.49593820201\n",
      "Done 480 batches in 117.88 sec.    training loss:\t\t3.49577341243\n",
      "Done 490 batches in 120.17 sec.    training loss:\t\t3.49565585396\n",
      "Done 500 batches in 122.59 sec.    training loss:\t\t3.49489178561\n",
      "Done 510 batches in 125.01 sec.    training loss:\t\t3.49460843813\n",
      "Done 520 batches in 127.56 sec.    training loss:\t\t3.49460752942\n",
      "Done 530 batches in 129.91 sec.    training loss:\t\t3.4945179637\n",
      "Done 540 batches in 132.33 sec.    training loss:\t\t3.49419786775\n",
      "Done 550 batches in 134.79 sec.    training loss:\t\t3.49400347148\n",
      "Done 560 batches in 137.32 sec.    training loss:\t\t3.49440023002\n",
      "Done 570 batches in 139.87 sec.    training loss:\t\t3.49476677202\n",
      "Done 580 batches in 142.09 sec.    training loss:\t\t3.49461098067\n",
      "Done 590 batches in 144.49 sec.    training loss:\t\t3.49425699198\n",
      "Done 600 batches in 146.99 sec.    training loss:\t\t3.49463412137\n",
      "Done 610 batches in 149.38 sec.    training loss:\t\t3.49417126659\n",
      "Done 620 batches in 152.09 sec.    training loss:\t\t3.49369531433\n",
      "Done 630 batches in 154.30 sec.    training loss:\t\t3.49316630928\n",
      "Done 640 batches in 156.64 sec.    training loss:\t\t3.49285007954\n",
      "Done 650 batches in 159.16 sec.    training loss:\t\t3.49236163039\n",
      "Done 660 batches in 161.45 sec.    training loss:\t\t3.49233519068\n",
      "Done 670 batches in 164.14 sec.    training loss:\t\t3.49285026787\n",
      "Done 680 batches in 166.51 sec.    training loss:\t\t3.49281645725\n",
      "Done 690 batches in 168.94 sec.    training loss:\t\t3.492511865\n",
      "Done 700 batches in 171.18 sec.    training loss:\t\t3.49242573997\n",
      "Done 710 batches in 173.73 sec.    training loss:\t\t3.49248681449\n",
      "Done 720 batches in 176.34 sec.    training loss:\t\t3.49218553639\n",
      "Done 730 batches in 178.62 sec.    training loss:\t\t3.49179149308\n",
      "Done 740 batches in 181.08 sec.    training loss:\t\t3.49159572129\n",
      "Done 750 batches in 183.52 sec.    training loss:\t\t3.49128110009\n",
      "Done 760 batches in 185.94 sec.    training loss:\t\t3.49129787517\n",
      "Done 770 batches in 188.44 sec.    training loss:\t\t3.49148052913\n",
      "Done 780 batches in 190.84 sec.    training loss:\t\t3.49115588086\n",
      "Done 790 batches in 193.31 sec.    training loss:\t\t3.49088005853\n",
      "Done 800 batches in 195.86 sec.    training loss:\t\t3.49062550207\n",
      "Done 810 batches in 198.20 sec.    training loss:\t\t3.49044524216\n",
      "Done 820 batches in 200.62 sec.    training loss:\t\t3.48995895485\n",
      "Done 830 batches in 202.87 sec.    training loss:\t\t3.49063434582\n",
      "Done 840 batches in 205.26 sec.    training loss:\t\t3.49061822261\n",
      "Done 850 batches in 207.56 sec.    training loss:\t\t3.49029144149\n",
      "Done 860 batches in 209.98 sec.    training loss:\t\t3.49023060745\n",
      "Done 870 batches in 212.40 sec.    training loss:\t\t3.49012462942\n",
      "Done 880 batches in 214.71 sec.    training loss:\t\t3.48992706035\n",
      "Done 890 batches in 217.21 sec.    training loss:\t\t3.49025144987\n",
      "Done 900 batches in 219.58 sec.    training loss:\t\t3.49041669025\n",
      "Done 910 batches in 221.74 sec.    training loss:\t\t3.48985242623\n",
      "Done 920 batches in 224.05 sec.    training loss:\t\t3.4896454927\n",
      "Done 930 batches in 226.58 sec.    training loss:\t\t3.48950430483\n",
      "Done 940 batches in 229.00 sec.    training loss:\t\t3.48930322155\n",
      "Done 950 batches in 231.46 sec.    training loss:\t\t3.48886590031\n",
      "Done 960 batches in 233.97 sec.    training loss:\t\t3.48863902177\n",
      "Done 970 batches in 236.27 sec.    training loss:\t\t3.4886952387\n",
      "Done 980 batches in 238.70 sec.    training loss:\t\t3.48863177008\n",
      "Done 990 batches in 240.97 sec.    training loss:\t\t3.4882827587\n",
      "Done 1000 batches in 243.39 sec.    training loss:\t\t3.48823462096\n",
      "Done 1010 batches in 245.90 sec.    training loss:\t\t3.48819718827\n",
      "Done 1020 batches in 248.06 sec.    training loss:\t\t3.48779732781\n",
      "Done 1030 batches in 250.64 sec.    training loss:\t\t3.48757796225\n",
      "Done 1040 batches in 253.27 sec.    training loss:\t\t3.48774061405\n",
      "Done 1050 batches in 255.71 sec.    training loss:\t\t3.48766357593\n",
      "Done 1060 batches in 258.15 sec.    training loss:\t\t3.48775495907\n",
      "Done 1070 batches in 260.52 sec.    training loss:\t\t3.48740718851\n",
      "Done 1080 batches in 262.89 sec.    training loss:\t\t3.48721617044\n",
      "Done 1090 batches in 265.54 sec.    training loss:\t\t3.48740067177\n",
      "Done 1100 batches in 267.98 sec.    training loss:\t\t3.48726579438\n",
      "Done 1110 batches in 270.43 sec.    training loss:\t\t3.48704137143\n",
      "Done 1120 batches in 273.00 sec.    training loss:\t\t3.48687266233\n",
      "Done 1130 batches in 275.44 sec.    training loss:\t\t3.48672834044\n",
      "Done 1140 batches in 277.95 sec.    training loss:\t\t3.48632251484\n",
      "Done 1150 batches in 280.27 sec.    training loss:\t\t3.48642118434\n",
      "Done 1160 batches in 282.80 sec.    training loss:\t\t3.48627674636\n",
      "Done 1170 batches in 285.43 sec.    training loss:\t\t3.48636302064\n",
      "Done 1180 batches in 287.89 sec.    training loss:\t\t3.48628812414\n",
      "Done 1190 batches in 290.56 sec.    training loss:\t\t3.48622349799\n",
      "Done 1200 batches in 293.14 sec.    training loss:\t\t3.48598456161\n",
      "Done 1210 batches in 295.71 sec.    training loss:\t\t3.48583997006\n",
      "Done 1220 batches in 298.09 sec.    training loss:\t\t3.48568731385\n",
      "Done 1230 batches in 300.55 sec.    training loss:\t\t3.48550064358\n",
      "Done 1240 batches in 303.08 sec.    training loss:\t\t3.48546592038\n",
      "Done 1250 batches in 305.74 sec.    training loss:\t\t3.48525223055\n",
      "Done 1260 batches in 308.20 sec.    training loss:\t\t3.48528938663\n",
      "Done 1270 batches in 310.59 sec.    training loss:\t\t3.48527624393\n",
      "Done 1280 batches in 313.08 sec.    training loss:\t\t3.48518291332\n",
      "Done 1290 batches in 315.52 sec.    training loss:\t\t3.48510583507\n",
      "Done 1300 batches in 318.01 sec.    training loss:\t\t3.48514123146\n",
      "Done 1310 batches in 320.63 sec.    training loss:\t\t3.48506460183\n",
      "Done 1320 batches in 322.87 sec.    training loss:\t\t3.48501182895\n",
      "Done 1330 batches in 325.31 sec.    training loss:\t\t3.48509555511\n",
      "Done 1340 batches in 327.70 sec.    training loss:\t\t3.48480519516\n",
      "Done 1350 batches in 330.31 sec.    training loss:\t\t3.4845221195\n",
      "Done 1360 batches in 332.94 sec.    training loss:\t\t3.48432231225\n",
      "Done 1370 batches in 335.46 sec.    training loss:\t\t3.48402739241\n",
      "Done 1380 batches in 337.78 sec.    training loss:\t\t3.48395633194\n",
      "Done 1390 batches in 340.40 sec.    training loss:\t\t3.48379410901\n",
      "Done 1400 batches in 342.90 sec.    training loss:\t\t3.48352062153\n",
      "Done 1410 batches in 345.42 sec.    training loss:\t\t3.48330343075\n",
      "Done 1420 batches in 347.64 sec.    training loss:\t\t3.48320030813\n",
      "Done 1430 batches in 350.04 sec.    training loss:\t\t3.48334621772\n",
      "Done 1440 batches in 352.54 sec.    training loss:\t\t3.48328370891\n",
      "Done 1450 batches in 355.17 sec.    training loss:\t\t3.48336473074\n",
      "Done 1460 batches in 357.53 sec.    training loss:\t\t3.48318931957\n",
      "Done 1470 batches in 359.83 sec.    training loss:\t\t3.48329538761\n",
      "Done 1480 batches in 362.52 sec.    training loss:\t\t3.48344148499\n",
      "Done 1490 batches in 365.13 sec.    training loss:\t\t3.48328346383\n",
      "Done 1500 batches in 367.69 sec.    training loss:\t\t3.4832189304\n",
      "Done 1510 batches in 370.23 sec.    training loss:\t\t3.48309206656\n",
      "Done 1520 batches in 372.71 sec.    training loss:\t\t3.48314597214\n",
      "Done 1530 batches in 375.26 sec.    training loss:\t\t3.48312780453\n",
      "Done 1540 batches in 377.74 sec.    training loss:\t\t3.48296553595\n",
      "Done 1550 batches in 380.12 sec.    training loss:\t\t3.4828586441\n",
      "Done 1560 batches in 382.68 sec.    training loss:\t\t3.48268325536\n",
      "Done 1570 batches in 385.16 sec.    training loss:\t\t3.48273559356\n",
      "Done 1580 batches in 387.44 sec.    training loss:\t\t3.48253396886\n",
      "Done 1590 batches in 390.23 sec.    training loss:\t\t3.48244114166\n",
      "Done 1600 batches in 392.57 sec.    training loss:\t\t3.48229468455\n",
      "Done 1610 batches in 394.73 sec.    training loss:\t\t3.48238538449\n",
      "Done 1620 batches in 397.35 sec.    training loss:\t\t3.48226728263\n",
      "Done 1630 batches in 399.81 sec.    training loss:\t\t3.48210810687\n",
      "Done 1640 batches in 402.58 sec.    training loss:\t\t3.48208248165\n",
      "Done 1650 batches in 405.28 sec.    training loss:\t\t3.4819434358\n",
      "Done 1660 batches in 407.73 sec.    training loss:\t\t3.48181999228\n",
      "Done 1670 batches in 410.34 sec.    training loss:\t\t3.48157406873\n",
      "Done 1680 batches in 412.98 sec.    training loss:\t\t3.48154399952\n",
      "Done 1690 batches in 415.62 sec.    training loss:\t\t3.48150816611\n",
      "Done 1700 batches in 417.87 sec.    training loss:\t\t3.4814207291\n",
      "Done 1710 batches in 420.34 sec.    training loss:\t\t3.48128458261\n",
      "Done 1720 batches in 422.73 sec.    training loss:\t\t3.4811066711\n",
      "Done 1730 batches in 425.40 sec.    training loss:\t\t3.48126009931\n",
      "Done 1740 batches in 427.78 sec.    training loss:\t\t3.48128770864\n",
      "Done 1750 batches in 430.29 sec.    training loss:\t\t3.48133649785\n",
      "Done 1760 batches in 432.84 sec.    training loss:\t\t3.48126140965\n",
      "Done 1770 batches in 435.12 sec.    training loss:\t\t3.48118820827\n",
      "Done 1780 batches in 437.77 sec.    training loss:\t\t3.48120263048\n",
      "Done 1790 batches in 440.25 sec.    training loss:\t\t3.48121242159\n",
      "Done 1800 batches in 442.74 sec.    training loss:\t\t3.48114922234\n",
      "Done 1810 batches in 445.28 sec.    training loss:\t\t3.48109435782\n",
      "Done 1820 batches in 448.02 sec.    training loss:\t\t3.48096935918\n",
      "Done 1830 batches in 450.51 sec.    training loss:\t\t3.4808589287\n",
      "Done 1840 batches in 452.81 sec.    training loss:\t\t3.48072457144\n",
      "Done 1850 batches in 455.13 sec.    training loss:\t\t3.48065407465\n",
      "Done 1860 batches in 457.29 sec.    training loss:\t\t3.48045264889\n",
      "Done 1870 batches in 459.79 sec.    training loss:\t\t3.48039122729\n",
      "Done 1880 batches in 462.07 sec.    training loss:\t\t3.48016890303\n",
      "Done 1890 batches in 464.71 sec.    training loss:\t\t3.48023145923\n",
      "Done 1900 batches in 467.17 sec.    training loss:\t\t3.48015329662\n",
      "Done 1910 batches in 469.69 sec.    training loss:\t\t3.4800802708\n",
      "Done 1920 batches in 472.10 sec.    training loss:\t\t3.479903395\n",
      "Done 1930 batches in 474.55 sec.    training loss:\t\t3.4798491231\n",
      "Done 1940 batches in 477.23 sec.    training loss:\t\t3.47981995971\n",
      "Done 1950 batches in 479.62 sec.    training loss:\t\t3.47967887934\n",
      "Done 1960 batches in 482.14 sec.    training loss:\t\t3.47964645529\n",
      "Done 1970 batches in 484.26 sec.    training loss:\t\t3.47956564908\n",
      "Done 1980 batches in 486.56 sec.    training loss:\t\t3.47939779202\n",
      "Done 1990 batches in 488.99 sec.    training loss:\t\t3.47940219683\n",
      "Done 2000 batches in 491.63 sec.    training loss:\t\t3.47924898468\n",
      "Done 2010 batches in 494.32 sec.    training loss:\t\t3.47911679941\n",
      "Done 2020 batches in 496.76 sec.    training loss:\t\t3.47890564455\n",
      "Done 2030 batches in 499.28 sec.    training loss:\t\t3.4787423214\n",
      "Done 2040 batches in 501.68 sec.    training loss:\t\t3.47866275513\n",
      "Done 2050 batches in 503.94 sec.    training loss:\t\t3.47860956192\n",
      "Done 2060 batches in 506.49 sec.    training loss:\t\t3.47857767622\n",
      "Done 2070 batches in 508.91 sec.    training loss:\t\t3.47839770116\n",
      "Done 2080 batches in 511.41 sec.    training loss:\t\t3.4783657861\n",
      "Done 2090 batches in 513.82 sec.    training loss:\t\t3.47824404481\n",
      "Done 2100 batches in 516.20 sec.    training loss:\t\t3.4781441151\n",
      "Done 2110 batches in 518.46 sec.    training loss:\t\t3.47800524389\n",
      "Done 2120 batches in 520.87 sec.    training loss:\t\t3.47798598918\n",
      "Done 2130 batches in 523.09 sec.    training loss:\t\t3.47787729656\n",
      "Done 2140 batches in 525.43 sec.    training loss:\t\t3.47778742067\n",
      "Done 2150 batches in 527.98 sec.    training loss:\t\t3.47773029955\n",
      "Done 2160 batches in 530.49 sec.    training loss:\t\t3.47765172507\n",
      "Done 2170 batches in 533.09 sec.    training loss:\t\t3.47770207479\n",
      "Done 2180 batches in 535.37 sec.    training loss:\t\t3.4776675145\n",
      "Done 2190 batches in 537.58 sec.    training loss:\t\t3.47771281523\n",
      "Done 2200 batches in 540.07 sec.    training loss:\t\t3.47754123802\n",
      "Done 2210 batches in 542.58 sec.    training loss:\t\t3.47756587433\n",
      "Done 2220 batches in 544.95 sec.    training loss:\t\t3.47747134404\n",
      "Done 2230 batches in 547.17 sec.    training loss:\t\t3.47731739308\n",
      "Done 2240 batches in 549.74 sec.    training loss:\t\t3.47721643684\n",
      "Done 2250 batches in 552.25 sec.    training loss:\t\t3.47712485734\n",
      "Done 2260 batches in 554.57 sec.    training loss:\t\t3.47697540779\n",
      "Done 2270 batches in 557.01 sec.    training loss:\t\t3.47692829357\n",
      "Done 2280 batches in 559.55 sec.    training loss:\t\t3.47680587257\n",
      "Done 2290 batches in 562.22 sec.    training loss:\t\t3.4767700611\n",
      "Done 2300 batches in 564.77 sec.    training loss:\t\t3.47675209868\n",
      "Done 2310 batches in 567.35 sec.    training loss:\t\t3.47670602939\n",
      "Done 2320 batches in 569.83 sec.    training loss:\t\t3.47660261015\n",
      "Done 2330 batches in 572.52 sec.    training loss:\t\t3.47659565282\n",
      "Done 2340 batches in 574.94 sec.    training loss:\t\t3.47646589916\n",
      "Done 2350 batches in 577.41 sec.    training loss:\t\t3.47637170452\n",
      "Done 2360 batches in 579.75 sec.    training loss:\t\t3.47626972848\n",
      "Done 2370 batches in 582.19 sec.    training loss:\t\t3.47629346652\n",
      "Done 2380 batches in 584.54 sec.    training loss:\t\t3.47622406599\n",
      "Done 2390 batches in 586.91 sec.    training loss:\t\t3.4761659248\n",
      "Done 2400 batches in 589.37 sec.    training loss:\t\t3.47615948922\n",
      "Done 2410 batches in 591.93 sec.    training loss:\t\t3.47609157015\n",
      "Done 2420 batches in 594.58 sec.    training loss:\t\t3.47612839213\n",
      "Done 2430 batches in 597.03 sec.    training loss:\t\t3.47607641372\n",
      "Done 2440 batches in 599.39 sec.    training loss:\t\t3.47617655928\n",
      "Done 2450 batches in 601.81 sec.    training loss:\t\t3.47623269942\n",
      "Done 2460 batches in 604.23 sec.    training loss:\t\t3.47624949566\n",
      "Done 2470 batches in 606.74 sec.    training loss:\t\t3.47624548535\n",
      "Done 2480 batches in 609.29 sec.    training loss:\t\t3.47617213927\n",
      "Done 2490 batches in 611.92 sec.    training loss:\t\t3.47608248234\n",
      "Done 2500 batches in 614.44 sec.    training loss:\t\t3.47606236304\n",
      "Done 2510 batches in 616.82 sec.    training loss:\t\t3.47602612439\n",
      "Done 2520 batches in 619.21 sec.    training loss:\t\t3.47591641083\n",
      "Done 2530 batches in 621.99 sec.    training loss:\t\t3.4758283191\n",
      "Done 2540 batches in 624.41 sec.    training loss:\t\t3.47590174077\n",
      "Done 2550 batches in 626.99 sec.    training loss:\t\t3.47589801549\n",
      "Done 2560 batches in 629.49 sec.    training loss:\t\t3.47579030553\n",
      "Done 2570 batches in 631.97 sec.    training loss:\t\t3.47559340959\n",
      "Done 2580 batches in 634.71 sec.    training loss:\t\t3.4755463379\n",
      "Done 2590 batches in 637.08 sec.    training loss:\t\t3.47545700986\n",
      "Done 2600 batches in 639.57 sec.    training loss:\t\t3.47542335964\n",
      "Done 2610 batches in 642.16 sec.    training loss:\t\t3.47535351668\n",
      "Done 2620 batches in 644.80 sec.    training loss:\t\t3.47517884051\n",
      "Done 2630 batches in 647.24 sec.    training loss:\t\t3.47515184266\n",
      "Done 2640 batches in 649.95 sec.    training loss:\t\t3.47513269696\n",
      "Done 2650 batches in 652.43 sec.    training loss:\t\t3.47508663678\n",
      "Done 2660 batches in 654.80 sec.    training loss:\t\t3.47499887967\n",
      "Done 2670 batches in 657.07 sec.    training loss:\t\t3.47485188253\n",
      "Done 2680 batches in 659.75 sec.    training loss:\t\t3.4747961378\n",
      "Done 2690 batches in 662.22 sec.    training loss:\t\t3.47477724124\n",
      "Done 2700 batches in 664.75 sec.    training loss:\t\t3.47474120019\n",
      "Done 2710 batches in 666.98 sec.    training loss:\t\t3.47466187666\n",
      "Done 2720 batches in 669.37 sec.    training loss:\t\t3.47464007622\n",
      "Done 2730 batches in 671.72 sec.    training loss:\t\t3.47459437727\n",
      "Done 2740 batches in 674.09 sec.    training loss:\t\t3.47445101696\n",
      "Done 2750 batches in 676.68 sec.    training loss:\t\t3.47435614084\n",
      "Done 2760 batches in 679.17 sec.    training loss:\t\t3.47438577902\n",
      "Done 2770 batches in 681.88 sec.    training loss:\t\t3.47433725727\n",
      "Done 2780 batches in 684.00 sec.    training loss:\t\t3.47421359483\n",
      "Done 2790 batches in 686.54 sec.    training loss:\t\t3.47410045406\n",
      "Done 2800 batches in 688.87 sec.    training loss:\t\t3.47411992719\n",
      "Done 2810 batches in 691.27 sec.    training loss:\t\t3.47412912726\n",
      "Done 2820 batches in 693.67 sec.    training loss:\t\t3.47406375308\n",
      "Done 2830 batches in 696.50 sec.    training loss:\t\t3.47419627952\n",
      "Done 2840 batches in 698.98 sec.    training loss:\t\t3.47406542668\n",
      "Done 2850 batches in 701.39 sec.    training loss:\t\t3.47397163329\n",
      "Done 2860 batches in 704.14 sec.    training loss:\t\t3.473886627\n",
      "Done 2870 batches in 706.75 sec.    training loss:\t\t3.47380155057\n",
      "Done 2880 batches in 709.04 sec.    training loss:\t\t3.47375544124\n",
      "Done 2890 batches in 711.62 sec.    training loss:\t\t3.47362409393\n",
      "Done 2900 batches in 714.17 sec.    training loss:\t\t3.47354046291\n",
      "Done 2910 batches in 716.32 sec.    training loss:\t\t3.47346486554\n",
      "Done 2920 batches in 718.75 sec.    training loss:\t\t3.47335535297\n",
      "Done 2930 batches in 721.14 sec.    training loss:\t\t3.47328686411\n",
      "Done 2940 batches in 723.64 sec.    training loss:\t\t3.47321065531\n",
      "Done 2950 batches in 726.20 sec.    training loss:\t\t3.47310102261\n",
      "Done 2960 batches in 728.77 sec.    training loss:\t\t3.47308204731\n",
      "Done 2970 batches in 731.08 sec.    training loss:\t\t3.47303624206\n",
      "Done 2980 batches in 733.46 sec.    training loss:\t\t3.47301846519\n",
      "Done 2990 batches in 735.95 sec.    training loss:\t\t3.47290877977\n",
      "Done 3000 batches in 738.54 sec.    training loss:\t\t3.47281013926\n",
      "Done 3010 batches in 740.91 sec.    training loss:\t\t3.47278495119\n",
      "Done 3020 batches in 743.16 sec.    training loss:\t\t3.47269550445\n",
      "Done 3030 batches in 745.66 sec.    training loss:\t\t3.47262929158\n",
      "Done 3040 batches in 748.24 sec.    training loss:\t\t3.47255147802\n",
      "Done 3050 batches in 750.89 sec.    training loss:\t\t3.47241800953\n",
      "Done 3060 batches in 753.31 sec.    training loss:\t\t3.47231775035\n",
      "Done 3070 batches in 755.79 sec.    training loss:\t\t3.47221014928\n",
      "Done 3080 batches in 758.23 sec.    training loss:\t\t3.47214558634\n",
      "Done 3090 batches in 760.70 sec.    training loss:\t\t3.47198917798\n",
      "Done 3100 batches in 763.02 sec.    training loss:\t\t3.47196718122\n",
      "Done 3110 batches in 765.48 sec.    training loss:\t\t3.47202436509\n",
      "Done 3120 batches in 768.00 sec.    training loss:\t\t3.47186428793\n",
      "Done 3130 batches in 770.62 sec.    training loss:\t\t3.47174332575\n",
      "Done 3140 batches in 772.92 sec.    training loss:\t\t3.47174450242\n",
      "Done 3150 batches in 775.03 sec.    training loss:\t\t3.47168531097\n",
      "Done 3160 batches in 777.52 sec.    training loss:\t\t3.47166394021\n",
      "Done 3170 batches in 779.98 sec.    training loss:\t\t3.47148631802\n",
      "Done 3180 batches in 782.56 sec.    training loss:\t\t3.47145198202\n",
      "Done 3190 batches in 785.01 sec.    training loss:\t\t3.47143533638\n",
      "Done 3200 batches in 787.45 sec.    training loss:\t\t3.47146574445\n",
      "Done 3210 batches in 789.89 sec.    training loss:\t\t3.47149674892\n",
      "Done 3220 batches in 792.39 sec.    training loss:\t\t3.47143031219\n",
      "Done 3230 batches in 795.00 sec.    training loss:\t\t3.47135862422\n",
      "Done 3240 batches in 797.52 sec.    training loss:\t\t3.47134633079\n",
      "Done 3250 batches in 799.98 sec.    training loss:\t\t3.47128200643\n",
      "Done 3260 batches in 802.33 sec.    training loss:\t\t3.4711748894\n",
      "Done 3270 batches in 804.89 sec.    training loss:\t\t3.47109055042\n",
      "Done 3280 batches in 807.56 sec.    training loss:\t\t3.47100523079\n",
      "Done 3290 batches in 809.84 sec.    training loss:\t\t3.47087401317\n",
      "Done 3300 batches in 812.40 sec.    training loss:\t\t3.47078697511\n",
      "Done 3310 batches in 814.75 sec.    training loss:\t\t3.47074240002\n",
      "Done 3320 batches in 817.02 sec.    training loss:\t\t3.47064704592\n",
      "Done 3330 batches in 819.64 sec.    training loss:\t\t3.47058980456\n",
      "Done 3340 batches in 822.09 sec.    training loss:\t\t3.47048351265\n",
      "Done 3350 batches in 824.59 sec.    training loss:\t\t3.47050304439\n",
      "Done 3360 batches in 827.00 sec.    training loss:\t\t3.47042144325\n",
      "Done 3370 batches in 829.48 sec.    training loss:\t\t3.47028811822\n",
      "Done 3380 batches in 831.91 sec.    training loss:\t\t3.47029071101\n",
      "Done 3390 batches in 834.25 sec.    training loss:\t\t3.47021877617\n",
      "Done 3400 batches in 836.73 sec.    training loss:\t\t3.47013669336\n",
      "Done 3410 batches in 839.23 sec.    training loss:\t\t3.47008887537\n",
      "Done 3420 batches in 841.44 sec.    training loss:\t\t3.47003294309\n",
      "Done 3430 batches in 843.75 sec.    training loss:\t\t3.47002705859\n",
      "Done 3440 batches in 846.18 sec.    training loss:\t\t3.46995576401\n",
      "Done 3450 batches in 848.46 sec.    training loss:\t\t3.46996893793\n",
      "Done 3460 batches in 851.04 sec.    training loss:\t\t3.46985217408\n",
      "Done 3470 batches in 853.49 sec.    training loss:\t\t3.46977788757\n",
      "Done 3480 batches in 855.99 sec.    training loss:\t\t3.4696657491\n",
      "Done 3490 batches in 858.52 sec.    training loss:\t\t3.46970646601\n",
      "Done 3500 batches in 861.10 sec.    training loss:\t\t3.46964846665\n",
      "Done 3510 batches in 863.53 sec.    training loss:\t\t3.46963354613\n",
      "Done 3520 batches in 865.95 sec.    training loss:\t\t3.46959931065\n",
      "Done 3530 batches in 868.24 sec.    training loss:\t\t3.46954266096\n",
      "Done 3540 batches in 870.72 sec.    training loss:\t\t3.46947194428\n",
      "Done 3550 batches in 872.98 sec.    training loss:\t\t3.46934894676\n",
      "Done 3560 batches in 875.65 sec.    training loss:\t\t3.46927861071\n",
      "Done 3570 batches in 878.19 sec.    training loss:\t\t3.46922162098\n",
      "Done 3580 batches in 880.70 sec.    training loss:\t\t3.46911111012\n",
      "Done 3590 batches in 883.28 sec.    training loss:\t\t3.46904016778\n",
      "Done 3600 batches in 885.99 sec.    training loss:\t\t3.46898688314\n",
      "Done 3610 batches in 888.56 sec.    training loss:\t\t3.46895165023\n",
      "Done 3620 batches in 890.78 sec.    training loss:\t\t3.46880577125\n",
      "Done 3630 batches in 893.05 sec.    training loss:\t\t3.46871244544\n",
      "Done 3640 batches in 895.58 sec.    training loss:\t\t3.46861913008\n",
      "Done 3650 batches in 898.10 sec.    training loss:\t\t3.46853230006\n",
      "Done 3660 batches in 900.44 sec.    training loss:\t\t3.46855658923\n",
      "Done 3670 batches in 902.75 sec.    training loss:\t\t3.46848681271\n",
      "Done 3680 batches in 905.06 sec.    training loss:\t\t3.46836039475\n",
      "Done 3690 batches in 907.39 sec.    training loss:\t\t3.46826205001\n",
      "Done 3700 batches in 909.92 sec.    training loss:\t\t3.46822089487\n",
      "Done 3710 batches in 912.23 sec.    training loss:\t\t3.46812526898\n",
      "Done 3720 batches in 914.64 sec.    training loss:\t\t3.46806899702\n",
      "Done 3730 batches in 916.99 sec.    training loss:\t\t3.46798267676\n",
      "Done 3740 batches in 919.48 sec.    training loss:\t\t3.46790995816\n",
      "Done 3750 batches in 921.89 sec.    training loss:\t\t3.46790195092\n",
      "Done 3760 batches in 924.47 sec.    training loss:\t\t3.46777424058\n",
      "Done 3770 batches in 926.82 sec.    training loss:\t\t3.46775471042\n",
      "Done 3780 batches in 929.17 sec.    training loss:\t\t3.46769092138\n",
      "Done 3790 batches in 931.67 sec.    training loss:\t\t3.4676878705\n",
      "Done 3800 batches in 934.19 sec.    training loss:\t\t3.46764353258\n",
      "Done 3810 batches in 936.74 sec.    training loss:\t\t3.4675552571\n",
      "Done 3820 batches in 939.35 sec.    training loss:\t\t3.46762475509\n",
      "Done 3830 batches in 941.68 sec.    training loss:\t\t3.46752599914\n",
      "Done 3840 batches in 944.24 sec.    training loss:\t\t3.46745508451\n",
      "Done 3850 batches in 947.02 sec.    training loss:\t\t3.46743855503\n",
      "Done 3860 batches in 949.56 sec.    training loss:\t\t3.46739916649\n",
      "Done 3870 batches in 952.10 sec.    training loss:\t\t3.46734947014\n",
      "Done 3880 batches in 954.58 sec.    training loss:\t\t3.46729997912\n",
      "Done 3890 batches in 956.87 sec.    training loss:\t\t3.46725193675\n",
      "Done 100 batches in 7.35 sec.\n",
      "Done 200 batches in 14.52 sec.\n",
      "Done 300 batches in 21.66 sec.\n",
      "Done 400 batches in 28.88 sec.\n",
      "Done 500 batches in 36.49 sec.\n",
      "Done 600 batches in 43.60 sec.\n",
      "Done 700 batches in 50.94 sec.\n",
      "Done 800 batches in 58.24 sec.\n",
      "Done 900 batches in 65.74 sec.\n",
      "Epoch 3 of 5 took 1031.114s\n",
      "  training loss:\t\t3.467233\n",
      "  validation loss:\t\t3.522795\n",
      "Done 10 batches in 2.43 sec.    training loss:\t\t3.42017001263\n",
      "Done 20 batches in 4.92 sec.    training loss:\t\t3.42883111195\n",
      "Done 30 batches in 7.26 sec.    training loss:\t\t3.43834118698\n",
      "Done 40 batches in 9.66 sec.    training loss:\t\t3.44452668387\n",
      "Done 50 batches in 12.39 sec.    training loss:\t\t3.44791316066\n",
      "Done 60 batches in 14.74 sec.    training loss:\t\t3.44834858689\n",
      "Done 70 batches in 17.18 sec.    training loss:\t\t3.44819348429\n",
      "Done 80 batches in 19.53 sec.    training loss:\t\t3.44753502579\n",
      "Done 90 batches in 22.20 sec.    training loss:\t\t3.44658345521\n",
      "Done 100 batches in 24.50 sec.    training loss:\t\t3.44644017665\n",
      "Done 110 batches in 27.03 sec.    training loss:\t\t3.44504985894\n",
      "Done 120 batches in 29.64 sec.    training loss:\t\t3.44470782097\n",
      "Done 130 batches in 32.08 sec.    training loss:\t\t3.44290491005\n",
      "Done 140 batches in 34.59 sec.    training loss:\t\t3.44123274982\n",
      "Done 150 batches in 36.87 sec.    training loss:\t\t3.44480728767\n",
      "Done 160 batches in 39.24 sec.    training loss:\t\t3.44393712824\n",
      "Done 170 batches in 41.61 sec.    training loss:\t\t3.44369888687\n",
      "Done 180 batches in 44.11 sec.    training loss:\t\t3.44241852287\n",
      "Done 190 batches in 46.52 sec.    training loss:\t\t3.44298748343\n",
      "Done 200 batches in 49.38 sec.    training loss:\t\t3.44438183079\n",
      "Done 210 batches in 51.88 sec.    training loss:\t\t3.4438156945\n",
      "Done 220 batches in 54.34 sec.    training loss:\t\t3.44346613638\n",
      "Done 230 batches in 56.72 sec.    training loss:\t\t3.44413945944\n",
      "Done 240 batches in 59.00 sec.    training loss:\t\t3.44318033173\n",
      "Done 250 batches in 61.63 sec.    training loss:\t\t3.44403783046\n",
      "Done 260 batches in 64.07 sec.    training loss:\t\t3.44346882749\n",
      "Done 270 batches in 66.43 sec.    training loss:\t\t3.44384028452\n",
      "Done 280 batches in 68.85 sec.    training loss:\t\t3.44265522824\n",
      "Done 290 batches in 71.29 sec.    training loss:\t\t3.44257594066\n",
      "Done 300 batches in 73.60 sec.    training loss:\t\t3.44176139631\n",
      "Done 310 batches in 75.90 sec.    training loss:\t\t3.44147023573\n",
      "Done 320 batches in 78.31 sec.    training loss:\t\t3.44152749798\n",
      "Done 330 batches in 80.80 sec.    training loss:\t\t3.44148391174\n",
      "Done 340 batches in 83.25 sec.    training loss:\t\t3.44149077273\n",
      "Done 350 batches in 85.91 sec.    training loss:\t\t3.44153921595\n",
      "Done 360 batches in 88.29 sec.    training loss:\t\t3.44174897115\n",
      "Done 370 batches in 90.77 sec.    training loss:\t\t3.44190877619\n",
      "Done 380 batches in 93.37 sec.    training loss:\t\t3.4419943143\n",
      "Done 390 batches in 95.85 sec.    training loss:\t\t3.44186729428\n",
      "Done 400 batches in 98.24 sec.    training loss:\t\t3.44169926755\n",
      "Done 410 batches in 100.69 sec.    training loss:\t\t3.44147913727\n",
      "Done 420 batches in 103.34 sec.    training loss:\t\t3.4410745803\n",
      "Done 430 batches in 105.87 sec.    training loss:\t\t3.44105879125\n",
      "Done 440 batches in 108.49 sec.    training loss:\t\t3.44079854063\n",
      "Done 450 batches in 111.03 sec.    training loss:\t\t3.44015064369\n",
      "Done 460 batches in 113.44 sec.    training loss:\t\t3.44104069433\n",
      "Done 470 batches in 116.10 sec.    training loss:\t\t3.44105987816\n",
      "Done 480 batches in 118.52 sec.    training loss:\t\t3.44078089887\n",
      "Done 490 batches in 120.80 sec.    training loss:\t\t3.44102527323\n",
      "Done 500 batches in 123.22 sec.    training loss:\t\t3.44043881414\n",
      "Done 510 batches in 125.63 sec.    training loss:\t\t3.44056818696\n",
      "Done 520 batches in 128.18 sec.    training loss:\t\t3.44056539225\n",
      "Done 530 batches in 130.56 sec.    training loss:\t\t3.44066979271\n",
      "Done 540 batches in 132.97 sec.    training loss:\t\t3.44045211594\n",
      "Done 550 batches in 135.42 sec.    training loss:\t\t3.44035681444\n",
      "Done 560 batches in 137.94 sec.    training loss:\t\t3.44037919164\n",
      "Done 570 batches in 140.49 sec.    training loss:\t\t3.44008844019\n",
      "Done 580 batches in 142.71 sec.    training loss:\t\t3.43974711279\n",
      "Done 590 batches in 145.13 sec.    training loss:\t\t3.43938452368\n",
      "Done 600 batches in 147.62 sec.    training loss:\t\t3.43933405719\n",
      "Done 610 batches in 150.03 sec.    training loss:\t\t3.43912240271\n",
      "Done 620 batches in 152.74 sec.    training loss:\t\t3.43892028816\n",
      "Done 630 batches in 154.97 sec.    training loss:\t\t3.4384324899\n",
      "Done 640 batches in 157.32 sec.    training loss:\t\t3.43822040845\n",
      "Done 650 batches in 159.84 sec.    training loss:\t\t3.43771978947\n",
      "Done 660 batches in 162.13 sec.    training loss:\t\t3.43761313368\n",
      "Done 670 batches in 164.84 sec.    training loss:\t\t3.43795466303\n",
      "Done 680 batches in 167.21 sec.    training loss:\t\t3.43800754123\n",
      "Done 690 batches in 169.65 sec.    training loss:\t\t3.43796224738\n",
      "Done 700 batches in 171.91 sec.    training loss:\t\t3.4373721332\n",
      "Done 710 batches in 174.48 sec.    training loss:\t\t3.43763388789\n",
      "Done 720 batches in 177.10 sec.    training loss:\t\t3.43754179679\n",
      "Done 730 batches in 179.38 sec.    training loss:\t\t3.43725754154\n",
      "Done 740 batches in 181.86 sec.    training loss:\t\t3.43712838454\n",
      "Done 750 batches in 184.33 sec.    training loss:\t\t3.43688650128\n",
      "Done 760 batches in 186.75 sec.    training loss:\t\t3.43677273458\n",
      "Done 770 batches in 189.25 sec.    training loss:\t\t3.43669444375\n",
      "Done 780 batches in 191.63 sec.    training loss:\t\t3.43637081837\n",
      "Done 790 batches in 194.12 sec.    training loss:\t\t3.43632967915\n",
      "Done 800 batches in 196.67 sec.    training loss:\t\t3.43607378301\n",
      "Done 810 batches in 199.02 sec.    training loss:\t\t3.43577043973\n",
      "Done 820 batches in 201.41 sec.    training loss:\t\t3.4353321808\n",
      "Done 830 batches in 203.64 sec.    training loss:\t\t3.4354209933\n",
      "Done 840 batches in 206.03 sec.    training loss:\t\t3.43548548418\n",
      "Done 850 batches in 208.32 sec.    training loss:\t\t3.4351246766\n",
      "Done 860 batches in 210.74 sec.    training loss:\t\t3.43492107526\n",
      "Done 870 batches in 213.16 sec.    training loss:\t\t3.43493743953\n",
      "Done 880 batches in 215.49 sec.    training loss:\t\t3.43522842846\n",
      "Done 890 batches in 217.99 sec.    training loss:\t\t3.43561835219\n",
      "Done 900 batches in 220.36 sec.    training loss:\t\t3.4358907676\n",
      "Done 910 batches in 222.53 sec.    training loss:\t\t3.43542329256\n",
      "Done 920 batches in 224.83 sec.    training loss:\t\t3.43530501173\n",
      "Done 930 batches in 227.33 sec.    training loss:\t\t3.4352499054\n",
      "Done 940 batches in 229.78 sec.    training loss:\t\t3.43510856409\n",
      "Done 950 batches in 232.25 sec.    training loss:\t\t3.43486137062\n",
      "Done 960 batches in 234.79 sec.    training loss:\t\t3.43469667812\n",
      "Done 970 batches in 237.08 sec.    training loss:\t\t3.43511891279\n",
      "Done 980 batches in 239.51 sec.    training loss:\t\t3.43483517084\n",
      "Done 990 batches in 241.79 sec.    training loss:\t\t3.43465974385\n",
      "Done 1000 batches in 244.22 sec.    training loss:\t\t3.43442655371\n",
      "Done 1010 batches in 246.72 sec.    training loss:\t\t3.43437730262\n",
      "Done 1020 batches in 248.89 sec.    training loss:\t\t3.43396447433\n",
      "Done 1030 batches in 251.48 sec.    training loss:\t\t3.43385129076\n",
      "Done 1040 batches in 254.13 sec.    training loss:\t\t3.43392426784\n",
      "Done 1050 batches in 256.58 sec.    training loss:\t\t3.4338479387\n",
      "Done 1060 batches in 259.02 sec.    training loss:\t\t3.43383423769\n",
      "Done 1070 batches in 261.39 sec.    training loss:\t\t3.43351834256\n",
      "Done 1080 batches in 263.75 sec.    training loss:\t\t3.43318923625\n",
      "Done 1090 batches in 266.41 sec.    training loss:\t\t3.43335182149\n",
      "Done 1100 batches in 268.87 sec.    training loss:\t\t3.43319205252\n",
      "Done 1110 batches in 271.32 sec.    training loss:\t\t3.43306869815\n",
      "Done 1120 batches in 273.90 sec.    training loss:\t\t3.43297875688\n",
      "Done 1130 batches in 276.35 sec.    training loss:\t\t3.43282951755\n",
      "Done 1140 batches in 278.88 sec.    training loss:\t\t3.43249208031\n",
      "Done 1150 batches in 281.21 sec.    training loss:\t\t3.43269217339\n",
      "Done 1160 batches in 283.76 sec.    training loss:\t\t3.43250948046\n",
      "Done 1170 batches in 286.39 sec.    training loss:\t\t3.43231553496\n",
      "Done 1180 batches in 288.87 sec.    training loss:\t\t3.43219572328\n",
      "Done 1190 batches in 291.52 sec.    training loss:\t\t3.43220479088\n",
      "Done 1200 batches in 294.09 sec.    training loss:\t\t3.43203576147\n",
      "Done 1210 batches in 296.66 sec.    training loss:\t\t3.43194016085\n",
      "Done 1220 batches in 299.04 sec.    training loss:\t\t3.43181874775\n",
      "Done 1230 batches in 301.52 sec.    training loss:\t\t3.43172807516\n",
      "Done 1240 batches in 304.06 sec.    training loss:\t\t3.4317008682\n",
      "Done 1250 batches in 306.71 sec.    training loss:\t\t3.43150184317\n",
      "Done 1260 batches in 309.17 sec.    training loss:\t\t3.43147777712\n",
      "Done 1270 batches in 311.56 sec.    training loss:\t\t3.43137357489\n",
      "Done 1280 batches in 314.05 sec.    training loss:\t\t3.43124722968\n",
      "Done 1290 batches in 316.49 sec.    training loss:\t\t3.4312018222\n",
      "Done 1300 batches in 319.00 sec.    training loss:\t\t3.4311760135\n",
      "Done 1310 batches in 321.62 sec.    training loss:\t\t3.43117599641\n",
      "Done 1320 batches in 323.86 sec.    training loss:\t\t3.4312533532\n",
      "Done 1330 batches in 326.31 sec.    training loss:\t\t3.4312679784\n",
      "Done 1340 batches in 328.69 sec.    training loss:\t\t3.43095469144\n",
      "Done 1350 batches in 331.30 sec.    training loss:\t\t3.43075831936\n",
      "Done 1360 batches in 333.92 sec.    training loss:\t\t3.4306277739\n",
      "Done 1370 batches in 336.44 sec.    training loss:\t\t3.43040988878\n",
      "Done 1380 batches in 338.75 sec.    training loss:\t\t3.43041999674\n",
      "Done 1390 batches in 341.36 sec.    training loss:\t\t3.43043877316\n",
      "Done 1400 batches in 343.87 sec.    training loss:\t\t3.43022293453\n",
      "Done 1410 batches in 346.40 sec.    training loss:\t\t3.43001313045\n",
      "Done 1420 batches in 348.64 sec.    training loss:\t\t3.42995893992\n",
      "Done 1430 batches in 351.01 sec.    training loss:\t\t3.42983364394\n",
      "Done 1440 batches in 353.51 sec.    training loss:\t\t3.42977345409\n",
      "Done 1450 batches in 356.14 sec.    training loss:\t\t3.42977332077\n",
      "Done 1460 batches in 358.51 sec.    training loss:\t\t3.42961588146\n",
      "Done 1470 batches in 360.79 sec.    training loss:\t\t3.42949581162\n",
      "Done 1480 batches in 363.46 sec.    training loss:\t\t3.42948203492\n",
      "Done 1490 batches in 366.06 sec.    training loss:\t\t3.42945972179\n",
      "Done 1500 batches in 368.65 sec.    training loss:\t\t3.42932745927\n",
      "Done 1510 batches in 371.18 sec.    training loss:\t\t3.42929595826\n",
      "Done 1520 batches in 373.65 sec.    training loss:\t\t3.42934521131\n",
      "Done 1530 batches in 376.22 sec.    training loss:\t\t3.42937741638\n",
      "Done 1540 batches in 378.69 sec.    training loss:\t\t3.42922563208\n",
      "Done 1550 batches in 381.08 sec.    training loss:\t\t3.42918460032\n",
      "Done 1560 batches in 383.62 sec.    training loss:\t\t3.42910426717\n",
      "Done 1570 batches in 386.10 sec.    training loss:\t\t3.42923835912\n",
      "Done 1580 batches in 388.36 sec.    training loss:\t\t3.42912135646\n",
      "Done 1590 batches in 391.14 sec.    training loss:\t\t3.42902317252\n",
      "Done 1600 batches in 393.48 sec.    training loss:\t\t3.42883014855\n",
      "Done 1610 batches in 395.65 sec.    training loss:\t\t3.42877375975\n",
      "Done 1620 batches in 398.26 sec.    training loss:\t\t3.42872059783\n",
      "Done 1630 batches in 400.72 sec.    training loss:\t\t3.42859113493\n",
      "Done 1640 batches in 403.49 sec.    training loss:\t\t3.42874610916\n",
      "Done 1650 batches in 406.18 sec.    training loss:\t\t3.42862890126\n",
      "Done 1660 batches in 408.62 sec.    training loss:\t\t3.42877977851\n",
      "Done 1670 batches in 411.22 sec.    training loss:\t\t3.42855686631\n",
      "Done 1680 batches in 413.85 sec.    training loss:\t\t3.42858366378\n",
      "Done 1690 batches in 416.50 sec.    training loss:\t\t3.42854351129\n",
      "Done 1700 batches in 418.75 sec.    training loss:\t\t3.42848277232\n",
      "Done 1710 batches in 421.21 sec.    training loss:\t\t3.42825979495\n",
      "Done 1720 batches in 423.58 sec.    training loss:\t\t3.42811999791\n",
      "Done 1730 batches in 426.25 sec.    training loss:\t\t3.42813511826\n",
      "Done 1740 batches in 428.62 sec.    training loss:\t\t3.42798305483\n",
      "Done 1750 batches in 431.12 sec.    training loss:\t\t3.4280646396\n",
      "Done 1760 batches in 433.68 sec.    training loss:\t\t3.42804299764\n",
      "Done 1770 batches in 435.95 sec.    training loss:\t\t3.42804121068\n",
      "Done 1780 batches in 438.58 sec.    training loss:\t\t3.42806901306\n",
      "Done 1790 batches in 441.06 sec.    training loss:\t\t3.4280854413\n",
      "Done 1800 batches in 443.56 sec.    training loss:\t\t3.428094318\n",
      "Done 1810 batches in 446.10 sec.    training loss:\t\t3.42818134559\n",
      "Done 1820 batches in 448.85 sec.    training loss:\t\t3.428051024\n",
      "Done 1830 batches in 451.32 sec.    training loss:\t\t3.42802324859\n",
      "Done 1840 batches in 453.61 sec.    training loss:\t\t3.42796061459\n",
      "Done 1850 batches in 455.92 sec.    training loss:\t\t3.42792473177\n",
      "Done 1860 batches in 458.10 sec.    training loss:\t\t3.4277438184\n",
      "Done 1870 batches in 460.60 sec.    training loss:\t\t3.42775876215\n",
      "Done 1880 batches in 462.88 sec.    training loss:\t\t3.42759329147\n",
      "Done 1890 batches in 465.50 sec.    training loss:\t\t3.4276700297\n",
      "Done 1900 batches in 467.94 sec.    training loss:\t\t3.42779871548\n",
      "Done 1910 batches in 470.47 sec.    training loss:\t\t3.42775304054\n",
      "Done 1920 batches in 472.85 sec.    training loss:\t\t3.42762301265\n",
      "Done 1930 batches in 475.31 sec.    training loss:\t\t3.42761928902\n",
      "Done 1940 batches in 477.96 sec.    training loss:\t\t3.42770620554\n",
      "Done 1950 batches in 480.35 sec.    training loss:\t\t3.42759235192\n",
      "Done 1960 batches in 482.90 sec.    training loss:\t\t3.42751535161\n",
      "Done 1970 batches in 485.03 sec.    training loss:\t\t3.42744155061\n",
      "Done 1980 batches in 487.34 sec.    training loss:\t\t3.42737479986\n",
      "Done 1990 batches in 489.77 sec.    training loss:\t\t3.42735012969\n",
      "Done 2000 batches in 492.40 sec.    training loss:\t\t3.4272371597\n",
      "Done 2010 batches in 495.09 sec.    training loss:\t\t3.42713002295\n",
      "Done 2020 batches in 497.53 sec.    training loss:\t\t3.42691322802\n",
      "Done 2030 batches in 500.03 sec.    training loss:\t\t3.42675800563\n",
      "Done 2040 batches in 502.40 sec.    training loss:\t\t3.42675922447\n",
      "Done 2050 batches in 504.68 sec.    training loss:\t\t3.426742381\n",
      "Done 2060 batches in 507.26 sec.    training loss:\t\t3.42672851032\n",
      "Done 2070 batches in 509.69 sec.    training loss:\t\t3.4265823039\n",
      "Done 2080 batches in 512.18 sec.    training loss:\t\t3.42648561991\n",
      "Done 2090 batches in 514.59 sec.    training loss:\t\t3.42642059404\n",
      "Done 2100 batches in 516.96 sec.    training loss:\t\t3.42624875194\n",
      "Done 2110 batches in 519.21 sec.    training loss:\t\t3.42611217512\n",
      "Done 2120 batches in 521.62 sec.    training loss:\t\t3.42605509704\n",
      "Done 2130 batches in 523.84 sec.    training loss:\t\t3.42597243729\n",
      "Done 2140 batches in 526.20 sec.    training loss:\t\t3.42583544167\n",
      "Done 2150 batches in 528.76 sec.    training loss:\t\t3.42567442225\n",
      "Done 2160 batches in 531.28 sec.    training loss:\t\t3.42561487893\n",
      "Done 2170 batches in 533.88 sec.    training loss:\t\t3.4255525817\n",
      "Done 2180 batches in 536.18 sec.    training loss:\t\t3.42558171555\n",
      "Done 2190 batches in 538.39 sec.    training loss:\t\t3.42557818757\n",
      "Done 2200 batches in 540.89 sec.    training loss:\t\t3.42543264324\n",
      "Done 2210 batches in 543.42 sec.    training loss:\t\t3.42534466834\n",
      "Done 2220 batches in 545.80 sec.    training loss:\t\t3.42523933074\n",
      "Done 2230 batches in 548.03 sec.    training loss:\t\t3.42515358836\n",
      "Done 2240 batches in 550.62 sec.    training loss:\t\t3.42510750014\n",
      "Done 2250 batches in 553.13 sec.    training loss:\t\t3.42505216617\n",
      "Done 2260 batches in 555.48 sec.    training loss:\t\t3.42498009085\n",
      "Done 2270 batches in 557.95 sec.    training loss:\t\t3.4249953981\n",
      "Done 2280 batches in 560.47 sec.    training loss:\t\t3.42488897133\n",
      "Done 2290 batches in 563.13 sec.    training loss:\t\t3.42487464879\n",
      "Done 2300 batches in 565.69 sec.    training loss:\t\t3.42491125632\n",
      "Done 2310 batches in 568.27 sec.    training loss:\t\t3.42488061732\n",
      "Done 2320 batches in 570.74 sec.    training loss:\t\t3.42480342206\n",
      "Done 2330 batches in 573.44 sec.    training loss:\t\t3.42481257121\n",
      "Done 2340 batches in 575.86 sec.    training loss:\t\t3.42468448565\n",
      "Done 2350 batches in 578.34 sec.    training loss:\t\t3.42457353778\n",
      "Done 2360 batches in 580.67 sec.    training loss:\t\t3.42454156413\n",
      "Done 2370 batches in 583.12 sec.    training loss:\t\t3.42454293154\n",
      "Done 2380 batches in 585.48 sec.    training loss:\t\t3.42445128473\n",
      "Done 2390 batches in 587.87 sec.    training loss:\t\t3.42440947385\n",
      "Done 2400 batches in 590.34 sec.    training loss:\t\t3.42447717008\n",
      "Done 2410 batches in 592.89 sec.    training loss:\t\t3.42444745263\n",
      "Done 2420 batches in 595.54 sec.    training loss:\t\t3.42448481931\n",
      "Done 2430 batches in 598.00 sec.    training loss:\t\t3.42449295268\n",
      "Done 2440 batches in 600.37 sec.    training loss:\t\t3.42444663677\n",
      "Done 2450 batches in 602.79 sec.    training loss:\t\t3.42449576258\n",
      "Done 2460 batches in 605.20 sec.    training loss:\t\t3.42449819523\n",
      "Done 2470 batches in 607.71 sec.    training loss:\t\t3.42447304859\n",
      "Done 2480 batches in 610.28 sec.    training loss:\t\t3.42439959171\n",
      "Done 2490 batches in 612.91 sec.    training loss:\t\t3.42436433541\n",
      "Done 2500 batches in 615.43 sec.    training loss:\t\t3.42434649372\n",
      "Done 2510 batches in 617.81 sec.    training loss:\t\t3.42432611203\n",
      "Done 2520 batches in 620.18 sec.    training loss:\t\t3.42424538324\n",
      "Done 2530 batches in 622.97 sec.    training loss:\t\t3.42416587885\n",
      "Done 2540 batches in 625.39 sec.    training loss:\t\t3.42411211551\n",
      "Done 2550 batches in 627.97 sec.    training loss:\t\t3.42420273886\n",
      "Done 2560 batches in 630.50 sec.    training loss:\t\t3.42420396051\n",
      "Done 2570 batches in 632.98 sec.    training loss:\t\t3.42405458889\n",
      "Done 2580 batches in 635.70 sec.    training loss:\t\t3.42401984971\n",
      "Done 2590 batches in 638.07 sec.    training loss:\t\t3.4239510866\n",
      "Done 2600 batches in 640.57 sec.    training loss:\t\t3.42389487002\n",
      "Done 2610 batches in 643.17 sec.    training loss:\t\t3.423872906\n",
      "Done 2620 batches in 645.80 sec.    training loss:\t\t3.42374409846\n",
      "Done 2630 batches in 648.24 sec.    training loss:\t\t3.42372102472\n",
      "Done 2640 batches in 650.94 sec.    training loss:\t\t3.42367638478\n",
      "Done 2650 batches in 653.41 sec.    training loss:\t\t3.42364905989\n",
      "Done 2660 batches in 655.76 sec.    training loss:\t\t3.42362864805\n",
      "Done 2670 batches in 658.05 sec.    training loss:\t\t3.42349055261\n",
      "Done 2680 batches in 660.73 sec.    training loss:\t\t3.42345272136\n",
      "Done 2690 batches in 663.21 sec.    training loss:\t\t3.42346559168\n",
      "Done 2700 batches in 665.75 sec.    training loss:\t\t3.42342197569\n",
      "Done 2710 batches in 667.99 sec.    training loss:\t\t3.42339656018\n",
      "Done 2720 batches in 670.36 sec.    training loss:\t\t3.42341767105\n",
      "Done 2730 batches in 672.71 sec.    training loss:\t\t3.42339683366\n",
      "Done 2740 batches in 675.08 sec.    training loss:\t\t3.42328052049\n",
      "Done 2750 batches in 677.66 sec.    training loss:\t\t3.42320584764\n",
      "Done 2760 batches in 680.16 sec.    training loss:\t\t3.4232791204\n",
      "Done 2770 batches in 682.89 sec.    training loss:\t\t3.42327704772\n",
      "Done 2780 batches in 685.02 sec.    training loss:\t\t3.42322134368\n",
      "Done 2790 batches in 687.56 sec.    training loss:\t\t3.42314755554\n",
      "Done 2800 batches in 689.90 sec.    training loss:\t\t3.4231047666\n",
      "Done 2810 batches in 692.31 sec.    training loss:\t\t3.42313102002\n",
      "Done 2820 batches in 694.70 sec.    training loss:\t\t3.42306315734\n",
      "Done 2830 batches in 697.53 sec.    training loss:\t\t3.42306207499\n",
      "Done 2840 batches in 700.03 sec.    training loss:\t\t3.4229782495\n",
      "Done 2850 batches in 702.42 sec.    training loss:\t\t3.42291191661\n",
      "Done 2860 batches in 705.19 sec.    training loss:\t\t3.42286821778\n",
      "Done 2870 batches in 707.78 sec.    training loss:\t\t3.42282291162\n",
      "Done 2880 batches in 710.08 sec.    training loss:\t\t3.422770694\n",
      "Done 2890 batches in 712.64 sec.    training loss:\t\t3.4226947181\n",
      "Done 2900 batches in 715.17 sec.    training loss:\t\t3.42254690199\n",
      "Done 2910 batches in 717.35 sec.    training loss:\t\t3.42246338808\n",
      "Done 2920 batches in 719.81 sec.    training loss:\t\t3.42239772765\n",
      "Done 2930 batches in 722.18 sec.    training loss:\t\t3.42232142838\n",
      "Done 2940 batches in 724.67 sec.    training loss:\t\t3.42224108165\n",
      "Done 2950 batches in 727.25 sec.    training loss:\t\t3.42217169233\n",
      "Done 2960 batches in 729.83 sec.    training loss:\t\t3.42213988308\n",
      "Done 2970 batches in 732.15 sec.    training loss:\t\t3.42210971572\n",
      "Done 2980 batches in 734.55 sec.    training loss:\t\t3.42211351227\n",
      "Done 2990 batches in 737.06 sec.    training loss:\t\t3.42201449156\n",
      "Done 3000 batches in 739.65 sec.    training loss:\t\t3.42194927572\n",
      "Done 3010 batches in 742.03 sec.    training loss:\t\t3.42197527032\n",
      "Done 3020 batches in 744.27 sec.    training loss:\t\t3.42192158073\n",
      "Done 3030 batches in 746.78 sec.    training loss:\t\t3.42187262274\n",
      "Done 3040 batches in 749.37 sec.    training loss:\t\t3.42183792069\n",
      "Done 3050 batches in 752.01 sec.    training loss:\t\t3.42173650534\n",
      "Done 3060 batches in 754.44 sec.    training loss:\t\t3.42165480147\n",
      "Done 3070 batches in 756.93 sec.    training loss:\t\t3.42160671339\n",
      "Done 3080 batches in 759.37 sec.    training loss:\t\t3.42154210357\n",
      "Done 3090 batches in 761.83 sec.    training loss:\t\t3.42157001115\n",
      "Done 3100 batches in 764.16 sec.    training loss:\t\t3.42153035245\n",
      "Done 3110 batches in 766.62 sec.    training loss:\t\t3.42156549761\n",
      "Done 3120 batches in 769.14 sec.    training loss:\t\t3.42146726412\n",
      "Done 3130 batches in 771.77 sec.    training loss:\t\t3.42137918504\n",
      "Done 3140 batches in 774.09 sec.    training loss:\t\t3.42134152809\n",
      "Done 3150 batches in 776.19 sec.    training loss:\t\t3.42132860257\n",
      "Done 3160 batches in 778.70 sec.    training loss:\t\t3.42135394911\n",
      "Done 3170 batches in 781.15 sec.    training loss:\t\t3.42122317292\n",
      "Done 3180 batches in 783.71 sec.    training loss:\t\t3.42120547361\n",
      "Done 3190 batches in 786.17 sec.    training loss:\t\t3.42116926046\n",
      "Done 3200 batches in 788.61 sec.    training loss:\t\t3.42119205891\n",
      "Done 3210 batches in 791.04 sec.    training loss:\t\t3.42124957611\n",
      "Done 3220 batches in 793.55 sec.    training loss:\t\t3.4211981118\n",
      "Done 3230 batches in 796.14 sec.    training loss:\t\t3.42118089859\n",
      "Done 3240 batches in 798.63 sec.    training loss:\t\t3.42120783008\n",
      "Done 3250 batches in 801.10 sec.    training loss:\t\t3.42117851538\n",
      "Done 3260 batches in 803.45 sec.    training loss:\t\t3.42113722274\n",
      "Done 3270 batches in 806.02 sec.    training loss:\t\t3.42103418666\n",
      "Done 3280 batches in 808.69 sec.    training loss:\t\t3.42097868165\n",
      "Done 3290 batches in 810.99 sec.    training loss:\t\t3.42091230019\n",
      "Done 3300 batches in 813.55 sec.    training loss:\t\t3.42089259585\n",
      "Done 3310 batches in 815.89 sec.    training loss:\t\t3.42087714053\n",
      "Done 3320 batches in 818.16 sec.    training loss:\t\t3.42080840057\n",
      "Done 3330 batches in 820.76 sec.    training loss:\t\t3.4207586636\n",
      "Done 3340 batches in 823.21 sec.    training loss:\t\t3.42068008444\n",
      "Done 3350 batches in 825.72 sec.    training loss:\t\t3.42074323103\n",
      "Done 3360 batches in 828.13 sec.    training loss:\t\t3.42075211116\n",
      "Done 3370 batches in 830.63 sec.    training loss:\t\t3.42065774626\n",
      "Done 3380 batches in 833.05 sec.    training loss:\t\t3.42060376361\n",
      "Done 3390 batches in 835.38 sec.    training loss:\t\t3.42056534756\n",
      "Done 3400 batches in 837.86 sec.    training loss:\t\t3.42049645386\n",
      "Done 3410 batches in 840.38 sec.    training loss:\t\t3.42047246672\n",
      "Done 3420 batches in 842.61 sec.    training loss:\t\t3.42044806634\n",
      "Done 3430 batches in 844.91 sec.    training loss:\t\t3.42039334497\n",
      "Done 3440 batches in 847.34 sec.    training loss:\t\t3.42034328113\n",
      "Done 3450 batches in 849.65 sec.    training loss:\t\t3.42037293313\n",
      "Done 3460 batches in 852.23 sec.    training loss:\t\t3.42031009027\n",
      "Done 3470 batches in 854.68 sec.    training loss:\t\t3.42027084129\n",
      "Done 3480 batches in 857.16 sec.    training loss:\t\t3.42022651677\n",
      "Done 3490 batches in 859.68 sec.    training loss:\t\t3.42020746156\n",
      "Done 3500 batches in 862.28 sec.    training loss:\t\t3.42020578059\n",
      "Done 3510 batches in 864.73 sec.    training loss:\t\t3.42019504216\n",
      "Done 3520 batches in 867.16 sec.    training loss:\t\t3.42019873545\n",
      "Done 3530 batches in 869.46 sec.    training loss:\t\t3.4201481319\n",
      "Done 3540 batches in 871.91 sec.    training loss:\t\t3.42009254437\n",
      "Done 3550 batches in 874.19 sec.    training loss:\t\t3.41997006688\n",
      "Done 3560 batches in 876.84 sec.    training loss:\t\t3.41994523108\n",
      "Done 3570 batches in 879.37 sec.    training loss:\t\t3.41997296154\n",
      "Done 3580 batches in 881.87 sec.    training loss:\t\t3.41990192766\n",
      "Done 3590 batches in 884.46 sec.    training loss:\t\t3.41986139234\n",
      "Done 3600 batches in 887.16 sec.    training loss:\t\t3.41982411551\n",
      "Done 3610 batches in 889.75 sec.    training loss:\t\t3.41982727272\n",
      "Done 3620 batches in 891.98 sec.    training loss:\t\t3.41971333295\n",
      "Done 3630 batches in 894.25 sec.    training loss:\t\t3.41964657357\n",
      "Done 3640 batches in 896.77 sec.    training loss:\t\t3.4195783647\n",
      "Done 3650 batches in 899.27 sec.    training loss:\t\t3.41952057168\n",
      "Done 3660 batches in 901.61 sec.    training loss:\t\t3.41953598009\n",
      "Done 3670 batches in 903.91 sec.    training loss:\t\t3.41951248235\n",
      "Done 3680 batches in 906.21 sec.    training loss:\t\t3.4194394433\n",
      "Done 3690 batches in 908.55 sec.    training loss:\t\t3.41936620404\n",
      "Done 3700 batches in 911.07 sec.    training loss:\t\t3.41938928198\n",
      "Done 3710 batches in 913.36 sec.    training loss:\t\t3.41932549354\n",
      "Done 3720 batches in 915.76 sec.    training loss:\t\t3.41928768342\n",
      "Done 3730 batches in 918.09 sec.    training loss:\t\t3.41924066651\n",
      "Done 3740 batches in 920.58 sec.    training loss:\t\t3.41918762196\n",
      "Done 3750 batches in 922.98 sec.    training loss:\t\t3.41922798554\n",
      "Done 3760 batches in 925.55 sec.    training loss:\t\t3.41915308477\n",
      "Done 3770 batches in 927.91 sec.    training loss:\t\t3.41916556894\n",
      "Done 3780 batches in 930.27 sec.    training loss:\t\t3.41908411589\n",
      "Done 3790 batches in 932.77 sec.    training loss:\t\t3.41906499979\n",
      "Done 3800 batches in 935.29 sec.    training loss:\t\t3.41905841412\n",
      "Done 3810 batches in 937.82 sec.    training loss:\t\t3.41902167185\n",
      "Done 3820 batches in 940.42 sec.    training loss:\t\t3.41897912891\n",
      "Done 3830 batches in 942.71 sec.    training loss:\t\t3.41888898553\n",
      "Done 3840 batches in 945.28 sec.    training loss:\t\t3.41886110916\n",
      "Done 3850 batches in 948.05 sec.    training loss:\t\t3.4187900564\n",
      "Done 3860 batches in 950.59 sec.    training loss:\t\t3.41875943782\n",
      "Done 3870 batches in 953.11 sec.    training loss:\t\t3.41875386083\n",
      "Done 3880 batches in 955.58 sec.    training loss:\t\t3.41873700603\n",
      "Done 3890 batches in 957.84 sec.    training loss:\t\t3.41871127937\n",
      "Done 100 batches in 7.25 sec.\n",
      "Done 200 batches in 14.43 sec.\n",
      "Done 300 batches in 21.50 sec.\n",
      "Done 400 batches in 28.62 sec.\n",
      "Done 500 batches in 36.20 sec.\n",
      "Done 600 batches in 43.19 sec.\n",
      "Done 700 batches in 50.59 sec.\n",
      "Done 800 batches in 57.82 sec.\n",
      "Done 900 batches in 65.28 sec.\n",
      "Epoch 4 of 5 took 1031.618s\n",
      "  training loss:\t\t3.418715\n",
      "  validation loss:\t\t3.491904\n",
      "Done 10 batches in 2.43 sec.    training loss:\t\t3.37862883221\n",
      "Done 20 batches in 4.91 sec.    training loss:\t\t3.39523589569\n",
      "Done 30 batches in 7.23 sec.    training loss:\t\t3.3980559445\n",
      "Done 40 batches in 9.62 sec.    training loss:\t\t3.4022439056\n",
      "Done 50 batches in 12.34 sec.    training loss:\t\t3.40754293027\n",
      "Done 60 batches in 14.68 sec.    training loss:\t\t3.40555367356\n",
      "Done 70 batches in 17.11 sec.    training loss:\t\t3.40558910707\n",
      "Done 80 batches in 19.45 sec.    training loss:\t\t3.40607766836\n",
      "Done 90 batches in 22.11 sec.    training loss:\t\t3.40286915917\n",
      "Done 100 batches in 24.42 sec.    training loss:\t\t3.40325055109\n",
      "Done 110 batches in 26.92 sec.    training loss:\t\t3.40399911662\n",
      "Done 120 batches in 29.53 sec.    training loss:\t\t3.40380372222\n",
      "Done 130 batches in 31.96 sec.    training loss:\t\t3.40261345754\n",
      "Done 140 batches in 34.48 sec.    training loss:\t\t3.40152363974\n",
      "Done 150 batches in 36.76 sec.    training loss:\t\t3.40263474418\n",
      "Done 160 batches in 39.13 sec.    training loss:\t\t3.40225706561\n",
      "Done 170 batches in 41.51 sec.    training loss:\t\t3.40212260661\n",
      "Done 180 batches in 44.00 sec.    training loss:\t\t3.40072834333\n",
      "Done 190 batches in 46.39 sec.    training loss:\t\t3.40120188583\n",
      "Done 200 batches in 49.23 sec.    training loss:\t\t3.40095023116\n",
      "Done 210 batches in 51.74 sec.    training loss:\t\t3.4003540779\n",
      "Done 220 batches in 54.17 sec.    training loss:\t\t3.40064388633\n",
      "Done 230 batches in 56.54 sec.    training loss:\t\t3.40107923999\n",
      "Done 240 batches in 58.82 sec.    training loss:\t\t3.4001488969\n",
      "Done 250 batches in 61.46 sec.    training loss:\t\t3.40128010635\n",
      "Done 260 batches in 63.92 sec.    training loss:\t\t3.40089220896\n",
      "Done 270 batches in 66.27 sec.    training loss:\t\t3.40140588964\n",
      "Done 280 batches in 68.69 sec.    training loss:\t\t3.40046186605\n",
      "Done 290 batches in 71.12 sec.    training loss:\t\t3.40079071189\n",
      "Done 300 batches in 73.43 sec.    training loss:\t\t3.40000975742\n",
      "Done 310 batches in 75.71 sec.    training loss:\t\t3.39965962389\n",
      "Done 320 batches in 78.12 sec.    training loss:\t\t3.39941087332\n",
      "Done 330 batches in 80.61 sec.    training loss:\t\t3.39997460052\n",
      "Done 340 batches in 83.07 sec.    training loss:\t\t3.40020031098\n",
      "Done 350 batches in 85.72 sec.    training loss:\t\t3.40055751951\n",
      "Done 360 batches in 88.09 sec.    training loss:\t\t3.40080150902\n",
      "Done 370 batches in 90.55 sec.    training loss:\t\t3.40075170381\n",
      "Done 380 batches in 93.15 sec.    training loss:\t\t3.40093892102\n",
      "Done 390 batches in 95.64 sec.    training loss:\t\t3.40119397058\n",
      "Done 400 batches in 98.02 sec.    training loss:\t\t3.4007369928\n",
      "Done 410 batches in 100.46 sec.    training loss:\t\t3.40048265788\n",
      "Done 420 batches in 103.09 sec.    training loss:\t\t3.39972521927\n",
      "Done 430 batches in 105.61 sec.    training loss:\t\t3.39990286002\n",
      "Done 440 batches in 108.23 sec.    training loss:\t\t3.39929831519\n",
      "Done 450 batches in 110.77 sec.    training loss:\t\t3.39885427172\n",
      "Done 460 batches in 113.17 sec.    training loss:\t\t3.39875599821\n",
      "Done 470 batches in 115.80 sec.    training loss:\t\t3.39880172417\n",
      "Done 480 batches in 118.22 sec.    training loss:\t\t3.39866865693\n",
      "Done 490 batches in 120.50 sec.    training loss:\t\t3.39868957067\n",
      "Done 500 batches in 122.93 sec.    training loss:\t\t3.39817533992\n",
      "Done 510 batches in 125.34 sec.    training loss:\t\t3.3980822612\n",
      "Done 520 batches in 127.91 sec.    training loss:\t\t3.39816218762\n",
      "Done 530 batches in 130.27 sec.    training loss:\t\t3.39823947105\n",
      "Done 540 batches in 132.69 sec.    training loss:\t\t3.39819183568\n",
      "Done 550 batches in 135.15 sec.    training loss:\t\t3.39811517081\n",
      "Done 560 batches in 137.70 sec.    training loss:\t\t3.39817344437\n",
      "Done 570 batches in 140.24 sec.    training loss:\t\t3.39781959354\n",
      "Done 580 batches in 142.44 sec.    training loss:\t\t3.39777507971\n",
      "Done 590 batches in 144.84 sec.    training loss:\t\t3.39754063246\n",
      "Done 600 batches in 147.36 sec.    training loss:\t\t3.39744805082\n",
      "Done 610 batches in 149.77 sec.    training loss:\t\t3.39734422579\n",
      "Done 620 batches in 152.49 sec.    training loss:\t\t3.39712448116\n",
      "Done 630 batches in 154.70 sec.    training loss:\t\t3.39672594535\n",
      "Done 640 batches in 157.05 sec.    training loss:\t\t3.39669459483\n",
      "Done 650 batches in 159.56 sec.    training loss:\t\t3.39629304467\n",
      "Done 660 batches in 161.87 sec.    training loss:\t\t3.39642780748\n",
      "Done 670 batches in 164.57 sec.    training loss:\t\t3.3967167089\n",
      "Done 680 batches in 166.92 sec.    training loss:\t\t3.39687792422\n",
      "Done 690 batches in 169.36 sec.    training loss:\t\t3.39672447859\n",
      "Done 700 batches in 171.62 sec.    training loss:\t\t3.39621592307\n",
      "Done 710 batches in 174.19 sec.    training loss:\t\t3.3963517031\n",
      "Done 720 batches in 176.80 sec.    training loss:\t\t3.39618854086\n",
      "Done 730 batches in 179.07 sec.    training loss:\t\t3.39594152082\n",
      "Done 740 batches in 181.54 sec.    training loss:\t\t3.39606795136\n",
      "Done 750 batches in 184.00 sec.    training loss:\t\t3.39588473835\n",
      "Done 760 batches in 186.42 sec.    training loss:\t\t3.39586903694\n",
      "Done 770 batches in 188.92 sec.    training loss:\t\t3.39604956866\n",
      "Done 780 batches in 191.33 sec.    training loss:\t\t3.39587921027\n",
      "Done 790 batches in 193.81 sec.    training loss:\t\t3.39639142473\n",
      "Done 800 batches in 196.35 sec.    training loss:\t\t3.39626342341\n",
      "Done 810 batches in 198.69 sec.    training loss:\t\t3.39600412556\n",
      "Done 820 batches in 201.09 sec.    training loss:\t\t3.39564783816\n",
      "Done 830 batches in 203.33 sec.    training loss:\t\t3.39578935059\n",
      "Done 840 batches in 205.71 sec.    training loss:\t\t3.39593387665\n",
      "Done 850 batches in 208.01 sec.    training loss:\t\t3.39562504793\n",
      "Done 860 batches in 210.42 sec.    training loss:\t\t3.39548537671\n",
      "Done 870 batches in 212.85 sec.    training loss:\t\t3.39549365808\n",
      "Done 880 batches in 215.17 sec.    training loss:\t\t3.39556370923\n",
      "Done 890 batches in 217.70 sec.    training loss:\t\t3.39599666456\n",
      "Done 900 batches in 220.06 sec.    training loss:\t\t3.39603105537\n",
      "Done 910 batches in 222.24 sec.    training loss:\t\t3.39567451739\n",
      "Done 920 batches in 224.54 sec.    training loss:\t\t3.39557642518\n",
      "Done 930 batches in 227.05 sec.    training loss:\t\t3.39554930441\n",
      "Done 940 batches in 229.48 sec.    training loss:\t\t3.39545910498\n",
      "Done 950 batches in 231.97 sec.    training loss:\t\t3.39517514998\n",
      "Done 960 batches in 234.49 sec.    training loss:\t\t3.39516980721\n",
      "Done 970 batches in 236.79 sec.    training loss:\t\t3.39515943665\n",
      "Done 980 batches in 239.22 sec.    training loss:\t\t3.39496930597\n",
      "Done 990 batches in 241.49 sec.    training loss:\t\t3.3945837419\n",
      "Done 1000 batches in 243.91 sec.    training loss:\t\t3.39434212127\n",
      "Done 1010 batches in 246.39 sec.    training loss:\t\t3.39441648929\n",
      "Done 1020 batches in 248.56 sec.    training loss:\t\t3.39410142025\n",
      "Done 1030 batches in 251.16 sec.    training loss:\t\t3.39401391353\n",
      "Done 1040 batches in 253.82 sec.    training loss:\t\t3.39417077434\n",
      "Done 1050 batches in 256.25 sec.    training loss:\t\t3.39409782105\n",
      "Done 1060 batches in 258.70 sec.    training loss:\t\t3.39415227328\n",
      "Done 1070 batches in 261.08 sec.    training loss:\t\t3.39387061616\n",
      "Done 1080 batches in 263.46 sec.    training loss:\t\t3.39364463145\n",
      "Done 1090 batches in 266.11 sec.    training loss:\t\t3.39379821832\n",
      "Done 1100 batches in 268.55 sec.    training loss:\t\t3.39362370661\n",
      "Done 1110 batches in 271.00 sec.    training loss:\t\t3.39347478489\n",
      "Done 1120 batches in 273.59 sec.    training loss:\t\t3.3934751577\n",
      "Done 1130 batches in 276.03 sec.    training loss:\t\t3.39348955804\n",
      "Done 1140 batches in 278.55 sec.    training loss:\t\t3.39319777799\n",
      "Done 1150 batches in 280.88 sec.    training loss:\t\t3.39330306512\n",
      "Done 1160 batches in 283.41 sec.    training loss:\t\t3.39311403141\n",
      "Done 1170 batches in 286.02 sec.    training loss:\t\t3.3930498155\n",
      "Done 1180 batches in 288.47 sec.    training loss:\t\t3.39304780009\n",
      "Done 1190 batches in 291.13 sec.    training loss:\t\t3.39307083517\n",
      "Done 1200 batches in 293.71 sec.    training loss:\t\t3.39292625748\n",
      "Done 1210 batches in 296.28 sec.    training loss:\t\t3.39286707977\n",
      "Done 1220 batches in 298.67 sec.    training loss:\t\t3.39275208728\n",
      "Done 1230 batches in 301.13 sec.    training loss:\t\t3.39273256642\n",
      "Done 1240 batches in 303.68 sec.    training loss:\t\t3.39264501477\n",
      "Done 1250 batches in 306.32 sec.    training loss:\t\t3.3926589585\n",
      "Done 1260 batches in 308.78 sec.    training loss:\t\t3.39271363748\n",
      "Done 1270 batches in 311.17 sec.    training loss:\t\t3.39265875321\n",
      "Done 1280 batches in 313.66 sec.    training loss:\t\t3.39256090798\n",
      "Done 1290 batches in 316.09 sec.    training loss:\t\t3.392526649\n",
      "Done 1300 batches in 318.59 sec.    training loss:\t\t3.39248487608\n",
      "Done 1310 batches in 321.19 sec.    training loss:\t\t3.39254850508\n",
      "Done 1320 batches in 323.42 sec.    training loss:\t\t3.39255336175\n",
      "Done 1330 batches in 325.85 sec.    training loss:\t\t3.39253497027\n",
      "Done 1340 batches in 328.24 sec.    training loss:\t\t3.39230242465\n",
      "Done 1350 batches in 330.86 sec.    training loss:\t\t3.39211082055\n",
      "Done 1360 batches in 333.49 sec.    training loss:\t\t3.39199025981\n",
      "Done 1370 batches in 335.99 sec.    training loss:\t\t3.39176066554\n",
      "Done 1380 batches in 338.32 sec.    training loss:\t\t3.39185510043\n",
      "Done 1390 batches in 340.94 sec.    training loss:\t\t3.391752651\n",
      "Done 1400 batches in 343.45 sec.    training loss:\t\t3.39160475988\n",
      "Done 1410 batches in 345.99 sec.    training loss:\t\t3.39148057021\n",
      "Done 1420 batches in 348.21 sec.    training loss:\t\t3.39141856074\n",
      "Done 1430 batches in 350.59 sec.    training loss:\t\t3.39134560796\n",
      "Done 1440 batches in 353.11 sec.    training loss:\t\t3.39129258062\n",
      "Done 1450 batches in 355.73 sec.    training loss:\t\t3.39129350499\n",
      "Done 1460 batches in 358.11 sec.    training loss:\t\t3.39123221371\n",
      "Done 1470 batches in 360.43 sec.    training loss:\t\t3.39118746494\n",
      "Done 1480 batches in 363.10 sec.    training loss:\t\t3.39117652412\n",
      "Done 1490 batches in 365.70 sec.    training loss:\t\t3.39113859459\n",
      "Done 1500 batches in 368.27 sec.    training loss:\t\t3.39103640826\n",
      "Done 1510 batches in 370.81 sec.    training loss:\t\t3.39104320851\n",
      "Done 1520 batches in 373.31 sec.    training loss:\t\t3.39105377856\n",
      "Done 1530 batches in 375.86 sec.    training loss:\t\t3.39113550734\n",
      "Done 1540 batches in 378.34 sec.    training loss:\t\t3.39102511856\n",
      "Done 1550 batches in 380.72 sec.    training loss:\t\t3.39096917711\n",
      "Done 1560 batches in 383.27 sec.    training loss:\t\t3.3908810929\n",
      "Done 1570 batches in 385.75 sec.    training loss:\t\t3.39099030551\n",
      "Done 1580 batches in 388.02 sec.    training loss:\t\t3.39083604872\n",
      "Done 1590 batches in 390.82 sec.    training loss:\t\t3.390823075\n",
      "Done 1600 batches in 393.17 sec.    training loss:\t\t3.39067721018\n",
      "Done 1610 batches in 395.34 sec.    training loss:\t\t3.39070674854\n",
      "Done 1620 batches in 397.98 sec.    training loss:\t\t3.3906722414\n",
      "Done 1630 batches in 400.43 sec.    training loss:\t\t3.39059555939\n",
      "Done 1640 batches in 403.23 sec.    training loss:\t\t3.39066346016\n",
      "Done 1650 batches in 405.91 sec.    training loss:\t\t3.3905548628\n",
      "Done 1660 batches in 408.38 sec.    training loss:\t\t3.39050624094\n",
      "Done 1670 batches in 410.96 sec.    training loss:\t\t3.39035522378\n",
      "Done 1680 batches in 413.60 sec.    training loss:\t\t3.39036985017\n",
      "Done 1690 batches in 416.23 sec.    training loss:\t\t3.39034465541\n",
      "Done 1700 batches in 418.49 sec.    training loss:\t\t3.39063871114\n",
      "Done 1710 batches in 420.95 sec.    training loss:\t\t3.39053120039\n",
      "Done 1720 batches in 423.32 sec.    training loss:\t\t3.39050519966\n",
      "Done 1730 batches in 425.98 sec.    training loss:\t\t3.39048784261\n",
      "Done 1740 batches in 428.35 sec.    training loss:\t\t3.39035050249\n",
      "Done 1750 batches in 430.86 sec.    training loss:\t\t3.39043099833\n",
      "Done 1760 batches in 433.41 sec.    training loss:\t\t3.39048176985\n",
      "Done 1770 batches in 435.70 sec.    training loss:\t\t3.39053001396\n",
      "Done 1780 batches in 438.35 sec.    training loss:\t\t3.39050825523\n",
      "Done 1790 batches in 440.85 sec.    training loss:\t\t3.39051611057\n",
      "Done 1800 batches in 443.35 sec.    training loss:\t\t3.39048409881\n",
      "Done 1810 batches in 445.90 sec.    training loss:\t\t3.39048039733\n",
      "Done 1820 batches in 448.64 sec.    training loss:\t\t3.3906800201\n",
      "Done 1830 batches in 451.11 sec.    training loss:\t\t3.39069260174\n",
      "Done 1840 batches in 453.42 sec.    training loss:\t\t3.39068016523\n",
      "Done 1850 batches in 455.76 sec.    training loss:\t\t3.39070287154\n",
      "Done 1860 batches in 457.94 sec.    training loss:\t\t3.3905736153\n",
      "Done 1870 batches in 460.45 sec.    training loss:\t\t3.39055033585\n",
      "Done 1880 batches in 462.74 sec.    training loss:\t\t3.39041278668\n",
      "Done 1890 batches in 465.39 sec.    training loss:\t\t3.39046785882\n",
      "Done 1900 batches in 467.84 sec.    training loss:\t\t3.39041370069\n",
      "Done 1910 batches in 470.37 sec.    training loss:\t\t3.39041323599\n",
      "Done 1920 batches in 472.77 sec.    training loss:\t\t3.3902759635\n",
      "Done 1930 batches in 475.23 sec.    training loss:\t\t3.39027035019\n",
      "Done 1940 batches in 477.90 sec.    training loss:\t\t3.39032621047\n",
      "Done 1950 batches in 480.30 sec.    training loss:\t\t3.3902550818\n",
      "Done 1960 batches in 482.85 sec.    training loss:\t\t3.39022649518\n",
      "Done 1970 batches in 484.98 sec.    training loss:\t\t3.39015255261\n",
      "Done 1980 batches in 487.30 sec.    training loss:\t\t3.39002692243\n",
      "Done 1990 batches in 489.74 sec.    training loss:\t\t3.38998916971\n",
      "Done 2000 batches in 492.39 sec.    training loss:\t\t3.38994406836\n",
      "Done 2010 batches in 495.09 sec.    training loss:\t\t3.38986994102\n",
      "Done 2020 batches in 497.55 sec.    training loss:\t\t3.38967081547\n",
      "Done 2030 batches in 500.06 sec.    training loss:\t\t3.38955967885\n",
      "Done 2040 batches in 502.44 sec.    training loss:\t\t3.38957230317\n",
      "Done 2050 batches in 504.72 sec.    training loss:\t\t3.38958133194\n",
      "Done 2060 batches in 507.30 sec.    training loss:\t\t3.38963316332\n",
      "Done 2070 batches in 509.73 sec.    training loss:\t\t3.38949105649\n",
      "Done 2080 batches in 512.22 sec.    training loss:\t\t3.38942145594\n",
      "Done 2090 batches in 514.66 sec.    training loss:\t\t3.38941508452\n",
      "Done 2100 batches in 517.06 sec.    training loss:\t\t3.38939491825\n",
      "Done 2110 batches in 519.30 sec.    training loss:\t\t3.38930410021\n",
      "Done 2120 batches in 521.71 sec.    training loss:\t\t3.38927151465\n",
      "Done 2130 batches in 523.93 sec.    training loss:\t\t3.38920385088\n",
      "Done 2140 batches in 526.27 sec.    training loss:\t\t3.38909712735\n",
      "Done 2150 batches in 528.82 sec.    training loss:\t\t3.38899343786\n",
      "Done 2160 batches in 531.35 sec.    training loss:\t\t3.38893772753\n",
      "Done 2170 batches in 533.97 sec.    training loss:\t\t3.38888755401\n",
      "Done 2180 batches in 536.25 sec.    training loss:\t\t3.38891541148\n",
      "Done 2190 batches in 538.47 sec.    training loss:\t\t3.38888864349\n",
      "Done 2200 batches in 540.95 sec.    training loss:\t\t3.3887554492\n",
      "Done 2210 batches in 543.48 sec.    training loss:\t\t3.3887255278\n",
      "Done 2220 batches in 545.88 sec.    training loss:\t\t3.38868289774\n",
      "Done 2230 batches in 548.12 sec.    training loss:\t\t3.38858904774\n",
      "Done 2240 batches in 550.71 sec.    training loss:\t\t3.38851352034\n",
      "Done 2250 batches in 553.25 sec.    training loss:\t\t3.3885042739\n",
      "Done 2260 batches in 555.57 sec.    training loss:\t\t3.38841793281\n",
      "Done 2270 batches in 558.03 sec.    training loss:\t\t3.38844068078\n",
      "Done 2280 batches in 560.55 sec.    training loss:\t\t3.38836229646\n",
      "Done 2290 batches in 563.25 sec.    training loss:\t\t3.38836899059\n",
      "Done 2300 batches in 565.80 sec.    training loss:\t\t3.38837700596\n",
      "Done 2310 batches in 568.39 sec.    training loss:\t\t3.38834010786\n",
      "Done 2320 batches in 570.86 sec.    training loss:\t\t3.38830521397\n",
      "Done 2330 batches in 573.56 sec.    training loss:\t\t3.38836447048\n",
      "Done 2340 batches in 575.99 sec.    training loss:\t\t3.38827877715\n",
      "Done 2350 batches in 578.49 sec.    training loss:\t\t3.38820421377\n",
      "Done 2360 batches in 580.84 sec.    training loss:\t\t3.38816826895\n",
      "Done 2370 batches in 583.29 sec.    training loss:\t\t3.38822387369\n",
      "Done 2380 batches in 585.64 sec.    training loss:\t\t3.38814926913\n",
      "Done 2390 batches in 588.02 sec.    training loss:\t\t3.38809021967\n",
      "Done 2400 batches in 590.49 sec.    training loss:\t\t3.3880962574\n",
      "Done 2410 batches in 593.06 sec.    training loss:\t\t3.38803799404\n",
      "Done 2420 batches in 595.72 sec.    training loss:\t\t3.38809757785\n",
      "Done 2430 batches in 598.17 sec.    training loss:\t\t3.3881008974\n",
      "Done 2440 batches in 600.54 sec.    training loss:\t\t3.38811310419\n",
      "Done 2450 batches in 602.98 sec.    training loss:\t\t3.38811245193\n",
      "Done 2460 batches in 605.39 sec.    training loss:\t\t3.38815976918\n",
      "Done 2470 batches in 607.90 sec.    training loss:\t\t3.38816319633\n",
      "Done 2480 batches in 610.48 sec.    training loss:\t\t3.38809072174\n",
      "Done 2490 batches in 613.14 sec.    training loss:\t\t3.38803439503\n",
      "Done 2500 batches in 615.66 sec.    training loss:\t\t3.38803356946\n",
      "Done 2510 batches in 618.07 sec.    training loss:\t\t3.38805177071\n",
      "Done 2520 batches in 620.45 sec.    training loss:\t\t3.38800346975\n",
      "Done 2530 batches in 623.21 sec.    training loss:\t\t3.38799290036\n",
      "Done 2540 batches in 625.64 sec.    training loss:\t\t3.38794705948\n",
      "Done 2550 batches in 628.23 sec.    training loss:\t\t3.38799752739\n",
      "Done 2560 batches in 630.75 sec.    training loss:\t\t3.38796665476\n",
      "Done 2570 batches in 633.24 sec.    training loss:\t\t3.38786872553\n",
      "Done 2580 batches in 635.98 sec.    training loss:\t\t3.38786427394\n",
      "Done 2590 batches in 638.37 sec.    training loss:\t\t3.38782084168\n",
      "Done 2600 batches in 640.88 sec.    training loss:\t\t3.38779181569\n",
      "Done 2610 batches in 643.50 sec.    training loss:\t\t3.3877983639\n",
      "Done 2620 batches in 646.13 sec.    training loss:\t\t3.38768538535\n",
      "Done 2630 batches in 648.58 sec.    training loss:\t\t3.38767696863\n",
      "Done 2640 batches in 651.27 sec.    training loss:\t\t3.38762688435\n",
      "Done 2650 batches in 653.74 sec.    training loss:\t\t3.38760257807\n",
      "Done 2660 batches in 656.10 sec.    training loss:\t\t3.38754670315\n",
      "Done 2670 batches in 658.38 sec.    training loss:\t\t3.38741153392\n",
      "Done 2680 batches in 661.05 sec.    training loss:\t\t3.38741320727\n",
      "Done 2690 batches in 663.56 sec.    training loss:\t\t3.38744925269\n",
      "Done 2700 batches in 666.07 sec.    training loss:\t\t3.38739559736\n",
      "Done 2710 batches in 668.33 sec.    training loss:\t\t3.38739656917\n",
      "Done 2720 batches in 670.71 sec.    training loss:\t\t3.38742601356\n",
      "Done 2730 batches in 673.08 sec.    training loss:\t\t3.38741241134\n",
      "Done 2740 batches in 675.43 sec.    training loss:\t\t3.38732787557\n",
      "Done 2750 batches in 678.03 sec.    training loss:\t\t3.38727989471\n",
      "Done 2760 batches in 680.53 sec.    training loss:\t\t3.38732198987\n",
      "Done 2770 batches in 683.26 sec.    training loss:\t\t3.38732081549\n",
      "Done 2780 batches in 685.39 sec.    training loss:\t\t3.38728693875\n",
      "Done 2790 batches in 687.94 sec.    training loss:\t\t3.38721672421\n",
      "Done 2800 batches in 690.27 sec.    training loss:\t\t3.38723806392\n",
      "Done 2810 batches in 692.68 sec.    training loss:\t\t3.38725605693\n",
      "Done 2820 batches in 695.08 sec.    training loss:\t\t3.38723753394\n",
      "Done 2830 batches in 697.93 sec.    training loss:\t\t3.38722218826\n",
      "Done 2840 batches in 700.44 sec.    training loss:\t\t3.38713101842\n",
      "Done 2850 batches in 702.86 sec.    training loss:\t\t3.38709144344\n",
      "Done 2860 batches in 705.63 sec.    training loss:\t\t3.38705991675\n",
      "Done 2870 batches in 708.23 sec.    training loss:\t\t3.38701567858\n",
      "Done 2880 batches in 710.54 sec.    training loss:\t\t3.38699951425\n",
      "Done 2890 batches in 713.12 sec.    training loss:\t\t3.38688141146\n",
      "Done 2900 batches in 715.68 sec.    training loss:\t\t3.38678116842\n",
      "Done 2910 batches in 717.85 sec.    training loss:\t\t3.38670489705\n",
      "Done 2920 batches in 720.29 sec.    training loss:\t\t3.38664349925\n",
      "Done 2930 batches in 722.67 sec.    training loss:\t\t3.38661267737\n",
      "Done 2940 batches in 725.16 sec.    training loss:\t\t3.38654028081\n",
      "Done 2950 batches in 727.72 sec.    training loss:\t\t3.38648221235\n",
      "Done 2960 batches in 730.32 sec.    training loss:\t\t3.38648105785\n",
      "Done 2970 batches in 732.63 sec.    training loss:\t\t3.38647819818\n",
      "Done 2980 batches in 735.03 sec.    training loss:\t\t3.38648219687\n",
      "Done 2990 batches in 737.54 sec.    training loss:\t\t3.38645099112\n",
      "Done 3000 batches in 740.13 sec.    training loss:\t\t3.3863731343\n",
      "Done 3010 batches in 742.52 sec.    training loss:\t\t3.38641007685\n",
      "Done 3020 batches in 744.77 sec.    training loss:\t\t3.38639667233\n",
      "Done 3030 batches in 747.28 sec.    training loss:\t\t3.38634782405\n",
      "Done 3040 batches in 749.88 sec.    training loss:\t\t3.38636228228\n",
      "Done 3050 batches in 752.54 sec.    training loss:\t\t3.38627767793\n",
      "Done 3060 batches in 754.98 sec.    training loss:\t\t3.38620742913\n",
      "Done 3070 batches in 757.47 sec.    training loss:\t\t3.38613131369\n",
      "Done 3080 batches in 759.94 sec.    training loss:\t\t3.3860794746\n",
      "Done 3090 batches in 762.41 sec.    training loss:\t\t3.38598933663\n",
      "Done 3100 batches in 764.75 sec.    training loss:\t\t3.38594842831\n",
      "Done 3110 batches in 767.22 sec.    training loss:\t\t3.38603859035\n",
      "Done 3120 batches in 769.74 sec.    training loss:\t\t3.38593583515\n",
      "Done 3130 batches in 772.38 sec.    training loss:\t\t3.38587507415\n",
      "Done 3140 batches in 774.70 sec.    training loss:\t\t3.38587303478\n",
      "Done 3150 batches in 776.81 sec.    training loss:\t\t3.38582848741\n",
      "Done 3160 batches in 779.34 sec.    training loss:\t\t3.3858775532\n",
      "Done 3170 batches in 781.80 sec.    training loss:\t\t3.38575958191\n",
      "Done 3180 batches in 784.38 sec.    training loss:\t\t3.38578348715\n",
      "Done 3190 batches in 786.83 sec.    training loss:\t\t3.38577119401\n",
      "Done 3200 batches in 789.29 sec.    training loss:\t\t3.38576179184\n",
      "Done 3210 batches in 791.72 sec.    training loss:\t\t3.38582719186\n",
      "Done 3220 batches in 794.24 sec.    training loss:\t\t3.38578445995\n",
      "Done 3230 batches in 796.85 sec.    training loss:\t\t3.38575010166\n",
      "Done 3240 batches in 799.38 sec.    training loss:\t\t3.38580888771\n",
      "Done 3250 batches in 801.84 sec.    training loss:\t\t3.38579666945\n",
      "Done 3260 batches in 804.21 sec.    training loss:\t\t3.38573232756\n",
      "Done 3270 batches in 806.75 sec.    training loss:\t\t3.38561619319\n",
      "Done 3280 batches in 809.43 sec.    training loss:\t\t3.3855645114\n",
      "Done 3290 batches in 811.72 sec.    training loss:\t\t3.38548311414\n",
      "Done 3300 batches in 814.29 sec.    training loss:\t\t3.38543130593\n",
      "Done 3310 batches in 816.63 sec.    training loss:\t\t3.38545100454\n",
      "Done 3320 batches in 818.91 sec.    training loss:\t\t3.38539105888\n",
      "Done 3330 batches in 821.51 sec.    training loss:\t\t3.38536595551\n",
      "Done 3340 batches in 823.97 sec.    training loss:\t\t3.38530841379\n",
      "Done 3350 batches in 826.49 sec.    training loss:\t\t3.38537096408\n",
      "Done 3360 batches in 828.92 sec.    training loss:\t\t3.38534518897\n",
      "Done 3370 batches in 831.43 sec.    training loss:\t\t3.38529285323\n",
      "Done 3380 batches in 833.89 sec.    training loss:\t\t3.38524061787\n",
      "Done 3390 batches in 836.21 sec.    training loss:\t\t3.38524945924\n",
      "Done 3400 batches in 838.72 sec.    training loss:\t\t3.38516931193\n",
      "Done 3410 batches in 841.23 sec.    training loss:\t\t3.38512111647\n",
      "Done 3420 batches in 843.45 sec.    training loss:\t\t3.38509710554\n",
      "Done 3430 batches in 845.76 sec.    training loss:\t\t3.38507313423\n",
      "Done 3440 batches in 848.22 sec.    training loss:\t\t3.38504938356\n",
      "Done 3450 batches in 850.52 sec.    training loss:\t\t3.38510572085\n",
      "Done 3460 batches in 853.11 sec.    training loss:\t\t3.38504210982\n",
      "Done 3470 batches in 855.55 sec.    training loss:\t\t3.38503941276\n",
      "Done 3480 batches in 858.03 sec.    training loss:\t\t3.3849952182\n",
      "Done 3490 batches in 860.57 sec.    training loss:\t\t3.38499603144\n",
      "Done 3500 batches in 863.17 sec.    training loss:\t\t3.38499551397\n",
      "Done 3510 batches in 865.62 sec.    training loss:\t\t3.38499806085\n",
      "Done 3520 batches in 868.05 sec.    training loss:\t\t3.38501245739\n",
      "Done 3530 batches in 870.37 sec.    training loss:\t\t3.38498319364\n",
      "Done 3540 batches in 872.85 sec.    training loss:\t\t3.38493072115\n",
      "Done 3550 batches in 875.13 sec.    training loss:\t\t3.38485361783\n",
      "Done 3560 batches in 877.80 sec.    training loss:\t\t3.38483674701\n",
      "Done 3570 batches in 880.35 sec.    training loss:\t\t3.3848401455\n",
      "Done 3580 batches in 882.87 sec.    training loss:\t\t3.38478400141\n",
      "Done 3590 batches in 885.48 sec.    training loss:\t\t3.38476662075\n",
      "Done 3600 batches in 888.20 sec.    training loss:\t\t3.38473415287\n",
      "Done 3610 batches in 890.79 sec.    training loss:\t\t3.38472918246\n",
      "Done 3620 batches in 893.03 sec.    training loss:\t\t3.38461732754\n",
      "Done 3630 batches in 895.31 sec.    training loss:\t\t3.38459129306\n",
      "Done 3640 batches in 897.84 sec.    training loss:\t\t3.38453891886\n",
      "Done 3650 batches in 900.37 sec.    training loss:\t\t3.38445933432\n",
      "Done 3660 batches in 902.72 sec.    training loss:\t\t3.38448179182\n",
      "Done 3670 batches in 905.03 sec.    training loss:\t\t3.38444256634\n",
      "Done 3680 batches in 907.33 sec.    training loss:\t\t3.38436708369\n",
      "Done 3690 batches in 909.68 sec.    training loss:\t\t3.38433741384\n",
      "Done 3700 batches in 912.23 sec.    training loss:\t\t3.38436218637\n",
      "Done 3710 batches in 914.53 sec.    training loss:\t\t3.38431348835\n",
      "Done 3720 batches in 916.95 sec.    training loss:\t\t3.38429026097\n",
      "Done 3730 batches in 919.31 sec.    training loss:\t\t3.38425958504\n",
      "Done 3740 batches in 921.81 sec.    training loss:\t\t3.38421080488\n",
      "Done 3750 batches in 924.22 sec.    training loss:\t\t3.38425616746\n",
      "Done 3760 batches in 926.81 sec.    training loss:\t\t3.38417614514\n",
      "Done 3770 batches in 929.17 sec.    training loss:\t\t3.38419109311\n",
      "Done 3780 batches in 931.52 sec.    training loss:\t\t3.38414874976\n",
      "Done 3790 batches in 934.01 sec.    training loss:\t\t3.3841643648\n",
      "Done 3800 batches in 936.55 sec.    training loss:\t\t3.38414036061\n",
      "Done 3810 batches in 939.11 sec.    training loss:\t\t3.38412831074\n",
      "Done 3820 batches in 941.72 sec.    training loss:\t\t3.38408839302\n",
      "Done 3830 batches in 944.05 sec.    training loss:\t\t3.38400797102\n",
      "Done 3840 batches in 946.62 sec.    training loss:\t\t3.38397729856\n",
      "Done 3850 batches in 949.41 sec.    training loss:\t\t3.38393263019\n",
      "Done 3860 batches in 951.96 sec.    training loss:\t\t3.38394696571\n",
      "Done 3870 batches in 954.50 sec.    training loss:\t\t3.38391334611\n",
      "Done 3880 batches in 956.99 sec.    training loss:\t\t3.38388944177\n",
      "Done 3890 batches in 959.28 sec.    training loss:\t\t3.38388762373\n",
      "Done 100 batches in 7.38 sec.\n",
      "Done 200 batches in 14.62 sec.\n",
      "Done 300 batches in 21.78 sec.\n",
      "Done 400 batches in 29.04 sec.\n",
      "Done 500 batches in 36.73 sec.\n",
      "Done 600 batches in 43.83 sec.\n",
      "Done 700 batches in 51.24 sec.\n",
      "Done 800 batches in 58.57 sec.\n",
      "Done 900 batches in 66.13 sec.\n",
      "Epoch 5 of 5 took 1033.856s\n",
      "  training loss:\t\t3.383930\n",
      "  validation loss:\t\t3.469067\n"
     ]
    }
   ],
   "source": [
    "# training, taken from mnist.py in lasagne examples\n",
    "\n",
    "num_epochs = 5\n",
    "batch_size = 50\n",
    "val_batch_size = 25\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch in iterate_minibatches(train, batch_size):\n",
    "        \n",
    "        inputs, targets, mask = batch\n",
    "        batch_err = train_fn(inputs, targets, mask)\n",
    "\n",
    "        train_err += batch_err\n",
    "        train_batches += 1\n",
    "        \n",
    "        if not train_batches % 10:\n",
    "            print \"Done {} batches in {:.2f} sec.    training loss:\\t\\t{}\".format(\n",
    "                train_batches, time.time() - start_time, train_err / train_batches)\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_err = 0\n",
    "    val_batches = 0\n",
    "    start_time2 = time.time()\n",
    "    \n",
    "    for batch in iterate_minibatches(valid, val_batch_size):\n",
    "        inputs, targets, mask = batch\n",
    "        \n",
    "        err = val_fn(inputs, targets, mask)\n",
    "        val_err += err\n",
    "        val_batches += 1\n",
    "        if not val_batches % 100:\n",
    "            print \"Done {} batches in {:.2f} sec.\".format(\n",
    "                val_batches, time.time() - start_time2)\n",
    "\n",
    "    # Then we print the results for this epoch:\n",
    "    print \"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time)\n",
    "    print \"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches)\n",
    "    print \"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches)\n",
    "    \n",
    "np.savez('5ep_w2vInit_300_300_ssoft(uni,200,non-unique)_bs50_cut200.npz', *L.layers.get_all_param_values(net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to byl test: non-unique, minusQ, unigram, num_sampled=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.savez('5ep_w2vInit_300_300_ssoft(uni,200,non-unique)_bs50_cut200.npz', *L.layers.get_all_param_values(net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with np.load('1ep_w2vInit_300_300_fullsoft_bs25_cut200.npz') as f:\n",
    "    param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "    L.layers.set_all_param_values(test_net, param_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_var = T.imatrix('inputs')\n",
    "gen_net = build_hsoft_rnnlm(input_var, None, None, voc_size, emb_size, rec_size)\n",
    "probs = L.layers.get_output(gen_net)[:,-1,:]\n",
    "get_probs = theano.function([input_var], probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_var = T.imatrix('inputs')\n",
    "gen_net = build_sampledsoft_rnnlm(input_var, None, -1, voc_size, emb_size, rec_size)\n",
    "probs = L.layers.get_output(gen_net)[:,-1,:]\n",
    "get_probs = theano.function([input_var], probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_var = T.imatrix('inputs')\n",
    "gen_net = build_simple_rnnlm(input_var, None, voc_size, emb_size, rec_size)\n",
    "probs = L.layers.get_output(gen_net)[:,-1,:]\n",
    "get_probs = theano.function([input_var], probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clone_param_values(net_from=net, net_to=gen_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with np.load('1ep_w2vPseudoFixed_300_300_fullsoft_bs25_cut200.npz') as f:\n",
    "    param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "    L.layers.set_all_param_values(gen_net, param_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnd_next_word(probs, size=1):\n",
    "    return np.random.choice(np.append(np.arange(probs.shape[0]-1), -1).astype(np.int32), \n",
    "                            size=size, p=probs)\n",
    "\n",
    "def beam_search(get_probs_fun, beam=10, init_seq='', mode='rr'):\n",
    "    utt = map(lambda w: w_to_idx.get(w, w_to_idx['<unk>']), init_seq.split())\n",
    "    if len(utt) == 0 or utt[0] != 1:\n",
    "        utt = [1] + utt\n",
    "    utt = np.asarray(utt, dtype=np.int32)[np.newaxis]\n",
    "    \n",
    "    if mode[0] == 's':\n",
    "        words = get_probs_fun(utt)[0].argpartition(-beam)[-beam:].astype(np.int32)\n",
    "        words[words==voc_size-1] = pad_value\n",
    "    elif mode[0] == 'r':\n",
    "        words = rnd_next_word(get_probs_fun(utt)[0], beam)\n",
    "    \n",
    "    candidates = utt.repeat(beam, axis=0)\n",
    "    candidates = np.hstack([candidates, words[np.newaxis].T])\n",
    "    scores = np.zeros(beam)\n",
    "    \n",
    "#     print candidates\n",
    "    \n",
    "    while candidates.shape[1] < 100 and pad_value not in candidates[:,-1]:\n",
    "        \n",
    "        if mode[1] == 's':\n",
    "            log_probs = np.log(get_probs_fun(candidates))\n",
    "            tot_scores = log_probs + scores[np.newaxis].T\n",
    "\n",
    "            idx = tot_scores.ravel().argpartition(-beam)[-beam:]\n",
    "            i,j = divmod(idx, tot_scores.shape[1])\n",
    "            j[j==voc_size-1] = pad_value\n",
    "            \n",
    "            scores = tot_scores[i,j]\n",
    "\n",
    "            candidates = np.hstack([candidates[i], j[np.newaxis].T.astype(np.int32)])\n",
    "            \n",
    "        elif mode[1] == 'r':\n",
    "            print L.layers.get_all_param_values(gen_net)[16]\n",
    "            probs = get_probs_fun(candidates)\n",
    "            words = []\n",
    "            for k in xrange(beam):\n",
    "                words.append(rnd_next_word(probs[k], beam)) # this doesn't have to be exactly 'beam'\n",
    "            words = np.array(words)\n",
    "            idx = np.indices((beam, words.shape[1]))[0]\n",
    "            tot_scores = scores[np.newaxis].T + np.log(probs)[idx, words]\n",
    "                \n",
    "            idx = tot_scores.ravel().argpartition(-beam)[-beam:]\n",
    "            i,j = divmod(idx, tot_scores.shape[1])\n",
    "\n",
    "            scores = tot_scores[i,j]\n",
    "\n",
    "            candidates = np.hstack([candidates[i], words[i,j][np.newaxis].T])\n",
    "            \n",
    "#     print candidates[:,:10]\n",
    "#     print scores[:10]\n",
    "        \n",
    "    cands = candidates[candidates[:,-1] == 0]\n",
    "    if cands.size > 0:\n",
    "        return candidates[candidates[:,-1] == 0][0]\n",
    "    return candidates[scores.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"mine is court , it ' s nothing . but i ' m not a full . and you know it really . let me tell you something . why not ?\""
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_seq = ''\n",
    "utt = [1] + map(lambda w: w_to_idx.get(w, w_to_idx['<unk>']), init_seq.split())\n",
    "utt = np.asarray(utt, dtype=np.int32)[np.newaxis]\n",
    "\n",
    "i = 0\n",
    "while utt[0,-1] != -1 and i < 100:\n",
    "    word_probs = get_probs(utt)[0]\n",
    "    next_idx = rnd_next_word(word_probs)\n",
    "    utt = np.append(utt, next_idx)[np.newaxis].astype(np.int32)\n",
    "    i += 1\n",
    "    \n",
    "text = map(lambda i: idx_to_w[i], list(utt[0]))\n",
    "' '.join([t for t in text if t not in ['<s>', '</s>', '<utt_end>']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

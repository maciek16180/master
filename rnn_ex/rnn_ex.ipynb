{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vendor:  Continuum Analytics, Inc.\n",
      "Package: mkl\n",
      "Message: trial mode expires in 30 days\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import time\n",
    "\n",
    "import lasagne as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mt_path = \"/pio/data/data/mtriples/\"\n",
    "\n",
    "beg_token = '<utt_beg>'\n",
    "end_token = '<utt_end>'\n",
    "\n",
    "def get_mt_voc(path=mt_path):\n",
    "    i_to_w, w_to_i = {}, {}\n",
    "    \n",
    "    i_to_w[0] = end_token   # separate tokens for beginning and ending of an utterance\n",
    "    w_to_i[end_token] = 0   # <utt_end> serves only as a target for the last word in the input sequence\n",
    "    i_to_w[1] = beg_token   # <utt_beg> will always be the first generated word\n",
    "    w_to_i[beg_token] = 1    \n",
    "    wc = 2\n",
    "    \n",
    "    with open(path + \"WordsList.txt\", \"r\") as wl:\n",
    "        for w in wl:\n",
    "            i_to_w[wc] = w[:-1]\n",
    "            w_to_i[w[:-1]] = wc\n",
    "            wc += 1\n",
    "    \n",
    "    return i_to_w, w_to_i, wc\n",
    "\n",
    "\n",
    "mt_batch_size = 50\n",
    "mt_i_to_w, mt_w_to_i, mt_voc_size = get_mt_voc()\n",
    "\n",
    "def load_mt(path=mt_path):\n",
    "    tr = None\n",
    "    vl = None\n",
    "    ts = None\n",
    "    \n",
    "    with open(path + \"Training_Shuffled_Dataset.txt\") as f:\n",
    "        tr = []\n",
    "        for l in f:\n",
    "            tr.insert(0, [1] + map(lambda w: mt_w_to_i.get(w, mt_w_to_i['<unk>']), l.split()) + [0])\n",
    "        \n",
    "    with open(path + \"Validation_Shuffled_Dataset.txt\") as f:\n",
    "        vl = []\n",
    "        for l in f:\n",
    "            vl.insert(0, [1] + map(lambda w: mt_w_to_i.get(w, mt_w_to_i['<unk>']), l.split()) + [0])\n",
    "            \n",
    "    with open(path + \"Test_Shuffled_Dataset.txt\") as f:\n",
    "        ts = []\n",
    "        for l in f:\n",
    "            ts.insert(0, [1] + map(lambda w: mt_w_to_i.get(w, mt_w_to_i['<unk>']), l.split()) + [0])\n",
    "    \n",
    "    return tr, vl, ts\n",
    "\n",
    "mt_train, mt_val, mt_test = load_mt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Similar to Lasagne mnist.py example, added input mask and different sequence lengths\n",
    "\n",
    "def iterate_minibatches(inputs, batchsize, shuffle=False):\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        t0 = time.time() # time wasted preparing data, just for the info\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        inp = inputs[excerpt]\n",
    "        \n",
    "        inp_max_len = len(max(inp, key=len))\n",
    "        inp = map(lambda l: l + [-1]*(inp_max_len-len(l)), inp)\n",
    "        inp = np.asarray(inp, dtype=np.int32)\n",
    "        tar = np.hstack((inp[:,1:], np.asarray([-1]*batchsize, dtype=np.int32).reshape((-1,1))))\n",
    "        def gr_zero(x):\n",
    "            if x > 0:\n",
    "                return 1.\n",
    "            return 0.\n",
    "        v_gr_zero = np.vectorize(gr_zero, otypes=[np.float32])\n",
    "        mask = v_gr_zero(inp) # 0 in vocabulary represents <utt_end>, we don't feed that into the net\n",
    "        \n",
    "        yield inp, tar, mask, (time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_simple_rnnlm(input_var, mask_input_var, voc_size, emb_size, rec_size):\n",
    "    l_in = L.layers.InputLayer(shape=(None, None), input_var=input_var)    \n",
    "    batch_size, seq_len = l_in.input_var.shape\n",
    "    l_mask = L.layers.InputLayer(shape=(batch_size, seq_len), input_var=mask_input_var)\n",
    "    \n",
    "    l_emb = L.layers.EmbeddingLayer(l_in,\n",
    "                                    input_size=voc_size, \n",
    "                                    output_size=emb_size)\n",
    "    \n",
    "    l_rec = L.layers.RecurrentLayer(l_emb,\n",
    "                                    num_units=rec_size, \n",
    "                                    W_in_to_hid=L.init.Orthogonal(), \n",
    "                                    W_hid_to_hid=L.init.Orthogonal(),\n",
    "                                    mask_input=l_mask)\n",
    "    \n",
    "    l_resh = L.layers.ReshapeLayer(l_rec, shape=(-1, rec_size))\n",
    "    \n",
    "    l_soft = L.layers.DenseLayer(l_resh,\n",
    "                                num_units=voc_size,\n",
    "                                nonlinearity=L.nonlinearities.softmax)\n",
    "    \n",
    "    l_out = L.layers.ReshapeLayer(l_soft, shape=(batch_size, seq_len, voc_size))\n",
    "    \n",
    "    return l_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "voc_size = mt_voc_size\n",
    "emb_size = 50\n",
    "rec_size = 100\n",
    "\n",
    "input_var = T.imatrix('inputs')\n",
    "targets = T.imatrix('targets') # these will be inputs shifted by 1\n",
    "mask_input_var = T.matrix('input_mask')\n",
    "\n",
    "net = build_simple_rnnlm(input_var, mask_input_var, voc_size, emb_size, rec_size)\n",
    "out = L.layers.get_output(net)\n",
    "\n",
    "loss = L.objectives.categorical_crossentropy(out.reshape((-1,voc_size)), targets.ravel())\n",
    "loss = loss.mean() # mean batch loss\n",
    "\n",
    "params = L.layers.get_all_params(net, trainable=True)\n",
    "updates = L.updates.rmsprop(loss, params)\n",
    "\n",
    "train_fn = theano.function([input_var, targets, mask_input_var], loss, updates=updates)\n",
    "\n",
    "#### for validation\n",
    "\n",
    "test_out = L.layers.get_output(net, deterministic=True)\n",
    "test_loss = L.objectives.categorical_crossentropy(test_out.reshape((-1,voc_size)), targets.ravel())\n",
    "test_loss = test_loss.mean()\n",
    "test_acc = T.mean(T.eq(T.argmax(test_out, axis=1), targets), dtype=theano.config.floatX)\n",
    "\n",
    "val_fn = theano.function([input_var, targets, mask_input_var], [test_loss, test_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 5 batches in 9.14288902283 sec. Time wasted: 0.00947022438049 sec. Training time: 9.13328886032 sec.\n",
      "Done 10 batches in 18.2482941151 sec. Time wasted: 0.0194792747498 sec. Training time: 18.2285096645 sec.\n",
      "Done 15 batches in 25.5649681091 sec. Time wasted: 0.0280480384827 sec. Training time: 25.536460638 sec.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-fb0febe4682e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mbatch_training_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mtrain_err\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mtraining_time\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbatch_training_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mtrain_batches\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/pio/os/anaconda/lib/python2.7/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    593\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 595\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    596\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/pio/os/anaconda/lib/python2.7/site-packages/theano/scan_module/scan_op.py\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[0;32m    668\u001b[0m         \u001b[0mallow_gc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mallow_gc\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mallow_gc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 670\u001b[1;33m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[0m\u001b[0;32m    671\u001b[0m                  allow_gc=allow_gc):\n\u001b[0;32m    672\u001b[0m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training, taken from mnist.py in lasagne examples\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    time_wasted = 0\n",
    "    training_time = 0\n",
    "    \n",
    "    for batch in iterate_minibatches(mt_train, mt_batch_size):\n",
    "        \n",
    "        inputs, targets, mask, t = batch\n",
    "        \n",
    "        batch_training_time = time.time()\n",
    "        train_err += train_fn(inputs, targets, mask)\n",
    "        training_time += time.time() - batch_training_time\n",
    "        train_batches += 1\n",
    "        \n",
    "        time_wasted += t\n",
    "        if not train_batches % 5:\n",
    "            print(\"Done {} batches in {} sec. Time wasted: {} sec. Training time: {} sec.\").format(\n",
    "                train_batches, time.time() - start_time, time_wasted, training_time)\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_err = 0\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(mt_val, mt_batch_size):\n",
    "        inputs, targets, mask = batch\n",
    "        err, acc = val_fn(inputs, targets, mask)\n",
    "        val_err += err\n",
    "        val_acc += acc\n",
    "        val_batches += 1\n",
    "\n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

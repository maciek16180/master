{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 780 (CNMeM is enabled with initial size: 30.0% of memory, cuDNN 4007)\n",
      "/home/i258346/.local/lib/python2.7/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import lasagne as L\n",
    "\n",
    "sys.path.insert(0, '../HSoftmaxLayerLasagne/')\n",
    "\n",
    "from HSoftmaxLayer import HierarchicalSoftmaxDenseLayer\n",
    "from SampledSoftmaxLayer import SampledSoftmaxDenseLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mt_path = \"/pio/data/data/mtriples/\"\n",
    "\n",
    "beg_token = '<utt_beg>'\n",
    "end_token = '<utt_end>'\n",
    "\n",
    "def get_mt_voc(path=mt_path):\n",
    "    i_to_w, w_to_i = {}, {}\n",
    "    \n",
    "    i_to_w[0] = end_token   # separate tokens for beginning and ending of an utterance\n",
    "    w_to_i[end_token] = 0   # <utt_end> serves only as a target for the last word in the input sequence\n",
    "    i_to_w[1] = beg_token   # <utt_beg> will always be the first generated word\n",
    "    w_to_i[beg_token] = 1    \n",
    "    wc = 2\n",
    "    \n",
    "    with open(path + \"WordsList.txt\", \"r\") as wl:\n",
    "        for w in wl:\n",
    "            i_to_w[wc] = w[:-1]\n",
    "            w_to_i[w[:-1]] = wc\n",
    "            wc += 1\n",
    "    \n",
    "    return i_to_w, w_to_i, wc\n",
    "\n",
    "mt_i_to_w, mt_w_to_i, mt_voc_size = get_mt_voc()\n",
    "\n",
    "\n",
    "def load_mt(path=mt_path):\n",
    "    tr = None\n",
    "    vl = None\n",
    "    ts = None\n",
    "    \n",
    "    with open(path + \"Training_Shuffled_Dataset.txt\") as f:\n",
    "        tr = []\n",
    "        for l in f:\n",
    "            if len(l.split()) < 1000:\n",
    "                tr.insert(0, [1] + map(lambda w: mt_w_to_i.get(w, mt_w_to_i['<unk>']), l.split()) + [0])\n",
    "        \n",
    "    with open(path + \"Validation_Shuffled_Dataset.txt\") as f:\n",
    "        vl = []\n",
    "        for l in f:\n",
    "            if len(l.split()) < 1000:\n",
    "                vl.insert(0, [1] + map(lambda w: mt_w_to_i.get(w, mt_w_to_i['<unk>']), l.split()) + [0])\n",
    "            \n",
    "    with open(path + \"Test_Shuffled_Dataset.txt\") as f:\n",
    "        ts = []\n",
    "        for l in f:\n",
    "            if len(l.split()) < 1000:\n",
    "                ts.insert(0, [1] + map(lambda w: mt_w_to_i.get(w, mt_w_to_i['<unk>']), l.split()) + [0])\n",
    "    \n",
    "    return tr, vl, ts\n",
    "\n",
    "mt_train, mt_val, mt_test = load_mt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Similar to Lasagne mnist.py example, added input mask and different sequence lengths\n",
    "\n",
    "def iterate_minibatches(inputs, batchsize):\n",
    "    pad_value = -1\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        t0 = time.time() # time wasted preparing data, just for the info\n",
    "        \n",
    "        excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        inp = inputs[excerpt]\n",
    "        \n",
    "        inp_max_len = len(max(inp, key=len))\n",
    "        inp = map(lambda l: l + [pad_value]*(inp_max_len-len(l)), inp)\n",
    "        inp = np.asarray(inp, dtype=np.int32)\n",
    "        tar = np.hstack((inp[:,1:], np.asarray([pad_value]*batchsize, dtype=np.int32).reshape((-1,1))))\n",
    "        def gr_zero(x):\n",
    "            if x > 0:\n",
    "                return 1.\n",
    "            return 0.\n",
    "        v_gr_zero = np.vectorize(gr_zero, otypes=[np.float32])\n",
    "        mask = v_gr_zero(inp) # 0 in vocabulary represents <utt_end>, we don't feed that into the net\n",
    "        \n",
    "        yield inp, tar, mask, (time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_simple_rnnlm(input_var, mask_input_var, voc_size, emb_size, rec_size):\n",
    "    l_in = L.layers.InputLayer(shape=(None, None), input_var=input_var)    \n",
    "    batch_size, seq_len = l_in.input_var.shape\n",
    "    \n",
    "    l_mask = None\n",
    "    if mask_input_var != None:\n",
    "        print 'setting up input mask...'\n",
    "        l_mask = L.layers.InputLayer(shape=(batch_size, seq_len), input_var=mask_input_var)\n",
    "    \n",
    "    l_emb = L.layers.EmbeddingLayer(l_in,\n",
    "                                    input_size=voc_size+1, \n",
    "                                    output_size=emb_size)\n",
    "    \n",
    "    l_lstm1 = L.layers.LSTMLayer(l_emb,\n",
    "                                 num_units=rec_size,\n",
    "                                 nonlinearity=L.nonlinearities.tanh,\n",
    "                                 grad_clipping=100,\n",
    "                                 mask_input=l_mask)\n",
    "    \n",
    "    l_resh = L.layers.ReshapeLayer(l_lstm1, shape=(-1, rec_size))\n",
    "    \n",
    "    l_soft = L.layers.DenseLayer(l_resh,\n",
    "                                num_units=voc_size,\n",
    "                                nonlinearity=L.nonlinearities.softmax)\n",
    "    \n",
    "    l_out = L.layers.ReshapeLayer(l_soft, shape=(batch_size, seq_len, voc_size))\n",
    "    \n",
    "    return l_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_hsoft_rnnlm(input_var, target_var, mask_input_var, voc_size, emb_size, rec_size):\n",
    "    l_in = L.layers.InputLayer(shape=(None, None), input_var=input_var)    \n",
    "    batch_size, seq_len = l_in.input_var.shape\n",
    "    l_mask = None\n",
    "    if mask_input_var != None:\n",
    "        print 'setting up input mask...'\n",
    "        l_mask = L.layers.InputLayer(shape=(batch_size, seq_len), input_var=mask_input_var)\n",
    "    \n",
    "    l_emb = L.layers.EmbeddingLayer(l_in,\n",
    "                                    input_size=voc_size+1, \n",
    "                                    output_size=emb_size)\n",
    "    \n",
    "    l_lstm1 = L.layers.LSTMLayer(l_emb,\n",
    "                                 num_units=rec_size,\n",
    "                                 nonlinearity=L.nonlinearities.tanh,\n",
    "                                 grad_clipping=100,\n",
    "                                 mask_input=l_mask)    \n",
    "    \n",
    "#     l_lstm2 = L.layers.LSTMLayer(l_lstm1,\n",
    "#                                  num_units=rec_size,\n",
    "#                                  nonlinearity=L.nonlinearities.tanh,\n",
    "#                                  grad_clipping=100,\n",
    "#                                  mask_input=l_mask)\n",
    "    \n",
    "    l_resh = L.layers.ReshapeLayer(l_lstm1, shape=(-1, rec_size))\n",
    "    \n",
    "    # hierarchical softmax\n",
    "    \n",
    "    l_resh_tar = None\n",
    "    if target_var != None:\n",
    "        print 'setting up targets for hsoftmax...'\n",
    "        l_tar = L.layers.InputLayer(shape=(None, None), input_var=target_var)\n",
    "        l_resh_tar = L.layers.ReshapeLayer(l_tar, shape=(-1, 1))\n",
    "        \n",
    "    l_hsoft = HierarchicalSoftmaxDenseLayer(l_resh,\n",
    "                                            num_units=voc_size,\n",
    "                                            target=l_resh_tar)\n",
    "    l_out = None\n",
    "    if target_var != None:\n",
    "        l_out = L.layers.ReshapeLayer(l_hsoft, shape=(batch_size, seq_len))\n",
    "    else:\n",
    "        l_out = L.layers.ReshapeLayer(l_hsoft, shape=(batch_size, seq_len, voc_size))\n",
    "    \n",
    "    return l_out\n",
    "\n",
    "# 1 epoch on gpu with hsoft took about 700s, batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_sampledsoft_rnnlm(input_var, mask_input_var, voc_mask, voc_size, emb_size, rec_size, target_var=None):\n",
    "    l_in = L.layers.InputLayer(shape=(None, None), input_var=input_var)    \n",
    "    batch_size, seq_len = l_in.input_var.shape\n",
    "    l_mask = None\n",
    "    if mask_input_var != None:\n",
    "        print 'setting up input mask...'\n",
    "        l_mask = L.layers.InputLayer(shape=(batch_size, seq_len), input_var=mask_input_var)\n",
    "    \n",
    "    l_emb = L.layers.EmbeddingLayer(l_in,\n",
    "                                    input_size=voc_size+1, \n",
    "                                    output_size=emb_size)\n",
    "    \n",
    "    l_lstm1 = L.layers.LSTMLayer(l_emb,\n",
    "                                 num_units=rec_size,\n",
    "                                 nonlinearity=L.nonlinearities.tanh,\n",
    "                                 grad_clipping=100,\n",
    "                                 mask_input=l_mask)\n",
    "    \n",
    "    l_lstm2 = L.layers.LSTMLayer(l_lstm1,\n",
    "                                 num_units=rec_size,\n",
    "                                 nonlinearity=L.nonlinearities.tanh,\n",
    "                                 grad_clipping=100,\n",
    "                                 mask_input=l_mask)\n",
    "      \n",
    "    l_resh = L.layers.ReshapeLayer(l_lstm2, shape=(-1, rec_size))\n",
    "    \n",
    "    l_tar = None\n",
    "    if target_var != None:\n",
    "        print 'setting up targets for sampled softmax...'\n",
    "        l_tar = L.layers.InputLayer(shape=(-1,), input_var=target_var.reshape(shape=(batch_size * seq_len,)))\n",
    "    \n",
    "    l_ssoft = SampledSoftmaxDenseLayer(l_resh, voc_mask, voc_size, l_tar)\n",
    "    \n",
    "    if target_var != None:\n",
    "        l_out = L.layers.ReshapeLayer(l_ssoft, shape=(batch_size, seq_len))\n",
    "    else:\n",
    "        l_out = L.layers.ReshapeLayer(l_ssoft, shape=(batch_size, seq_len, voc_size))\n",
    "    \n",
    "    return l_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "voc_size = mt_voc_size\n",
    "emb_size = 100\n",
    "rec_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# full softmax test\n",
    "\n",
    "input_var = T.imatrix('inputs')\n",
    "targets = T.imatrix('targets') # these will be inputs shifted by 1\n",
    "mask_input_var = T.matrix('input_mask')\n",
    "voc_mask = T.ivector('voc_mask')\n",
    "\n",
    "net = build_simple_rnnlm(input_var, mask_input_var, voc_size, emb_size, rec_size)\n",
    "out = L.layers.get_output(net)\n",
    "\n",
    "mask_idx = mask_input_var.nonzero()\n",
    "loss = L.objectives.categorical_crossentropy(out[mask_idx], targets[mask_idx])\n",
    "loss = loss.mean() # mean batch loss\n",
    "\n",
    "params = L.layers.get_all_params(net, trainable=True)\n",
    "updates = L.updates.adagrad(loss, params, learning_rate=.01)\n",
    "\n",
    "train_fn = theano.function([input_var, targets, mask_input_var], loss, updates=updates)\n",
    "\n",
    "### for validation\n",
    "\n",
    "test_out = L.layers.get_output(net, deterministic=True)\n",
    "test_loss = L.objectives.categorical_crossentropy(test_out[mask_idx], targets[mask_idx])\n",
    "test_loss = test_loss.mean()\n",
    "# test_acc = T.mean(T.eq(T.argmax(test_out, axis=1), targets), dtype=theano.config.floatX)\n",
    "\n",
    "val_fn = theano.function([input_var, targets, mask_input_var], test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up input mask...\n"
     ]
    }
   ],
   "source": [
    "# sampled softmax test\n",
    "\n",
    "input_var = T.imatrix('inputs')\n",
    "targets = T.imatrix('targets') # these will be inputs shifted by 1\n",
    "mask_input_var = T.matrix('input_mask')\n",
    "voc_mask = T.ivector('voc_mask')\n",
    "\n",
    "net = build_sampledsoft_rnnlm(input_var, mask_input_var, voc_mask, voc_size, emb_size, rec_size)\n",
    "out = L.layers.get_output(net)\n",
    "\n",
    "mask_idx = mask_input_var.nonzero()\n",
    "loss = L.objectives.categorical_crossentropy(out[mask_idx], targets[mask_idx])\n",
    "loss = loss.mean() # mean batch loss\n",
    "\n",
    "params = L.layers.get_all_params(net, trainable=True)\n",
    "updates = L.updates.adagrad(loss, params, learning_rate=.01)\n",
    "\n",
    "train_fn = theano.function([input_var, targets, mask_input_var, voc_mask], loss, updates=updates)\n",
    "\n",
    "### for validation\n",
    "\n",
    "test_out = L.layers.get_output(net, deterministic=True)\n",
    "test_loss = L.objectives.categorical_crossentropy(test_out[mask_idx], targets[mask_idx])\n",
    "test_loss = test_loss.mean()\n",
    "test_acc = T.mean(T.eq(T.argmax(test_out, axis=1), targets), dtype=theano.config.floatX)\n",
    "\n",
    "val_fn = theano.function([input_var, targets, mask_input_var, voc_mask], [test_loss, test_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up input mask...\n",
      "setting up targets for sampled softmax...\n"
     ]
    }
   ],
   "source": [
    "# sampled softmax test (with targets!)\n",
    "\n",
    "input_var = T.imatrix('inputs')\n",
    "targets = T.imatrix('targets') # these will be inputs shifted by 1\n",
    "mask_input_var = T.matrix('input_mask')\n",
    "voc_mask = T.ivector('voc_mask')\n",
    "\n",
    "net = build_sampledsoft_rnnlm(input_var, mask_input_var, voc_mask, voc_size, emb_size, rec_size, target_var=targets)\n",
    "out = L.layers.get_output(net)\n",
    "\n",
    "mask_idx = mask_input_var.nonzero()\n",
    "loss = -T.sum(T.log(out[mask_idx])) / T.sum(mask_input_var)\n",
    "\n",
    "params = L.layers.get_all_params(net, trainable=True)\n",
    "updates = L.updates.adagrad(loss, params, learning_rate=.1)\n",
    "# updates = L.updates.rmsprop(loss, params, learning_rate=.001, rho=.9, epsilon=1e-06)\n",
    "\n",
    "train_fn = theano.function([input_var, targets, mask_input_var, voc_mask], loss, updates=updates)\n",
    "\n",
    "### for validation\n",
    "\n",
    "test_out = L.layers.get_output(net, deterministic=True)\n",
    "test_loss = -T.sum(T.log(test_out[mask_idx])) / T.sum(mask_input_var)\n",
    "\n",
    "# test_acc = T.mean(T.eq(T.argmax(test_out, axis=1), targets), dtype=theano.config.floatX)\n",
    "\n",
    "val_fn = theano.function([input_var, targets, mask_input_var, voc_mask], test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up input mask...\n",
      "setting up targets for hsoftmax...\n"
     ]
    }
   ],
   "source": [
    "# hierarchical softmax test\n",
    "\n",
    "input_var = T.imatrix('inputs')\n",
    "targets = T.imatrix('targets') # these will be inputs shifted by 1\n",
    "mask_input_var = T.matrix('input_mask')\n",
    "\n",
    "net = build_hsoft_rnnlm(input_var, targets, mask_input_var, voc_size, emb_size, rec_size)\n",
    "out = L.layers.get_output(net)\n",
    "\n",
    "mask_idx = mask_input_var.nonzero()\n",
    "loss = -T.sum(T.log(out[mask_idx])) / T.sum(mask_input_var)\n",
    "\n",
    "params = L.layers.get_all_params(net, trainable=True)\n",
    "#updates = L.updates.rmsprop(loss, params, learning_rate=.001, rho=.9, epsilon=1e-06)\n",
    "updates = L.updates.adagrad(loss, params, learning_rate=.01)\n",
    "\n",
    "train_fn = theano.function([input_var, targets, mask_input_var], loss, updates=updates)\n",
    "\n",
    "#### for validation\n",
    "\n",
    "test_out = L.layers.get_output(net, deterministic=True)\n",
    "test_loss = -T.sum(T.log(test_out[mask_idx])) / T.sum(mask_input_var)\n",
    "\n",
    "#test_acc = T.mean(T.eq(T.argmax(test_out, axis=1), targets), dtype=theano.config.floatX)\n",
    "\n",
    "val_fn = theano.function([input_var, targets, mask_input_var], test_loss)\n",
    "\n",
    "### dump weights\n",
    "\n",
    "# np.savez('model.npz', *L.layers.get_all_param_values(net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with np.load('model_trained.npz') as f:\n",
    "#     param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "#     L.layers.set_all_param_values(net, param_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 10 batches in 1.74 sec.    training loss:\t\t8.01618280411\n",
      "Done 20 batches in 3.54 sec.    training loss:\t\t7.03781168461\n",
      "Done 30 batches in 5.21 sec.    training loss:\t\t6.62351954778\n",
      "Done 40 batches in 6.93 sec.    training loss:\t\t6.41977221966\n",
      "Done 50 batches in 8.89 sec.    training loss:\t\t6.2927291584\n",
      "Done 60 batches in 10.64 sec.    training loss:\t\t6.1947356383\n",
      "Done 70 batches in 11.98 sec.    training loss:\t\t6.12029126712\n",
      "Done 80 batches in 13.66 sec.    training loss:\t\t6.06091989875\n",
      "Done 90 batches in 15.43 sec.    training loss:\t\t6.01176055802\n",
      "Done 100 batches in 17.07 sec.    training loss:\t\t5.97600924969\n",
      "Done 110 batches in 18.78 sec.    training loss:\t\t5.93722072515\n",
      "Done 120 batches in 20.37 sec.    training loss:\t\t5.90387858947\n",
      "Done 130 batches in 21.77 sec.    training loss:\t\t5.87822682307\n",
      "Done 140 batches in 23.34 sec.    training loss:\t\t5.85469616822\n",
      "Done 150 batches in 25.38 sec.    training loss:\t\t5.83863047282\n",
      "Done 160 batches in 27.00 sec.    training loss:\t\t5.82116682529\n",
      "Done 170 batches in 28.84 sec.    training loss:\t\t5.80429037879\n",
      "Done 180 batches in 30.64 sec.    training loss:\t\t5.78704613845\n",
      "Done 190 batches in 32.57 sec.    training loss:\t\t5.77529234886\n",
      "Done 200 batches in 34.37 sec.    training loss:\t\t5.76129953146\n",
      "Done 210 batches in 36.52 sec.    training loss:\t\t5.74496393431\n",
      "Done 220 batches in 38.15 sec.    training loss:\t\t5.72630113255\n",
      "Done 230 batches in 40.01 sec.    training loss:\t\t5.71589875843\n",
      "Done 240 batches in 41.74 sec.    training loss:\t\t5.69994680285\n",
      "Done 250 batches in 43.33 sec.    training loss:\t\t5.68569083023\n",
      "Done 260 batches in 45.05 sec.    training loss:\t\t5.66905983595\n",
      "Done 270 batches in 46.76 sec.    training loss:\t\t5.6546087371\n",
      "Done 280 batches in 48.60 sec.    training loss:\t\t5.64050737449\n",
      "Done 290 batches in 50.15 sec.    training loss:\t\t5.62470512226\n",
      "Done 300 batches in 52.17 sec.    training loss:\t\t5.61134614944\n",
      "Done 310 batches in 53.90 sec.    training loss:\t\t5.60014277735\n",
      "Done 320 batches in 55.79 sec.    training loss:\t\t5.58679048866\n",
      "Done 330 batches in 57.69 sec.    training loss:\t\t5.5706880266\n",
      "Done 340 batches in 59.27 sec.    training loss:\t\t5.55835062756\n",
      "Done 350 batches in 61.28 sec.    training loss:\t\t5.54798319817\n",
      "Done 360 batches in 63.24 sec.    training loss:\t\t5.53690728876\n",
      "Done 370 batches in 65.18 sec.    training loss:\t\t5.52489152212\n",
      "Done 380 batches in 67.01 sec.    training loss:\t\t5.51229557489\n",
      "Done 390 batches in 68.63 sec.    training loss:\t\t5.50054623041\n",
      "Done 400 batches in 70.39 sec.    training loss:\t\t5.48915574193\n",
      "Done 410 batches in 71.97 sec.    training loss:\t\t5.47950582504\n",
      "Done 420 batches in 73.76 sec.    training loss:\t\t5.46857083979\n",
      "Done 430 batches in 75.49 sec.    training loss:\t\t5.45680805583\n",
      "Done 440 batches in 77.14 sec.    training loss:\t\t5.44808577191\n",
      "Done 450 batches in 78.98 sec.    training loss:\t\t5.43715621206\n",
      "Done 460 batches in 80.71 sec.    training loss:\t\t5.42649039911\n",
      "Done 470 batches in 82.24 sec.    training loss:\t\t5.41676587653\n",
      "Done 480 batches in 83.95 sec.    training loss:\t\t5.40835220714\n",
      "Done 490 batches in 85.68 sec.    training loss:\t\t5.40002284828\n",
      "Done 500 batches in 87.16 sec.    training loss:\t\t5.38893268776\n",
      "Done 510 batches in 88.72 sec.    training loss:\t\t5.37769361103\n",
      "Done 520 batches in 90.33 sec.    training loss:\t\t5.36922976787\n",
      "Done 530 batches in 91.80 sec.    training loss:\t\t5.36016165445\n",
      "Done 540 batches in 93.46 sec.    training loss:\t\t5.35001846243\n",
      "Done 550 batches in 95.09 sec.    training loss:\t\t5.34058024146\n",
      "Done 560 batches in 96.82 sec.    training loss:\t\t5.33265585048\n",
      "Done 570 batches in 98.70 sec.    training loss:\t\t5.32571769765\n",
      "Done 580 batches in 100.30 sec.    training loss:\t\t5.3179229243\n",
      "Done 590 batches in 101.85 sec.    training loss:\t\t5.30940847801\n",
      "Done 600 batches in 103.68 sec.    training loss:\t\t5.29884612481\n",
      "Done 610 batches in 105.76 sec.    training loss:\t\t5.29296443892\n",
      "Done 620 batches in 107.22 sec.    training loss:\t\t5.28602430128\n",
      "Done 630 batches in 109.06 sec.    training loss:\t\t5.27824624531\n",
      "Done 640 batches in 110.86 sec.    training loss:\t\t5.27045599893\n",
      "Done 650 batches in 112.62 sec.    training loss:\t\t5.26272340628\n",
      "Done 660 batches in 114.44 sec.    training loss:\t\t5.25648837884\n",
      "Done 670 batches in 116.21 sec.    training loss:\t\t5.24873056554\n",
      "Done 680 batches in 117.94 sec.    training loss:\t\t5.24263767495\n",
      "Done 690 batches in 119.50 sec.    training loss:\t\t5.23640368572\n",
      "Done 700 batches in 121.16 sec.    training loss:\t\t5.23126976286\n",
      "Done 710 batches in 122.99 sec.    training loss:\t\t5.22567747412\n",
      "Done 720 batches in 124.44 sec.    training loss:\t\t5.21880218652\n",
      "Done 730 batches in 126.24 sec.    training loss:\t\t5.21134484043\n",
      "Done 740 batches in 127.79 sec.    training loss:\t\t5.20206241157\n",
      "Done 750 batches in 129.40 sec.    training loss:\t\t5.1956232605\n",
      "Done 760 batches in 130.84 sec.    training loss:\t\t5.18947768149\n",
      "Done 770 batches in 132.30 sec.    training loss:\t\t5.18198829131\n",
      "Done 780 batches in 134.04 sec.    training loss:\t\t5.17551152156\n",
      "Done 790 batches in 135.82 sec.    training loss:\t\t5.17037231107\n",
      "Done 800 batches in 137.64 sec.    training loss:\t\t5.16568841457\n",
      "Done 810 batches in 139.38 sec.    training loss:\t\t5.15987132979\n",
      "Done 820 batches in 141.31 sec.    training loss:\t\t5.15414120511\n",
      "Done 830 batches in 143.27 sec.    training loss:\t\t5.1482129097\n",
      "Done 840 batches in 144.79 sec.    training loss:\t\t5.14323523045\n",
      "Done 850 batches in 146.29 sec.    training loss:\t\t5.13660036143\n",
      "Done 860 batches in 148.13 sec.    training loss:\t\t5.13013446719\n",
      "Done 870 batches in 149.72 sec.    training loss:\t\t5.12554511311\n",
      "Done 880 batches in 151.53 sec.    training loss:\t\t5.1193589747\n",
      "Done 890 batches in 153.45 sec.    training loss:\t\t5.1154310505\n",
      "Done 900 batches in 155.36 sec.    training loss:\t\t5.10949116654\n",
      "Done 910 batches in 156.91 sec.    training loss:\t\t5.10394632161\n",
      "Done 920 batches in 158.41 sec.    training loss:\t\t5.09945080021\n",
      "Done 930 batches in 160.20 sec.    training loss:\t\t5.09556511089\n",
      "Done 940 batches in 162.13 sec.    training loss:\t\t5.09030818533\n",
      "Done 950 batches in 163.70 sec.    training loss:\t\t5.08615626134\n",
      "Done 960 batches in 165.16 sec.    training loss:\t\t5.08109535724\n",
      "Done 970 batches in 166.69 sec.    training loss:\t\t5.07582636273\n",
      "Done 980 batches in 168.25 sec.    training loss:\t\t5.06946340191\n",
      "Done 990 batches in 169.68 sec.    training loss:\t\t5.06528469336\n",
      "Done 1000 batches in 171.28 sec.    training loss:\t\t5.06066754103\n",
      "Done 1010 batches in 173.62 sec.    training loss:\t\t5.05604867369\n",
      "Done 1020 batches in 175.27 sec.    training loss:\t\t5.05206488207\n",
      "Done 1030 batches in 177.02 sec.    training loss:\t\t5.04756225012\n",
      "Done 1040 batches in 178.71 sec.    training loss:\t\t5.04391147907\n",
      "Done 1050 batches in 180.28 sec.    training loss:\t\t5.04040555318\n",
      "Done 1060 batches in 182.07 sec.    training loss:\t\t5.03696121\n",
      "Done 1070 batches in 183.44 sec.    training loss:\t\t5.03203105258\n",
      "Done 1080 batches in 185.15 sec.    training loss:\t\t5.02800714793\n",
      "Done 1090 batches in 187.25 sec.    training loss:\t\t5.02535746426\n",
      "Done 1100 batches in 188.86 sec.    training loss:\t\t5.02166006262\n",
      "Done 1110 batches in 190.55 sec.    training loss:\t\t5.01745900807\n",
      "Done 1120 batches in 191.96 sec.    training loss:\t\t5.01302431268\n",
      "Done 1130 batches in 193.63 sec.    training loss:\t\t5.0087627605\n",
      "Done 1140 batches in 195.30 sec.    training loss:\t\t5.00532629657\n",
      "Done 1150 batches in 197.15 sec.    training loss:\t\t5.00106423129\n",
      "Done 1160 batches in 198.63 sec.    training loss:\t\t4.99711854335\n",
      "Done 1170 batches in 200.81 sec.    training loss:\t\t4.99480092913\n",
      "Done 1180 batches in 202.69 sec.    training loss:\t\t4.99212235435\n",
      "Done 1190 batches in 204.25 sec.    training loss:\t\t4.98900369276\n",
      "Done 1200 batches in 206.09 sec.    training loss:\t\t4.9852259481\n",
      "Done 1210 batches in 207.83 sec.    training loss:\t\t4.9816231105\n",
      "Done 1220 batches in 209.28 sec.    training loss:\t\t4.97807890314\n",
      "Done 1230 batches in 210.87 sec.    training loss:\t\t4.97486460577\n",
      "Done 1240 batches in 212.48 sec.    training loss:\t\t4.97185236639\n",
      "Done 1250 batches in 213.95 sec.    training loss:\t\t4.96796699905\n",
      "Done 1260 batches in 215.82 sec.    training loss:\t\t4.96479282682\n",
      "Done 1270 batches in 217.56 sec.    training loss:\t\t4.96186784346\n",
      "Done 1280 batches in 219.40 sec.    training loss:\t\t4.95818395615\n",
      "Done 1290 batches in 220.72 sec.    training loss:\t\t4.95584045418\n",
      "Done 1300 batches in 222.57 sec.    training loss:\t\t4.95347293817\n",
      "Done 1310 batches in 224.54 sec.    training loss:\t\t4.94983164955\n",
      "Done 1320 batches in 226.10 sec.    training loss:\t\t4.94642626336\n",
      "Done 1330 batches in 227.73 sec.    training loss:\t\t4.94308315148\n",
      "Done 1340 batches in 229.28 sec.    training loss:\t\t4.94027168146\n",
      "Done 1350 batches in 231.53 sec.    training loss:\t\t4.93795813066\n",
      "Done 1360 batches in 233.18 sec.    training loss:\t\t4.93432665818\n",
      "Done 1370 batches in 235.26 sec.    training loss:\t\t4.93123482579\n",
      "Done 1380 batches in 236.74 sec.    training loss:\t\t4.92839900894\n",
      "Done 1390 batches in 238.53 sec.    training loss:\t\t4.92637137097\n",
      "Done 1400 batches in 240.10 sec.    training loss:\t\t4.9240263067\n",
      "Done 1410 batches in 241.68 sec.    training loss:\t\t4.9210907439\n",
      "Done 1420 batches in 243.13 sec.    training loss:\t\t4.91845130853\n",
      "Done 1430 batches in 244.84 sec.    training loss:\t\t4.91494081237\n",
      "Done 1440 batches in 246.76 sec.    training loss:\t\t4.91140454014\n",
      "Done 1450 batches in 248.65 sec.    training loss:\t\t4.9094226528\n",
      "Done 1460 batches in 250.45 sec.    training loss:\t\t4.90609823122\n",
      "Done 1470 batches in 252.53 sec.    training loss:\t\t4.90386801415\n",
      "Done 1480 batches in 254.16 sec.    training loss:\t\t4.90102430195\n",
      "Done 1490 batches in 256.17 sec.    training loss:\t\t4.89869984908\n",
      "Done 1500 batches in 257.87 sec.    training loss:\t\t4.8964973472\n",
      "Done 1510 batches in 260.52 sec.    training loss:\t\t4.89424728984\n",
      "Done 1520 batches in 262.14 sec.    training loss:\t\t4.89144865792\n",
      "Done 1530 batches in 264.03 sec.    training loss:\t\t4.88945298927\n",
      "Done 1540 batches in 265.78 sec.    training loss:\t\t4.8866684525\n",
      "Done 1550 batches in 267.36 sec.    training loss:\t\t4.88360615407\n",
      "Done 1560 batches in 269.04 sec.    training loss:\t\t4.88166837371\n",
      "Done 1570 batches in 270.93 sec.    training loss:\t\t4.87963089168\n",
      "Done 1580 batches in 272.50 sec.    training loss:\t\t4.87734597318\n",
      "Done 1590 batches in 274.05 sec.    training loss:\t\t4.87492982352\n",
      "Done 1600 batches in 276.04 sec.    training loss:\t\t4.87274662063\n",
      "Done 1610 batches in 277.67 sec.    training loss:\t\t4.86964757028\n",
      "Done 1620 batches in 279.60 sec.    training loss:\t\t4.86777718994\n",
      "Done 1630 batches in 281.33 sec.    training loss:\t\t4.86553266019\n",
      "Done 1640 batches in 282.99 sec.    training loss:\t\t4.8633067871\n",
      "Done 1650 batches in 284.87 sec.    training loss:\t\t4.86198139552\n",
      "Done 1660 batches in 286.63 sec.    training loss:\t\t4.86039069202\n",
      "Done 1670 batches in 288.22 sec.    training loss:\t\t4.85724400689\n",
      "Done 1680 batches in 289.79 sec.    training loss:\t\t4.85539853559\n",
      "Done 1690 batches in 291.66 sec.    training loss:\t\t4.8534332037\n",
      "Done 1700 batches in 293.44 sec.    training loss:\t\t4.85126378494\n",
      "Done 1710 batches in 295.12 sec.    training loss:\t\t4.84888224644\n",
      "Done 1720 batches in 296.78 sec.    training loss:\t\t4.84651054851\n",
      "Done 1730 batches in 298.23 sec.    training loss:\t\t4.84414210912\n",
      "Done 1740 batches in 299.68 sec.    training loss:\t\t4.84189049608\n",
      "Done 1750 batches in 301.36 sec.    training loss:\t\t4.83932257353\n",
      "Done 1760 batches in 303.09 sec.    training loss:\t\t4.83773599673\n",
      "Done 1770 batches in 305.01 sec.    training loss:\t\t4.83570111108\n",
      "Done 1780 batches in 306.57 sec.    training loss:\t\t4.83373651719\n",
      "Done 1790 batches in 307.93 sec.    training loss:\t\t4.8310990183\n",
      "Done 1800 batches in 309.53 sec.    training loss:\t\t4.82889397025\n",
      "Done 1810 batches in 310.87 sec.    training loss:\t\t4.82641451411\n",
      "Done 1820 batches in 312.47 sec.    training loss:\t\t4.82365216253\n",
      "Done 1830 batches in 314.09 sec.    training loss:\t\t4.82176680187\n",
      "Done 1840 batches in 315.92 sec.    training loss:\t\t4.82008157256\n",
      "Done 1850 batches in 318.01 sec.    training loss:\t\t4.81914672736\n",
      "Done 1860 batches in 319.38 sec.    training loss:\t\t4.81672830966\n",
      "Done 1870 batches in 320.98 sec.    training loss:\t\t4.81500417676\n",
      "Done 1880 batches in 322.32 sec.    training loss:\t\t4.81254521317\n",
      "Done 1890 batches in 324.03 sec.    training loss:\t\t4.8106206986\n",
      "Done 1900 batches in 325.66 sec.    training loss:\t\t4.80892247965\n",
      "Done 1910 batches in 327.27 sec.    training loss:\t\t4.80692431128\n",
      "Done 1920 batches in 329.02 sec.    training loss:\t\t4.80547092197\n",
      "Done 1930 batches in 331.00 sec.    training loss:\t\t4.80345406915\n",
      "Done 1940 batches in 332.49 sec.    training loss:\t\t4.80218470306\n",
      "Done 1950 batches in 334.03 sec.    training loss:\t\t4.80009334332\n",
      "Done 1960 batches in 335.87 sec.    training loss:\t\t4.7989767448\n",
      "Done 1970 batches in 337.44 sec.    training loss:\t\t4.79720604952\n",
      "Done 1980 batches in 339.06 sec.    training loss:\t\t4.79572243558\n",
      "Done 1990 batches in 340.76 sec.    training loss:\t\t4.7938358737\n",
      "Done 2000 batches in 342.64 sec.    training loss:\t\t4.7927090143\n",
      "Done 2010 batches in 344.37 sec.    training loss:\t\t4.79105593067\n",
      "Done 2020 batches in 345.98 sec.    training loss:\t\t4.7888615777\n",
      "Done 2030 batches in 347.56 sec.    training loss:\t\t4.78666433572\n",
      "Done 2040 batches in 349.17 sec.    training loss:\t\t4.78439708375\n",
      "Done 2050 batches in 351.21 sec.    training loss:\t\t4.78249129644\n",
      "Done 2060 batches in 353.07 sec.    training loss:\t\t4.78124257824\n",
      "Done 2070 batches in 354.70 sec.    training loss:\t\t4.77946445009\n",
      "Done 2080 batches in 356.67 sec.    training loss:\t\t4.77819080834\n",
      "Done 2090 batches in 358.42 sec.    training loss:\t\t4.77680525118\n",
      "Done 2100 batches in 359.95 sec.    training loss:\t\t4.77487249215\n",
      "Done 2110 batches in 361.88 sec.    training loss:\t\t4.77370616045\n",
      "Done 2120 batches in 363.53 sec.    training loss:\t\t4.77163462808\n",
      "Done 2130 batches in 365.14 sec.    training loss:\t\t4.77028007384\n",
      "Done 2140 batches in 367.25 sec.    training loss:\t\t4.76865872835\n",
      "Done 2150 batches in 368.77 sec.    training loss:\t\t4.76724941908\n",
      "Done 2160 batches in 370.54 sec.    training loss:\t\t4.76604090013\n",
      "Done 2170 batches in 372.25 sec.    training loss:\t\t4.76436269899\n",
      "Done 2180 batches in 373.89 sec.    training loss:\t\t4.76305375263\n",
      "Done 2190 batches in 375.58 sec.    training loss:\t\t4.76108335621\n",
      "Done 2200 batches in 377.08 sec.    training loss:\t\t4.75908324968\n",
      "Done 2210 batches in 378.66 sec.    training loss:\t\t4.7577279271\n",
      "Done 2220 batches in 380.36 sec.    training loss:\t\t4.75577805569\n",
      "Done 2230 batches in 382.04 sec.    training loss:\t\t4.75434740253\n",
      "Done 2240 batches in 383.91 sec.    training loss:\t\t4.75292681211\n",
      "Done 2250 batches in 385.33 sec.    training loss:\t\t4.75141875892\n",
      "Done 2260 batches in 387.13 sec.    training loss:\t\t4.74982991271\n",
      "Done 2270 batches in 388.61 sec.    training loss:\t\t4.74845026107\n",
      "Done 2280 batches in 390.21 sec.    training loss:\t\t4.74727682199\n",
      "Done 2290 batches in 391.76 sec.    training loss:\t\t4.74638900934\n",
      "Done 2300 batches in 393.36 sec.    training loss:\t\t4.74508297951\n",
      "Done 2310 batches in 394.82 sec.    training loss:\t\t4.74346449809\n",
      "Done 2320 batches in 396.40 sec.    training loss:\t\t4.74200425929\n",
      "Done 2330 batches in 397.90 sec.    training loss:\t\t4.74065461282\n",
      "Done 2340 batches in 399.53 sec.    training loss:\t\t4.73905107282\n",
      "Done 2350 batches in 401.44 sec.    training loss:\t\t4.73767980494\n",
      "Done 2360 batches in 403.20 sec.    training loss:\t\t4.73602157387\n",
      "Done 2370 batches in 404.70 sec.    training loss:\t\t4.73470230374\n",
      "Done 2380 batches in 406.18 sec.    training loss:\t\t4.73307792109\n",
      "Done 2390 batches in 407.68 sec.    training loss:\t\t4.73134662626\n",
      "Done 2400 batches in 409.56 sec.    training loss:\t\t4.73027268757\n",
      "Done 2410 batches in 411.35 sec.    training loss:\t\t4.72889882964\n",
      "Done 2420 batches in 412.96 sec.    training loss:\t\t4.72771753112\n",
      "Done 2430 batches in 415.10 sec.    training loss:\t\t4.72664032542\n",
      "Done 2440 batches in 416.66 sec.    training loss:\t\t4.72512670413\n",
      "Done 2450 batches in 418.51 sec.    training loss:\t\t4.7241622955\n",
      "Done 2460 batches in 420.16 sec.    training loss:\t\t4.72295544448\n",
      "Done 2470 batches in 421.77 sec.    training loss:\t\t4.72136375373\n",
      "Done 2480 batches in 423.63 sec.    training loss:\t\t4.72011066252\n",
      "Done 2490 batches in 425.20 sec.    training loss:\t\t4.71890797835\n",
      "Done 2500 batches in 426.93 sec.    training loss:\t\t4.71811622496\n",
      "Done 2510 batches in 428.64 sec.    training loss:\t\t4.71708234002\n",
      "Done 2520 batches in 430.22 sec.    training loss:\t\t4.71585259523\n",
      "Done 2530 batches in 431.85 sec.    training loss:\t\t4.71450146432\n",
      "Done 2540 batches in 433.60 sec.    training loss:\t\t4.71302365046\n",
      "Done 2550 batches in 435.57 sec.    training loss:\t\t4.71196273813\n",
      "Done 2560 batches in 437.07 sec.    training loss:\t\t4.7109309095\n",
      "Done 2570 batches in 438.78 sec.    training loss:\t\t4.70976241757\n",
      "Done 2580 batches in 440.46 sec.    training loss:\t\t4.70884044577\n",
      "Done 2590 batches in 442.10 sec.    training loss:\t\t4.70752232645\n",
      "Done 2600 batches in 443.86 sec.    training loss:\t\t4.7062051299\n",
      "Done 2610 batches in 445.50 sec.    training loss:\t\t4.70503898206\n",
      "Done 2620 batches in 447.14 sec.    training loss:\t\t4.70348808875\n",
      "Done 2630 batches in 448.83 sec.    training loss:\t\t4.70273598174\n",
      "Done 2640 batches in 450.49 sec.    training loss:\t\t4.70142398061\n",
      "Done 2650 batches in 451.97 sec.    training loss:\t\t4.70037131004\n",
      "Done 2660 batches in 453.98 sec.    training loss:\t\t4.69945205602\n",
      "Done 2670 batches in 455.42 sec.    training loss:\t\t4.69797089975\n",
      "Done 2680 batches in 457.00 sec.    training loss:\t\t4.69664210232\n",
      "Done 2690 batches in 458.93 sec.    training loss:\t\t4.69602441212\n",
      "Done 2700 batches in 460.81 sec.    training loss:\t\t4.69497354234\n",
      "Done 2710 batches in 462.36 sec.    training loss:\t\t4.69361651392\n",
      "Done 2720 batches in 463.97 sec.    training loss:\t\t4.69216609685\n",
      "Done 2730 batches in 465.60 sec.    training loss:\t\t4.69091466792\n",
      "Done 2740 batches in 467.24 sec.    training loss:\t\t4.69029036995\n",
      "Done 2750 batches in 469.00 sec.    training loss:\t\t4.68910354848\n",
      "Done 2760 batches in 470.77 sec.    training loss:\t\t4.68816174571\n",
      "Done 2770 batches in 472.22 sec.    training loss:\t\t4.68741170138\n",
      "Done 2780 batches in 473.93 sec.    training loss:\t\t4.68655298108\n",
      "Done 2790 batches in 475.59 sec.    training loss:\t\t4.68540205007\n",
      "Done 2800 batches in 477.69 sec.    training loss:\t\t4.68479903298\n",
      "Done 2810 batches in 479.60 sec.    training loss:\t\t4.683703032\n",
      "Done 2820 batches in 481.16 sec.    training loss:\t\t4.68223755926\n",
      "Done 2830 batches in 482.97 sec.    training loss:\t\t4.68100252792\n",
      "Done 2840 batches in 484.54 sec.    training loss:\t\t4.67993247543\n",
      "Done 2850 batches in 486.08 sec.    training loss:\t\t4.67884292427\n",
      "Done 2860 batches in 487.68 sec.    training loss:\t\t4.67762515987\n",
      "Done 2870 batches in 489.63 sec.    training loss:\t\t4.67646742457\n",
      "Done 2880 batches in 492.58 sec.    training loss:\t\t4.67583228226\n",
      "Done 2890 batches in 494.34 sec.    training loss:\t\t4.67482327615\n",
      "Done 2900 batches in 496.00 sec.    training loss:\t\t4.67370468312\n",
      "Done 2910 batches in 497.67 sec.    training loss:\t\t4.67274018014\n",
      "Done 2920 batches in 498.98 sec.    training loss:\t\t4.67147077102\n",
      "Done 2930 batches in 500.94 sec.    training loss:\t\t4.67011140988\n",
      "Done 2940 batches in 502.82 sec.    training loss:\t\t4.66911451403\n",
      "Done 2950 batches in 504.43 sec.    training loss:\t\t4.66796037488\n",
      "Done 2960 batches in 506.42 sec.    training loss:\t\t4.66706085704\n",
      "Done 2970 batches in 507.68 sec.    training loss:\t\t4.6659702631\n",
      "Done 2980 batches in 509.37 sec.    training loss:\t\t4.66483982057\n",
      "Done 2990 batches in 511.32 sec.    training loss:\t\t4.66394491515\n",
      "Done 3000 batches in 513.12 sec.    training loss:\t\t4.66327875876\n",
      "Done 3010 batches in 514.81 sec.    training loss:\t\t4.66217869437\n",
      "Done 3020 batches in 516.59 sec.    training loss:\t\t4.6611156556\n",
      "Done 3030 batches in 518.40 sec.    training loss:\t\t4.6603833278\n",
      "Done 3040 batches in 520.13 sec.    training loss:\t\t4.65883471323\n",
      "Done 3050 batches in 521.78 sec.    training loss:\t\t4.65775412044\n",
      "Done 3060 batches in 523.31 sec.    training loss:\t\t4.65677663828\n",
      "Done 3070 batches in 525.09 sec.    training loss:\t\t4.65575916029\n",
      "Done 3080 batches in 526.94 sec.    training loss:\t\t4.65456357304\n",
      "Done 3090 batches in 528.83 sec.    training loss:\t\t4.6536545333\n",
      "Done 3100 batches in 530.48 sec.    training loss:\t\t4.65250515592\n",
      "Done 3110 batches in 531.94 sec.    training loss:\t\t4.65154812167\n",
      "Done 3120 batches in 533.64 sec.    training loss:\t\t4.65070339357\n",
      "Done 3130 batches in 535.31 sec.    training loss:\t\t4.65005709881\n",
      "Done 3140 batches in 536.86 sec.    training loss:\t\t4.64925234902\n",
      "Done 3150 batches in 538.90 sec.    training loss:\t\t4.64821254518\n",
      "Done 3160 batches in 540.60 sec.    training loss:\t\t4.64748579973\n",
      "Done 3170 batches in 542.13 sec.    training loss:\t\t4.64615993771\n",
      "Done 3180 batches in 543.68 sec.    training loss:\t\t4.64522068516\n",
      "Done 3190 batches in 545.46 sec.    training loss:\t\t4.64424136574\n",
      "Done 3200 batches in 547.29 sec.    training loss:\t\t4.64359064266\n",
      "Done 3210 batches in 549.10 sec.    training loss:\t\t4.6428339107\n",
      "Done 3220 batches in 550.80 sec.    training loss:\t\t4.64206784556\n",
      "Done 3230 batches in 552.42 sec.    training loss:\t\t4.64101064774\n",
      "Done 3240 batches in 554.62 sec.    training loss:\t\t4.64019847417\n",
      "Done 3250 batches in 556.48 sec.    training loss:\t\t4.63913301351\n",
      "Done 3260 batches in 558.19 sec.    training loss:\t\t4.63827714013\n",
      "Done 3270 batches in 559.98 sec.    training loss:\t\t4.63727134638\n",
      "Done 3280 batches in 561.60 sec.    training loss:\t\t4.63660191755\n",
      "Done 3290 batches in 563.38 sec.    training loss:\t\t4.63544463553\n",
      "Done 3300 batches in 564.98 sec.    training loss:\t\t4.6345085584\n",
      "Done 3310 batches in 566.61 sec.    training loss:\t\t4.63374341816\n",
      "Done 3320 batches in 568.46 sec.    training loss:\t\t4.63322757088\n",
      "Done 3330 batches in 570.15 sec.    training loss:\t\t4.63224496355\n",
      "Done 3340 batches in 571.63 sec.    training loss:\t\t4.63119499384\n",
      "Done 3350 batches in 573.53 sec.    training loss:\t\t4.63048214208\n",
      "Done 3360 batches in 575.41 sec.    training loss:\t\t4.62975799562\n",
      "Done 3370 batches in 576.95 sec.    training loss:\t\t4.62861537947\n",
      "Done 3380 batches in 578.82 sec.    training loss:\t\t4.62764537927\n",
      "Done 3390 batches in 580.52 sec.    training loss:\t\t4.62661903024\n",
      "Done 3400 batches in 582.23 sec.    training loss:\t\t4.62528144794\n",
      "Done 3410 batches in 583.97 sec.    training loss:\t\t4.62455688054\n",
      "Done 3420 batches in 585.82 sec.    training loss:\t\t4.62368155\n",
      "Done 3430 batches in 587.38 sec.    training loss:\t\t4.62274867703\n",
      "Done 3440 batches in 589.22 sec.    training loss:\t\t4.62206426235\n",
      "Done 3450 batches in 590.86 sec.    training loss:\t\t4.62126242375\n",
      "Done 3460 batches in 592.84 sec.    training loss:\t\t4.62054497424\n",
      "Done 3470 batches in 594.36 sec.    training loss:\t\t4.61988419236\n",
      "Done 3480 batches in 596.11 sec.    training loss:\t\t4.61943707548\n",
      "Done 3490 batches in 598.08 sec.    training loss:\t\t4.61904806968\n",
      "Done 3500 batches in 599.82 sec.    training loss:\t\t4.61825535829\n",
      "Done 3510 batches in 601.64 sec.    training loss:\t\t4.61768764702\n",
      "Done 3520 batches in 603.14 sec.    training loss:\t\t4.61678679748\n",
      "Done 3530 batches in 605.04 sec.    training loss:\t\t4.61619121941\n",
      "Done 3540 batches in 606.86 sec.    training loss:\t\t4.61522097076\n",
      "Done 3550 batches in 608.55 sec.    training loss:\t\t4.614745955\n",
      "Done 3560 batches in 610.27 sec.    training loss:\t\t4.6140000634\n",
      "Done 3570 batches in 611.90 sec.    training loss:\t\t4.61341594341\n",
      "Done 3580 batches in 613.61 sec.    training loss:\t\t4.6125263944\n",
      "Done 3590 batches in 615.70 sec.    training loss:\t\t4.61169372043\n",
      "Done 3600 batches in 617.54 sec.    training loss:\t\t4.61074511422\n",
      "Done 3610 batches in 619.22 sec.    training loss:\t\t4.61024787763\n",
      "Done 3620 batches in 620.81 sec.    training loss:\t\t4.60918142756\n",
      "Done 3630 batches in 622.69 sec.    training loss:\t\t4.60848388665\n",
      "Done 3640 batches in 624.66 sec.    training loss:\t\t4.6074526103\n",
      "Done 3650 batches in 626.32 sec.    training loss:\t\t4.60675264332\n",
      "Done 3660 batches in 628.42 sec.    training loss:\t\t4.60619146863\n",
      "Done 3670 batches in 629.82 sec.    training loss:\t\t4.60527256016\n",
      "Done 3680 batches in 631.38 sec.    training loss:\t\t4.60436398776\n",
      "Done 3690 batches in 633.02 sec.    training loss:\t\t4.60344750965\n",
      "Done 3700 batches in 634.75 sec.    training loss:\t\t4.60246821616\n",
      "Done 3710 batches in 636.19 sec.    training loss:\t\t4.60125353645\n",
      "Done 3720 batches in 637.92 sec.    training loss:\t\t4.600300217\n",
      "Done 3730 batches in 639.75 sec.    training loss:\t\t4.59969105676\n",
      "Done 3740 batches in 641.29 sec.    training loss:\t\t4.59877235214\n",
      "Done 3750 batches in 642.94 sec.    training loss:\t\t4.59804257736\n",
      "Done 3760 batches in 644.29 sec.    training loss:\t\t4.59715225938\n",
      "Done 3770 batches in 646.34 sec.    training loss:\t\t4.59700470793\n",
      "Done 3780 batches in 647.67 sec.    training loss:\t\t4.5959255168\n",
      "Done 3790 batches in 649.32 sec.    training loss:\t\t4.59535102888\n",
      "Done 3800 batches in 650.90 sec.    training loss:\t\t4.59443218143\n",
      "Done 3810 batches in 652.38 sec.    training loss:\t\t4.59367590063\n",
      "Done 3820 batches in 653.88 sec.    training loss:\t\t4.59265952759\n",
      "Done 3830 batches in 655.50 sec.    training loss:\t\t4.59219949625\n",
      "Done 3840 batches in 657.19 sec.    training loss:\t\t4.59147403613\n",
      "Done 3850 batches in 658.67 sec.    training loss:\t\t4.59057936798\n",
      "Done 3860 batches in 660.27 sec.    training loss:\t\t4.58974637868\n",
      "Done 3870 batches in 661.92 sec.    training loss:\t\t4.58900360821\n",
      "Done 3880 batches in 663.68 sec.    training loss:\t\t4.58826226297\n",
      "Done 3890 batches in 665.42 sec.    training loss:\t\t4.58741079143\n",
      "Done 3900 batches in 667.21 sec.    training loss:\t\t4.58677449025\n",
      "Done 3910 batches in 668.86 sec.    training loss:\t\t4.58600858575\n",
      "Done 3920 batches in 670.42 sec.    training loss:\t\t4.58514325503\n",
      "Done 3930 batches in 671.79 sec.    training loss:\t\t4.58427343326\n",
      "Done 3940 batches in 673.73 sec.    training loss:\t\t4.58353047734\n",
      "Done 3950 batches in 675.40 sec.    training loss:\t\t4.58289489764\n",
      "Done 3960 batches in 677.16 sec.    training loss:\t\t4.58223861432\n",
      "Done 3970 batches in 678.91 sec.    training loss:\t\t4.58168238753\n",
      "Done 3980 batches in 680.45 sec.    training loss:\t\t4.58098141943\n",
      "Done 3990 batches in 682.21 sec.    training loss:\t\t4.58050770497\n",
      "Done 4000 batches in 684.13 sec.    training loss:\t\t4.58017784518\n",
      "Done 4010 batches in 686.09 sec.    training loss:\t\t4.57961212067\n",
      "Done 4020 batches in 687.96 sec.    training loss:\t\t4.57926866113\n",
      "Done 4030 batches in 689.85 sec.    training loss:\t\t4.57863651405\n",
      "Done 4040 batches in 691.33 sec.    training loss:\t\t4.57786664072\n",
      "Done 4050 batches in 692.98 sec.    training loss:\t\t4.57752888956\n",
      "Done 4060 batches in 694.68 sec.    training loss:\t\t4.57659337468\n",
      "Done 4070 batches in 696.69 sec.    training loss:\t\t4.5758705312\n",
      "Done 4080 batches in 698.48 sec.    training loss:\t\t4.57531566941\n",
      "Done 4090 batches in 700.29 sec.    training loss:\t\t4.57453198555\n",
      "Done 4100 batches in 701.93 sec.    training loss:\t\t4.57410950446\n",
      "Done 4110 batches in 703.39 sec.    training loss:\t\t4.5732402635\n",
      "Done 4120 batches in 705.62 sec.    training loss:\t\t4.57268940668\n",
      "Done 4130 batches in 707.37 sec.    training loss:\t\t4.57213112996\n",
      "Done 4140 batches in 709.20 sec.    training loss:\t\t4.57151065627\n",
      "Done 4150 batches in 710.85 sec.    training loss:\t\t4.57063749543\n",
      "Done 4160 batches in 712.53 sec.    training loss:\t\t4.57023056339\n",
      "Done 4170 batches in 714.24 sec.    training loss:\t\t4.56942498249\n",
      "Done 4180 batches in 715.92 sec.    training loss:\t\t4.5686120469\n",
      "Done 4190 batches in 717.31 sec.    training loss:\t\t4.56780423219\n",
      "Done 4200 batches in 719.37 sec.    training loss:\t\t4.56713160475\n",
      "Done 4210 batches in 720.82 sec.    training loss:\t\t4.56649204263\n",
      "Done 4220 batches in 722.63 sec.    training loss:\t\t4.56596678445\n",
      "Done 4230 batches in 724.61 sec.    training loss:\t\t4.56547071624\n",
      "Done 4240 batches in 726.42 sec.    training loss:\t\t4.56500890154\n",
      "Done 4250 batches in 728.20 sec.    training loss:\t\t4.56456344851\n",
      "Done 4260 batches in 729.70 sec.    training loss:\t\t4.56383675091\n",
      "Done 4270 batches in 731.32 sec.    training loss:\t\t4.56323464213\n",
      "Done 4280 batches in 733.16 sec.    training loss:\t\t4.56279400788\n",
      "Done 4290 batches in 734.78 sec.    training loss:\t\t4.56212770944\n",
      "Done 4300 batches in 736.66 sec.    training loss:\t\t4.56143188626\n",
      "Done 4310 batches in 738.07 sec.    training loss:\t\t4.56054638135\n",
      "Done 4320 batches in 740.05 sec.    training loss:\t\t4.55989572284\n",
      "Done 4330 batches in 741.84 sec.    training loss:\t\t4.5594728656\n",
      "Done 4340 batches in 743.63 sec.    training loss:\t\t4.55881113973\n",
      "Done 4350 batches in 745.14 sec.    training loss:\t\t4.55784770379\n",
      "Done 4360 batches in 746.99 sec.    training loss:\t\t4.55711910856\n",
      "Done 4370 batches in 748.63 sec.    training loss:\t\t4.55670108653\n",
      "Done 4380 batches in 750.29 sec.    training loss:\t\t4.55596808162\n",
      "Done 4390 batches in 752.21 sec.    training loss:\t\t4.55543921102\n",
      "Done 4400 batches in 754.01 sec.    training loss:\t\t4.55485581067\n",
      "Done 4410 batches in 755.58 sec.    training loss:\t\t4.55440412935\n",
      "Done 4420 batches in 756.95 sec.    training loss:\t\t4.55366128349\n",
      "Done 4430 batches in 758.79 sec.    training loss:\t\t4.55305589397\n",
      "Done 4440 batches in 760.67 sec.    training loss:\t\t4.55247115407\n",
      "Done 4450 batches in 762.47 sec.    training loss:\t\t4.55197944796\n",
      "Done 4460 batches in 764.48 sec.    training loss:\t\t4.55141032635\n",
      "Done 4470 batches in 766.28 sec.    training loss:\t\t4.55100293335\n",
      "Done 4480 batches in 767.91 sec.    training loss:\t\t4.55029145548\n",
      "Done 4490 batches in 769.40 sec.    training loss:\t\t4.54984555207\n",
      "Done 4500 batches in 771.55 sec.    training loss:\t\t4.5494719117\n",
      "Done 4510 batches in 773.30 sec.    training loss:\t\t4.54905913511\n",
      "Done 4520 batches in 774.94 sec.    training loss:\t\t4.54836471439\n",
      "Done 4530 batches in 776.85 sec.    training loss:\t\t4.5479214132\n",
      "Done 4540 batches in 778.59 sec.    training loss:\t\t4.54760014953\n",
      "Done 4550 batches in 780.35 sec.    training loss:\t\t4.5469823177\n",
      "Done 4560 batches in 782.01 sec.    training loss:\t\t4.54649630649\n",
      "Done 4570 batches in 783.58 sec.    training loss:\t\t4.54577434626\n",
      "Done 4580 batches in 785.08 sec.    training loss:\t\t4.54516541203\n",
      "Done 4590 batches in 787.12 sec.    training loss:\t\t4.54461750647\n",
      "Done 4600 batches in 788.59 sec.    training loss:\t\t4.54397546711\n",
      "Done 4610 batches in 790.58 sec.    training loss:\t\t4.54335693915\n",
      "Done 4620 batches in 792.26 sec.    training loss:\t\t4.54263865153\n",
      "Done 4630 batches in 794.11 sec.    training loss:\t\t4.54203725026\n",
      "Done 4640 batches in 795.83 sec.    training loss:\t\t4.54159478936\n",
      "Done 4650 batches in 797.79 sec.    training loss:\t\t4.54112983509\n",
      "Done 4660 batches in 799.31 sec.    training loss:\t\t4.54062987346\n",
      "Done 4670 batches in 801.15 sec.    training loss:\t\t4.54025106604\n",
      "Done 4680 batches in 802.90 sec.    training loss:\t\t4.53958520089\n",
      "Done 4690 batches in 804.45 sec.    training loss:\t\t4.53927726191\n",
      "Done 4700 batches in 806.33 sec.    training loss:\t\t4.53873380428\n",
      "Done 4710 batches in 807.85 sec.    training loss:\t\t4.53800467078\n",
      "Done 4720 batches in 809.98 sec.    training loss:\t\t4.53758080844\n",
      "Done 4730 batches in 811.79 sec.    training loss:\t\t4.53702744157\n",
      "Done 4740 batches in 813.62 sec.    training loss:\t\t4.53671225244\n",
      "Done 4750 batches in 815.55 sec.    training loss:\t\t4.5363667721\n",
      "Done 4760 batches in 817.42 sec.    training loss:\t\t4.53589876039\n",
      "Done 4770 batches in 818.85 sec.    training loss:\t\t4.53526087247\n",
      "Done 4780 batches in 820.51 sec.    training loss:\t\t4.5348441741\n",
      "Done 4790 batches in 822.12 sec.    training loss:\t\t4.53413737199\n",
      "Done 4800 batches in 823.84 sec.    training loss:\t\t4.53336777394\n",
      "Done 4810 batches in 825.62 sec.    training loss:\t\t4.53268523449\n",
      "Done 4820 batches in 827.22 sec.    training loss:\t\t4.53232485867\n",
      "Done 4830 batches in 828.95 sec.    training loss:\t\t4.53179980938\n",
      "Done 4840 batches in 830.71 sec.    training loss:\t\t4.53115371948\n",
      "Done 4850 batches in 832.66 sec.    training loss:\t\t4.53061167933\n",
      "Done 4860 batches in 834.30 sec.    training loss:\t\t4.53021958306\n",
      "Done 4870 batches in 835.75 sec.    training loss:\t\t4.52974283137\n",
      "Done 4880 batches in 837.30 sec.    training loss:\t\t4.52931033031\n",
      "Done 4890 batches in 838.77 sec.    training loss:\t\t4.5286489044\n",
      "Done 4900 batches in 840.38 sec.    training loss:\t\t4.52821129896\n",
      "Done 4910 batches in 841.87 sec.    training loss:\t\t4.52777856751\n",
      "Done 4920 batches in 843.60 sec.    training loss:\t\t4.52722306848\n",
      "Done 4930 batches in 845.20 sec.    training loss:\t\t4.5267497158\n",
      "Done 4940 batches in 847.02 sec.    training loss:\t\t4.52620398134\n",
      "Done 4950 batches in 849.07 sec.    training loss:\t\t4.52569967809\n",
      "Done 4960 batches in 851.02 sec.    training loss:\t\t4.52517255747\n",
      "Done 4970 batches in 852.88 sec.    training loss:\t\t4.52468754118\n",
      "Done 4980 batches in 854.44 sec.    training loss:\t\t4.52394819968\n",
      "Done 4990 batches in 856.11 sec.    training loss:\t\t4.52340955854\n",
      "Done 5000 batches in 857.56 sec.    training loss:\t\t4.52274899259\n",
      "Done 5010 batches in 859.09 sec.    training loss:\t\t4.52211612937\n",
      "Done 5020 batches in 861.00 sec.    training loss:\t\t4.52168708346\n",
      "Done 5030 batches in 862.68 sec.    training loss:\t\t4.52133933749\n",
      "Done 5040 batches in 864.79 sec.    training loss:\t\t4.52083359266\n",
      "Done 5050 batches in 866.32 sec.    training loss:\t\t4.51996639861\n",
      "Done 5060 batches in 868.25 sec.    training loss:\t\t4.51956533231\n",
      "Done 5070 batches in 870.10 sec.    training loss:\t\t4.51910038272\n",
      "Done 5080 batches in 871.59 sec.    training loss:\t\t4.51862389943\n",
      "Done 5090 batches in 873.59 sec.    training loss:\t\t4.51807019373\n",
      "Done 5100 batches in 875.82 sec.    training loss:\t\t4.51762294447\n",
      "Done 5110 batches in 877.56 sec.    training loss:\t\t4.51703240946\n",
      "Done 5120 batches in 879.17 sec.    training loss:\t\t4.51645435295\n",
      "Done 5130 batches in 880.76 sec.    training loss:\t\t4.51599033111\n",
      "Done 5140 batches in 882.16 sec.    training loss:\t\t4.51535619757\n",
      "Done 5150 batches in 884.18 sec.    training loss:\t\t4.5151164984\n",
      "Done 5160 batches in 886.06 sec.    training loss:\t\t4.51464643945\n",
      "Done 5170 batches in 887.71 sec.    training loss:\t\t4.5143051788\n",
      "Done 5180 batches in 889.26 sec.    training loss:\t\t4.51402730403\n",
      "Done 5190 batches in 891.44 sec.    training loss:\t\t4.51388230062\n",
      "Done 5200 batches in 893.31 sec.    training loss:\t\t4.51342053523\n",
      "Done 5210 batches in 894.92 sec.    training loss:\t\t4.51299541861\n",
      "Done 5220 batches in 896.50 sec.    training loss:\t\t4.51227638498\n",
      "Done 5230 batches in 897.91 sec.    training loss:\t\t4.51185804503\n",
      "Done 5240 batches in 899.77 sec.    training loss:\t\t4.51141568063\n",
      "Done 5250 batches in 901.43 sec.    training loss:\t\t4.51081664081\n",
      "Done 5260 batches in 903.12 sec.    training loss:\t\t4.5106293162\n",
      "Done 5270 batches in 905.20 sec.    training loss:\t\t4.51013467592\n",
      "Done 5280 batches in 906.88 sec.    training loss:\t\t4.509665327\n",
      "Done 5290 batches in 908.46 sec.    training loss:\t\t4.50917264983\n",
      "Done 5300 batches in 909.82 sec.    training loss:\t\t4.50856993149\n",
      "Done 5310 batches in 911.76 sec.    training loss:\t\t4.50802381111\n",
      "Done 5320 batches in 913.51 sec.    training loss:\t\t4.50744967721\n",
      "Done 5330 batches in 915.43 sec.    training loss:\t\t4.5069701477\n",
      "Done 5340 batches in 917.35 sec.    training loss:\t\t4.50656194763\n",
      "Done 5350 batches in 919.13 sec.    training loss:\t\t4.50603731329\n",
      "Done 5360 batches in 920.87 sec.    training loss:\t\t4.50560353616\n",
      "Done 5370 batches in 922.50 sec.    training loss:\t\t4.50512865606\n",
      "Done 5380 batches in 924.21 sec.    training loss:\t\t4.50445122542\n",
      "Done 5390 batches in 925.72 sec.    training loss:\t\t4.50415286505\n",
      "Done 5400 batches in 927.59 sec.    training loss:\t\t4.50362904686\n",
      "Done 5410 batches in 929.58 sec.    training loss:\t\t4.50317595177\n",
      "Done 5420 batches in 931.70 sec.    training loss:\t\t4.50254525144\n",
      "Done 5430 batches in 933.56 sec.    training loss:\t\t4.50224061592\n",
      "Done 5440 batches in 935.06 sec.    training loss:\t\t4.50196447793\n",
      "Done 5450 batches in 936.89 sec.    training loss:\t\t4.50154997222\n",
      "Done 5460 batches in 938.50 sec.    training loss:\t\t4.50118054457\n",
      "Done 5470 batches in 940.28 sec.    training loss:\t\t4.50064989139\n",
      "Done 5480 batches in 941.83 sec.    training loss:\t\t4.50005729399\n",
      "Done 5490 batches in 943.81 sec.    training loss:\t\t4.49972679954\n",
      "Done 5500 batches in 945.21 sec.    training loss:\t\t4.49919365744\n",
      "Done 5510 batches in 947.04 sec.    training loss:\t\t4.49876275781\n",
      "Done 5520 batches in 948.85 sec.    training loss:\t\t4.49839610414\n",
      "Done 5530 batches in 950.64 sec.    training loss:\t\t4.49796913946\n",
      "Done 5540 batches in 952.22 sec.    training loss:\t\t4.49761893723\n",
      "Done 5550 batches in 953.90 sec.    training loss:\t\t4.49712479355\n",
      "Done 5560 batches in 955.39 sec.    training loss:\t\t4.4965626862\n",
      "Done 5570 batches in 957.06 sec.    training loss:\t\t4.49621448555\n",
      "Done 5580 batches in 959.08 sec.    training loss:\t\t4.4958434598\n",
      "Done 5590 batches in 961.01 sec.    training loss:\t\t4.49547867818\n",
      "Done 5600 batches in 962.46 sec.    training loss:\t\t4.49492468557\n",
      "Done 5610 batches in 964.08 sec.    training loss:\t\t4.49452891983\n",
      "Done 5620 batches in 966.01 sec.    training loss:\t\t4.49415451823\n",
      "Done 5630 batches in 967.75 sec.    training loss:\t\t4.49365877415\n",
      "Done 5640 batches in 969.51 sec.    training loss:\t\t4.49323680836\n",
      "Done 5650 batches in 971.10 sec.    training loss:\t\t4.49300246382\n",
      "Done 5660 batches in 972.56 sec.    training loss:\t\t4.49255617571\n",
      "Done 5670 batches in 974.26 sec.    training loss:\t\t4.492070796\n",
      "Done 5680 batches in 975.71 sec.    training loss:\t\t4.49145461841\n",
      "Done 5690 batches in 977.58 sec.    training loss:\t\t4.49113075226\n",
      "Done 5700 batches in 979.29 sec.    training loss:\t\t4.49076984439\n",
      "Done 5710 batches in 981.20 sec.    training loss:\t\t4.49052484933\n",
      "Done 5720 batches in 982.82 sec.    training loss:\t\t4.49008185747\n",
      "Done 5730 batches in 984.31 sec.    training loss:\t\t4.48956125161\n",
      "Done 5740 batches in 985.94 sec.    training loss:\t\t4.48929965363\n",
      "Done 5750 batches in 987.74 sec.    training loss:\t\t4.4890485356\n",
      "Done 5760 batches in 989.50 sec.    training loss:\t\t4.48867632105\n",
      "Done 5770 batches in 991.18 sec.    training loss:\t\t4.48838531802\n",
      "Done 5780 batches in 992.60 sec.    training loss:\t\t4.48808886378\n",
      "Done 5790 batches in 994.35 sec.    training loss:\t\t4.48753164947\n",
      "Done 5800 batches in 996.62 sec.    training loss:\t\t4.48705123831\n",
      "Done 5810 batches in 998.62 sec.    training loss:\t\t4.48671064529\n",
      "Done 5820 batches in 1000.43 sec.    training loss:\t\t4.48659398208\n",
      "Done 5830 batches in 1002.15 sec.    training loss:\t\t4.48597411983\n",
      "Done 5840 batches in 1004.22 sec.    training loss:\t\t4.48543005079\n",
      "Done 5850 batches in 1005.71 sec.    training loss:\t\t4.4848501551\n",
      "Done 5860 batches in 1007.29 sec.    training loss:\t\t4.48447504064\n",
      "Done 5870 batches in 1008.75 sec.    training loss:\t\t4.48404692024\n",
      "Done 5880 batches in 1010.45 sec.    training loss:\t\t4.48358546694\n",
      "Done 5890 batches in 1012.21 sec.    training loss:\t\t4.48320512986\n",
      "Done 5900 batches in 1014.33 sec.    training loss:\t\t4.48275960918\n",
      "Done 5910 batches in 1015.85 sec.    training loss:\t\t4.48232718188\n",
      "Done 5920 batches in 1017.48 sec.    training loss:\t\t4.48173985215\n",
      "Done 5930 batches in 1019.36 sec.    training loss:\t\t4.48138165856\n",
      "Done 5940 batches in 1021.66 sec.    training loss:\t\t4.48112759442\n",
      "Done 5950 batches in 1023.30 sec.    training loss:\t\t4.48075251615\n",
      "Done 5960 batches in 1024.88 sec.    training loss:\t\t4.48026635583\n",
      "Done 5970 batches in 1026.44 sec.    training loss:\t\t4.47976587681\n",
      "Done 5980 batches in 1027.87 sec.    training loss:\t\t4.47935224588\n",
      "Done 5990 batches in 1029.66 sec.    training loss:\t\t4.47907058094\n",
      "Done 6000 batches in 1031.23 sec.    training loss:\t\t4.47871471242\n",
      "Done 6010 batches in 1032.78 sec.    training loss:\t\t4.47808146961\n",
      "Done 6020 batches in 1034.36 sec.    training loss:\t\t4.47782937245\n",
      "Done 6030 batches in 1036.22 sec.    training loss:\t\t4.47753817612\n",
      "Done 6040 batches in 1037.65 sec.    training loss:\t\t4.47716204631\n",
      "Done 6050 batches in 1039.46 sec.    training loss:\t\t4.47675589676\n",
      "Done 6060 batches in 1041.19 sec.    training loss:\t\t4.47657053876\n",
      "Done 6070 batches in 1042.81 sec.    training loss:\t\t4.47577430157\n",
      "Done 6080 batches in 1044.88 sec.    training loss:\t\t4.47551896019\n",
      "Done 6090 batches in 1046.28 sec.    training loss:\t\t4.47508013104\n",
      "Done 6100 batches in 1047.90 sec.    training loss:\t\t4.47468483929\n",
      "Done 6110 batches in 1049.93 sec.    training loss:\t\t4.47452739346\n",
      "Done 6120 batches in 1051.79 sec.    training loss:\t\t4.47425366286\n",
      "Done 6130 batches in 1053.87 sec.    training loss:\t\t4.47402897251\n",
      "Done 6140 batches in 1055.55 sec.    training loss:\t\t4.4738976743\n",
      "Done 6150 batches in 1057.59 sec.    training loss:\t\t4.4734868951\n",
      "Done 6160 batches in 1059.12 sec.    training loss:\t\t4.47313341367\n",
      "Done 6170 batches in 1060.80 sec.    training loss:\t\t4.47261839071\n",
      "Done 6180 batches in 1062.76 sec.    training loss:\t\t4.47236444391\n",
      "Done 6190 batches in 1064.28 sec.    training loss:\t\t4.47201700438\n",
      "Done 6200 batches in 1066.15 sec.    training loss:\t\t4.47176317038\n",
      "Done 6210 batches in 1067.51 sec.    training loss:\t\t4.47110992733\n",
      "Done 6220 batches in 1069.29 sec.    training loss:\t\t4.47075524675\n",
      "Done 6230 batches in 1071.13 sec.    training loss:\t\t4.47036515032\n",
      "Done 6240 batches in 1073.23 sec.    training loss:\t\t4.46998954079\n",
      "Done 6250 batches in 1074.95 sec.    training loss:\t\t4.46958463623\n",
      "Done 6260 batches in 1076.77 sec.    training loss:\t\t4.46915871815\n",
      "Done 6270 batches in 1078.45 sec.    training loss:\t\t4.46881411597\n",
      "Done 6280 batches in 1080.28 sec.    training loss:\t\t4.46846227\n",
      "Done 6290 batches in 1081.85 sec.    training loss:\t\t4.46818551441\n",
      "Done 6300 batches in 1083.37 sec.    training loss:\t\t4.46781005432\n",
      "Done 6310 batches in 1084.95 sec.    training loss:\t\t4.46744789265\n",
      "Done 6320 batches in 1086.75 sec.    training loss:\t\t4.46715874268\n",
      "Done 6330 batches in 1088.73 sec.    training loss:\t\t4.46699873233\n",
      "Done 6340 batches in 1090.55 sec.    training loss:\t\t4.46657366553\n",
      "Done 6350 batches in 1092.21 sec.    training loss:\t\t4.46638916583\n",
      "Done 6360 batches in 1094.19 sec.    training loss:\t\t4.46608522294\n",
      "Done 6370 batches in 1096.07 sec.    training loss:\t\t4.46585227212\n",
      "Done 6380 batches in 1097.73 sec.    training loss:\t\t4.46541500364\n",
      "Done 6390 batches in 1099.54 sec.    training loss:\t\t4.46507787757\n",
      "Done 6400 batches in 1101.12 sec.    training loss:\t\t4.46461394858\n",
      "Done 6410 batches in 1103.04 sec.    training loss:\t\t4.46434788239\n",
      "Done 6420 batches in 1104.88 sec.    training loss:\t\t4.46408204665\n",
      "Done 6430 batches in 1106.63 sec.    training loss:\t\t4.46368688643\n",
      "Done 6440 batches in 1108.18 sec.    training loss:\t\t4.46303076963\n",
      "Done 6450 batches in 1109.95 sec.    training loss:\t\t4.46263514315\n",
      "Done 6460 batches in 1111.46 sec.    training loss:\t\t4.46219554095\n",
      "Done 6470 batches in 1113.09 sec.    training loss:\t\t4.46182309796\n",
      "Done 6480 batches in 1114.67 sec.    training loss:\t\t4.46141235619\n",
      "Done 6490 batches in 1116.50 sec.    training loss:\t\t4.46118883903\n",
      "Done 6500 batches in 1118.04 sec.    training loss:\t\t4.46092728116\n",
      "Done 6510 batches in 1119.65 sec.    training loss:\t\t4.46067447285\n",
      "Done 6520 batches in 1121.34 sec.    training loss:\t\t4.46040023353\n",
      "Done 6530 batches in 1122.83 sec.    training loss:\t\t4.45980253391\n",
      "Done 6540 batches in 1124.62 sec.    training loss:\t\t4.45944455443\n",
      "Done 6550 batches in 1126.28 sec.    training loss:\t\t4.45907500817\n",
      "Done 6560 batches in 1128.60 sec.    training loss:\t\t4.458610187\n",
      "Done 6570 batches in 1130.63 sec.    training loss:\t\t4.45827150581\n",
      "Done 6580 batches in 1132.21 sec.    training loss:\t\t4.45795007806\n",
      "Done 6590 batches in 1134.04 sec.    training loss:\t\t4.45750576589\n",
      "Done 6600 batches in 1136.14 sec.    training loss:\t\t4.45728877606\n",
      "Done 6610 batches in 1138.00 sec.    training loss:\t\t4.45707167569\n",
      "Done 6620 batches in 1139.44 sec.    training loss:\t\t4.45660370772\n",
      "Done 6630 batches in 1140.88 sec.    training loss:\t\t4.45625563247\n",
      "Done 6640 batches in 1142.48 sec.    training loss:\t\t4.45597488453\n",
      "Done 6650 batches in 1144.38 sec.    training loss:\t\t4.45578155657\n",
      "Done 6660 batches in 1146.25 sec.    training loss:\t\t4.45545388668\n",
      "Done 6670 batches in 1148.20 sec.    training loss:\t\t4.45506847934\n",
      "Done 6680 batches in 1149.85 sec.    training loss:\t\t4.4545078768\n",
      "Done 6690 batches in 1151.44 sec.    training loss:\t\t4.45406051185\n",
      "Done 6700 batches in 1153.15 sec.    training loss:\t\t4.45372030941\n",
      "Done 6710 batches in 1154.92 sec.    training loss:\t\t4.45333622355\n",
      "Done 6720 batches in 1156.91 sec.    training loss:\t\t4.45288476614\n",
      "Done 6730 batches in 1158.45 sec.    training loss:\t\t4.45264641353\n",
      "Done 6740 batches in 1160.28 sec.    training loss:\t\t4.45239188484\n",
      "Done 6750 batches in 1162.05 sec.    training loss:\t\t4.45206678285\n",
      "Done 6760 batches in 1163.80 sec.    training loss:\t\t4.45169868596\n",
      "Done 6770 batches in 1165.32 sec.    training loss:\t\t4.4513343144\n",
      "Done 6780 batches in 1167.00 sec.    training loss:\t\t4.4511781689\n",
      "Done 6790 batches in 1168.96 sec.    training loss:\t\t4.45098882799\n",
      "Done 6800 batches in 1170.90 sec.    training loss:\t\t4.45071246466\n",
      "Done 6810 batches in 1172.69 sec.    training loss:\t\t4.4505409748\n",
      "Done 6820 batches in 1174.49 sec.    training loss:\t\t4.45028938537\n",
      "Done 6830 batches in 1176.26 sec.    training loss:\t\t4.45001144605\n",
      "Done 6840 batches in 1178.00 sec.    training loss:\t\t4.44981050422\n",
      "Done 6850 batches in 1180.02 sec.    training loss:\t\t4.4495192833\n",
      "Done 6860 batches in 1181.85 sec.    training loss:\t\t4.44918127095\n",
      "Done 6870 batches in 1183.34 sec.    training loss:\t\t4.4489478055\n",
      "Done 6880 batches in 1185.63 sec.    training loss:\t\t4.44865145028\n",
      "Done 6890 batches in 1187.97 sec.    training loss:\t\t4.44828413121\n",
      "Done 6900 batches in 1189.56 sec.    training loss:\t\t4.44786637158\n",
      "Done 6910 batches in 1191.33 sec.    training loss:\t\t4.44755421437\n",
      "Done 6920 batches in 1193.07 sec.    training loss:\t\t4.44712889777\n",
      "Done 6930 batches in 1194.90 sec.    training loss:\t\t4.44679461131\n",
      "Done 6940 batches in 1196.71 sec.    training loss:\t\t4.4464670698\n",
      "Done 6950 batches in 1198.41 sec.    training loss:\t\t4.44590258739\n",
      "Done 6960 batches in 1200.08 sec.    training loss:\t\t4.44552753962\n",
      "Done 6970 batches in 1201.75 sec.    training loss:\t\t4.44513624587\n",
      "Done 6980 batches in 1203.85 sec.    training loss:\t\t4.44489730094\n",
      "Done 6990 batches in 1205.32 sec.    training loss:\t\t4.44456523034\n",
      "Done 7000 batches in 1206.96 sec.    training loss:\t\t4.44412730476\n",
      "Done 7010 batches in 1208.61 sec.    training loss:\t\t4.44380694245\n",
      "Done 7020 batches in 1210.58 sec.    training loss:\t\t4.44348615788\n",
      "Done 7030 batches in 1212.72 sec.    training loss:\t\t4.44327956433\n",
      "Done 7040 batches in 1214.69 sec.    training loss:\t\t4.44290433685\n",
      "Done 7050 batches in 1216.18 sec.    training loss:\t\t4.44259239437\n",
      "Done 7060 batches in 1217.76 sec.    training loss:\t\t4.44232875595\n",
      "Done 7070 batches in 1219.43 sec.    training loss:\t\t4.44195058636\n",
      "Done 7080 batches in 1221.29 sec.    training loss:\t\t4.44175369073\n",
      "Done 7090 batches in 1222.83 sec.    training loss:\t\t4.44147688687\n",
      "Done 7100 batches in 1224.51 sec.    training loss:\t\t4.44113437965\n",
      "Done 7110 batches in 1226.17 sec.    training loss:\t\t4.44071581944\n",
      "Done 7120 batches in 1227.71 sec.    training loss:\t\t4.44023085458\n",
      "Done 7130 batches in 1229.58 sec.    training loss:\t\t4.43993884374\n",
      "Done 7140 batches in 1231.09 sec.    training loss:\t\t4.43955293787\n",
      "Done 7150 batches in 1232.93 sec.    training loss:\t\t4.43926200317\n",
      "Done 7160 batches in 1234.52 sec.    training loss:\t\t4.43893820354\n",
      "Done 7170 batches in 1236.35 sec.    training loss:\t\t4.43860404874\n",
      "Done 7180 batches in 1237.96 sec.    training loss:\t\t4.43828709019\n",
      "Done 7190 batches in 1239.58 sec.    training loss:\t\t4.4380814236\n",
      "Done 7200 batches in 1241.27 sec.    training loss:\t\t4.43774368336\n",
      "Done 7210 batches in 1242.76 sec.    training loss:\t\t4.43751335858\n",
      "Done 7220 batches in 1244.42 sec.    training loss:\t\t4.43726871212\n",
      "Done 7230 batches in 1246.09 sec.    training loss:\t\t4.43704555061\n",
      "Done 7240 batches in 1248.20 sec.    training loss:\t\t4.43672682781\n",
      "Done 7250 batches in 1249.80 sec.    training loss:\t\t4.43651258699\n",
      "Done 7260 batches in 1251.55 sec.    training loss:\t\t4.43624049734\n",
      "Done 7270 batches in 1252.92 sec.    training loss:\t\t4.43587847664\n",
      "Done 7280 batches in 1254.68 sec.    training loss:\t\t4.43564415522\n",
      "Done 7290 batches in 1256.58 sec.    training loss:\t\t4.43542556867\n",
      "Done 7300 batches in 1258.35 sec.    training loss:\t\t4.43505545802\n",
      "Done 7310 batches in 1259.72 sec.    training loss:\t\t4.43470651732\n",
      "Done 7320 batches in 1261.54 sec.    training loss:\t\t4.434456234\n",
      "Done 7330 batches in 1263.45 sec.    training loss:\t\t4.43415688935\n",
      "Done 7340 batches in 1265.58 sec.    training loss:\t\t4.43381326377\n",
      "Done 7350 batches in 1267.64 sec.    training loss:\t\t4.43359659827\n",
      "Done 7360 batches in 1269.09 sec.    training loss:\t\t4.4332929802\n",
      "Done 7370 batches in 1270.71 sec.    training loss:\t\t4.43283677415\n",
      "Done 7380 batches in 1272.41 sec.    training loss:\t\t4.4325204227\n",
      "Done 7390 batches in 1274.72 sec.    training loss:\t\t4.43233868408\n",
      "Done 7400 batches in 1276.31 sec.    training loss:\t\t4.43200634431\n",
      "Done 7410 batches in 1278.04 sec.    training loss:\t\t4.43173591499\n",
      "Done 7420 batches in 1279.72 sec.    training loss:\t\t4.4315594187\n",
      "Done 7430 batches in 1281.24 sec.    training loss:\t\t4.43137165922\n",
      "Done 7440 batches in 1282.75 sec.    training loss:\t\t4.43104670817\n",
      "Done 7450 batches in 1284.82 sec.    training loss:\t\t4.43093120965\n",
      "Done 7460 batches in 1286.57 sec.    training loss:\t\t4.43086138839\n",
      "Done 7470 batches in 1288.10 sec.    training loss:\t\t4.43060988406\n",
      "Done 7480 batches in 1289.85 sec.    training loss:\t\t4.43032959999\n",
      "Done 7490 batches in 1291.50 sec.    training loss:\t\t4.43000213295\n",
      "Done 7500 batches in 1293.20 sec.    training loss:\t\t4.4297401399\n",
      "Done 7510 batches in 1295.31 sec.    training loss:\t\t4.42956838633\n",
      "Done 7520 batches in 1297.20 sec.    training loss:\t\t4.42923252332\n",
      "Done 7530 batches in 1298.89 sec.    training loss:\t\t4.42891246595\n",
      "Done 7540 batches in 1300.41 sec.    training loss:\t\t4.42858859196\n",
      "Done 7550 batches in 1301.93 sec.    training loss:\t\t4.42832316354\n",
      "Done 7560 batches in 1303.43 sec.    training loss:\t\t4.42796278012\n",
      "Done 7570 batches in 1305.03 sec.    training loss:\t\t4.42767097062\n",
      "Done 7580 batches in 1306.88 sec.    training loss:\t\t4.42749590889\n",
      "Done 7590 batches in 1308.51 sec.    training loss:\t\t4.42733004577\n",
      "Done 7600 batches in 1310.15 sec.    training loss:\t\t4.42719498512\n",
      "Done 7610 batches in 1311.70 sec.    training loss:\t\t4.42695728166\n",
      "Done 7620 batches in 1313.37 sec.    training loss:\t\t4.42669378178\n",
      "Done 7630 batches in 1315.13 sec.    training loss:\t\t4.4263397628\n",
      "Done 7640 batches in 1316.74 sec.    training loss:\t\t4.42594873212\n",
      "Done 7650 batches in 1318.36 sec.    training loss:\t\t4.42566591263\n",
      "Done 7660 batches in 1319.89 sec.    training loss:\t\t4.42533301541\n",
      "Done 7670 batches in 1321.44 sec.    training loss:\t\t4.42495900602\n",
      "Done 7680 batches in 1323.04 sec.    training loss:\t\t4.42464202242\n",
      "Done 7690 batches in 1324.80 sec.    training loss:\t\t4.42441999977\n",
      "Done 7700 batches in 1326.60 sec.    training loss:\t\t4.42399955523\n",
      "Done 7710 batches in 1328.12 sec.    training loss:\t\t4.42360376622\n",
      "Done 7720 batches in 1329.99 sec.    training loss:\t\t4.42326532244\n",
      "Done 7730 batches in 1331.59 sec.    training loss:\t\t4.42300042532\n",
      "Done 7740 batches in 1333.29 sec.    training loss:\t\t4.42279853476\n",
      "Done 7750 batches in 1335.39 sec.    training loss:\t\t4.42256022696\n",
      "Done 7760 batches in 1337.22 sec.    training loss:\t\t4.42228868564\n",
      "Done 7770 batches in 1339.16 sec.    training loss:\t\t4.4220522756\n",
      "Done 7780 batches in 1340.83 sec.    training loss:\t\t4.42174247935\n",
      "Done 7790 batches in 1342.82 sec.    training loss:\t\t4.42150081716\n",
      "Done 7800 batches in 1344.45 sec.    training loss:\t\t4.421176192\n",
      "Done 7810 batches in 1346.04 sec.    training loss:\t\t4.42078052611\n",
      "Done 7820 batches in 1348.12 sec.    training loss:\t\t4.42055606982\n",
      "Done 7830 batches in 1349.90 sec.    training loss:\t\t4.4202043487\n",
      "Done 7840 batches in 1352.10 sec.    training loss:\t\t4.41997694969\n",
      "Done 7850 batches in 1353.89 sec.    training loss:\t\t4.41959955525\n",
      "Done 7860 batches in 1355.71 sec.    training loss:\t\t4.41922611323\n",
      "Done 7870 batches in 1357.46 sec.    training loss:\t\t4.41900893033\n",
      "Done 7880 batches in 1359.12 sec.    training loss:\t\t4.41879849328\n",
      "Done 7890 batches in 1360.85 sec.    training loss:\t\t4.41847665812\n",
      "Done 7900 batches in 1362.54 sec.    training loss:\t\t4.41817074441\n",
      "Done 7910 batches in 1364.59 sec.    training loss:\t\t4.41794778535\n",
      "Done 7920 batches in 1366.18 sec.    training loss:\t\t4.41766502361\n",
      "Done 7930 batches in 1368.13 sec.    training loss:\t\t4.41748418384\n",
      "Done 7940 batches in 1369.70 sec.    training loss:\t\t4.41720894479\n",
      "Done 7950 batches in 1371.49 sec.    training loss:\t\t4.41695334462\n",
      "Done 7960 batches in 1373.26 sec.    training loss:\t\t4.416668411\n",
      "Done 7970 batches in 1375.00 sec.    training loss:\t\t4.41642888517\n",
      "Done 7980 batches in 1376.63 sec.    training loss:\t\t4.41610386222\n",
      "Done 7990 batches in 1378.40 sec.    training loss:\t\t4.41581951942\n",
      "Done 8000 batches in 1380.10 sec.    training loss:\t\t4.41550229469\n",
      "Done 8010 batches in 1381.85 sec.    training loss:\t\t4.41505803077\n",
      "Done 8020 batches in 1383.81 sec.    training loss:\t\t4.41480389557\n",
      "Done 8030 batches in 1385.55 sec.    training loss:\t\t4.4145483783\n",
      "Done 8040 batches in 1387.38 sec.    training loss:\t\t4.41441783893\n",
      "Done 8050 batches in 1389.02 sec.    training loss:\t\t4.41416412197\n",
      "Done 8060 batches in 1391.24 sec.    training loss:\t\t4.41397867765\n",
      "Done 8070 batches in 1393.27 sec.    training loss:\t\t4.41375607907\n",
      "Done 8080 batches in 1394.84 sec.    training loss:\t\t4.41339449608\n",
      "Done 8090 batches in 1396.76 sec.    training loss:\t\t4.41325848586\n",
      "Done 8100 batches in 1398.31 sec.    training loss:\t\t4.41302374442\n",
      "Done 8110 batches in 1400.04 sec.    training loss:\t\t4.41269166905\n",
      "Done 8120 batches in 1401.98 sec.    training loss:\t\t4.41253613809\n",
      "Done 8130 batches in 1403.66 sec.    training loss:\t\t4.4122393775\n",
      "Done 8140 batches in 1405.28 sec.    training loss:\t\t4.41196852539\n",
      "Done 8150 batches in 1406.97 sec.    training loss:\t\t4.41138765511\n",
      "Done 8160 batches in 1408.59 sec.    training loss:\t\t4.41120069372\n",
      "Done 8170 batches in 1410.34 sec.    training loss:\t\t4.4108579434\n",
      "Done 8180 batches in 1412.00 sec.    training loss:\t\t4.41053535647\n",
      "Done 8190 batches in 1413.77 sec.    training loss:\t\t4.41014330451\n",
      "Done 8200 batches in 1415.39 sec.    training loss:\t\t4.40980616343\n",
      "Done 8210 batches in 1416.95 sec.    training loss:\t\t4.4095181011\n",
      "Done 8220 batches in 1418.38 sec.    training loss:\t\t4.40927041528\n",
      "Done 8230 batches in 1420.10 sec.    training loss:\t\t4.40907664076\n",
      "Done 8240 batches in 1421.84 sec.    training loss:\t\t4.40884871434\n",
      "Done 8250 batches in 1423.74 sec.    training loss:\t\t4.40874277002\n",
      "Done 8260 batches in 1425.60 sec.    training loss:\t\t4.40844524553\n",
      "Done 8270 batches in 1427.62 sec.    training loss:\t\t4.40830800911\n",
      "Done 8280 batches in 1429.13 sec.    training loss:\t\t4.4079879992\n",
      "Done 8290 batches in 1430.69 sec.    training loss:\t\t4.40771712207\n",
      "Done 8300 batches in 1432.30 sec.    training loss:\t\t4.40737720297\n",
      "Done 8310 batches in 1434.15 sec.    training loss:\t\t4.4072062101\n",
      "Done 8320 batches in 1435.92 sec.    training loss:\t\t4.40681905701\n",
      "Done 8330 batches in 1437.88 sec.    training loss:\t\t4.40653856943\n",
      "Done 8340 batches in 1439.49 sec.    training loss:\t\t4.40624504567\n",
      "Done 8350 batches in 1441.12 sec.    training loss:\t\t4.40608495338\n",
      "Done 8360 batches in 1442.52 sec.    training loss:\t\t4.40571749687\n",
      "Done 8370 batches in 1444.27 sec.    training loss:\t\t4.40549734126\n",
      "Done 8380 batches in 1445.99 sec.    training loss:\t\t4.40523565766\n",
      "Done 8390 batches in 1447.55 sec.    training loss:\t\t4.40487091311\n",
      "Done 8400 batches in 1450.59 sec.    training loss:\t\t4.40470857546\n",
      "Done 8410 batches in 1452.20 sec.    training loss:\t\t4.40433266129\n",
      "Done 8420 batches in 1453.71 sec.    training loss:\t\t4.4041634661\n",
      "Done 8430 batches in 1455.30 sec.    training loss:\t\t4.4038729281\n",
      "Done 8440 batches in 1457.01 sec.    training loss:\t\t4.4035819515\n",
      "Done 8450 batches in 1458.41 sec.    training loss:\t\t4.40314643352\n",
      "Done 8460 batches in 1459.98 sec.    training loss:\t\t4.40270810527\n",
      "Done 8470 batches in 1461.40 sec.    training loss:\t\t4.40237636338\n",
      "Done 8480 batches in 1463.42 sec.    training loss:\t\t4.4020968059\n",
      "Done 8490 batches in 1465.32 sec.    training loss:\t\t4.40184252922\n",
      "Done 8500 batches in 1467.51 sec.    training loss:\t\t4.40156458538\n",
      "Done 8510 batches in 1469.33 sec.    training loss:\t\t4.40128384878\n",
      "Done 8520 batches in 1471.35 sec.    training loss:\t\t4.40092295674\n",
      "Done 8530 batches in 1472.83 sec.    training loss:\t\t4.40064740106\n",
      "Done 8540 batches in 1474.55 sec.    training loss:\t\t4.4004957899\n",
      "Done 8550 batches in 1476.33 sec.    training loss:\t\t4.40036656527\n",
      "Done 8560 batches in 1478.02 sec.    training loss:\t\t4.40001197597\n",
      "Done 8570 batches in 1479.69 sec.    training loss:\t\t4.39974806918\n",
      "Done 8580 batches in 1481.66 sec.    training loss:\t\t4.39940954939\n",
      "Done 8590 batches in 1483.57 sec.    training loss:\t\t4.39911423124\n",
      "Done 8600 batches in 1485.23 sec.    training loss:\t\t4.39883509115\n",
      "Done 8610 batches in 1486.77 sec.    training loss:\t\t4.39850951911\n",
      "Done 8620 batches in 1488.42 sec.    training loss:\t\t4.39821013345\n",
      "Done 8630 batches in 1490.06 sec.    training loss:\t\t4.39807258804\n",
      "Done 8640 batches in 1491.64 sec.    training loss:\t\t4.39785243322\n",
      "Done 8650 batches in 1493.42 sec.    training loss:\t\t4.39758095733\n",
      "Done 8660 batches in 1495.05 sec.    training loss:\t\t4.39726674023\n",
      "Done 8670 batches in 1496.66 sec.    training loss:\t\t4.39697105632\n",
      "Done 8680 batches in 1498.22 sec.    training loss:\t\t4.3968306275\n",
      "Done 8690 batches in 1499.75 sec.    training loss:\t\t4.39658948311\n",
      "Done 8700 batches in 1501.30 sec.    training loss:\t\t4.39631696597\n",
      "Done 8710 batches in 1503.07 sec.    training loss:\t\t4.39604534235\n",
      "Done 8720 batches in 1504.89 sec.    training loss:\t\t4.39587322512\n",
      "Done 8730 batches in 1506.90 sec.    training loss:\t\t4.39558012029\n",
      "Done 8740 batches in 1508.48 sec.    training loss:\t\t4.39523738279\n",
      "Done 8750 batches in 1510.33 sec.    training loss:\t\t4.39504962823\n",
      "Done 8760 batches in 1512.41 sec.    training loss:\t\t4.39480579368\n",
      "Done 8770 batches in 1513.99 sec.    training loss:\t\t4.39453641227\n",
      "Done 8780 batches in 1515.78 sec.    training loss:\t\t4.39429730739\n",
      "Done 8790 batches in 1517.47 sec.    training loss:\t\t4.39404969771\n",
      "Done 8800 batches in 1519.11 sec.    training loss:\t\t4.39386076746\n",
      "Done 8810 batches in 1520.58 sec.    training loss:\t\t4.39351663278\n",
      "Done 8820 batches in 1522.08 sec.    training loss:\t\t4.39319436053\n",
      "Done 8830 batches in 1523.83 sec.    training loss:\t\t4.39295821611\n",
      "Done 8840 batches in 1525.67 sec.    training loss:\t\t4.39255680919\n",
      "Done 8850 batches in 1527.55 sec.    training loss:\t\t4.3923495558\n",
      "Done 8860 batches in 1529.38 sec.    training loss:\t\t4.39213963284\n",
      "Done 8870 batches in 1530.76 sec.    training loss:\t\t4.39182107131\n",
      "Done 8880 batches in 1532.19 sec.    training loss:\t\t4.39141941822\n",
      "Done 8890 batches in 1534.08 sec.    training loss:\t\t4.39112466677\n",
      "Done 8900 batches in 1535.70 sec.    training loss:\t\t4.39083661473\n",
      "Done 8910 batches in 1537.54 sec.    training loss:\t\t4.39056666018\n",
      "Done 8920 batches in 1539.56 sec.    training loss:\t\t4.39031974604\n",
      "Done 8930 batches in 1541.19 sec.    training loss:\t\t4.38999626169\n",
      "Done 8940 batches in 1542.66 sec.    training loss:\t\t4.3898879576\n",
      "Done 8950 batches in 1544.00 sec.    training loss:\t\t4.38951902376\n",
      "Done 8960 batches in 1545.78 sec.    training loss:\t\t4.38936081952\n",
      "Done 8970 batches in 1547.16 sec.    training loss:\t\t4.38900898106\n",
      "Done 8980 batches in 1548.95 sec.    training loss:\t\t4.38882219521\n",
      "Done 8990 batches in 1550.40 sec.    training loss:\t\t4.38851262388\n",
      "Done 9000 batches in 1552.55 sec.    training loss:\t\t4.38834216523\n",
      "Done 9010 batches in 1554.35 sec.    training loss:\t\t4.38815652393\n",
      "Done 9020 batches in 1555.95 sec.    training loss:\t\t4.38792232687\n",
      "Done 9030 batches in 1557.72 sec.    training loss:\t\t4.3876890596\n",
      "Done 9040 batches in 1559.66 sec.    training loss:\t\t4.3874088088\n",
      "Done 9050 batches in 1561.11 sec.    training loss:\t\t4.38701639099\n",
      "Done 9060 batches in 1562.76 sec.    training loss:\t\t4.38669598268\n",
      "Done 9070 batches in 1564.88 sec.    training loss:\t\t4.38647283688\n",
      "Done 9080 batches in 1566.47 sec.    training loss:\t\t4.38620779845\n",
      "Done 9090 batches in 1568.31 sec.    training loss:\t\t4.38592884436\n",
      "Done 9100 batches in 1569.88 sec.    training loss:\t\t4.38566507625\n",
      "Done 9110 batches in 1571.57 sec.    training loss:\t\t4.38545468419\n",
      "Done 9120 batches in 1573.38 sec.    training loss:\t\t4.38516244721\n",
      "Done 9130 batches in 1574.84 sec.    training loss:\t\t4.38478883159\n",
      "Done 9140 batches in 1576.67 sec.    training loss:\t\t4.38457333269\n",
      "Done 9150 batches in 1578.14 sec.    training loss:\t\t4.38431218661\n",
      "Done 9160 batches in 1579.70 sec.    training loss:\t\t4.38398256333\n",
      "Done 9170 batches in 1582.02 sec.    training loss:\t\t4.3839145727\n",
      "Done 9180 batches in 1584.15 sec.    training loss:\t\t4.38368104966\n",
      "Done 9190 batches in 1585.93 sec.    training loss:\t\t4.38343185899\n",
      "Done 9200 batches in 1587.53 sec.    training loss:\t\t4.38316778787\n",
      "Done 9210 batches in 1589.12 sec.    training loss:\t\t4.3828930628\n",
      "Done 9220 batches in 1590.70 sec.    training loss:\t\t4.3825937477\n",
      "Done 9230 batches in 1592.61 sec.    training loss:\t\t4.38241842201\n",
      "Done 9240 batches in 1594.22 sec.    training loss:\t\t4.38229189866\n",
      "Done 9250 batches in 1595.72 sec.    training loss:\t\t4.38193444917\n",
      "Done 9260 batches in 1597.50 sec.    training loss:\t\t4.38167260633\n",
      "Done 9270 batches in 1598.76 sec.    training loss:\t\t4.38140012606\n",
      "Done 9280 batches in 1600.48 sec.    training loss:\t\t4.38126852384\n",
      "Done 9290 batches in 1602.37 sec.    training loss:\t\t4.38086671262\n",
      "Done 9300 batches in 1604.25 sec.    training loss:\t\t4.38078971753\n",
      "Done 9310 batches in 1605.66 sec.    training loss:\t\t4.3805738896\n",
      "Done 9320 batches in 1607.79 sec.    training loss:\t\t4.38053178797\n",
      "Done 9330 batches in 1609.53 sec.    training loss:\t\t4.3801464931\n",
      "Done 9340 batches in 1611.18 sec.    training loss:\t\t4.37986045639\n",
      "Done 9350 batches in 1612.97 sec.    training loss:\t\t4.37960606363\n",
      "Done 9360 batches in 1614.65 sec.    training loss:\t\t4.37937931599\n",
      "Done 9370 batches in 1616.34 sec.    training loss:\t\t4.37902956462\n",
      "Done 9380 batches in 1617.79 sec.    training loss:\t\t4.37875727141\n",
      "Done 9390 batches in 1619.48 sec.    training loss:\t\t4.37865211687\n",
      "Done 9400 batches in 1621.27 sec.    training loss:\t\t4.37845485459\n",
      "Done 9410 batches in 1623.41 sec.    training loss:\t\t4.37814177142\n",
      "Done 9420 batches in 1624.99 sec.    training loss:\t\t4.37785149157\n",
      "Done 9430 batches in 1626.76 sec.    training loss:\t\t4.37757068462\n",
      "Done 9440 batches in 1628.40 sec.    training loss:\t\t4.37723506985\n",
      "Done 9450 batches in 1630.54 sec.    training loss:\t\t4.37698922079\n",
      "Done 9460 batches in 1632.14 sec.    training loss:\t\t4.37677541447\n",
      "Done 9470 batches in 1633.83 sec.    training loss:\t\t4.37656253945\n",
      "Done 9480 batches in 1635.83 sec.    training loss:\t\t4.37648096117\n",
      "Done 9490 batches in 1637.20 sec.    training loss:\t\t4.37616052972\n",
      "Done 9500 batches in 1639.11 sec.    training loss:\t\t4.37582104603\n",
      "Done 9510 batches in 1640.70 sec.    training loss:\t\t4.37555955274\n",
      "Done 9520 batches in 1642.58 sec.    training loss:\t\t4.37522890713\n",
      "Done 9530 batches in 1644.42 sec.    training loss:\t\t4.37502762736\n",
      "Done 9540 batches in 1646.46 sec.    training loss:\t\t4.3748773116\n",
      "Done 9550 batches in 1648.17 sec.    training loss:\t\t4.37473255986\n",
      "Done 9560 batches in 1649.97 sec.    training loss:\t\t4.37455676186\n",
      "Done 9570 batches in 1651.55 sec.    training loss:\t\t4.37435090634\n",
      "Done 9580 batches in 1653.74 sec.    training loss:\t\t4.37414145333\n",
      "Done 9590 batches in 1655.60 sec.    training loss:\t\t4.37392130052\n",
      "Done 9600 batches in 1657.32 sec.    training loss:\t\t4.37373612215\n",
      "Done 9610 batches in 1658.96 sec.    training loss:\t\t4.37354963024\n",
      "Done 9620 batches in 1660.68 sec.    training loss:\t\t4.37331149281\n",
      "Done 9630 batches in 1662.38 sec.    training loss:\t\t4.37299570671\n",
      "Done 9640 batches in 1664.32 sec.    training loss:\t\t4.3728489348\n",
      "Done 9650 batches in 1665.88 sec.    training loss:\t\t4.37262995221\n",
      "Done 9660 batches in 1667.48 sec.    training loss:\t\t4.37242550149\n",
      "Done 9670 batches in 1669.07 sec.    training loss:\t\t4.37220264906\n",
      "Done 9680 batches in 1670.57 sec.    training loss:\t\t4.3720071946\n",
      "Done 9690 batches in 1672.58 sec.    training loss:\t\t4.37180092613\n",
      "Done 9700 batches in 1674.21 sec.    training loss:\t\t4.37142013756\n",
      "Done 9710 batches in 1675.86 sec.    training loss:\t\t4.37120379547\n",
      "Done 9720 batches in 1677.47 sec.    training loss:\t\t4.3709523527\n",
      "Done 9730 batches in 1678.96 sec.    training loss:\t\t4.37066503519\n",
      "Done 9740 batches in 1680.21 sec.    training loss:\t\t4.37049251577\n",
      "Done 9750 batches in 1681.72 sec.    training loss:\t\t4.37030352319\n",
      "Done 9760 batches in 1683.26 sec.    training loss:\t\t4.3700765437\n",
      "Done 9770 batches in 1685.09 sec.    training loss:\t\t4.36987051688\n",
      "Done 9780 batches in 1686.70 sec.    training loss:\t\t4.36959829796\n",
      "Done 9790 batches in 1688.98 sec.    training loss:\t\t4.36946940921\n",
      "Done 9800 batches in 1690.86 sec.    training loss:\t\t4.36927007026\n",
      "Done 9810 batches in 1692.69 sec.    training loss:\t\t4.36911853888\n",
      "Done 9820 batches in 1694.77 sec.    training loss:\t\t4.36885493909\n",
      "Done 9830 batches in 1696.34 sec.    training loss:\t\t4.36862795494\n",
      "Done 9840 batches in 1697.88 sec.    training loss:\t\t4.36839963375\n",
      "Done 9850 batches in 1699.39 sec.    training loss:\t\t4.36819372695\n",
      "Done 9860 batches in 1700.91 sec.    training loss:\t\t4.36796049349\n",
      "Done 9870 batches in 1702.65 sec.    training loss:\t\t4.36770894247\n",
      "Done 9880 batches in 1704.45 sec.    training loss:\t\t4.36749976842\n",
      "Done 9890 batches in 1706.52 sec.    training loss:\t\t4.3672983676\n",
      "Done 9900 batches in 1708.40 sec.    training loss:\t\t4.3670271138\n",
      "Done 9910 batches in 1710.34 sec.    training loss:\t\t4.36677442299\n",
      "Done 9920 batches in 1711.95 sec.    training loss:\t\t4.36644314713\n",
      "Done 9930 batches in 1713.50 sec.    training loss:\t\t4.36614993769\n",
      "Done 9940 batches in 1715.24 sec.    training loss:\t\t4.36604469508\n",
      "Done 9950 batches in 1716.99 sec.    training loss:\t\t4.36583030813\n",
      "Done 9960 batches in 1719.17 sec.    training loss:\t\t4.3656846631\n",
      "Done 9970 batches in 1720.66 sec.    training loss:\t\t4.36548337013\n",
      "Done 9980 batches in 1722.41 sec.    training loss:\t\t4.36521333442\n",
      "Done 9990 batches in 1724.02 sec.    training loss:\t\t4.36497028163\n",
      "Done 10000 batches in 1725.77 sec.    training loss:\t\t4.3646824157\n",
      "Done 10010 batches in 1727.46 sec.    training loss:\t\t4.3643492249\n",
      "Done 10020 batches in 1729.12 sec.    training loss:\t\t4.36415994391\n",
      "Done 10030 batches in 1730.62 sec.    training loss:\t\t4.3638221102\n",
      "Done 10040 batches in 1732.20 sec.    training loss:\t\t4.36365692841\n",
      "Done 10050 batches in 1734.17 sec.    training loss:\t\t4.36338574606\n",
      "Done 10060 batches in 1735.59 sec.    training loss:\t\t4.36318338638\n",
      "Done 10070 batches in 1737.64 sec.    training loss:\t\t4.36313475625\n",
      "Done 10080 batches in 1739.35 sec.    training loss:\t\t4.36288316219\n",
      "Done 10090 batches in 1741.18 sec.    training loss:\t\t4.36268156527\n",
      "Done 10100 batches in 1742.89 sec.    training loss:\t\t4.36249066294\n",
      "Done 10110 batches in 1744.68 sec.    training loss:\t\t4.36233438497\n",
      "Done 10120 batches in 1746.28 sec.    training loss:\t\t4.36204860168\n",
      "Done 10130 batches in 1748.17 sec.    training loss:\t\t4.36188137978\n",
      "Done 10140 batches in 1749.93 sec.    training loss:\t\t4.36177070308\n",
      "Done 10150 batches in 1751.72 sec.    training loss:\t\t4.36156444416\n",
      "Done 10160 batches in 1753.62 sec.    training loss:\t\t4.36147332755\n",
      "Done 10170 batches in 1755.13 sec.    training loss:\t\t4.3613737382\n",
      "Done 10180 batches in 1756.91 sec.    training loss:\t\t4.3611057774\n",
      "Done 10190 batches in 1758.61 sec.    training loss:\t\t4.36095154779\n",
      "Done 10200 batches in 1760.14 sec.    training loss:\t\t4.36072937152\n",
      "Done 10210 batches in 1762.02 sec.    training loss:\t\t4.36042699889\n",
      "Done 10220 batches in 1763.68 sec.    training loss:\t\t4.36023054417\n",
      "Done 10230 batches in 1765.04 sec.    training loss:\t\t4.36002287699\n",
      "Done 10240 batches in 1766.53 sec.    training loss:\t\t4.35979040747\n",
      "Done 10250 batches in 1769.18 sec.    training loss:\t\t4.35975931454\n",
      "Done 10260 batches in 1770.80 sec.    training loss:\t\t4.35954309523\n",
      "Done 10270 batches in 1772.84 sec.    training loss:\t\t4.35935216651\n",
      "Done 10280 batches in 1774.12 sec.    training loss:\t\t4.35903561043\n",
      "Done 10290 batches in 1775.84 sec.    training loss:\t\t4.35884092086\n",
      "Done 10300 batches in 1777.17 sec.    training loss:\t\t4.35847069694\n",
      "Done 10310 batches in 1778.61 sec.    training loss:\t\t4.35817819553\n",
      "Done 10320 batches in 1780.19 sec.    training loss:\t\t4.35796317546\n",
      "Done 10330 batches in 1781.66 sec.    training loss:\t\t4.35775185146\n",
      "Done 10340 batches in 1783.51 sec.    training loss:\t\t4.35752145564\n",
      "Done 10350 batches in 1785.20 sec.    training loss:\t\t4.35726279408\n",
      "Done 10360 batches in 1787.11 sec.    training loss:\t\t4.35712044423\n",
      "Done 10370 batches in 1788.92 sec.    training loss:\t\t4.35701937682\n",
      "Done 10380 batches in 1790.74 sec.    training loss:\t\t4.35676249433\n",
      "Done 10390 batches in 1792.79 sec.    training loss:\t\t4.35651410083\n",
      "Done 10400 batches in 1794.44 sec.    training loss:\t\t4.35628268476\n",
      "Done 10410 batches in 1796.00 sec.    training loss:\t\t4.35598524382\n",
      "Done 10420 batches in 1797.87 sec.    training loss:\t\t4.35587316722\n",
      "Done 10430 batches in 1799.52 sec.    training loss:\t\t4.35567715765\n",
      "Done 10440 batches in 1801.35 sec.    training loss:\t\t4.35542247001\n",
      "Done 10450 batches in 1803.48 sec.    training loss:\t\t4.35531509472\n",
      "Done 10460 batches in 1805.15 sec.    training loss:\t\t4.35509620116\n",
      "Done 10470 batches in 1807.27 sec.    training loss:\t\t4.35486053006\n",
      "Done 10480 batches in 1809.14 sec.    training loss:\t\t4.35462910717\n",
      "Done 10490 batches in 1810.92 sec.    training loss:\t\t4.35456149367\n",
      "Done 10500 batches in 1812.85 sec.    training loss:\t\t4.35431242366\n",
      "Done 10510 batches in 1814.67 sec.    training loss:\t\t4.35408279193\n",
      "Done 10520 batches in 1816.51 sec.    training loss:\t\t4.35392340569\n",
      "Done 10530 batches in 1818.21 sec.    training loss:\t\t4.35369990024\n",
      "Done 10540 batches in 1820.43 sec.    training loss:\t\t4.35357752423\n",
      "Done 10550 batches in 1822.45 sec.    training loss:\t\t4.35355331138\n",
      "Done 10560 batches in 1824.21 sec.    training loss:\t\t4.35339478868\n",
      "Done 10570 batches in 1826.09 sec.    training loss:\t\t4.3531196001\n",
      "Done 10580 batches in 1827.79 sec.    training loss:\t\t4.35301602081\n",
      "Done 10590 batches in 1829.35 sec.    training loss:\t\t4.35277066645\n",
      "Done 10600 batches in 1831.12 sec.    training loss:\t\t4.35253352937\n",
      "Done 10610 batches in 1832.86 sec.    training loss:\t\t4.35238210647\n",
      "Done 10620 batches in 1834.45 sec.    training loss:\t\t4.35215918973\n",
      "Done 10630 batches in 1836.16 sec.    training loss:\t\t4.35200227872\n",
      "Done 10640 batches in 1838.09 sec.    training loss:\t\t4.35187360257\n",
      "Done 10650 batches in 1839.62 sec.    training loss:\t\t4.3517651589\n",
      "Done 10660 batches in 1841.37 sec.    training loss:\t\t4.35158770898\n",
      "Done 10670 batches in 1843.06 sec.    training loss:\t\t4.35144513136\n",
      "Done 10680 batches in 1844.80 sec.    training loss:\t\t4.35133922808\n",
      "Done 10690 batches in 1846.75 sec.    training loss:\t\t4.35114311924\n",
      "Done 10700 batches in 1848.49 sec.    training loss:\t\t4.35092344028\n",
      "Done 10710 batches in 1850.27 sec.    training loss:\t\t4.35071135876\n",
      "Done 10720 batches in 1852.08 sec.    training loss:\t\t4.35057189654\n",
      "Done 10730 batches in 1853.89 sec.    training loss:\t\t4.3503603542\n",
      "Done 10740 batches in 1855.40 sec.    training loss:\t\t4.35013294997\n",
      "Done 10750 batches in 1856.89 sec.    training loss:\t\t4.34986038916\n",
      "Done 10760 batches in 1858.51 sec.    training loss:\t\t4.34967428147\n",
      "Done 10770 batches in 1860.33 sec.    training loss:\t\t4.34944776413\n",
      "Done 10780 batches in 1862.17 sec.    training loss:\t\t4.34926293224\n",
      "Done 10790 batches in 1863.69 sec.    training loss:\t\t4.34904251664\n",
      "Done 10800 batches in 1865.47 sec.    training loss:\t\t4.34880714812\n",
      "Done 10810 batches in 1867.34 sec.    training loss:\t\t4.3487328772\n",
      "Done 10820 batches in 1869.12 sec.    training loss:\t\t4.34843294195\n",
      "Done 10830 batches in 1870.91 sec.    training loss:\t\t4.34820454123\n",
      "Done 10840 batches in 1872.50 sec.    training loss:\t\t4.34801550026\n",
      "Done 10850 batches in 1874.35 sec.    training loss:\t\t4.34793706769\n",
      "Done 10860 batches in 1876.16 sec.    training loss:\t\t4.34774735413\n",
      "Done 10870 batches in 1877.68 sec.    training loss:\t\t4.34758567871\n",
      "Done 10880 batches in 1879.85 sec.    training loss:\t\t4.34730185163\n",
      "Done 10890 batches in 1881.33 sec.    training loss:\t\t4.3471543693\n",
      "Done 10900 batches in 1882.59 sec.    training loss:\t\t4.34691471187\n",
      "Done 10910 batches in 1884.27 sec.    training loss:\t\t4.34679249017\n",
      "Done 10920 batches in 1886.08 sec.    training loss:\t\t4.34667694451\n",
      "Done 10930 batches in 1887.76 sec.    training loss:\t\t4.34651905402\n",
      "Done 10940 batches in 1889.72 sec.    training loss:\t\t4.34633809441\n",
      "Done 10950 batches in 1891.89 sec.    training loss:\t\t4.34615586934\n",
      "Done 10960 batches in 1894.19 sec.    training loss:\t\t4.34599014875\n",
      "Done 10970 batches in 1896.30 sec.    training loss:\t\t4.345832262\n",
      "Done 10980 batches in 1897.95 sec.    training loss:\t\t4.34561068532\n",
      "Done 10990 batches in 1899.65 sec.    training loss:\t\t4.34542567845\n",
      "Done 11000 batches in 1901.17 sec.    training loss:\t\t4.34515494024\n",
      "Done 11010 batches in 1902.94 sec.    training loss:\t\t4.34501363157\n",
      "Done 11020 batches in 1904.69 sec.    training loss:\t\t4.34477141412\n",
      "Done 11030 batches in 1906.65 sec.    training loss:\t\t4.34460467339\n",
      "Done 11040 batches in 1908.77 sec.    training loss:\t\t4.34446904743\n",
      "Done 11050 batches in 1910.58 sec.    training loss:\t\t4.34427814415\n",
      "Done 11060 batches in 1912.23 sec.    training loss:\t\t4.34405331668\n",
      "Done 11070 batches in 1913.89 sec.    training loss:\t\t4.34383252837\n",
      "Done 11080 batches in 1915.44 sec.    training loss:\t\t4.34364317832\n",
      "Done 11090 batches in 1917.21 sec.    training loss:\t\t4.34338047618\n",
      "Done 11100 batches in 1919.62 sec.    training loss:\t\t4.34332098667\n",
      "Done 11110 batches in 1921.10 sec.    training loss:\t\t4.34315839874\n",
      "Done 11120 batches in 1922.53 sec.    training loss:\t\t4.34298400598\n",
      "Done 11130 batches in 1924.62 sec.    training loss:\t\t4.34278475437\n",
      "Done 11140 batches in 1926.20 sec.    training loss:\t\t4.34250407405\n",
      "Done 11150 batches in 1927.90 sec.    training loss:\t\t4.3423473031\n",
      "Done 11160 batches in 1929.64 sec.    training loss:\t\t4.34212148354\n",
      "Done 11170 batches in 1931.53 sec.    training loss:\t\t4.34203223599\n",
      "Done 11180 batches in 1933.55 sec.    training loss:\t\t4.34187228748\n",
      "Done 11190 batches in 1935.30 sec.    training loss:\t\t4.34163719248\n",
      "Done 11200 batches in 1937.03 sec.    training loss:\t\t4.34139197552\n",
      "Done 11210 batches in 1938.50 sec.    training loss:\t\t4.34118016417\n",
      "Done 11220 batches in 1940.21 sec.    training loss:\t\t4.34101655702\n",
      "Done 11230 batches in 1941.88 sec.    training loss:\t\t4.34080405766\n",
      "Done 11240 batches in 1943.60 sec.    training loss:\t\t4.34053365206\n",
      "Done 11250 batches in 1945.61 sec.    training loss:\t\t4.34037990879\n",
      "Done 11260 batches in 1947.11 sec.    training loss:\t\t4.34015321683\n",
      "Done 11270 batches in 1949.08 sec.    training loss:\t\t4.33996354277\n",
      "Done 11280 batches in 1950.71 sec.    training loss:\t\t4.3397482865\n",
      "Done 11290 batches in 1952.11 sec.    training loss:\t\t4.33948264713\n",
      "Done 11300 batches in 1953.90 sec.    training loss:\t\t4.33934693693\n",
      "Done 11310 batches in 1955.81 sec.    training loss:\t\t4.33912728708\n",
      "Done 11320 batches in 1957.72 sec.    training loss:\t\t4.33897460253\n",
      "Done 11330 batches in 1959.44 sec.    training loss:\t\t4.33876787229\n",
      "Done 11340 batches in 1960.94 sec.    training loss:\t\t4.33847428634\n",
      "Done 11350 batches in 1962.47 sec.    training loss:\t\t4.33831533274\n",
      "Done 11360 batches in 1964.24 sec.    training loss:\t\t4.33806621124\n",
      "Done 11370 batches in 1966.07 sec.    training loss:\t\t4.33800085486\n",
      "Done 11380 batches in 1967.74 sec.    training loss:\t\t4.33780274412\n",
      "Done 11390 batches in 1969.63 sec.    training loss:\t\t4.3377724087\n",
      "Done 11400 batches in 1971.21 sec.    training loss:\t\t4.3375995815\n",
      "Done 11410 batches in 1973.16 sec.    training loss:\t\t4.33744939015\n",
      "Done 11420 batches in 1974.91 sec.    training loss:\t\t4.33726391116\n",
      "Done 11430 batches in 1976.71 sec.    training loss:\t\t4.33715647821\n",
      "Done 11440 batches in 1978.38 sec.    training loss:\t\t4.33694522235\n",
      "Done 11450 batches in 1979.92 sec.    training loss:\t\t4.33679262099\n",
      "Done 11460 batches in 1981.46 sec.    training loss:\t\t4.33653605467\n",
      "Done 11470 batches in 1982.93 sec.    training loss:\t\t4.33625250085\n",
      "Done 11480 batches in 1984.70 sec.    training loss:\t\t4.33602872483\n",
      "Done 11490 batches in 1986.49 sec.    training loss:\t\t4.33579865268\n",
      "Done 11500 batches in 1988.13 sec.    training loss:\t\t4.33566704267\n",
      "Done 11510 batches in 1989.91 sec.    training loss:\t\t4.33553394907\n",
      "Done 11520 batches in 1991.58 sec.    training loss:\t\t4.3353655319\n",
      "Done 11530 batches in 1993.51 sec.    training loss:\t\t4.33510756253\n",
      "Done 11540 batches in 1995.05 sec.    training loss:\t\t4.33494942254\n",
      "Done 11550 batches in 1996.77 sec.    training loss:\t\t4.3347203469\n",
      "Done 11560 batches in 1998.24 sec.    training loss:\t\t4.33451505072\n",
      "Done 11570 batches in 1999.84 sec.    training loss:\t\t4.33437241181\n",
      "Done 11580 batches in 2001.22 sec.    training loss:\t\t4.3341575912\n",
      "Done 11590 batches in 2003.15 sec.    training loss:\t\t4.33407916179\n",
      "Done 11600 batches in 2005.03 sec.    training loss:\t\t4.33391275352\n",
      "Done 11610 batches in 2007.35 sec.    training loss:\t\t4.33362302308\n",
      "Done 11620 batches in 2008.93 sec.    training loss:\t\t4.33341990406\n",
      "Done 11630 batches in 2011.10 sec.    training loss:\t\t4.33321613983\n",
      "Done 11640 batches in 2013.17 sec.    training loss:\t\t4.33304065796\n",
      "Done 11650 batches in 2014.76 sec.    training loss:\t\t4.33277260627\n",
      "Done 11660 batches in 2016.31 sec.    training loss:\t\t4.33262119841\n",
      "Done 11670 batches in 2017.75 sec.    training loss:\t\t4.33239647247\n",
      "Done 11680 batches in 2019.41 sec.    training loss:\t\t4.3322402763\n",
      "Done 11690 batches in 2021.00 sec.    training loss:\t\t4.33201381909\n",
      "Done 11700 batches in 2022.51 sec.    training loss:\t\t4.33180585971\n",
      "Done 11710 batches in 2024.80 sec.    training loss:\t\t4.331681692\n",
      "Done 11720 batches in 2026.38 sec.    training loss:\t\t4.33155054692\n",
      "Done 11730 batches in 2028.04 sec.    training loss:\t\t4.33131011723\n",
      "Done 11740 batches in 2030.01 sec.    training loss:\t\t4.33120034647\n",
      "Done 11750 batches in 2032.13 sec.    training loss:\t\t4.33112762628\n",
      "Done 11760 batches in 2033.90 sec.    training loss:\t\t4.33094570787\n",
      "Done 11770 batches in 2035.66 sec.    training loss:\t\t4.3307534307\n",
      "Done 11780 batches in 2037.27 sec.    training loss:\t\t4.33060377506\n",
      "Done 11790 batches in 2038.70 sec.    training loss:\t\t4.33043336407\n",
      "Done 11800 batches in 2040.66 sec.    training loss:\t\t4.33027504969\n",
      "Done 11810 batches in 2042.27 sec.    training loss:\t\t4.33008718337\n",
      "Done 11820 batches in 2044.09 sec.    training loss:\t\t4.3299807705\n",
      "Done 11830 batches in 2045.91 sec.    training loss:\t\t4.32985989929\n",
      "Done 11840 batches in 2047.55 sec.    training loss:\t\t4.3297000355\n",
      "Done 11850 batches in 2049.31 sec.    training loss:\t\t4.32950738484\n",
      "Done 11860 batches in 2050.91 sec.    training loss:\t\t4.32930821976\n",
      "Done 11870 batches in 2052.32 sec.    training loss:\t\t4.32905317357\n",
      "Done 11880 batches in 2053.89 sec.    training loss:\t\t4.32887569824\n",
      "Done 11890 batches in 2055.53 sec.    training loss:\t\t4.32874307751\n",
      "Done 11900 batches in 2057.07 sec.    training loss:\t\t4.32859796778\n",
      "Done 11910 batches in 2058.75 sec.    training loss:\t\t4.32840873651\n",
      "Done 11920 batches in 2060.63 sec.    training loss:\t\t4.32820163545\n",
      "Done 11930 batches in 2062.24 sec.    training loss:\t\t4.32804795788\n",
      "Done 11940 batches in 2063.85 sec.    training loss:\t\t4.32783600278\n",
      "Done 11950 batches in 2065.73 sec.    training loss:\t\t4.32763514184\n",
      "Done 11960 batches in 2067.40 sec.    training loss:\t\t4.32754577592\n",
      "Done 11970 batches in 2069.43 sec.    training loss:\t\t4.32738645746\n",
      "Done 11980 batches in 2071.07 sec.    training loss:\t\t4.32714813972\n",
      "Done 11990 batches in 2072.92 sec.    training loss:\t\t4.326937643\n",
      "Done 12000 batches in 2074.46 sec.    training loss:\t\t4.32674322736\n",
      "Done 12010 batches in 2076.17 sec.    training loss:\t\t4.32655634225\n",
      "Done 12020 batches in 2077.88 sec.    training loss:\t\t4.32642104041\n",
      "Done 12030 batches in 2079.54 sec.    training loss:\t\t4.32620931292\n",
      "Done 12040 batches in 2081.47 sec.    training loss:\t\t4.32603477164\n",
      "Done 12050 batches in 2083.18 sec.    training loss:\t\t4.32587845517\n",
      "Done 12060 batches in 2084.83 sec.    training loss:\t\t4.32580344398\n",
      "Done 12070 batches in 2086.81 sec.    training loss:\t\t4.32564976992\n",
      "Done 12080 batches in 2088.35 sec.    training loss:\t\t4.32549932459\n",
      "Done 12090 batches in 2090.34 sec.    training loss:\t\t4.32535560338\n",
      "Done 12100 batches in 2092.17 sec.    training loss:\t\t4.32516781478\n",
      "Done 12110 batches in 2093.99 sec.    training loss:\t\t4.32495480261\n",
      "Done 12120 batches in 2095.69 sec.    training loss:\t\t4.32478335874\n",
      "Done 12130 batches in 2097.32 sec.    training loss:\t\t4.32455862197\n",
      "Done 12140 batches in 2099.14 sec.    training loss:\t\t4.32430578398\n",
      "Done 12150 batches in 2101.18 sec.    training loss:\t\t4.32409768326\n",
      "Done 12160 batches in 2102.70 sec.    training loss:\t\t4.32391569097\n",
      "Done 12170 batches in 2104.55 sec.    training loss:\t\t4.32371596348\n",
      "Done 12180 batches in 2106.20 sec.    training loss:\t\t4.32365861963\n",
      "Done 12190 batches in 2108.28 sec.    training loss:\t\t4.32339680385\n",
      "Done 12200 batches in 2110.14 sec.    training loss:\t\t4.32330755988\n",
      "Done 12210 batches in 2111.72 sec.    training loss:\t\t4.32310427815\n",
      "Done 12220 batches in 2113.43 sec.    training loss:\t\t4.32303217681\n",
      "Done 12230 batches in 2114.99 sec.    training loss:\t\t4.32281689256\n",
      "Done 12240 batches in 2116.56 sec.    training loss:\t\t4.32261854447\n",
      "Done 12250 batches in 2118.41 sec.    training loss:\t\t4.32249080212\n",
      "Done 12260 batches in 2120.34 sec.    training loss:\t\t4.32230232544\n",
      "Done 12270 batches in 2121.73 sec.    training loss:\t\t4.3221069544\n",
      "Done 12280 batches in 2123.38 sec.    training loss:\t\t4.32194081677\n",
      "Done 12290 batches in 2125.41 sec.    training loss:\t\t4.32178753332\n",
      "Done 12300 batches in 2127.11 sec.    training loss:\t\t4.32154329872\n",
      "Done 12310 batches in 2129.19 sec.    training loss:\t\t4.32133211987\n",
      "Done 12320 batches in 2131.06 sec.    training loss:\t\t4.3211395351\n",
      "Done 12330 batches in 2132.55 sec.    training loss:\t\t4.32086226153\n",
      "Done 12340 batches in 2134.45 sec.    training loss:\t\t4.32063387828\n",
      "Done 12350 batches in 2136.48 sec.    training loss:\t\t4.3205560991\n",
      "Done 12360 batches in 2138.15 sec.    training loss:\t\t4.32034525948\n",
      "Done 12370 batches in 2139.83 sec.    training loss:\t\t4.32023327646\n",
      "Done 12380 batches in 2141.55 sec.    training loss:\t\t4.32004075771\n",
      "Done 12390 batches in 2143.53 sec.    training loss:\t\t4.31994514858\n",
      "Done 12400 batches in 2144.90 sec.    training loss:\t\t4.31977050893\n",
      "Done 12410 batches in 2146.74 sec.    training loss:\t\t4.31972608597\n",
      "Done 12420 batches in 2148.68 sec.    training loss:\t\t4.31955120694\n",
      "Done 12430 batches in 2150.70 sec.    training loss:\t\t4.31940472126\n",
      "Done 12440 batches in 2152.62 sec.    training loss:\t\t4.31933020014\n",
      "Done 12450 batches in 2154.47 sec.    training loss:\t\t4.31918068754\n",
      "Done 12460 batches in 2155.80 sec.    training loss:\t\t4.31894177675\n",
      "Done 12470 batches in 2157.61 sec.    training loss:\t\t4.31882247267\n",
      "Done 12480 batches in 2159.49 sec.    training loss:\t\t4.31859379913\n",
      "Done 12490 batches in 2161.20 sec.    training loss:\t\t4.31839571818\n",
      "Done 12500 batches in 2162.59 sec.    training loss:\t\t4.31821141768\n",
      "Done 12510 batches in 2164.15 sec.    training loss:\t\t4.31802565038\n",
      "Done 12520 batches in 2165.87 sec.    training loss:\t\t4.31785212424\n",
      "Done 12530 batches in 2167.62 sec.    training loss:\t\t4.31765451045\n",
      "Done 12540 batches in 2169.47 sec.    training loss:\t\t4.31747274638\n",
      "Done 12550 batches in 2170.98 sec.    training loss:\t\t4.31724093232\n",
      "Done 12560 batches in 2172.76 sec.    training loss:\t\t4.31709798835\n",
      "Done 12570 batches in 2174.46 sec.    training loss:\t\t4.31704014216\n",
      "Done 12580 batches in 2176.00 sec.    training loss:\t\t4.31683915905\n",
      "Done 12590 batches in 2177.64 sec.    training loss:\t\t4.31661041408\n",
      "Done 12600 batches in 2179.38 sec.    training loss:\t\t4.31642066026\n",
      "Done 12610 batches in 2181.04 sec.    training loss:\t\t4.31622360104\n",
      "Done 12620 batches in 2182.72 sec.    training loss:\t\t4.31598995669\n",
      "Done 12630 batches in 2184.21 sec.    training loss:\t\t4.31575379668\n",
      "Done 12640 batches in 2185.88 sec.    training loss:\t\t4.31558531525\n",
      "Done 12650 batches in 2187.66 sec.    training loss:\t\t4.3153483558\n",
      "Done 12660 batches in 2189.69 sec.    training loss:\t\t4.31523499365\n",
      "Done 12670 batches in 2191.59 sec.    training loss:\t\t4.31504811019\n",
      "Done 12680 batches in 2193.16 sec.    training loss:\t\t4.31492568337\n",
      "Done 12690 batches in 2194.95 sec.    training loss:\t\t4.31476896961\n",
      "Done 12700 batches in 2196.83 sec.    training loss:\t\t4.31469438198\n",
      "Done 12710 batches in 2198.47 sec.    training loss:\t\t4.31458385722\n",
      "Done 12720 batches in 2200.23 sec.    training loss:\t\t4.31441341573\n",
      "Done 12730 batches in 2201.83 sec.    training loss:\t\t4.3143000327\n",
      "Done 12740 batches in 2203.61 sec.    training loss:\t\t4.31419888295\n",
      "Done 12750 batches in 2205.25 sec.    training loss:\t\t4.31398747545\n",
      "Done 12760 batches in 2206.87 sec.    training loss:\t\t4.31381698\n",
      "Done 12770 batches in 2208.84 sec.    training loss:\t\t4.31359095333\n",
      "Done 12780 batches in 2210.86 sec.    training loss:\t\t4.31339189792\n",
      "Done 12790 batches in 2212.53 sec.    training loss:\t\t4.31328956771\n",
      "Done 12800 batches in 2214.43 sec.    training loss:\t\t4.31313936504\n",
      "Done 12810 batches in 2216.09 sec.    training loss:\t\t4.31295156881\n",
      "Done 12820 batches in 2218.24 sec.    training loss:\t\t4.31271018312\n",
      "Done 12830 batches in 2220.16 sec.    training loss:\t\t4.31260532257\n",
      "Done 12840 batches in 2221.91 sec.    training loss:\t\t4.31246077181\n",
      "Done 12850 batches in 2223.57 sec.    training loss:\t\t4.31226175824\n",
      "Done 12860 batches in 2225.35 sec.    training loss:\t\t4.31203285617\n",
      "Done 12870 batches in 2227.26 sec.    training loss:\t\t4.31182694796\n",
      "Done 12880 batches in 2228.87 sec.    training loss:\t\t4.31156420863\n",
      "Done 12890 batches in 2230.89 sec.    training loss:\t\t4.31139865758\n",
      "Done 12900 batches in 2232.50 sec.    training loss:\t\t4.31123205882\n",
      "Done 12910 batches in 2234.23 sec.    training loss:\t\t4.31101145096\n",
      "Done 12920 batches in 2235.63 sec.    training loss:\t\t4.3108306521\n",
      "Done 12930 batches in 2237.29 sec.    training loss:\t\t4.31060141962\n",
      "Done 12940 batches in 2239.12 sec.    training loss:\t\t4.31039017691\n",
      "Done 12950 batches in 2240.78 sec.    training loss:\t\t4.31025069806\n",
      "Done 12960 batches in 2242.35 sec.    training loss:\t\t4.30996823603\n",
      "Done 12970 batches in 2243.87 sec.    training loss:\t\t4.30981513123\n",
      "Done 12980 batches in 2245.26 sec.    training loss:\t\t4.30968665515\n",
      "Done 12990 batches in 2246.73 sec.    training loss:\t\t4.30954170572\n",
      "Done 13000 batches in 2248.51 sec.    training loss:\t\t4.30946450266\n",
      "Done 13010 batches in 2250.16 sec.    training loss:\t\t4.30930021988\n",
      "Done 13020 batches in 2252.18 sec.    training loss:\t\t4.30914176144\n",
      "Done 13030 batches in 2253.87 sec.    training loss:\t\t4.3089772807\n",
      "Done 13040 batches in 2255.47 sec.    training loss:\t\t4.30882196565\n",
      "Done 13050 batches in 2257.31 sec.    training loss:\t\t4.30861868734\n",
      "Done 13060 batches in 2259.70 sec.    training loss:\t\t4.30856579307\n",
      "Done 13070 batches in 2261.53 sec.    training loss:\t\t4.30847860214\n",
      "Done 13080 batches in 2263.32 sec.    training loss:\t\t4.30831658474\n",
      "Done 13090 batches in 2265.26 sec.    training loss:\t\t4.30816466011\n",
      "Done 13100 batches in 2267.11 sec.    training loss:\t\t4.30792723366\n",
      "Done 13110 batches in 2268.64 sec.    training loss:\t\t4.30771233464\n",
      "Done 13120 batches in 2270.48 sec.    training loss:\t\t4.30764909018\n",
      "Done 13130 batches in 2272.25 sec.    training loss:\t\t4.3074705517\n",
      "Done 13140 batches in 2273.89 sec.    training loss:\t\t4.30733492512\n",
      "Done 13150 batches in 2275.96 sec.    training loss:\t\t4.30717475554\n",
      "Done 13160 batches in 2277.55 sec.    training loss:\t\t4.30693025087\n",
      "Done 13170 batches in 2279.01 sec.    training loss:\t\t4.30668446473\n",
      "Done 13180 batches in 2280.66 sec.    training loss:\t\t4.30652556023\n",
      "Done 13190 batches in 2282.23 sec.    training loss:\t\t4.30632750308\n",
      "Done 13200 batches in 2283.84 sec.    training loss:\t\t4.30621914948\n",
      "Done 13210 batches in 2285.56 sec.    training loss:\t\t4.30601477988\n",
      "Done 13220 batches in 2287.03 sec.    training loss:\t\t4.30579006279\n",
      "Done 13230 batches in 2288.86 sec.    training loss:\t\t4.30565045803\n",
      "Done 13240 batches in 2291.27 sec.    training loss:\t\t4.30556426119\n",
      "Done 13250 batches in 2293.54 sec.    training loss:\t\t4.30548100939\n",
      "Done 13260 batches in 2295.49 sec.    training loss:\t\t4.30532790214\n",
      "Done 13270 batches in 2297.31 sec.    training loss:\t\t4.30523965909\n",
      "Done 13280 batches in 2298.89 sec.    training loss:\t\t4.30498568453\n",
      "Done 13290 batches in 2300.72 sec.    training loss:\t\t4.30482077355\n",
      "Done 13300 batches in 2302.78 sec.    training loss:\t\t4.30471495826\n",
      "Done 13310 batches in 2304.40 sec.    training loss:\t\t4.30456243727\n",
      "Done 13320 batches in 2306.01 sec.    training loss:\t\t4.30437026092\n",
      "Done 13330 batches in 2307.73 sec.    training loss:\t\t4.30426014514\n",
      "Done 13340 batches in 2309.57 sec.    training loss:\t\t4.30410866301\n",
      "Done 13350 batches in 2311.28 sec.    training loss:\t\t4.30406438599\n",
      "Done 13360 batches in 2313.18 sec.    training loss:\t\t4.30390324841\n",
      "Done 13370 batches in 2314.97 sec.    training loss:\t\t4.30375875706\n",
      "Done 13380 batches in 2316.70 sec.    training loss:\t\t4.30359842421\n",
      "Done 13390 batches in 2318.52 sec.    training loss:\t\t4.30338712406\n",
      "Done 13400 batches in 2320.35 sec.    training loss:\t\t4.30325297333\n",
      "Done 13410 batches in 2321.96 sec.    training loss:\t\t4.30307697565\n",
      "Done 13420 batches in 2323.78 sec.    training loss:\t\t4.30294872938\n",
      "Done 13430 batches in 2325.58 sec.    training loss:\t\t4.30276982136\n",
      "Done 13440 batches in 2327.24 sec.    training loss:\t\t4.3026077433\n",
      "Done 13450 batches in 2329.05 sec.    training loss:\t\t4.30243235228\n",
      "Done 13460 batches in 2330.65 sec.    training loss:\t\t4.30224936492\n",
      "Done 13470 batches in 2332.47 sec.    training loss:\t\t4.30207184184\n",
      "Done 13480 batches in 2334.00 sec.    training loss:\t\t4.30187429554\n",
      "Done 13490 batches in 2336.19 sec.    training loss:\t\t4.30183800849\n",
      "Done 13500 batches in 2337.77 sec.    training loss:\t\t4.30167328838\n",
      "Done 13510 batches in 2339.49 sec.    training loss:\t\t4.30146940402\n",
      "Done 13520 batches in 2341.18 sec.    training loss:\t\t4.30134857457\n",
      "Done 13530 batches in 2342.95 sec.    training loss:\t\t4.30118434519\n",
      "Done 13540 batches in 2344.52 sec.    training loss:\t\t4.30098846182\n",
      "Done 13550 batches in 2346.19 sec.    training loss:\t\t4.30073297376\n",
      "Done 13560 batches in 2347.99 sec.    training loss:\t\t4.30057447166\n",
      "Done 13570 batches in 2350.11 sec.    training loss:\t\t4.30049896636\n",
      "Done 13580 batches in 2351.72 sec.    training loss:\t\t4.300366009\n",
      "Done 13590 batches in 2353.79 sec.    training loss:\t\t4.30026095659\n",
      "Done 13600 batches in 2355.91 sec.    training loss:\t\t4.30009202633\n",
      "Done 13610 batches in 2357.68 sec.    training loss:\t\t4.29991579313\n",
      "Done 13620 batches in 2359.37 sec.    training loss:\t\t4.29971551687\n",
      "Done 13630 batches in 2361.45 sec.    training loss:\t\t4.29963542945\n",
      "Done 13640 batches in 2363.22 sec.    training loss:\t\t4.29948979209\n",
      "Done 13650 batches in 2365.23 sec.    training loss:\t\t4.29939626226\n",
      "Done 13660 batches in 2366.96 sec.    training loss:\t\t4.29920570842\n",
      "Done 13670 batches in 2368.95 sec.    training loss:\t\t4.29905834057\n",
      "Done 13680 batches in 2371.25 sec.    training loss:\t\t4.29899524245\n",
      "Done 13690 batches in 2373.04 sec.    training loss:\t\t4.29886028234\n",
      "Done 13700 batches in 2374.64 sec.    training loss:\t\t4.29869440684\n",
      "Done 13710 batches in 2376.55 sec.    training loss:\t\t4.29857139001\n",
      "Done 13720 batches in 2378.40 sec.    training loss:\t\t4.29855633757\n",
      "Done 13730 batches in 2380.04 sec.    training loss:\t\t4.29843701714\n",
      "Done 13740 batches in 2382.02 sec.    training loss:\t\t4.29830596987\n",
      "Done 13750 batches in 2383.68 sec.    training loss:\t\t4.29813499846\n",
      "Done 13760 batches in 2385.42 sec.    training loss:\t\t4.29799006443\n",
      "Done 13770 batches in 2386.97 sec.    training loss:\t\t4.29773995836\n",
      "Done 13780 batches in 2388.72 sec.    training loss:\t\t4.29761889398\n",
      "Done 13790 batches in 2390.60 sec.    training loss:\t\t4.29747959316\n",
      "Done 13800 batches in 2392.03 sec.    training loss:\t\t4.29730512781\n",
      "Done 13810 batches in 2393.72 sec.    training loss:\t\t4.29711171997\n",
      "Done 13820 batches in 2395.17 sec.    training loss:\t\t4.29693615992\n",
      "Done 13830 batches in 2397.16 sec.    training loss:\t\t4.29684606671\n",
      "Done 13840 batches in 2398.98 sec.    training loss:\t\t4.29674139402\n",
      "Done 13850 batches in 2400.93 sec.    training loss:\t\t4.29664755374\n",
      "Done 13860 batches in 2402.88 sec.    training loss:\t\t4.29656899949\n",
      "Done 13870 batches in 2404.57 sec.    training loss:\t\t4.29648237294\n",
      "Done 13880 batches in 2406.20 sec.    training loss:\t\t4.29637977635\n",
      "Done 13890 batches in 2407.76 sec.    training loss:\t\t4.29621228913\n",
      "Done 13900 batches in 2409.35 sec.    training loss:\t\t4.29604595438\n",
      "Done 13910 batches in 2411.18 sec.    training loss:\t\t4.29587454278\n",
      "Done 13920 batches in 2412.87 sec.    training loss:\t\t4.29564965837\n",
      "Done 13930 batches in 2414.55 sec.    training loss:\t\t4.29545316273\n",
      "Done 13940 batches in 2416.19 sec.    training loss:\t\t4.2953062234\n",
      "Done 13950 batches in 2418.49 sec.    training loss:\t\t4.29519516278\n",
      "Done 13960 batches in 2420.86 sec.    training loss:\t\t4.29514447949\n",
      "Done 13970 batches in 2422.28 sec.    training loss:\t\t4.29494590024\n",
      "Done 13980 batches in 2423.93 sec.    training loss:\t\t4.29475730475\n",
      "Done 13990 batches in 2425.52 sec.    training loss:\t\t4.29460566613\n",
      "Done 14000 batches in 2427.26 sec.    training loss:\t\t4.29448898378\n",
      "Done 14010 batches in 2429.06 sec.    training loss:\t\t4.29431012242\n",
      "Done 14020 batches in 2430.76 sec.    training loss:\t\t4.29415783171\n",
      "Done 14030 batches in 2432.59 sec.    training loss:\t\t4.29407276796\n",
      "Done 14040 batches in 2434.21 sec.    training loss:\t\t4.29393080225\n",
      "Done 14050 batches in 2436.41 sec.    training loss:\t\t4.29382182583\n",
      "Done 14060 batches in 2438.02 sec.    training loss:\t\t4.29372207196\n",
      "Done 14070 batches in 2439.65 sec.    training loss:\t\t4.29361083831\n",
      "Done 14080 batches in 2441.27 sec.    training loss:\t\t4.29341102772\n",
      "Done 14090 batches in 2442.95 sec.    training loss:\t\t4.29323719829\n",
      "Done 14100 batches in 2444.79 sec.    training loss:\t\t4.29303452473\n",
      "Done 14110 batches in 2446.21 sec.    training loss:\t\t4.29286228093\n",
      "Done 14120 batches in 2447.98 sec.    training loss:\t\t4.29277936529\n",
      "Done 14130 batches in 2449.66 sec.    training loss:\t\t4.29260753359\n",
      "Done 14140 batches in 2451.22 sec.    training loss:\t\t4.2925275392\n",
      "Done 14150 batches in 2452.99 sec.    training loss:\t\t4.29238677057\n",
      "Done 14160 batches in 2454.83 sec.    training loss:\t\t4.29228575366\n",
      "Done 14170 batches in 2456.96 sec.    training loss:\t\t4.29215751944\n",
      "Done 14180 batches in 2458.66 sec.    training loss:\t\t4.29210061398\n",
      "Done 14190 batches in 2460.19 sec.    training loss:\t\t4.29200311295\n",
      "Done 14200 batches in 2461.99 sec.    training loss:\t\t4.29186307743\n",
      "Done 14210 batches in 2463.54 sec.    training loss:\t\t4.29168353488\n",
      "Done 14220 batches in 2465.23 sec.    training loss:\t\t4.29158403186\n",
      "Done 14230 batches in 2466.84 sec.    training loss:\t\t4.29136326596\n",
      "Done 14240 batches in 2468.39 sec.    training loss:\t\t4.29124110758\n",
      "Done 14250 batches in 2470.02 sec.    training loss:\t\t4.29105809961\n",
      "Done 14260 batches in 2471.57 sec.    training loss:\t\t4.29093460348\n",
      "Done 14270 batches in 2473.18 sec.    training loss:\t\t4.29069149942\n",
      "Done 14280 batches in 2474.78 sec.    training loss:\t\t4.29054015783\n",
      "Done 14290 batches in 2476.59 sec.    training loss:\t\t4.29038675409\n",
      "Done 14300 batches in 2478.27 sec.    training loss:\t\t4.2902537732\n",
      "Done 14310 batches in 2480.00 sec.    training loss:\t\t4.29012823855\n",
      "Done 14320 batches in 2481.59 sec.    training loss:\t\t4.28999440612\n",
      "Done 14330 batches in 2483.40 sec.    training loss:\t\t4.28985478592\n",
      "Done 14340 batches in 2484.73 sec.    training loss:\t\t4.28970346388\n",
      "Done 14350 batches in 2486.52 sec.    training loss:\t\t4.28952913874\n",
      "Done 14360 batches in 2488.17 sec.    training loss:\t\t4.289362392\n",
      "Done 14370 batches in 2489.67 sec.    training loss:\t\t4.28923720535\n",
      "Done 14380 batches in 2491.39 sec.    training loss:\t\t4.28910601278\n",
      "Done 14390 batches in 2493.15 sec.    training loss:\t\t4.28890682316\n",
      "Done 14400 batches in 2494.85 sec.    training loss:\t\t4.28880818049\n",
      "Done 14410 batches in 2496.52 sec.    training loss:\t\t4.28866978683\n",
      "Done 14420 batches in 2497.99 sec.    training loss:\t\t4.28844095965\n",
      "Done 14430 batches in 2499.79 sec.    training loss:\t\t4.28836304094\n",
      "Done 14440 batches in 2501.38 sec.    training loss:\t\t4.28817056178\n",
      "Done 14450 batches in 2503.28 sec.    training loss:\t\t4.28805793424\n",
      "Done 14460 batches in 2505.18 sec.    training loss:\t\t4.28787762751\n",
      "Done 14470 batches in 2506.95 sec.    training loss:\t\t4.28784295112\n",
      "Done 14480 batches in 2508.44 sec.    training loss:\t\t4.28769624908\n",
      "Done 14490 batches in 2510.18 sec.    training loss:\t\t4.28751070223\n",
      "Done 14500 batches in 2511.79 sec.    training loss:\t\t4.28738570776\n",
      "Done 14510 batches in 2513.36 sec.    training loss:\t\t4.28712994821\n",
      "Done 14520 batches in 2515.39 sec.    training loss:\t\t4.28692784891\n",
      "Done 14530 batches in 2516.94 sec.    training loss:\t\t4.28677045974\n",
      "Done 14540 batches in 2518.36 sec.    training loss:\t\t4.28661477361\n",
      "Done 14550 batches in 2520.16 sec.    training loss:\t\t4.28648304372\n",
      "Done 14560 batches in 2521.86 sec.    training loss:\t\t4.28636896167\n",
      "Done 14570 batches in 2524.04 sec.    training loss:\t\t4.2862848821\n",
      "Done 14580 batches in 2525.78 sec.    training loss:\t\t4.28612300143\n",
      "Done 14590 batches in 2527.72 sec.    training loss:\t\t4.2859649043\n",
      "Done 14600 batches in 2529.36 sec.    training loss:\t\t4.28579213567\n",
      "Done 14610 batches in 2530.76 sec.    training loss:\t\t4.28560497815\n",
      "Done 14620 batches in 2532.29 sec.    training loss:\t\t4.28542409421\n",
      "Done 14630 batches in 2534.13 sec.    training loss:\t\t4.28525225696\n",
      "Done 14640 batches in 2535.77 sec.    training loss:\t\t4.28510658554\n",
      "Done 14650 batches in 2537.22 sec.    training loss:\t\t4.28496448815\n",
      "Done 14660 batches in 2538.61 sec.    training loss:\t\t4.28480319575\n",
      "Done 14670 batches in 2539.96 sec.    training loss:\t\t4.28465644267\n",
      "Done 14680 batches in 2541.75 sec.    training loss:\t\t4.2844550475\n",
      "Done 14690 batches in 2543.42 sec.    training loss:\t\t4.2843302221\n",
      "Done 14700 batches in 2545.68 sec.    training loss:\t\t4.2842651831\n",
      "Done 14710 batches in 2547.84 sec.    training loss:\t\t4.284112601\n",
      "Done 14720 batches in 2549.66 sec.    training loss:\t\t4.28395903574\n",
      "Done 14730 batches in 2551.04 sec.    training loss:\t\t4.28381171887\n",
      "Done 14740 batches in 2552.74 sec.    training loss:\t\t4.28363803318\n",
      "Done 14750 batches in 2554.49 sec.    training loss:\t\t4.28348392144\n",
      "Done 14760 batches in 2556.20 sec.    training loss:\t\t4.2833521688\n",
      "Done 14770 batches in 2557.79 sec.    training loss:\t\t4.28311306563\n",
      "Done 14780 batches in 2559.19 sec.    training loss:\t\t4.28297730991\n",
      "Done 14790 batches in 2560.89 sec.    training loss:\t\t4.28286014042\n",
      "Done 14800 batches in 2562.64 sec.    training loss:\t\t4.2827992822\n",
      "Done 14810 batches in 2564.23 sec.    training loss:\t\t4.28270421344\n",
      "Done 14820 batches in 2566.06 sec.    training loss:\t\t4.28257099903\n",
      "Done 14830 batches in 2567.68 sec.    training loss:\t\t4.28233458827\n",
      "Done 14840 batches in 2569.42 sec.    training loss:\t\t4.28213120339\n",
      "Done 14850 batches in 2570.93 sec.    training loss:\t\t4.28198935634\n",
      "Done 14860 batches in 2572.45 sec.    training loss:\t\t4.28182170701\n",
      "Done 14870 batches in 2574.12 sec.    training loss:\t\t4.28177287377\n",
      "Done 14880 batches in 2575.82 sec.    training loss:\t\t4.28160788495\n",
      "Done 14890 batches in 2577.45 sec.    training loss:\t\t4.28143540393\n",
      "Done 14900 batches in 2579.47 sec.    training loss:\t\t4.28134822989\n",
      "Done 14910 batches in 2581.35 sec.    training loss:\t\t4.28125971325\n",
      "Done 14920 batches in 2582.84 sec.    training loss:\t\t4.28114567538\n",
      "Done 14930 batches in 2584.48 sec.    training loss:\t\t4.28097580086\n",
      "Done 14940 batches in 2585.89 sec.    training loss:\t\t4.2808683193\n",
      "Done 14950 batches in 2587.41 sec.    training loss:\t\t4.28073670744\n",
      "Done 14960 batches in 2589.11 sec.    training loss:\t\t4.28061340902\n",
      "Done 14970 batches in 2590.70 sec.    training loss:\t\t4.28045491218\n",
      "Done 14980 batches in 2592.30 sec.    training loss:\t\t4.28032432335\n",
      "Done 14990 batches in 2594.14 sec.    training loss:\t\t4.28016040443\n",
      "Done 15000 batches in 2596.37 sec.    training loss:\t\t4.28002433553\n",
      "Done 15010 batches in 2598.14 sec.    training loss:\t\t4.27989178754\n",
      "Done 15020 batches in 2599.87 sec.    training loss:\t\t4.27983984909\n",
      "Done 15030 batches in 2601.65 sec.    training loss:\t\t4.27973068485\n",
      "Done 15040 batches in 2603.43 sec.    training loss:\t\t4.27961475898\n",
      "Done 15050 batches in 2605.23 sec.    training loss:\t\t4.27943820239\n",
      "Done 15060 batches in 2606.93 sec.    training loss:\t\t4.27931072163\n",
      "Done 15070 batches in 2608.49 sec.    training loss:\t\t4.27912906218\n",
      "Done 15080 batches in 2610.01 sec.    training loss:\t\t4.27895787338\n",
      "Done 15090 batches in 2611.58 sec.    training loss:\t\t4.2787180716\n",
      "Done 15100 batches in 2613.06 sec.    training loss:\t\t4.27858455258\n",
      "Done 15110 batches in 2614.69 sec.    training loss:\t\t4.27846803086\n",
      "Done 15120 batches in 2616.45 sec.    training loss:\t\t4.27837165928\n",
      "Done 15130 batches in 2618.62 sec.    training loss:\t\t4.27831656592\n",
      "Done 15140 batches in 2620.17 sec.    training loss:\t\t4.27821228986\n",
      "Done 15150 batches in 2621.89 sec.    training loss:\t\t4.2780568453\n",
      "Done 15160 batches in 2624.43 sec.    training loss:\t\t4.27806189221\n",
      "Done 15170 batches in 2626.11 sec.    training loss:\t\t4.27796372701\n",
      "Done 15180 batches in 2627.66 sec.    training loss:\t\t4.27784298399\n",
      "Done 15190 batches in 2629.77 sec.    training loss:\t\t4.27767822921\n",
      "Done 15200 batches in 2631.35 sec.    training loss:\t\t4.27760124319\n",
      "Done 15210 batches in 2633.13 sec.    training loss:\t\t4.27743382026\n",
      "Done 15220 batches in 2635.16 sec.    training loss:\t\t4.27729741623\n",
      "Done 15230 batches in 2636.65 sec.    training loss:\t\t4.27719788105\n",
      "Done 15240 batches in 2638.09 sec.    training loss:\t\t4.27704322878\n",
      "Done 15250 batches in 2640.03 sec.    training loss:\t\t4.27696926073\n",
      "Done 15260 batches in 2641.51 sec.    training loss:\t\t4.27672479723\n",
      "Done 15270 batches in 2643.49 sec.    training loss:\t\t4.27667003173\n",
      "Done 15280 batches in 2645.19 sec.    training loss:\t\t4.27650347574\n",
      "Done 15290 batches in 2646.77 sec.    training loss:\t\t4.27641754233\n",
      "Done 15300 batches in 2648.36 sec.    training loss:\t\t4.27625298411\n",
      "Done 15310 batches in 2650.71 sec.    training loss:\t\t4.27614385021\n",
      "Done 15320 batches in 2652.23 sec.    training loss:\t\t4.27600272402\n",
      "Done 15330 batches in 2654.02 sec.    training loss:\t\t4.27585983088\n",
      "Done 15340 batches in 2655.84 sec.    training loss:\t\t4.27571653392\n",
      "Done 15350 batches in 2657.80 sec.    training loss:\t\t4.27558521341\n",
      "Done 15360 batches in 2659.45 sec.    training loss:\t\t4.27545471528\n",
      "Done 15370 batches in 2661.13 sec.    training loss:\t\t4.2753059269\n",
      "Done 15380 batches in 2662.67 sec.    training loss:\t\t4.27513526168\n",
      "Done 15390 batches in 2664.39 sec.    training loss:\t\t4.27502518373\n",
      "Done 15400 batches in 2666.03 sec.    training loss:\t\t4.27486087219\n",
      "Done 15410 batches in 2667.58 sec.    training loss:\t\t4.27473485816\n",
      "Done 15420 batches in 2669.21 sec.    training loss:\t\t4.27459507469\n",
      "Done 15430 batches in 2671.14 sec.    training loss:\t\t4.27450363406\n",
      "Done 15440 batches in 2672.77 sec.    training loss:\t\t4.27436308978\n",
      "Done 15450 batches in 2674.38 sec.    training loss:\t\t4.27426595901\n",
      "Done 15460 batches in 2675.93 sec.    training loss:\t\t4.27413307633\n",
      "Done 15470 batches in 2677.35 sec.    training loss:\t\t4.27399148852\n",
      "Done 15480 batches in 2678.92 sec.    training loss:\t\t4.27388691118\n",
      "Done 15490 batches in 2680.79 sec.    training loss:\t\t4.27385239673\n",
      "Done 15500 batches in 2682.85 sec.    training loss:\t\t4.27379506448\n",
      "Done 15510 batches in 2684.83 sec.    training loss:\t\t4.27367403804\n",
      "Done 15520 batches in 2686.60 sec.    training loss:\t\t4.27352138489\n",
      "Done 15530 batches in 2688.16 sec.    training loss:\t\t4.27334355329\n",
      "Done 15540 batches in 2689.54 sec.    training loss:\t\t4.27317265938\n",
      "Done 15550 batches in 2690.87 sec.    training loss:\t\t4.27299418741\n",
      "Done 15560 batches in 2692.80 sec.    training loss:\t\t4.27289587849\n",
      "Done 15570 batches in 2694.49 sec.    training loss:\t\t4.27281882708\n",
      "Done 15580 batches in 2696.86 sec.    training loss:\t\t4.2727276322\n",
      "Done 15590 batches in 2698.32 sec.    training loss:\t\t4.27256889025\n",
      "Done 15600 batches in 2700.15 sec.    training loss:\t\t4.27242867378\n",
      "Done 15610 batches in 2701.76 sec.    training loss:\t\t4.27223066135\n",
      "Done 15620 batches in 2704.17 sec.    training loss:\t\t4.27215690491\n",
      "Done 15630 batches in 2705.72 sec.    training loss:\t\t4.27206637449\n",
      "Done 15640 batches in 2707.45 sec.    training loss:\t\t4.27191964046\n",
      "Done 15650 batches in 2709.15 sec.    training loss:\t\t4.27177411798\n",
      "Done 15660 batches in 2710.95 sec.    training loss:\t\t4.27167653092\n",
      "Done 15670 batches in 2712.39 sec.    training loss:\t\t4.27146947165\n",
      "Done 15680 batches in 2714.82 sec.    training loss:\t\t4.27140980138\n",
      "Done 15690 batches in 2716.41 sec.    training loss:\t\t4.27129274829\n",
      "Done 15700 batches in 2718.20 sec.    training loss:\t\t4.27116147286\n",
      "Done 15710 batches in 2719.75 sec.    training loss:\t\t4.27098146836\n",
      "Done 15720 batches in 2721.40 sec.    training loss:\t\t4.27084550186\n",
      "Done 15730 batches in 2723.19 sec.    training loss:\t\t4.2707232659\n",
      "Done 15740 batches in 2725.05 sec.    training loss:\t\t4.27062320432\n",
      "Done 15750 batches in 2726.63 sec.    training loss:\t\t4.27043845475\n",
      "Done 15760 batches in 2728.26 sec.    training loss:\t\t4.27024078212\n",
      "Done 15770 batches in 2730.01 sec.    training loss:\t\t4.27008667357\n",
      "Done 15780 batches in 2731.76 sec.    training loss:\t\t4.26997044969\n",
      "Done 15790 batches in 2733.80 sec.    training loss:\t\t4.26990490286\n",
      "Done 15800 batches in 2735.81 sec.    training loss:\t\t4.26980282921\n",
      "Done 15810 batches in 2737.52 sec.    training loss:\t\t4.26970283047\n",
      "Done 15820 batches in 2739.29 sec.    training loss:\t\t4.26964981738\n",
      "Done 15830 batches in 2740.68 sec.    training loss:\t\t4.26953670723\n",
      "Done 15840 batches in 2742.38 sec.    training loss:\t\t4.26939411124\n",
      "Done 15850 batches in 2743.74 sec.    training loss:\t\t4.26930665239\n",
      "Done 15860 batches in 2745.91 sec.    training loss:\t\t4.26923542102\n",
      "Done 15870 batches in 2747.55 sec.    training loss:\t\t4.26913291089\n",
      "Done 15880 batches in 2749.37 sec.    training loss:\t\t4.2690298345\n",
      "Done 15890 batches in 2751.09 sec.    training loss:\t\t4.26891115105\n",
      "Done 15900 batches in 2752.62 sec.    training loss:\t\t4.26876626722\n",
      "Done 15910 batches in 2755.01 sec.    training loss:\t\t4.26867004325\n",
      "Done 15920 batches in 2757.13 sec.    training loss:\t\t4.26858420498\n",
      "Done 15930 batches in 2758.67 sec.    training loss:\t\t4.26843659528\n",
      "Done 15940 batches in 2760.65 sec.    training loss:\t\t4.26832250973\n",
      "Done 15950 batches in 2762.52 sec.    training loss:\t\t4.26817584166\n",
      "Done 15960 batches in 2763.93 sec.    training loss:\t\t4.26797796288\n",
      "Done 15970 batches in 2765.41 sec.    training loss:\t\t4.2677484671\n",
      "Done 15980 batches in 2767.48 sec.    training loss:\t\t4.26761754117\n",
      "Done 15990 batches in 2769.22 sec.    training loss:\t\t4.2674421082\n",
      "Done 16000 batches in 2770.88 sec.    training loss:\t\t4.26730472937\n",
      "Done 16010 batches in 2772.95 sec.    training loss:\t\t4.26722706197\n",
      "Done 16020 batches in 2774.53 sec.    training loss:\t\t4.2670588557\n",
      "Done 16030 batches in 2776.46 sec.    training loss:\t\t4.26689741216\n",
      "Done 16040 batches in 2778.20 sec.    training loss:\t\t4.26680766796\n",
      "Done 16050 batches in 2779.69 sec.    training loss:\t\t4.2666372711\n",
      "Done 16060 batches in 2781.45 sec.    training loss:\t\t4.26654533409\n",
      "Done 16070 batches in 2783.15 sec.    training loss:\t\t4.26647135701\n",
      "Done 16080 batches in 2785.04 sec.    training loss:\t\t4.26635477095\n",
      "Done 16090 batches in 2786.62 sec.    training loss:\t\t4.2662964005\n",
      "Done 16100 batches in 2788.40 sec.    training loss:\t\t4.26622391065\n",
      "Done 16110 batches in 2790.29 sec.    training loss:\t\t4.26613455794\n",
      "Done 16120 batches in 2791.99 sec.    training loss:\t\t4.26593136645\n",
      "Done 16130 batches in 2793.67 sec.    training loss:\t\t4.26579219856\n",
      "Done 16140 batches in 2795.41 sec.    training loss:\t\t4.26560159928\n",
      "Done 16150 batches in 2797.36 sec.    training loss:\t\t4.26545475712\n",
      "Done 16160 batches in 2798.72 sec.    training loss:\t\t4.26528090632\n",
      "Done 16170 batches in 2801.09 sec.    training loss:\t\t4.26516666349\n",
      "Done 16180 batches in 2803.04 sec.    training loss:\t\t4.26503356657\n",
      "Done 16190 batches in 2804.72 sec.    training loss:\t\t4.26493935659\n",
      "Done 16200 batches in 2806.35 sec.    training loss:\t\t4.26481032996\n",
      "Done 16210 batches in 2808.28 sec.    training loss:\t\t4.26475126571\n",
      "Done 16220 batches in 2809.81 sec.    training loss:\t\t4.2645923293\n",
      "Done 16230 batches in 2811.49 sec.    training loss:\t\t4.26447870958\n",
      "Done 16240 batches in 2813.27 sec.    training loss:\t\t4.26438422003\n",
      "Done 16250 batches in 2814.90 sec.    training loss:\t\t4.26430093624\n",
      "Done 16260 batches in 2816.56 sec.    training loss:\t\t4.26418829351\n",
      "Done 16270 batches in 2818.37 sec.    training loss:\t\t4.26407811733\n",
      "Done 16280 batches in 2819.93 sec.    training loss:\t\t4.26397302622\n",
      "Done 16290 batches in 2821.63 sec.    training loss:\t\t4.26388560634\n",
      "Done 16300 batches in 2823.35 sec.    training loss:\t\t4.26377145191\n",
      "Done 16310 batches in 2825.30 sec.    training loss:\t\t4.2637329459\n",
      "Done 16320 batches in 2827.43 sec.    training loss:\t\t4.26366351105\n",
      "Done 16330 batches in 2828.94 sec.    training loss:\t\t4.2634772511\n",
      "Done 16340 batches in 2830.75 sec.    training loss:\t\t4.26331974525\n",
      "Done 16350 batches in 2832.75 sec.    training loss:\t\t4.26324131425\n",
      "Done 16360 batches in 2834.63 sec.    training loss:\t\t4.26314729333\n",
      "Done 16370 batches in 2836.53 sec.    training loss:\t\t4.26298911748\n",
      "Done 16380 batches in 2838.25 sec.    training loss:\t\t4.26287677482\n",
      "Done 16390 batches in 2839.91 sec.    training loss:\t\t4.2627639334\n",
      "Done 16400 batches in 2841.30 sec.    training loss:\t\t4.26254230323\n",
      "Done 16410 batches in 2843.36 sec.    training loss:\t\t4.26244802767\n",
      "Done 16420 batches in 2844.83 sec.    training loss:\t\t4.26220375921\n",
      "Done 16430 batches in 2846.79 sec.    training loss:\t\t4.26212574891\n",
      "Done 16440 batches in 2848.39 sec.    training loss:\t\t4.26206534756\n",
      "Done 16450 batches in 2850.26 sec.    training loss:\t\t4.2619398581\n",
      "Done 16460 batches in 2851.88 sec.    training loss:\t\t4.26183476961\n",
      "Done 16470 batches in 2853.79 sec.    training loss:\t\t4.26171360343\n",
      "Done 16480 batches in 2855.81 sec.    training loss:\t\t4.26154493108\n",
      "Done 16490 batches in 2857.51 sec.    training loss:\t\t4.26144603574\n",
      "Done 16500 batches in 2859.15 sec.    training loss:\t\t4.2612743029\n",
      "Done 16510 batches in 2860.72 sec.    training loss:\t\t4.2611606903\n",
      "Done 16520 batches in 2862.54 sec.    training loss:\t\t4.26105252041\n",
      "Done 16530 batches in 2864.26 sec.    training loss:\t\t4.26088120497\n",
      "Done 16540 batches in 2865.93 sec.    training loss:\t\t4.26079935849\n",
      "Done 16550 batches in 2867.73 sec.    training loss:\t\t4.26064230088\n",
      "Done 16560 batches in 2869.21 sec.    training loss:\t\t4.26048492588\n",
      "Done 16570 batches in 2870.74 sec.    training loss:\t\t4.26036350631\n",
      "Done 16580 batches in 2872.66 sec.    training loss:\t\t4.26025411985\n",
      "Done 16590 batches in 2874.15 sec.    training loss:\t\t4.26017084064\n",
      "Done 16600 batches in 2875.96 sec.    training loss:\t\t4.26005571354\n",
      "Done 16610 batches in 2877.47 sec.    training loss:\t\t4.25987824435\n",
      "Done 16620 batches in 2879.34 sec.    training loss:\t\t4.25980172562\n",
      "Done 16630 batches in 2880.99 sec.    training loss:\t\t4.25971343943\n",
      "Done 16640 batches in 2882.62 sec.    training loss:\t\t4.25960118991\n",
      "Done 16650 batches in 2884.60 sec.    training loss:\t\t4.25955351467\n",
      "Done 16660 batches in 2886.07 sec.    training loss:\t\t4.25939159187\n",
      "Done 16670 batches in 2887.41 sec.    training loss:\t\t4.25918073604\n",
      "Done 16680 batches in 2889.17 sec.    training loss:\t\t4.258982036\n",
      "Done 16690 batches in 2891.19 sec.    training loss:\t\t4.2587948986\n",
      "Done 16700 batches in 2892.92 sec.    training loss:\t\t4.25872223206\n",
      "Done 16710 batches in 2894.50 sec.    training loss:\t\t4.2585897988\n",
      "Done 16720 batches in 2896.24 sec.    training loss:\t\t4.25848979963\n",
      "Done 16730 batches in 2898.25 sec.    training loss:\t\t4.25845330019\n",
      "Done 16740 batches in 2900.01 sec.    training loss:\t\t4.25838037262\n",
      "Done 16750 batches in 2901.55 sec.    training loss:\t\t4.25825157691\n",
      "Done 16760 batches in 2903.10 sec.    training loss:\t\t4.25817856774\n",
      "Done 16770 batches in 2904.87 sec.    training loss:\t\t4.25808205752\n",
      "Done 16780 batches in 2907.07 sec.    training loss:\t\t4.25801240734\n",
      "Done 16790 batches in 2908.80 sec.    training loss:\t\t4.25792084694\n",
      "Done 16800 batches in 2910.39 sec.    training loss:\t\t4.25775585921\n",
      "Done 16810 batches in 2912.03 sec.    training loss:\t\t4.25763992356\n",
      "Done 16820 batches in 2913.69 sec.    training loss:\t\t4.25754062222\n",
      "Done 16830 batches in 2915.46 sec.    training loss:\t\t4.25742560664\n",
      "Done 16840 batches in 2917.61 sec.    training loss:\t\t4.25733549949\n",
      "Done 16850 batches in 2919.48 sec.    training loss:\t\t4.25733649947\n",
      "Done 16860 batches in 2921.09 sec.    training loss:\t\t4.25715814844\n",
      "Done 16870 batches in 2922.65 sec.    training loss:\t\t4.25706530481\n",
      "Done 16880 batches in 2924.16 sec.    training loss:\t\t4.25693668768\n",
      "Done 16890 batches in 2925.71 sec.    training loss:\t\t4.25682535214\n",
      "Done 16900 batches in 2927.56 sec.    training loss:\t\t4.25675255269\n",
      "Done 16910 batches in 2929.14 sec.    training loss:\t\t4.25662924317\n",
      "Done 16920 batches in 2930.93 sec.    training loss:\t\t4.25647398986\n",
      "Done 16930 batches in 2932.70 sec.    training loss:\t\t4.25641602369\n",
      "Done 16940 batches in 2934.48 sec.    training loss:\t\t4.25629200395\n",
      "Done 16950 batches in 2936.32 sec.    training loss:\t\t4.25619992642\n",
      "Done 16960 batches in 2937.66 sec.    training loss:\t\t4.25603609141\n",
      "Done 16970 batches in 2939.16 sec.    training loss:\t\t4.25591561595\n",
      "Done 16980 batches in 2941.08 sec.    training loss:\t\t4.25579177589\n",
      "Done 16990 batches in 2942.73 sec.    training loss:\t\t4.25566079841\n",
      "Done 17000 batches in 2944.72 sec.    training loss:\t\t4.2555526065\n",
      "Done 17010 batches in 2946.68 sec.    training loss:\t\t4.25543386254\n",
      "Done 17020 batches in 2948.66 sec.    training loss:\t\t4.25537065436\n",
      "Done 17030 batches in 2950.50 sec.    training loss:\t\t4.2552841634\n",
      "Done 17040 batches in 2952.23 sec.    training loss:\t\t4.25516462148\n",
      "Done 17050 batches in 2953.98 sec.    training loss:\t\t4.25507492911\n",
      "Done 17060 batches in 2955.66 sec.    training loss:\t\t4.25494976136\n",
      "Done 17070 batches in 2957.54 sec.    training loss:\t\t4.25478298886\n",
      "Done 17080 batches in 2959.16 sec.    training loss:\t\t4.25464230781\n",
      "Done 17090 batches in 2960.72 sec.    training loss:\t\t4.25452667601\n",
      "Done 17100 batches in 2962.24 sec.    training loss:\t\t4.25441523573\n",
      "Done 17110 batches in 2963.96 sec.    training loss:\t\t4.25426658131\n",
      "Done 17120 batches in 2965.43 sec.    training loss:\t\t4.25404525935\n",
      "Done 17130 batches in 2967.09 sec.    training loss:\t\t4.25390008092\n",
      "Done 17140 batches in 2968.53 sec.    training loss:\t\t4.25372595567\n",
      "Done 17150 batches in 2970.65 sec.    training loss:\t\t4.25360131632\n",
      "Done 17160 batches in 2972.08 sec.    training loss:\t\t4.25343308346\n",
      "Done 17170 batches in 2973.71 sec.    training loss:\t\t4.25324975394\n",
      "Done 17180 batches in 2975.22 sec.    training loss:\t\t4.25313692128\n",
      "Done 17190 batches in 2976.92 sec.    training loss:\t\t4.25300684824\n",
      "Done 17200 batches in 2978.60 sec.    training loss:\t\t4.25287130593\n",
      "Done 17210 batches in 2980.10 sec.    training loss:\t\t4.25279478775\n",
      "Done 17220 batches in 2981.79 sec.    training loss:\t\t4.25268840631\n",
      "Done 17230 batches in 2983.62 sec.    training loss:\t\t4.25253091655\n",
      "Done 17240 batches in 2985.43 sec.    training loss:\t\t4.25244568129\n",
      "Done 17250 batches in 2987.17 sec.    training loss:\t\t4.25233125108\n",
      "Done 17260 batches in 2989.01 sec.    training loss:\t\t4.2522165329\n",
      "Done 17270 batches in 2990.67 sec.    training loss:\t\t4.25206459024\n",
      "Done 17280 batches in 2992.69 sec.    training loss:\t\t4.25197091568\n",
      "Done 17290 batches in 2994.56 sec.    training loss:\t\t4.25185719488\n",
      "Done 17300 batches in 2996.36 sec.    training loss:\t\t4.25176355117\n",
      "Done 17310 batches in 2998.34 sec.    training loss:\t\t4.25171077494\n",
      "Done 17320 batches in 3000.20 sec.    training loss:\t\t4.25159385091\n",
      "Done 17330 batches in 3001.78 sec.    training loss:\t\t4.25142064251\n",
      "Done 17340 batches in 3003.23 sec.    training loss:\t\t4.25137598594\n",
      "Done 17350 batches in 3005.14 sec.    training loss:\t\t4.25123612192\n",
      "Done 17360 batches in 3006.73 sec.    training loss:\t\t4.25114055075\n",
      "Done 17370 batches in 3008.18 sec.    training loss:\t\t4.25096334404\n",
      "Done 17380 batches in 3009.85 sec.    training loss:\t\t4.25085195085\n",
      "Done 17390 batches in 3011.41 sec.    training loss:\t\t4.25071803945\n",
      "Done 17400 batches in 3013.24 sec.    training loss:\t\t4.2505816193\n",
      "Done 17410 batches in 3014.82 sec.    training loss:\t\t4.25038020357\n",
      "Done 17420 batches in 3016.34 sec.    training loss:\t\t4.250278574\n",
      "Done 17430 batches in 3017.98 sec.    training loss:\t\t4.25010832881\n",
      "Done 17440 batches in 3019.80 sec.    training loss:\t\t4.24995717445\n",
      "Done 17450 batches in 3021.86 sec.    training loss:\t\t4.24983738454\n",
      "Done 17460 batches in 3023.72 sec.    training loss:\t\t4.24968753824\n",
      "Done 17470 batches in 3025.49 sec.    training loss:\t\t4.24951390175\n",
      "Done 17480 batches in 3027.04 sec.    training loss:\t\t4.24940455314\n",
      "Done 17490 batches in 3028.47 sec.    training loss:\t\t4.24931474433\n",
      "Done 17500 batches in 3030.25 sec.    training loss:\t\t4.24921045949\n",
      "Done 17510 batches in 3032.13 sec.    training loss:\t\t4.24913250311\n",
      "Done 17520 batches in 3033.93 sec.    training loss:\t\t4.2490424855\n",
      "Done 17530 batches in 3035.68 sec.    training loss:\t\t4.24892594375\n",
      "Done 17540 batches in 3037.55 sec.    training loss:\t\t4.24889763392\n",
      "Done 17550 batches in 3039.33 sec.    training loss:\t\t4.24875590591\n",
      "Done 17560 batches in 3041.38 sec.    training loss:\t\t4.24859381716\n",
      "Done 17570 batches in 3043.30 sec.    training loss:\t\t4.24846945034\n",
      "Done 17580 batches in 3045.01 sec.    training loss:\t\t4.24838038574\n",
      "Done 17590 batches in 3046.60 sec.    training loss:\t\t4.24829480244\n",
      "Done 17600 batches in 3048.23 sec.    training loss:\t\t4.2481181051\n",
      "Done 17610 batches in 3049.91 sec.    training loss:\t\t4.24802640474\n",
      "Done 17620 batches in 3052.24 sec.    training loss:\t\t4.24789548049\n",
      "Done 17630 batches in 3054.48 sec.    training loss:\t\t4.24781219405\n",
      "Done 17640 batches in 3056.05 sec.    training loss:\t\t4.24772731439\n",
      "Done 17650 batches in 3057.60 sec.    training loss:\t\t4.24758122155\n",
      "Done 17660 batches in 3059.13 sec.    training loss:\t\t4.24746832467\n",
      "Done 17670 batches in 3061.07 sec.    training loss:\t\t4.24732648173\n",
      "Done 17680 batches in 3063.02 sec.    training loss:\t\t4.24725745554\n",
      "Done 17690 batches in 3064.62 sec.    training loss:\t\t4.24714265637\n",
      "Done 17700 batches in 3066.09 sec.    training loss:\t\t4.24705095752\n",
      "Done 17710 batches in 3067.77 sec.    training loss:\t\t4.24692064049\n",
      "Done 17720 batches in 3069.63 sec.    training loss:\t\t4.24690242813\n",
      "Done 17730 batches in 3071.15 sec.    training loss:\t\t4.24673119459\n",
      "Done 17740 batches in 3072.69 sec.    training loss:\t\t4.2466541118\n",
      "Done 17750 batches in 3074.47 sec.    training loss:\t\t4.24658728219\n",
      "Done 17760 batches in 3076.11 sec.    training loss:\t\t4.24648171422\n",
      "Done 17770 batches in 3077.71 sec.    training loss:\t\t4.24637375042\n",
      "Done 17780 batches in 3079.40 sec.    training loss:\t\t4.2462894455\n",
      "Done 17790 batches in 3081.17 sec.    training loss:\t\t4.24625883853\n",
      "Done 17800 batches in 3083.06 sec.    training loss:\t\t4.2461255893\n",
      "Done 17810 batches in 3084.56 sec.    training loss:\t\t4.2459583993\n",
      "Done 17820 batches in 3086.25 sec.    training loss:\t\t4.24587067434\n",
      "Done 17830 batches in 3087.91 sec.    training loss:\t\t4.245730533\n",
      "Done 17840 batches in 3089.82 sec.    training loss:\t\t4.24565667567\n",
      "Done 17850 batches in 3091.49 sec.    training loss:\t\t4.24552025382\n",
      "Done 17860 batches in 3093.31 sec.    training loss:\t\t4.24543003872\n",
      "Done 17870 batches in 3095.07 sec.    training loss:\t\t4.24532174508\n",
      "Done 17880 batches in 3096.74 sec.    training loss:\t\t4.24524481273\n",
      "Done 17890 batches in 3098.54 sec.    training loss:\t\t4.24520258091\n",
      "Done 17900 batches in 3100.16 sec.    training loss:\t\t4.24510766678\n",
      "Done 17910 batches in 3101.69 sec.    training loss:\t\t4.24501335202\n",
      "Done 17920 batches in 3103.43 sec.    training loss:\t\t4.24497411661\n",
      "Done 17930 batches in 3104.87 sec.    training loss:\t\t4.24485671051\n",
      "Done 17940 batches in 3106.39 sec.    training loss:\t\t4.24471821891\n",
      "Done 17950 batches in 3108.33 sec.    training loss:\t\t4.24461176619\n",
      "Done 17960 batches in 3110.41 sec.    training loss:\t\t4.24456731508\n",
      "Done 17970 batches in 3112.10 sec.    training loss:\t\t4.24444758234\n",
      "Done 17980 batches in 3113.61 sec.    training loss:\t\t4.24438365358\n",
      "Done 17990 batches in 3115.46 sec.    training loss:\t\t4.24433163911\n",
      "Done 18000 batches in 3116.93 sec.    training loss:\t\t4.24418296448\n",
      "Done 18010 batches in 3118.84 sec.    training loss:\t\t4.2441366128\n",
      "Done 18020 batches in 3120.55 sec.    training loss:\t\t4.24403449604\n",
      "Done 18030 batches in 3122.66 sec.    training loss:\t\t4.24391756763\n",
      "Done 18040 batches in 3124.07 sec.    training loss:\t\t4.24382624856\n",
      "Done 18050 batches in 3125.54 sec.    training loss:\t\t4.24373706053\n",
      "Done 18060 batches in 3127.44 sec.    training loss:\t\t4.24363060416\n",
      "Done 18070 batches in 3129.53 sec.    training loss:\t\t4.24346621663\n",
      "Done 18080 batches in 3131.24 sec.    training loss:\t\t4.24333615605\n",
      "Done 18090 batches in 3132.91 sec.    training loss:\t\t4.24320896452\n",
      "Done 18100 batches in 3134.96 sec.    training loss:\t\t4.24314505275\n",
      "Done 18110 batches in 3136.63 sec.    training loss:\t\t4.24303469507\n",
      "Done 18120 batches in 3138.26 sec.    training loss:\t\t4.24293595391\n",
      "Done 18130 batches in 3140.05 sec.    training loss:\t\t4.24282251685\n",
      "Done 18140 batches in 3141.50 sec.    training loss:\t\t4.24262047382\n",
      "Done 18150 batches in 3143.41 sec.    training loss:\t\t4.24257477856\n",
      "Done 18160 batches in 3144.79 sec.    training loss:\t\t4.24238509998\n",
      "Done 18170 batches in 3146.61 sec.    training loss:\t\t4.24227284994\n",
      "Done 18180 batches in 3148.54 sec.    training loss:\t\t4.24221606266\n",
      "Done 18190 batches in 3150.15 sec.    training loss:\t\t4.24213108804\n",
      "Done 18200 batches in 3151.73 sec.    training loss:\t\t4.24206952061\n",
      "Done 18210 batches in 3153.37 sec.    training loss:\t\t4.24198224999\n",
      "Done 18220 batches in 3155.02 sec.    training loss:\t\t4.2418346491\n",
      "Done 18230 batches in 3157.06 sec.    training loss:\t\t4.24175923046\n",
      "Done 18240 batches in 3158.41 sec.    training loss:\t\t4.24154827778\n",
      "Done 18250 batches in 3160.03 sec.    training loss:\t\t4.24145897661\n",
      "Done 18260 batches in 3161.54 sec.    training loss:\t\t4.24139474886\n",
      "Done 18270 batches in 3163.35 sec.    training loss:\t\t4.2412502331\n",
      "Done 18280 batches in 3164.99 sec.    training loss:\t\t4.24111162878\n",
      "Done 18290 batches in 3166.74 sec.    training loss:\t\t4.24101236597\n",
      "Done 18300 batches in 3169.07 sec.    training loss:\t\t4.24096020317\n",
      "Done 18310 batches in 3171.35 sec.    training loss:\t\t4.24094025883\n",
      "Done 18320 batches in 3173.09 sec.    training loss:\t\t4.2408661803\n",
      "Done 18330 batches in 3174.56 sec.    training loss:\t\t4.24071048167\n",
      "Done 18340 batches in 3176.39 sec.    training loss:\t\t4.24060078259\n",
      "Done 18350 batches in 3178.05 sec.    training loss:\t\t4.24050661513\n",
      "Done 18360 batches in 3179.76 sec.    training loss:\t\t4.24041620579\n",
      "Done 18370 batches in 3181.70 sec.    training loss:\t\t4.24038338195\n",
      "Done 18380 batches in 3183.47 sec.    training loss:\t\t4.24026775594\n",
      "Done 18390 batches in 3185.21 sec.    training loss:\t\t4.2401906851\n",
      "Done 18400 batches in 3187.11 sec.    training loss:\t\t4.24017339703\n",
      "Done 18410 batches in 3189.07 sec.    training loss:\t\t4.24007135585\n",
      "Done 18420 batches in 3191.03 sec.    training loss:\t\t4.2400121368\n",
      "Done 18430 batches in 3192.40 sec.    training loss:\t\t4.23993553516\n",
      "Done 18440 batches in 3194.11 sec.    training loss:\t\t4.2398016246\n",
      "Done 18450 batches in 3195.75 sec.    training loss:\t\t4.23963770099\n",
      "Done 18460 batches in 3197.42 sec.    training loss:\t\t4.23949395589\n",
      "Done 18470 batches in 3198.75 sec.    training loss:\t\t4.23934244388\n",
      "Done 18480 batches in 3200.42 sec.    training loss:\t\t4.23918289024\n",
      "Done 18490 batches in 3202.09 sec.    training loss:\t\t4.23908242097\n",
      "Done 18500 batches in 3203.67 sec.    training loss:\t\t4.23896766636\n",
      "Done 18510 batches in 3205.67 sec.    training loss:\t\t4.23888468621\n",
      "Done 18520 batches in 3207.12 sec.    training loss:\t\t4.23877617172\n",
      "Done 18530 batches in 3208.76 sec.    training loss:\t\t4.23874066158\n",
      "Done 18540 batches in 3210.42 sec.    training loss:\t\t4.2386243456\n",
      "Done 18550 batches in 3212.03 sec.    training loss:\t\t4.23848431919\n",
      "Done 18560 batches in 3214.20 sec.    training loss:\t\t4.23845045717\n",
      "Done 18570 batches in 3216.04 sec.    training loss:\t\t4.23831828252\n",
      "Done 18580 batches in 3217.66 sec.    training loss:\t\t4.23819097984\n",
      "Done 18590 batches in 3219.49 sec.    training loss:\t\t4.23806314803\n",
      "Done 18600 batches in 3221.10 sec.    training loss:\t\t4.23793585489\n",
      "Done 18610 batches in 3222.68 sec.    training loss:\t\t4.23784100264\n",
      "Done 18620 batches in 3224.22 sec.    training loss:\t\t4.23774667037\n",
      "Done 18630 batches in 3225.96 sec.    training loss:\t\t4.23763366831\n",
      "Done 18640 batches in 3227.71 sec.    training loss:\t\t4.23753882905\n",
      "Done 18650 batches in 3229.37 sec.    training loss:\t\t4.23745813288\n",
      "Done 18660 batches in 3231.03 sec.    training loss:\t\t4.23737732852\n",
      "Done 18670 batches in 3232.61 sec.    training loss:\t\t4.23722926463\n",
      "Done 18680 batches in 3234.71 sec.    training loss:\t\t4.23714643149\n",
      "Done 18690 batches in 3236.35 sec.    training loss:\t\t4.23711947506\n",
      "Done 18700 batches in 3238.06 sec.    training loss:\t\t4.23704272727\n",
      "Done 18710 batches in 3239.64 sec.    training loss:\t\t4.23693831684\n",
      "Done 18720 batches in 3241.39 sec.    training loss:\t\t4.23684098829\n",
      "Done 18730 batches in 3243.52 sec.    training loss:\t\t4.23678407003\n",
      "Done 18740 batches in 3245.19 sec.    training loss:\t\t4.23654349581\n",
      "Done 18750 batches in 3246.75 sec.    training loss:\t\t4.23641393693\n",
      "Done 18760 batches in 3248.35 sec.    training loss:\t\t4.23628690674\n",
      "Done 18770 batches in 3249.94 sec.    training loss:\t\t4.23616720303\n",
      "Done 18780 batches in 3251.29 sec.    training loss:\t\t4.23604620853\n",
      "Done 18790 batches in 3252.82 sec.    training loss:\t\t4.23594517898\n",
      "Done 18800 batches in 3254.39 sec.    training loss:\t\t4.23581368037\n",
      "Done 18810 batches in 3256.09 sec.    training loss:\t\t4.23575231834\n",
      "Done 18820 batches in 3258.21 sec.    training loss:\t\t4.23562545248\n",
      "Done 18830 batches in 3259.96 sec.    training loss:\t\t4.23551834299\n",
      "Done 18840 batches in 3261.45 sec.    training loss:\t\t4.23537228001\n",
      "Done 18850 batches in 3262.95 sec.    training loss:\t\t4.23527089516\n",
      "Done 18860 batches in 3264.78 sec.    training loss:\t\t4.2351879486\n",
      "Done 18870 batches in 3266.45 sec.    training loss:\t\t4.23508629497\n",
      "Done 18880 batches in 3268.02 sec.    training loss:\t\t4.23494061932\n",
      "Done 18890 batches in 3269.46 sec.    training loss:\t\t4.23477800377\n",
      "Done 18900 batches in 3271.03 sec.    training loss:\t\t4.23469799246\n",
      "Done 18910 batches in 3273.08 sec.    training loss:\t\t4.23458063876\n",
      "Done 18920 batches in 3275.17 sec.    training loss:\t\t4.23452659607\n",
      "Done 18930 batches in 3277.10 sec.    training loss:\t\t4.23443738398\n",
      "Done 18940 batches in 3278.85 sec.    training loss:\t\t4.23436743319\n",
      "Done 18950 batches in 3280.52 sec.    training loss:\t\t4.23428783238\n",
      "Done 18960 batches in 3282.41 sec.    training loss:\t\t4.23419199319\n",
      "Done 18970 batches in 3284.04 sec.    training loss:\t\t4.23409234524\n",
      "Done 18980 batches in 3285.70 sec.    training loss:\t\t4.23400681918\n",
      "Done 18990 batches in 3287.36 sec.    training loss:\t\t4.2338391957\n",
      "Done 19000 batches in 3289.19 sec.    training loss:\t\t4.23374837252\n",
      "Done 19010 batches in 3290.77 sec.    training loss:\t\t4.23356123091\n",
      "Done 19020 batches in 3292.48 sec.    training loss:\t\t4.23344230776\n",
      "Done 19030 batches in 3294.26 sec.    training loss:\t\t4.23333854657\n",
      "Done 19040 batches in 3295.99 sec.    training loss:\t\t4.23325717432\n",
      "Done 19050 batches in 3297.62 sec.    training loss:\t\t4.23319863691\n",
      "Done 19060 batches in 3299.13 sec.    training loss:\t\t4.23309910691\n",
      "Done 19070 batches in 3301.23 sec.    training loss:\t\t4.23297738348\n",
      "Done 19080 batches in 3302.92 sec.    training loss:\t\t4.2328971975\n",
      "Done 19090 batches in 3304.70 sec.    training loss:\t\t4.23279539759\n",
      "Done 19100 batches in 3306.33 sec.    training loss:\t\t4.23269315782\n",
      "Done 19110 batches in 3307.97 sec.    training loss:\t\t4.23262449519\n",
      "Done 19120 batches in 3309.70 sec.    training loss:\t\t4.23255694097\n",
      "Done 19130 batches in 3311.19 sec.    training loss:\t\t4.23242979721\n",
      "Done 19140 batches in 3312.70 sec.    training loss:\t\t4.23228696789\n",
      "Done 19150 batches in 3314.52 sec.    training loss:\t\t4.23222799456\n",
      "Done 19160 batches in 3316.24 sec.    training loss:\t\t4.23216996088\n",
      "Done 19170 batches in 3317.83 sec.    training loss:\t\t4.2321196284\n",
      "Done 19180 batches in 3319.57 sec.    training loss:\t\t4.23201655322\n",
      "Done 19190 batches in 3321.57 sec.    training loss:\t\t4.23188285968\n",
      "Done 19200 batches in 3323.50 sec.    training loss:\t\t4.23182135028\n",
      "Done 19210 batches in 3325.30 sec.    training loss:\t\t4.23167454092\n",
      "Done 19220 batches in 3326.88 sec.    training loss:\t\t4.23159721105\n",
      "Done 19230 batches in 3328.58 sec.    training loss:\t\t4.23146695401\n",
      "Done 19240 batches in 3329.95 sec.    training loss:\t\t4.23137359443\n",
      "Done 19250 batches in 3331.52 sec.    training loss:\t\t4.23127419037\n",
      "Done 19260 batches in 3333.10 sec.    training loss:\t\t4.23116679921\n",
      "Done 19270 batches in 3334.83 sec.    training loss:\t\t4.23100152365\n",
      "Done 19280 batches in 3336.65 sec.    training loss:\t\t4.23092128893\n",
      "Done 19290 batches in 3338.36 sec.    training loss:\t\t4.23082225592\n",
      "Done 19300 batches in 3340.08 sec.    training loss:\t\t4.23072796008\n",
      "Done 19310 batches in 3341.98 sec.    training loss:\t\t4.23060238124\n",
      "Done 19320 batches in 3343.59 sec.    training loss:\t\t4.23049410724\n",
      "Done 19330 batches in 3345.34 sec.    training loss:\t\t4.230373492\n",
      "Done 19340 batches in 3347.58 sec.    training loss:\t\t4.23027714098\n",
      "Done 19350 batches in 3349.53 sec.    training loss:\t\t4.23014782823\n",
      "Done 19360 batches in 3351.02 sec.    training loss:\t\t4.23004832786\n",
      "Done 19370 batches in 3352.79 sec.    training loss:\t\t4.22994144359\n",
      "Done 19380 batches in 3354.48 sec.    training loss:\t\t4.22981575736\n",
      "Done 19390 batches in 3356.46 sec.    training loss:\t\t4.22971934982\n",
      "Done 19400 batches in 3358.20 sec.    training loss:\t\t4.22963598576\n",
      "Done 19410 batches in 3359.82 sec.    training loss:\t\t4.22951098875\n",
      "Done 19420 batches in 3361.55 sec.    training loss:\t\t4.2293419867\n",
      "Done 19430 batches in 3363.45 sec.    training loss:\t\t4.22924906393\n",
      "Done 19440 batches in 3365.05 sec.    training loss:\t\t4.22915969289\n",
      "Done 19450 batches in 3366.68 sec.    training loss:\t\t4.22904672963\n",
      "Done 19460 batches in 3368.32 sec.    training loss:\t\t4.22896807344\n",
      "Done 19470 batches in 3369.74 sec.    training loss:\t\t4.22880292708\n",
      "Done 19480 batches in 3371.63 sec.    training loss:\t\t4.22875710701\n",
      "Done 19490 batches in 3373.08 sec.    training loss:\t\t4.22864503049\n",
      "Done 19500 batches in 3375.05 sec.    training loss:\t\t4.22853011994\n",
      "Done 19510 batches in 3376.92 sec.    training loss:\t\t4.22841278375\n",
      "Done 19520 batches in 3378.90 sec.    training loss:\t\t4.22840589472\n",
      "Done 19530 batches in 3380.21 sec.    training loss:\t\t4.228283063\n",
      "Done 19540 batches in 3381.87 sec.    training loss:\t\t4.22815250167\n",
      "Done 19550 batches in 3383.78 sec.    training loss:\t\t4.22808495778\n",
      "Done 19560 batches in 3385.89 sec.    training loss:\t\t4.22798858606\n",
      "Done 19570 batches in 3387.44 sec.    training loss:\t\t4.22787677541\n",
      "Done 19580 batches in 3389.57 sec.    training loss:\t\t4.22777625322\n",
      "Done 19590 batches in 3391.23 sec.    training loss:\t\t4.22767432377\n",
      "Done 19600 batches in 3392.91 sec.    training loss:\t\t4.22756454338\n",
      "Done 19610 batches in 3395.07 sec.    training loss:\t\t4.22745083422\n",
      "Done 19620 batches in 3396.59 sec.    training loss:\t\t4.22736393197\n",
      "Done 19630 batches in 3398.47 sec.    training loss:\t\t4.22722957801\n",
      "Done 100 batches in 2.94 sec.\n",
      "Done 200 batches in 5.82 sec.\n",
      "Done 300 batches in 8.75 sec.\n",
      "Done 400 batches in 11.55 sec.\n",
      "Done 500 batches in 14.38 sec.\n",
      "Done 600 batches in 17.18 sec.\n",
      "Done 700 batches in 19.97 sec.\n",
      "Done 800 batches in 23.03 sec.\n",
      "Done 900 batches in 25.76 sec.\n",
      "Done 1000 batches in 28.46 sec.\n",
      "Done 1100 batches in 31.13 sec.\n",
      "Done 1200 batches in 33.83 sec.\n",
      "Done 1300 batches in 36.76 sec.\n",
      "Done 1400 batches in 39.68 sec.\n",
      "Done 1500 batches in 42.59 sec.\n",
      "Done 1600 batches in 45.39 sec.\n",
      "Done 1700 batches in 48.12 sec.\n",
      "Done 1800 batches in 50.69 sec.\n",
      "Done 1900 batches in 53.28 sec.\n",
      "Done 2000 batches in 55.82 sec.\n",
      "Done 2100 batches in 58.47 sec.\n",
      "Done 2200 batches in 61.16 sec.\n",
      "Done 2300 batches in 63.93 sec.\n",
      "Done 2400 batches in 66.98 sec.\n",
      "Epoch 1 of 2 took 3467.485s\n",
      "  training loss:\t\t4.227230\n",
      "  validation loss:\t\t4.060919\n",
      "Done 10 batches in 1.66 sec.    training loss:\t\t4.00778689384\n",
      "Done 20 batches in 3.46 sec.    training loss:\t\t3.99504277706\n",
      "Done 30 batches in 5.14 sec.    training loss:\t\t4.00013631185\n",
      "Done 40 batches in 6.84 sec.    training loss:\t\t4.01992598772\n",
      "Done 50 batches in 8.78 sec.    training loss:\t\t4.03615493298\n",
      "Done 60 batches in 10.51 sec.    training loss:\t\t4.0350628535\n",
      "Done 70 batches in 11.84 sec.    training loss:\t\t4.02661725453\n",
      "Done 80 batches in 13.53 sec.    training loss:\t\t4.02716093063\n",
      "Done 90 batches in 15.30 sec.    training loss:\t\t4.02295333015\n",
      "Done 100 batches in 16.94 sec.    training loss:\t\t4.02730006218\n",
      "Done 110 batches in 18.65 sec.    training loss:\t\t4.02364451018\n",
      "Done 120 batches in 20.25 sec.    training loss:\t\t4.01872894367\n",
      "Done 130 batches in 21.65 sec.    training loss:\t\t4.02031266139\n",
      "Done 140 batches in 23.23 sec.    training loss:\t\t4.01589751073\n",
      "Done 150 batches in 25.29 sec.    training loss:\t\t4.01764600436\n",
      "Done 160 batches in 26.91 sec.    training loss:\t\t4.02050665319\n",
      "Done 170 batches in 28.76 sec.    training loss:\t\t4.02145485177\n",
      "Done 180 batches in 30.55 sec.    training loss:\t\t4.02031757434\n",
      "Done 190 batches in 32.50 sec.    training loss:\t\t4.02335099296\n",
      "Done 200 batches in 34.30 sec.    training loss:\t\t4.02482539773\n",
      "Done 210 batches in 36.46 sec.    training loss:\t\t4.02476490906\n",
      "Done 220 batches in 38.09 sec.    training loss:\t\t4.02195801193\n",
      "Done 230 batches in 39.95 sec.    training loss:\t\t4.02846299254\n",
      "Done 240 batches in 41.69 sec.    training loss:\t\t4.02717434267\n",
      "Done 250 batches in 43.28 sec.    training loss:\t\t4.02841180325\n",
      "Done 260 batches in 45.02 sec.    training loss:\t\t4.02654208403\n",
      "Done 270 batches in 46.74 sec.    training loss:\t\t4.0265824053\n",
      "Done 280 batches in 48.58 sec.    training loss:\t\t4.02596590519\n",
      "Done 290 batches in 50.13 sec.    training loss:\t\t4.02293871025\n",
      "Done 300 batches in 52.15 sec.    training loss:\t\t4.02187548319\n",
      "Done 310 batches in 53.88 sec.    training loss:\t\t4.02447952301\n",
      "Done 320 batches in 55.75 sec.    training loss:\t\t4.02349670827\n",
      "Done 330 batches in 57.64 sec.    training loss:\t\t4.01907806974\n",
      "Done 340 batches in 59.23 sec.    training loss:\t\t4.02037076599\n",
      "Done 350 batches in 61.20 sec.    training loss:\t\t4.02315187522\n",
      "Done 360 batches in 63.16 sec.    training loss:\t\t4.0255608658\n",
      "Done 370 batches in 65.11 sec.    training loss:\t\t4.0248565706\n",
      "Done 380 batches in 66.94 sec.    training loss:\t\t4.0235192004\n",
      "Done 390 batches in 68.57 sec.    training loss:\t\t4.0230027859\n",
      "Done 400 batches in 70.33 sec.    training loss:\t\t4.02215280771\n",
      "Done 410 batches in 71.90 sec.    training loss:\t\t4.0239738854\n",
      "Done 420 batches in 73.69 sec.    training loss:\t\t4.02420019933\n",
      "Done 430 batches in 75.43 sec.    training loss:\t\t4.02302041996\n",
      "Done 440 batches in 77.09 sec.    training loss:\t\t4.02547876076\n",
      "Done 450 batches in 78.94 sec.    training loss:\t\t4.02492058913\n",
      "Done 460 batches in 80.68 sec.    training loss:\t\t4.02385308691\n",
      "Done 470 batches in 82.20 sec.    training loss:\t\t4.02452432805\n",
      "Done 480 batches in 83.91 sec.    training loss:\t\t4.0262053142\n",
      "Done 490 batches in 85.64 sec.    training loss:\t\t4.02696116165\n",
      "Done 500 batches in 87.11 sec.    training loss:\t\t4.02435918283\n",
      "Done 510 batches in 88.68 sec.    training loss:\t\t4.02298147632\n",
      "Done 520 batches in 90.32 sec.    training loss:\t\t4.02443582232\n",
      "Done 530 batches in 91.80 sec.    training loss:\t\t4.02450302052\n",
      "Done 540 batches in 93.46 sec.    training loss:\t\t4.02318586332\n",
      "Done 550 batches in 95.09 sec.    training loss:\t\t4.02282055248\n",
      "Done 560 batches in 96.82 sec.    training loss:\t\t4.02324581487\n",
      "Done 570 batches in 98.68 sec.    training loss:\t\t4.02488679007\n",
      "Done 580 batches in 100.29 sec.    training loss:\t\t4.02541993815\n",
      "Done 590 batches in 101.83 sec.    training loss:\t\t4.02503733918\n",
      "Done 600 batches in 103.67 sec.    training loss:\t\t4.02192398508\n",
      "Done 610 batches in 105.75 sec.    training loss:\t\t4.02361951734\n",
      "Done 620 batches in 107.21 sec.    training loss:\t\t4.02451891861\n",
      "Done 630 batches in 109.06 sec.    training loss:\t\t4.02391745968\n",
      "Done 640 batches in 110.86 sec.    training loss:\t\t4.02386026643\n",
      "Done 650 batches in 112.60 sec.    training loss:\t\t4.02344844305\n",
      "Done 660 batches in 114.43 sec.    training loss:\t\t4.02454651088\n",
      "Done 670 batches in 116.18 sec.    training loss:\t\t4.02353051634\n",
      "Done 680 batches in 117.91 sec.    training loss:\t\t4.02473204416\n",
      "Done 690 batches in 119.47 sec.    training loss:\t\t4.02521543503\n",
      "Done 700 batches in 121.13 sec.    training loss:\t\t4.02701183966\n",
      "Done 710 batches in 122.95 sec.    training loss:\t\t4.02827436823\n",
      "Done 720 batches in 124.41 sec.    training loss:\t\t4.02800708546\n",
      "Done 730 batches in 126.22 sec.    training loss:\t\t4.02679254845\n",
      "Done 740 batches in 127.78 sec.    training loss:\t\t4.02400114697\n",
      "Done 750 batches in 129.39 sec.    training loss:\t\t4.02426587105\n",
      "Done 760 batches in 130.82 sec.    training loss:\t\t4.0247016706\n",
      "Done 770 batches in 132.29 sec.    training loss:\t\t4.02388022906\n",
      "Done 780 batches in 134.03 sec.    training loss:\t\t4.02365683715\n",
      "Done 790 batches in 135.82 sec.    training loss:\t\t4.02433497876\n",
      "Done 800 batches in 137.64 sec.    training loss:\t\t4.02548820019\n",
      "Done 810 batches in 139.37 sec.    training loss:\t\t4.02513704624\n",
      "Done 820 batches in 141.30 sec.    training loss:\t\t4.02507192158\n",
      "Done 830 batches in 143.26 sec.    training loss:\t\t4.0245560017\n",
      "Done 840 batches in 144.78 sec.    training loss:\t\t4.02500912973\n",
      "Done 850 batches in 146.28 sec.    training loss:\t\t4.02343613344\n",
      "Done 860 batches in 148.12 sec.    training loss:\t\t4.02239167274\n",
      "Done 870 batches in 149.71 sec.    training loss:\t\t4.02295655492\n",
      "Done 880 batches in 151.52 sec.    training loss:\t\t4.02198976766\n",
      "Done 890 batches in 153.44 sec.    training loss:\t\t4.02303840444\n",
      "Done 900 batches in 155.36 sec.    training loss:\t\t4.02232531044\n",
      "Done 910 batches in 156.91 sec.    training loss:\t\t4.02168529427\n",
      "Done 920 batches in 158.39 sec.    training loss:\t\t4.02224544701\n",
      "Done 930 batches in 160.19 sec.    training loss:\t\t4.02356950006\n",
      "Done 940 batches in 162.12 sec.    training loss:\t\t4.02311456812\n",
      "Done 950 batches in 163.69 sec.    training loss:\t\t4.02332982088\n",
      "Done 960 batches in 165.16 sec.    training loss:\t\t4.02288918942\n",
      "Done 970 batches in 166.70 sec.    training loss:\t\t4.02210390076\n",
      "Done 980 batches in 168.26 sec.    training loss:\t\t4.02021680267\n",
      "Done 990 batches in 169.69 sec.    training loss:\t\t4.02072912443\n",
      "Done 1000 batches in 171.29 sec.    training loss:\t\t4.02080711341\n",
      "Done 1010 batches in 173.62 sec.    training loss:\t\t4.02053809874\n",
      "Done 1020 batches in 175.29 sec.    training loss:\t\t4.0210258\n",
      "Done 1030 batches in 177.04 sec.    training loss:\t\t4.02076923477\n",
      "Done 1040 batches in 178.76 sec.    training loss:\t\t4.02160531856\n",
      "Done 1050 batches in 180.33 sec.    training loss:\t\t4.02227915923\n",
      "Done 1060 batches in 182.12 sec.    training loss:\t\t4.02290708479\n",
      "Done 1070 batches in 183.51 sec.    training loss:\t\t4.02196432853\n",
      "Done 1080 batches in 185.20 sec.    training loss:\t\t4.02203538926\n",
      "Done 1090 batches in 187.29 sec.    training loss:\t\t4.02327021328\n",
      "Done 1100 batches in 188.90 sec.    training loss:\t\t4.02343531999\n",
      "Done 1110 batches in 190.58 sec.    training loss:\t\t4.02319234449\n",
      "Done 1120 batches in 191.99 sec.    training loss:\t\t4.02250813863\n",
      "Done 1130 batches in 193.67 sec.    training loss:\t\t4.02192702779\n",
      "Done 1140 batches in 195.35 sec.    training loss:\t\t4.02229869596\n",
      "Done 1150 batches in 197.19 sec.    training loss:\t\t4.02179617322\n",
      "Done 1160 batches in 198.67 sec.    training loss:\t\t4.02151615332\n",
      "Done 1170 batches in 200.85 sec.    training loss:\t\t4.02231102907\n",
      "Done 1180 batches in 202.74 sec.    training loss:\t\t4.02304286532\n",
      "Done 1190 batches in 204.31 sec.    training loss:\t\t4.02335113758\n",
      "Done 1200 batches in 206.16 sec.    training loss:\t\t4.02292716106\n",
      "Done 1210 batches in 207.90 sec.    training loss:\t\t4.02247367063\n",
      "Done 1220 batches in 209.35 sec.    training loss:\t\t4.02235572612\n",
      "Done 1230 batches in 210.96 sec.    training loss:\t\t4.02245492586\n",
      "Done 1240 batches in 212.58 sec.    training loss:\t\t4.02283412583\n",
      "Done 1250 batches in 214.06 sec.    training loss:\t\t4.02214595089\n",
      "Done 1260 batches in 215.93 sec.    training loss:\t\t4.02217603487\n",
      "Done 1270 batches in 217.70 sec.    training loss:\t\t4.02257101892\n",
      "Done 1280 batches in 219.55 sec.    training loss:\t\t4.0220661208\n",
      "Done 1290 batches in 220.88 sec.    training loss:\t\t4.02254509981\n",
      "Done 1300 batches in 222.73 sec.    training loss:\t\t4.02341900349\n",
      "Done 1310 batches in 224.72 sec.    training loss:\t\t4.02257138755\n",
      "Done 1320 batches in 226.29 sec.    training loss:\t\t4.02214595748\n",
      "Done 1330 batches in 227.92 sec.    training loss:\t\t4.02168174095\n",
      "Done 1340 batches in 229.48 sec.    training loss:\t\t4.02190947639\n",
      "Done 1350 batches in 231.75 sec.    training loss:\t\t4.02255524671\n",
      "Done 1360 batches in 233.39 sec.    training loss:\t\t4.02188882828\n",
      "Done 1370 batches in 235.48 sec.    training loss:\t\t4.02149329168\n",
      "Done 1380 batches in 236.97 sec.    training loss:\t\t4.02172499999\n",
      "Done 1390 batches in 238.76 sec.    training loss:\t\t4.02261393139\n",
      "Done 1400 batches in 240.34 sec.    training loss:\t\t4.02303636312\n",
      "Done 1410 batches in 241.94 sec.    training loss:\t\t4.02291634252\n",
      "Done 1420 batches in 243.41 sec.    training loss:\t\t4.02299418584\n",
      "Done 1430 batches in 245.13 sec.    training loss:\t\t4.0221226072\n",
      "Done 1440 batches in 247.07 sec.    training loss:\t\t4.02131294807\n",
      "Done 1450 batches in 248.98 sec.    training loss:\t\t4.02206820636\n",
      "Done 1460 batches in 250.81 sec.    training loss:\t\t4.02118415457\n",
      "Done 1470 batches in 252.89 sec.    training loss:\t\t4.02157019579\n",
      "Done 1480 batches in 254.53 sec.    training loss:\t\t4.02141850446\n",
      "Done 1490 batches in 256.55 sec.    training loss:\t\t4.02136288473\n",
      "Done 1500 batches in 258.25 sec.    training loss:\t\t4.02170390956\n",
      "Done 1510 batches in 260.92 sec.    training loss:\t\t4.02191081284\n",
      "Done 1520 batches in 262.55 sec.    training loss:\t\t4.02165179378\n",
      "Done 1530 batches in 264.46 sec.    training loss:\t\t4.02229021094\n",
      "Done 1540 batches in 266.22 sec.    training loss:\t\t4.02197415163\n",
      "Done 1550 batches in 267.81 sec.    training loss:\t\t4.02128660171\n",
      "Done 1560 batches in 269.51 sec.    training loss:\t\t4.02166626912\n",
      "Done 1570 batches in 271.41 sec.    training loss:\t\t4.02197192915\n",
      "Done 1580 batches in 272.99 sec.    training loss:\t\t4.02193667798\n",
      "Done 1590 batches in 274.55 sec.    training loss:\t\t4.02184262171\n",
      "Done 1600 batches in 276.52 sec.    training loss:\t\t4.02204856411\n",
      "Done 1610 batches in 278.16 sec.    training loss:\t\t4.02124419183\n",
      "Done 1620 batches in 280.07 sec.    training loss:\t\t4.02173768356\n",
      "Done 1630 batches in 281.82 sec.    training loss:\t\t4.02168557776\n",
      "Done 1640 batches in 283.48 sec.    training loss:\t\t4.02152702358\n",
      "Done 1650 batches in 285.37 sec.    training loss:\t\t4.02243824785\n",
      "Done 1660 batches in 287.14 sec.    training loss:\t\t4.02306538303\n",
      "Done 1670 batches in 288.76 sec.    training loss:\t\t4.02201265903\n",
      "Done 1680 batches in 290.33 sec.    training loss:\t\t4.02230591632\n",
      "Done 1690 batches in 292.20 sec.    training loss:\t\t4.02261088346\n",
      "Done 1700 batches in 293.99 sec.    training loss:\t\t4.02261754485\n",
      "Done 1710 batches in 295.68 sec.    training loss:\t\t4.02242497506\n",
      "Done 1720 batches in 297.35 sec.    training loss:\t\t4.02231402952\n",
      "Done 1730 batches in 298.80 sec.    training loss:\t\t4.02216665717\n",
      "Done 1740 batches in 300.26 sec.    training loss:\t\t4.02203153824\n",
      "Done 1750 batches in 301.95 sec.    training loss:\t\t4.02150123664\n",
      "Done 1760 batches in 303.69 sec.    training loss:\t\t4.02187105864\n",
      "Done 1770 batches in 305.64 sec.    training loss:\t\t4.02177285138\n",
      "Done 1780 batches in 307.21 sec.    training loss:\t\t4.02202678142\n",
      "Done 1790 batches in 308.59 sec.    training loss:\t\t4.02130001524\n",
      "Done 1800 batches in 310.21 sec.    training loss:\t\t4.02109710217\n",
      "Done 1810 batches in 311.57 sec.    training loss:\t\t4.02055722969\n",
      "Done 1820 batches in 313.18 sec.    training loss:\t\t4.01956060902\n",
      "Done 1830 batches in 314.81 sec.    training loss:\t\t4.01958749685\n",
      "Done 1840 batches in 316.63 sec.    training loss:\t\t4.01986352115\n",
      "Done 1850 batches in 318.74 sec.    training loss:\t\t4.02085260945\n",
      "Done 1860 batches in 320.13 sec.    training loss:\t\t4.02036480763\n",
      "Done 1870 batches in 321.75 sec.    training loss:\t\t4.02050715105\n",
      "Done 1880 batches in 323.11 sec.    training loss:\t\t4.01990174537\n",
      "Done 1890 batches in 324.82 sec.    training loss:\t\t4.01989374804\n",
      "Done 1900 batches in 326.48 sec.    training loss:\t\t4.02014087589\n",
      "Done 1910 batches in 328.10 sec.    training loss:\t\t4.01995010164\n",
      "Done 1920 batches in 329.87 sec.    training loss:\t\t4.02033055772\n",
      "Done 1930 batches in 331.86 sec.    training loss:\t\t4.02005586179\n",
      "Done 1940 batches in 333.36 sec.    training loss:\t\t4.02059458777\n",
      "Done 1950 batches in 334.91 sec.    training loss:\t\t4.02015551812\n",
      "Done 1960 batches in 336.77 sec.    training loss:\t\t4.02074535331\n",
      "Done 1970 batches in 338.35 sec.    training loss:\t\t4.02078831341\n",
      "Done 1980 batches in 339.98 sec.    training loss:\t\t4.02093698075\n",
      "Done 1990 batches in 341.70 sec.    training loss:\t\t4.02074403284\n",
      "Done 2000 batches in 343.59 sec.    training loss:\t\t4.02131525671\n",
      "Done 2010 batches in 345.32 sec.    training loss:\t\t4.02130781537\n",
      "Done 2020 batches in 346.95 sec.    training loss:\t\t4.02065892703\n",
      "Done 2030 batches in 348.54 sec.    training loss:\t\t4.02031493774\n",
      "Done 2040 batches in 350.16 sec.    training loss:\t\t4.01976567951\n",
      "Done 2050 batches in 352.22 sec.    training loss:\t\t4.01968725181\n",
      "Done 2060 batches in 354.08 sec.    training loss:\t\t4.02008666957\n",
      "Done 2070 batches in 355.74 sec.    training loss:\t\t4.0201243138\n",
      "Done 2080 batches in 357.72 sec.    training loss:\t\t4.02053622145\n",
      "Done 2090 batches in 359.49 sec.    training loss:\t\t4.02085929476\n",
      "Done 2100 batches in 360.99 sec.    training loss:\t\t4.0205474245\n",
      "Done 2110 batches in 362.94 sec.    training loss:\t\t4.02090660961\n",
      "Done 2120 batches in 364.60 sec.    training loss:\t\t4.0203069246\n",
      "Done 2130 batches in 366.21 sec.    training loss:\t\t4.02048300701\n",
      "Done 2140 batches in 368.34 sec.    training loss:\t\t4.02027510583\n",
      "Done 2150 batches in 369.87 sec.    training loss:\t\t4.02044294535\n",
      "Done 2160 batches in 371.65 sec.    training loss:\t\t4.02068905588\n",
      "Done 2170 batches in 373.36 sec.    training loss:\t\t4.0204186957\n",
      "Done 2180 batches in 375.02 sec.    training loss:\t\t4.02061613525\n",
      "Done 2190 batches in 376.72 sec.    training loss:\t\t4.02008160652\n",
      "Done 2200 batches in 378.24 sec.    training loss:\t\t4.01961036769\n",
      "Done 2210 batches in 379.83 sec.    training loss:\t\t4.01971490782\n",
      "Done 2220 batches in 381.52 sec.    training loss:\t\t4.01924463586\n",
      "Done 2230 batches in 383.21 sec.    training loss:\t\t4.01910721443\n",
      "Done 2240 batches in 385.08 sec.    training loss:\t\t4.0191564214\n",
      "Done 2250 batches in 386.52 sec.    training loss:\t\t4.01910326799\n",
      "Done 2260 batches in 388.31 sec.    training loss:\t\t4.01899467675\n",
      "Done 2270 batches in 389.78 sec.    training loss:\t\t4.01918367909\n",
      "Done 2280 batches in 391.39 sec.    training loss:\t\t4.01948849538\n",
      "Done 2290 batches in 392.94 sec.    training loss:\t\t4.0200439729\n",
      "Done 2300 batches in 394.55 sec.    training loss:\t\t4.02006688398\n",
      "Done 2310 batches in 396.02 sec.    training loss:\t\t4.01993842982\n",
      "Done 2320 batches in 397.61 sec.    training loss:\t\t4.01986229995\n",
      "Done 2330 batches in 399.14 sec.    training loss:\t\t4.01984874363\n",
      "Done 2340 batches in 400.77 sec.    training loss:\t\t4.0196364684\n",
      "Done 2350 batches in 402.69 sec.    training loss:\t\t4.01962257385\n",
      "Done 2360 batches in 404.46 sec.    training loss:\t\t4.01933534822\n",
      "Done 2370 batches in 405.97 sec.    training loss:\t\t4.01940960693\n",
      "Done 2380 batches in 407.47 sec.    training loss:\t\t4.01912452624\n",
      "Done 2390 batches in 408.98 sec.    training loss:\t\t4.01873237728\n",
      "Done 2400 batches in 410.87 sec.    training loss:\t\t4.01899814248\n",
      "Done 2410 batches in 412.66 sec.    training loss:\t\t4.0189134303\n",
      "Done 2420 batches in 414.27 sec.    training loss:\t\t4.0189958268\n",
      "Done 2430 batches in 416.41 sec.    training loss:\t\t4.01913488981\n",
      "Done 2440 batches in 417.99 sec.    training loss:\t\t4.0190221509\n",
      "Done 2450 batches in 419.84 sec.    training loss:\t\t4.01937323317\n",
      "Done 2460 batches in 421.49 sec.    training loss:\t\t4.01943031423\n",
      "Done 2470 batches in 423.11 sec.    training loss:\t\t4.01910224357\n",
      "Done 2480 batches in 424.95 sec.    training loss:\t\t4.01919010685\n",
      "Done 2490 batches in 426.53 sec.    training loss:\t\t4.01927138763\n",
      "Done 2500 batches in 428.27 sec.    training loss:\t\t4.01975939436\n",
      "Done 2510 batches in 429.99 sec.    training loss:\t\t4.01989873633\n",
      "Done 2520 batches in 431.58 sec.    training loss:\t\t4.01997179361\n",
      "Done 2530 batches in 433.21 sec.    training loss:\t\t4.01982112534\n",
      "Done 2540 batches in 434.96 sec.    training loss:\t\t4.01961606722\n",
      "Done 2550 batches in 436.93 sec.    training loss:\t\t4.01979188545\n",
      "Done 2560 batches in 438.44 sec.    training loss:\t\t4.01989975935\n",
      "Done 2570 batches in 440.15 sec.    training loss:\t\t4.01998235957\n",
      "Done 2580 batches in 441.85 sec.    training loss:\t\t4.02014216369\n",
      "Done 2590 batches in 443.50 sec.    training loss:\t\t4.01993772671\n",
      "Done 2600 batches in 445.28 sec.    training loss:\t\t4.01983468019\n",
      "Done 2610 batches in 446.93 sec.    training loss:\t\t4.01992631571\n",
      "Done 2620 batches in 448.56 sec.    training loss:\t\t4.01951842272\n",
      "Done 2630 batches in 450.24 sec.    training loss:\t\t4.01980499777\n",
      "Done 2640 batches in 451.92 sec.    training loss:\t\t4.01966950017\n",
      "Done 2650 batches in 453.42 sec.    training loss:\t\t4.01968156446\n",
      "Done 2660 batches in 455.44 sec.    training loss:\t\t4.01994380906\n",
      "Done 2670 batches in 456.88 sec.    training loss:\t\t4.01951952329\n",
      "Done 2680 batches in 458.47 sec.    training loss:\t\t4.01926931837\n",
      "Done 2690 batches in 460.42 sec.    training loss:\t\t4.01978850879\n",
      "Done 2700 batches in 462.29 sec.    training loss:\t\t4.01985770605\n",
      "Done 2710 batches in 463.85 sec.    training loss:\t\t4.01963189352\n",
      "Done 2720 batches in 465.46 sec.    training loss:\t\t4.01928767615\n",
      "Done 2730 batches in 467.09 sec.    training loss:\t\t4.01917263868\n",
      "Done 2740 batches in 468.72 sec.    training loss:\t\t4.01963362528\n",
      "Done 2750 batches in 470.49 sec.    training loss:\t\t4.01952021772\n",
      "Done 2760 batches in 472.27 sec.    training loss:\t\t4.01963640866\n",
      "Done 2770 batches in 473.73 sec.    training loss:\t\t4.01994204805\n",
      "Done 2780 batches in 475.44 sec.    training loss:\t\t4.02007004812\n",
      "Done 2790 batches in 477.12 sec.    training loss:\t\t4.01998779047\n",
      "Done 2800 batches in 479.23 sec.    training loss:\t\t4.02044296299\n",
      "Done 2810 batches in 481.15 sec.    training loss:\t\t4.0203974079\n",
      "Done 2820 batches in 482.71 sec.    training loss:\t\t4.02004855169\n",
      "Done 2830 batches in 484.54 sec.    training loss:\t\t4.01989096003\n",
      "Done 2840 batches in 486.12 sec.    training loss:\t\t4.01982252455\n",
      "Done 2850 batches in 487.64 sec.    training loss:\t\t4.01974860526\n",
      "Done 2860 batches in 489.24 sec.    training loss:\t\t4.01959870679\n",
      "Done 2870 batches in 491.19 sec.    training loss:\t\t4.01944990175\n",
      "Done 2880 batches in 494.15 sec.    training loss:\t\t4.01971295923\n",
      "Done 2890 batches in 495.91 sec.    training loss:\t\t4.01969752592\n",
      "Done 2900 batches in 497.57 sec.    training loss:\t\t4.01958040015\n",
      "Done 2910 batches in 499.24 sec.    training loss:\t\t4.01955906099\n",
      "Done 2920 batches in 500.56 sec.    training loss:\t\t4.01932898327\n",
      "Done 2930 batches in 502.53 sec.    training loss:\t\t4.01896680565\n",
      "Done 2940 batches in 504.42 sec.    training loss:\t\t4.0189506092\n",
      "Done 2950 batches in 506.02 sec.    training loss:\t\t4.01878151053\n",
      "Done 2960 batches in 508.01 sec.    training loss:\t\t4.01880642933\n",
      "Done 2970 batches in 509.25 sec.    training loss:\t\t4.01873677676\n",
      "Done 2980 batches in 510.85 sec.    training loss:\t\t4.01855723546\n",
      "Done 2990 batches in 512.70 sec.    training loss:\t\t4.01862596445\n",
      "Done 3000 batches in 514.50 sec.    training loss:\t\t4.01894429747\n",
      "Done 3010 batches in 516.21 sec.    training loss:\t\t4.01883319818\n",
      "Done 3020 batches in 518.00 sec.    training loss:\t\t4.01869659274\n",
      "Done 3030 batches in 519.80 sec.    training loss:\t\t4.01891710428\n",
      "Done 3040 batches in 521.49 sec.    training loss:\t\t4.01829564085\n",
      "Done 3050 batches in 523.14 sec.    training loss:\t\t4.01817138648\n",
      "Done 3060 batches in 524.68 sec.    training loss:\t\t4.01816117654\n",
      "Done 3070 batches in 526.46 sec.    training loss:\t\t4.01798540777\n",
      "Done 3080 batches in 528.32 sec.    training loss:\t\t4.01774249913\n",
      "Done 3090 batches in 530.21 sec.    training loss:\t\t4.01772436054\n",
      "Done 3100 batches in 531.86 sec.    training loss:\t\t4.01755658173\n",
      "Done 3110 batches in 533.32 sec.    training loss:\t\t4.01752403143\n",
      "Done 3120 batches in 535.03 sec.    training loss:\t\t4.0176358926\n",
      "Done 3130 batches in 536.70 sec.    training loss:\t\t4.01788282707\n",
      "Done 3140 batches in 538.25 sec.    training loss:\t\t4.01795385727\n",
      "Done 3150 batches in 540.29 sec.    training loss:\t\t4.01777380027\n",
      "Done 3160 batches in 541.99 sec.    training loss:\t\t4.01787804773\n",
      "Done 3170 batches in 543.53 sec.    training loss:\t\t4.01745431002\n",
      "Done 3180 batches in 545.08 sec.    training loss:\t\t4.01737752561\n",
      "Done 3190 batches in 546.85 sec.    training loss:\t\t4.01730804279\n",
      "Done 3200 batches in 548.68 sec.    training loss:\t\t4.01745033368\n",
      "Done 3210 batches in 550.50 sec.    training loss:\t\t4.01749581793\n",
      "Done 3220 batches in 552.20 sec.    training loss:\t\t4.01763474904\n",
      "Done 3230 batches in 553.83 sec.    training loss:\t\t4.01745579479\n",
      "Done 3240 batches in 556.04 sec.    training loss:\t\t4.01750468943\n",
      "Done 3250 batches in 557.91 sec.    training loss:\t\t4.01728541059\n",
      "Done 3260 batches in 559.62 sec.    training loss:\t\t4.01732255198\n",
      "Done 3270 batches in 561.42 sec.    training loss:\t\t4.01716421241\n",
      "Done 3280 batches in 563.05 sec.    training loss:\t\t4.0173537484\n",
      "Done 3290 batches in 564.85 sec.    training loss:\t\t4.01700148075\n",
      "Done 3300 batches in 566.45 sec.    training loss:\t\t4.01695082209\n",
      "Done 3310 batches in 568.07 sec.    training loss:\t\t4.01700632457\n",
      "Done 3320 batches in 569.94 sec.    training loss:\t\t4.0173736099\n",
      "Done 3330 batches in 571.63 sec.    training loss:\t\t4.01725392521\n",
      "Done 3340 batches in 573.12 sec.    training loss:\t\t4.0170839486\n",
      "Done 3350 batches in 575.01 sec.    training loss:\t\t4.01717440121\n",
      "Done 3360 batches in 576.90 sec.    training loss:\t\t4.01720046167\n",
      "Done 3370 batches in 578.43 sec.    training loss:\t\t4.01688370931\n",
      "Done 3380 batches in 580.31 sec.    training loss:\t\t4.01671888306\n",
      "Done 3390 batches in 582.01 sec.    training loss:\t\t4.01656762829\n",
      "Done 3400 batches in 583.72 sec.    training loss:\t\t4.01599767285\n",
      "Done 3410 batches in 585.47 sec.    training loss:\t\t4.01614733414\n",
      "Done 3420 batches in 587.32 sec.    training loss:\t\t4.01609116646\n",
      "Done 3430 batches in 588.89 sec.    training loss:\t\t4.01592532525\n",
      "Done 3440 batches in 590.74 sec.    training loss:\t\t4.01599894097\n",
      "Done 3450 batches in 592.39 sec.    training loss:\t\t4.01602260458\n",
      "Done 3460 batches in 594.36 sec.    training loss:\t\t4.01605411589\n",
      "Done 3470 batches in 595.90 sec.    training loss:\t\t4.01619968222\n",
      "Done 3480 batches in 597.67 sec.    training loss:\t\t4.01647448471\n",
      "Done 3490 batches in 599.64 sec.    training loss:\t\t4.01684344389\n",
      "Done 3500 batches in 601.40 sec.    training loss:\t\t4.01685856165\n",
      "Done 3510 batches in 603.25 sec.    training loss:\t\t4.0170901647\n",
      "Done 3520 batches in 604.74 sec.    training loss:\t\t4.01692643023\n",
      "Done 3530 batches in 606.66 sec.    training loss:\t\t4.01711725365\n",
      "Done 3540 batches in 608.46 sec.    training loss:\t\t4.01691920582\n",
      "Done 3550 batches in 610.16 sec.    training loss:\t\t4.01721017811\n",
      "Done 3560 batches in 611.89 sec.    training loss:\t\t4.0171812456\n",
      "Done 3570 batches in 613.52 sec.    training loss:\t\t4.0173412886\n",
      "Done 3580 batches in 615.24 sec.    training loss:\t\t4.0171916614\n",
      "Done 3590 batches in 617.31 sec.    training loss:\t\t4.01708588534\n",
      "Done 3600 batches in 619.16 sec.    training loss:\t\t4.01682781365\n",
      "Done 3610 batches in 620.81 sec.    training loss:\t\t4.01710875365\n",
      "Done 3620 batches in 622.43 sec.    training loss:\t\t4.01677389375\n",
      "Done 3630 batches in 624.30 sec.    training loss:\t\t4.01681228433\n",
      "Done 3640 batches in 626.28 sec.    training loss:\t\t4.01649230168\n",
      "Done 3650 batches in 627.96 sec.    training loss:\t\t4.01654450201\n",
      "Done 3660 batches in 630.04 sec.    training loss:\t\t4.01673645556\n",
      "Done 3670 batches in 631.46 sec.    training loss:\t\t4.01652240272\n",
      "Done 3680 batches in 633.02 sec.    training loss:\t\t4.01635381262\n",
      "Done 3690 batches in 634.66 sec.    training loss:\t\t4.01617661227\n",
      "Done 3700 batches in 636.40 sec.    training loss:\t\t4.01591860552\n",
      "Done 3710 batches in 637.84 sec.    training loss:\t\t4.01542509855\n",
      "Done 3720 batches in 639.56 sec.    training loss:\t\t4.01520483776\n",
      "Done 3730 batches in 641.40 sec.    training loss:\t\t4.01522953197\n",
      "Done 3740 batches in 642.95 sec.    training loss:\t\t4.0149720985\n",
      "Done 3750 batches in 644.59 sec.    training loss:\t\t4.01492659359\n",
      "Done 3760 batches in 645.94 sec.    training loss:\t\t4.01468516322\n",
      "Done 3770 batches in 647.98 sec.    training loss:\t\t4.01524027045\n",
      "Done 3780 batches in 649.31 sec.    training loss:\t\t4.01485513159\n",
      "Done 3790 batches in 650.93 sec.    training loss:\t\t4.01492925426\n",
      "Done 3800 batches in 652.51 sec.    training loss:\t\t4.01471751113\n",
      "Done 3810 batches in 653.97 sec.    training loss:\t\t4.01462011875\n",
      "Done 3820 batches in 655.46 sec.    training loss:\t\t4.01433617239\n",
      "Done 3830 batches in 657.08 sec.    training loss:\t\t4.01457030829\n",
      "Done 3840 batches in 658.77 sec.    training loss:\t\t4.01455446339\n",
      "Done 3850 batches in 660.26 sec.    training loss:\t\t4.01433642418\n",
      "Done 3860 batches in 661.87 sec.    training loss:\t\t4.01415739659\n",
      "Done 3870 batches in 663.53 sec.    training loss:\t\t4.01407116661\n",
      "Done 3880 batches in 665.25 sec.    training loss:\t\t4.01395839748\n",
      "Done 3890 batches in 667.00 sec.    training loss:\t\t4.01380474506\n",
      "Done 3900 batches in 668.79 sec.    training loss:\t\t4.01384866611\n",
      "Done 3910 batches in 670.44 sec.    training loss:\t\t4.01373826146\n",
      "Done 3920 batches in 672.00 sec.    training loss:\t\t4.0134808249\n",
      "Done 3930 batches in 673.35 sec.    training loss:\t\t4.0132636106\n",
      "Done 3940 batches in 675.27 sec.    training loss:\t\t4.01320928577\n",
      "Done 3950 batches in 676.93 sec.    training loss:\t\t4.01320971706\n",
      "Done 3960 batches in 678.68 sec.    training loss:\t\t4.01318882595\n",
      "Done 3970 batches in 680.39 sec.    training loss:\t\t4.01325794125\n",
      "Done 3980 batches in 681.92 sec.    training loss:\t\t4.01319424447\n",
      "Done 3990 batches in 683.68 sec.    training loss:\t\t4.01335940259\n",
      "Done 4000 batches in 685.60 sec.    training loss:\t\t4.01369703108\n",
      "Done 4010 batches in 687.54 sec.    training loss:\t\t4.01377833674\n",
      "Done 4020 batches in 689.41 sec.    training loss:\t\t4.01408675802\n",
      "Done 4030 batches in 691.29 sec.    training loss:\t\t4.01404652945\n",
      "Done 4040 batches in 692.74 sec.    training loss:\t\t4.01388258456\n",
      "Done 4050 batches in 694.40 sec.    training loss:\t\t4.014117434\n",
      "Done 4060 batches in 696.10 sec.    training loss:\t\t4.01380479471\n",
      "Done 4070 batches in 698.10 sec.    training loss:\t\t4.01368854714\n",
      "Done 4080 batches in 699.90 sec.    training loss:\t\t4.0137442798\n",
      "Done 4090 batches in 701.68 sec.    training loss:\t\t4.01357991206\n",
      "Done 4100 batches in 703.31 sec.    training loss:\t\t4.01376996267\n",
      "Done 4110 batches in 704.76 sec.    training loss:\t\t4.01353437477\n",
      "Done 4120 batches in 706.97 sec.    training loss:\t\t4.01354269733\n",
      "Done 4130 batches in 708.71 sec.    training loss:\t\t4.01354060242\n",
      "Done 4140 batches in 710.54 sec.    training loss:\t\t4.01354311336\n",
      "Done 4150 batches in 712.18 sec.    training loss:\t\t4.01328973799\n",
      "Done 4160 batches in 713.87 sec.    training loss:\t\t4.01348702214\n",
      "Done 4170 batches in 715.58 sec.    training loss:\t\t4.01327900709\n",
      "Done 4180 batches in 717.24 sec.    training loss:\t\t4.01306625873\n",
      "Done 4190 batches in 718.62 sec.    training loss:\t\t4.01290125932\n",
      "Done 4200 batches in 720.69 sec.    training loss:\t\t4.01275830297\n",
      "Done 4210 batches in 722.14 sec.    training loss:\t\t4.0126498138\n",
      "Done 4220 batches in 723.94 sec.    training loss:\t\t4.01268289304\n",
      "Done 4230 batches in 725.94 sec.    training loss:\t\t4.01276112054\n",
      "Done 4240 batches in 727.74 sec.    training loss:\t\t4.01288990941\n",
      "Done 4250 batches in 729.52 sec.    training loss:\t\t4.01294072291\n",
      "Done 4260 batches in 731.01 sec.    training loss:\t\t4.01279167732\n",
      "Done 4270 batches in 732.63 sec.    training loss:\t\t4.01280590287\n",
      "Done 4280 batches in 734.48 sec.    training loss:\t\t4.01293104336\n",
      "Done 4290 batches in 736.09 sec.    training loss:\t\t4.01284010138\n",
      "Done 4300 batches in 737.97 sec.    training loss:\t\t4.01272115402\n",
      "Done 4310 batches in 739.37 sec.    training loss:\t\t4.01240025856\n",
      "Done 4320 batches in 741.35 sec.    training loss:\t\t4.01230834491\n",
      "Done 4330 batches in 743.16 sec.    training loss:\t\t4.0124542624\n",
      "Done 4340 batches in 744.94 sec.    training loss:\t\t4.01235620481\n",
      "Done 4350 batches in 746.43 sec.    training loss:\t\t4.01195033917\n",
      "Done 4360 batches in 748.27 sec.    training loss:\t\t4.011795389\n",
      "Done 4370 batches in 749.91 sec.    training loss:\t\t4.01197980394\n",
      "Done 4380 batches in 751.57 sec.    training loss:\t\t4.01180612796\n",
      "Done 4390 batches in 753.49 sec.    training loss:\t\t4.01181529969\n",
      "Done 4400 batches in 755.30 sec.    training loss:\t\t4.01175607996\n",
      "Done 4410 batches in 756.87 sec.    training loss:\t\t4.01188406447\n",
      "Done 4420 batches in 758.22 sec.    training loss:\t\t4.01174988752\n",
      "Done 4430 batches in 760.07 sec.    training loss:\t\t4.0116598576\n",
      "Done 4440 batches in 761.95 sec.    training loss:\t\t4.01162278974\n",
      "Done 4450 batches in 763.76 sec.    training loss:\t\t4.01162708277\n",
      "Done 4460 batches in 765.78 sec.    training loss:\t\t4.0116010428\n",
      "Done 4470 batches in 767.56 sec.    training loss:\t\t4.01172306362\n",
      "Done 4480 batches in 769.19 sec.    training loss:\t\t4.0115627796\n",
      "Done 4490 batches in 770.69 sec.    training loss:\t\t4.01162973592\n",
      "Done 4500 batches in 772.84 sec.    training loss:\t\t4.01180679136\n",
      "Done 4510 batches in 774.60 sec.    training loss:\t\t4.01195482721\n",
      "Done 4520 batches in 776.25 sec.    training loss:\t\t4.01177484836\n",
      "Done 4530 batches in 778.16 sec.    training loss:\t\t4.0118604689\n",
      "Done 4540 batches in 779.91 sec.    training loss:\t\t4.01206300154\n",
      "Done 4550 batches in 781.66 sec.    training loss:\t\t4.01194384203\n",
      "Done 4560 batches in 783.31 sec.    training loss:\t\t4.01198655241\n",
      "Done 4570 batches in 784.89 sec.    training loss:\t\t4.0118395027\n",
      "Done 4580 batches in 786.39 sec.    training loss:\t\t4.01173724696\n",
      "Done 4590 batches in 788.44 sec.    training loss:\t\t4.01173678296\n",
      "Done 4600 batches in 789.90 sec.    training loss:\t\t4.01156410767\n",
      "Done 4610 batches in 791.88 sec.    training loss:\t\t4.01148390982\n",
      "Done 4620 batches in 793.55 sec.    training loss:\t\t4.01132098849\n",
      "Done 4630 batches in 795.40 sec.    training loss:\t\t4.01124779586\n",
      "Done 4640 batches in 797.12 sec.    training loss:\t\t4.01128835221\n",
      "Done 4650 batches in 799.08 sec.    training loss:\t\t4.01131399483\n",
      "Done 4660 batches in 800.60 sec.    training loss:\t\t4.0113047696\n",
      "Done 4670 batches in 802.43 sec.    training loss:\t\t4.01141092384\n",
      "Done 4680 batches in 804.15 sec.    training loss:\t\t4.01123223911\n",
      "Done 4690 batches in 805.71 sec.    training loss:\t\t4.01143109697\n",
      "Done 4700 batches in 807.59 sec.    training loss:\t\t4.01141748951\n",
      "Done 4710 batches in 809.12 sec.    training loss:\t\t4.01118164786\n",
      "Done 4720 batches in 811.25 sec.    training loss:\t\t4.01126522078\n",
      "Done 4730 batches in 813.03 sec.    training loss:\t\t4.01120238959\n",
      "Done 4740 batches in 814.85 sec.    training loss:\t\t4.01138922665\n",
      "Done 4750 batches in 816.78 sec.    training loss:\t\t4.01154902749\n",
      "Done 4760 batches in 818.66 sec.    training loss:\t\t4.01158165631\n",
      "Done 4770 batches in 820.08 sec.    training loss:\t\t4.01142626304\n",
      "Done 4780 batches in 821.73 sec.    training loss:\t\t4.01150725023\n",
      "Done 4790 batches in 823.34 sec.    training loss:\t\t4.01127878475\n",
      "Done 4800 batches in 825.05 sec.    training loss:\t\t4.01101204703\n",
      "Done 4810 batches in 826.83 sec.    training loss:\t\t4.01079388205\n",
      "Done 4820 batches in 828.42 sec.    training loss:\t\t4.01091566669\n",
      "Done 4830 batches in 830.15 sec.    training loss:\t\t4.01087557475\n",
      "Done 4840 batches in 831.93 sec.    training loss:\t\t4.01074072482\n",
      "Done 4850 batches in 833.85 sec.    training loss:\t\t4.0106651551\n",
      "Done 4860 batches in 835.49 sec.    training loss:\t\t4.01075156235\n",
      "Done 4870 batches in 836.96 sec.    training loss:\t\t4.01074018366\n",
      "Done 4880 batches in 838.49 sec.    training loss:\t\t4.01079205558\n",
      "Done 4890 batches in 839.96 sec.    training loss:\t\t4.01061288638\n",
      "Done 4900 batches in 841.57 sec.    training loss:\t\t4.01070925849\n",
      "Done 4910 batches in 843.06 sec.    training loss:\t\t4.01077087091\n",
      "Done 4920 batches in 844.79 sec.    training loss:\t\t4.0106751096\n",
      "Done 4930 batches in 846.35 sec.    training loss:\t\t4.01071338252\n",
      "Done 4940 batches in 848.17 sec.    training loss:\t\t4.01063260738\n",
      "Done 4950 batches in 850.22 sec.    training loss:\t\t4.01056967827\n",
      "Done 4960 batches in 852.18 sec.    training loss:\t\t4.0105103038\n",
      "Done 4970 batches in 854.05 sec.    training loss:\t\t4.01044556049\n",
      "Done 4980 batches in 855.62 sec.    training loss:\t\t4.01020818235\n",
      "Done 4990 batches in 857.27 sec.    training loss:\t\t4.01016981224\n",
      "Done 5000 batches in 858.71 sec.    training loss:\t\t4.009969625\n",
      "Done 5010 batches in 860.24 sec.    training loss:\t\t4.00979945469\n",
      "Done 5020 batches in 862.17 sec.    training loss:\t\t4.00982260913\n",
      "Done 5030 batches in 863.85 sec.    training loss:\t\t4.00991616254\n",
      "Done 5040 batches in 865.97 sec.    training loss:\t\t4.00986006487\n",
      "Done 5050 batches in 867.50 sec.    training loss:\t\t4.00947363391\n",
      "Done 5060 batches in 869.44 sec.    training loss:\t\t4.00949967713\n",
      "Done 5070 batches in 871.30 sec.    training loss:\t\t4.00948404951\n",
      "Done 5080 batches in 872.79 sec.    training loss:\t\t4.00945915847\n",
      "Done 5090 batches in 874.82 sec.    training loss:\t\t4.00934364768\n",
      "Done 5100 batches in 877.09 sec.    training loss:\t\t4.0093542214\n",
      "Done 5110 batches in 878.85 sec.    training loss:\t\t4.00922836875\n",
      "Done 5120 batches in 880.47 sec.    training loss:\t\t4.00906406008\n",
      "Done 5130 batches in 882.08 sec.    training loss:\t\t4.00903850136\n",
      "Done 5140 batches in 883.47 sec.    training loss:\t\t4.00885918428\n",
      "Done 5150 batches in 885.49 sec.    training loss:\t\t4.00901452912\n",
      "Done 5160 batches in 887.39 sec.    training loss:\t\t4.00900014791\n",
      "Done 5170 batches in 889.04 sec.    training loss:\t\t4.00907253905\n",
      "Done 5180 batches in 890.60 sec.    training loss:\t\t4.00925127033\n",
      "Done 5190 batches in 892.83 sec.    training loss:\t\t4.00957116093\n",
      "Done 5200 batches in 894.70 sec.    training loss:\t\t4.00956372293\n",
      "Done 5210 batches in 896.33 sec.    training loss:\t\t4.00959342089\n",
      "Done 5220 batches in 897.93 sec.    training loss:\t\t4.00934334884\n",
      "Done 5230 batches in 899.34 sec.    training loss:\t\t4.00934207895\n",
      "Done 5240 batches in 901.19 sec.    training loss:\t\t4.00930855024\n",
      "Done 5250 batches in 902.86 sec.    training loss:\t\t4.00914867905\n",
      "Done 5260 batches in 904.57 sec.    training loss:\t\t4.0093808001\n",
      "Done 5270 batches in 906.64 sec.    training loss:\t\t4.00930555098\n",
      "Done 5280 batches in 908.33 sec.    training loss:\t\t4.00925030306\n",
      "Done 5290 batches in 909.92 sec.    training loss:\t\t4.00922403133\n",
      "Done 5300 batches in 911.29 sec.    training loss:\t\t4.00907210355\n",
      "Done 5310 batches in 913.29 sec.    training loss:\t\t4.00896064786\n",
      "Done 5320 batches in 915.04 sec.    training loss:\t\t4.00876714749\n",
      "Done 5330 batches in 916.98 sec.    training loss:\t\t4.00874333015\n",
      "Done 5340 batches in 918.91 sec.    training loss:\t\t4.00877489338\n",
      "Done 5350 batches in 920.70 sec.    training loss:\t\t4.00865970834\n",
      "Done 5360 batches in 922.45 sec.    training loss:\t\t4.00863410114\n",
      "Done 5370 batches in 924.09 sec.    training loss:\t\t4.00853220018\n",
      "Done 5380 batches in 925.79 sec.    training loss:\t\t4.00827895983\n",
      "Done 5390 batches in 927.30 sec.    training loss:\t\t4.00839915156\n",
      "Done 5400 batches in 929.17 sec.    training loss:\t\t4.00830192932\n",
      "Done 5410 batches in 931.12 sec.    training loss:\t\t4.00825546325\n",
      "Done 5420 batches in 933.23 sec.    training loss:\t\t4.00804158502\n",
      "Done 5430 batches in 935.10 sec.    training loss:\t\t4.00816498334\n",
      "Done 5440 batches in 936.61 sec.    training loss:\t\t4.00830858854\n",
      "Done 5450 batches in 938.46 sec.    training loss:\t\t4.00833829862\n",
      "Done 5460 batches in 940.05 sec.    training loss:\t\t4.00837458263\n",
      "Done 5470 batches in 941.83 sec.    training loss:\t\t4.00825338734\n",
      "Done 5480 batches in 943.39 sec.    training loss:\t\t4.00804926157\n",
      "Done 5490 batches in 945.38 sec.    training loss:\t\t4.00814081498\n",
      "Done 5500 batches in 946.79 sec.    training loss:\t\t4.00801396916\n",
      "Done 5510 batches in 948.61 sec.    training loss:\t\t4.0079761914\n",
      "Done 5520 batches in 950.44 sec.    training loss:\t\t4.00799166422\n",
      "Done 5530 batches in 952.24 sec.    training loss:\t\t4.0079952644\n",
      "Done 5540 batches in 953.83 sec.    training loss:\t\t4.00804800953\n",
      "Done 5550 batches in 955.51 sec.    training loss:\t\t4.00794693646\n",
      "Done 5560 batches in 957.01 sec.    training loss:\t\t4.00777480118\n",
      "Done 5570 batches in 958.69 sec.    training loss:\t\t4.00782671364\n",
      "Done 5580 batches in 960.72 sec.    training loss:\t\t4.00784862563\n",
      "Done 5590 batches in 962.66 sec.    training loss:\t\t4.0078736472\n",
      "Done 5600 batches in 964.12 sec.    training loss:\t\t4.00771515646\n",
      "Done 5610 batches in 965.74 sec.    training loss:\t\t4.00771914196\n",
      "Done 5620 batches in 967.67 sec.    training loss:\t\t4.00773248282\n",
      "Done 5630 batches in 969.42 sec.    training loss:\t\t4.00761526016\n",
      "Done 5640 batches in 971.19 sec.    training loss:\t\t4.00759778674\n",
      "Done 5650 batches in 972.76 sec.    training loss:\t\t4.00772969368\n",
      "Done 5660 batches in 974.23 sec.    training loss:\t\t4.00764477969\n",
      "Done 5670 batches in 975.92 sec.    training loss:\t\t4.00756702192\n",
      "Done 5680 batches in 977.38 sec.    training loss:\t\t4.00733029083\n",
      "Done 5690 batches in 979.26 sec.    training loss:\t\t4.00740375129\n",
      "Done 5700 batches in 980.98 sec.    training loss:\t\t4.00739950506\n",
      "Done 5710 batches in 982.87 sec.    training loss:\t\t4.00753847835\n",
      "Done 5720 batches in 984.50 sec.    training loss:\t\t4.00750204316\n",
      "Done 5730 batches in 985.98 sec.    training loss:\t\t4.00737923477\n",
      "Done 5740 batches in 987.62 sec.    training loss:\t\t4.00751952498\n",
      "Done 5750 batches in 989.43 sec.    training loss:\t\t4.00770246348\n",
      "Done 5760 batches in 991.19 sec.    training loss:\t\t4.00770157009\n",
      "Done 5770 batches in 992.88 sec.    training loss:\t\t4.00781442103\n",
      "Done 5780 batches in 994.31 sec.    training loss:\t\t4.00787442118\n",
      "Done 5790 batches in 996.06 sec.    training loss:\t\t4.0076824432\n",
      "Done 5800 batches in 998.34 sec.    training loss:\t\t4.0075784051\n",
      "Done 5810 batches in 1000.32 sec.    training loss:\t\t4.00760032425\n",
      "Done 5820 batches in 1002.14 sec.    training loss:\t\t4.00780824345\n",
      "Done 5830 batches in 1003.87 sec.    training loss:\t\t4.00757660796\n",
      "Done 5840 batches in 1005.96 sec.    training loss:\t\t4.00741072048\n",
      "Done 5850 batches in 1007.45 sec.    training loss:\t\t4.0072054484\n",
      "Done 5860 batches in 1009.03 sec.    training loss:\t\t4.00718435912\n",
      "Done 5870 batches in 1010.50 sec.    training loss:\t\t4.0071261648\n",
      "Done 5880 batches in 1012.20 sec.    training loss:\t\t4.00702766456\n",
      "Done 5890 batches in 1013.97 sec.    training loss:\t\t4.00701337265\n",
      "Done 5900 batches in 1016.10 sec.    training loss:\t\t4.00695007724\n",
      "Done 5910 batches in 1017.63 sec.    training loss:\t\t4.00687009433\n",
      "Done 5920 batches in 1019.25 sec.    training loss:\t\t4.00663572316\n",
      "Done 5930 batches in 1021.12 sec.    training loss:\t\t4.00663561893\n",
      "Done 5940 batches in 1023.42 sec.    training loss:\t\t4.00676190042\n",
      "Done 5950 batches in 1025.07 sec.    training loss:\t\t4.00675179914\n",
      "Done 5960 batches in 1026.65 sec.    training loss:\t\t4.00662729652\n",
      "Done 5970 batches in 1028.22 sec.    training loss:\t\t4.00650460628\n",
      "Done 5980 batches in 1029.65 sec.    training loss:\t\t4.00645407749\n",
      "Done 5990 batches in 1031.44 sec.    training loss:\t\t4.00652368964\n",
      "Done 6000 batches in 1033.03 sec.    training loss:\t\t4.00654933755\n",
      "Done 6010 batches in 1034.59 sec.    training loss:\t\t4.006300614\n",
      "Done 6020 batches in 1036.18 sec.    training loss:\t\t4.00639976509\n",
      "Done 6030 batches in 1038.07 sec.    training loss:\t\t4.00645357353\n",
      "Done 6040 batches in 1039.50 sec.    training loss:\t\t4.00641446781\n",
      "Done 6050 batches in 1041.32 sec.    training loss:\t\t4.00637281737\n",
      "Done 6060 batches in 1043.07 sec.    training loss:\t\t4.00655058689\n",
      "Done 6070 batches in 1044.70 sec.    training loss:\t\t4.0060891228\n",
      "Done 6080 batches in 1046.79 sec.    training loss:\t\t4.00618139862\n",
      "Done 6090 batches in 1048.20 sec.    training loss:\t\t4.00607588444\n",
      "Done 6100 batches in 1049.84 sec.    training loss:\t\t4.00604446356\n",
      "Done 6110 batches in 1051.89 sec.    training loss:\t\t4.0062125096\n",
      "Done 6120 batches in 1053.76 sec.    training loss:\t\t4.00629681022\n",
      "Done 6130 batches in 1055.86 sec.    training loss:\t\t4.00641888942\n",
      "Done 6140 batches in 1057.54 sec.    training loss:\t\t4.0066567261\n",
      "Done 6150 batches in 1059.59 sec.    training loss:\t\t4.00657993476\n",
      "Done 6160 batches in 1061.12 sec.    training loss:\t\t4.00657026303\n",
      "Done 6170 batches in 1062.79 sec.    training loss:\t\t4.00639407635\n",
      "Done 6180 batches in 1064.75 sec.    training loss:\t\t4.00650673819\n",
      "Done 6190 batches in 1066.31 sec.    training loss:\t\t4.00649597375\n",
      "Done 6200 batches in 1068.19 sec.    training loss:\t\t4.00655968051\n",
      "Done 6210 batches in 1069.55 sec.    training loss:\t\t4.0062476162\n",
      "Done 6220 batches in 1071.34 sec.    training loss:\t\t4.00623240007\n",
      "Done 6230 batches in 1073.19 sec.    training loss:\t\t4.00619536904\n",
      "Done 6240 batches in 1075.31 sec.    training loss:\t\t4.00614519887\n",
      "Done 6250 batches in 1077.06 sec.    training loss:\t\t4.00607830807\n",
      "Done 6260 batches in 1078.86 sec.    training loss:\t\t4.00601014733\n",
      "Done 6270 batches in 1080.54 sec.    training loss:\t\t4.00598444307\n",
      "Done 6280 batches in 1082.37 sec.    training loss:\t\t4.00596497097\n",
      "Done 6290 batches in 1083.94 sec.    training loss:\t\t4.00603497673\n",
      "Done 6300 batches in 1085.49 sec.    training loss:\t\t4.00599331205\n",
      "Done 6310 batches in 1087.07 sec.    training loss:\t\t4.00595589538\n",
      "Done 6320 batches in 1088.86 sec.    training loss:\t\t4.00597873198\n",
      "Done 6330 batches in 1090.86 sec.    training loss:\t\t4.00612471819\n",
      "Done 6340 batches in 1092.67 sec.    training loss:\t\t4.00605589324\n",
      "Done 6350 batches in 1094.34 sec.    training loss:\t\t4.00620929102\n",
      "Done 6360 batches in 1096.36 sec.    training loss:\t\t4.00626499087\n",
      "Done 6370 batches in 1098.25 sec.    training loss:\t\t4.00636103142\n",
      "Done 6380 batches in 1099.92 sec.    training loss:\t\t4.00625847361\n",
      "Done 6390 batches in 1101.76 sec.    training loss:\t\t4.00622987128\n",
      "Done 6400 batches in 1103.33 sec.    training loss:\t\t4.00610957038\n",
      "Done 6410 batches in 1105.26 sec.    training loss:\t\t4.0062001594\n",
      "Done 6420 batches in 1107.10 sec.    training loss:\t\t4.00626246298\n",
      "Done 6430 batches in 1108.86 sec.    training loss:\t\t4.00619661971\n",
      "Done 6440 batches in 1110.42 sec.    training loss:\t\t4.00587314786\n",
      "Done 6450 batches in 1112.18 sec.    training loss:\t\t4.0057893532\n",
      "Done 6460 batches in 1113.68 sec.    training loss:\t\t4.00566874658\n",
      "Done 6470 batches in 1115.30 sec.    training loss:\t\t4.00561979447\n",
      "Done 6480 batches in 1116.91 sec.    training loss:\t\t4.00553828857\n",
      "Done 6490 batches in 1118.75 sec.    training loss:\t\t4.00563476626\n",
      "Done 6500 batches in 1120.30 sec.    training loss:\t\t4.00569169133\n",
      "Done 6510 batches in 1121.91 sec.    training loss:\t\t4.00572920545\n",
      "Done 6520 batches in 1123.60 sec.    training loss:\t\t4.00575422944\n",
      "Done 6530 batches in 1125.10 sec.    training loss:\t\t4.00547780896\n",
      "Done 6540 batches in 1126.90 sec.    training loss:\t\t4.00545049348\n",
      "Done 6550 batches in 1128.56 sec.    training loss:\t\t4.00539007576\n",
      "Done 6560 batches in 1130.87 sec.    training loss:\t\t4.00523347735\n",
      "Done 6570 batches in 1132.91 sec.    training loss:\t\t4.00522539293\n",
      "Done 6580 batches in 1134.49 sec.    training loss:\t\t4.00523685003\n",
      "Done 6590 batches in 1136.30 sec.    training loss:\t\t4.00509891702\n",
      "Done 6600 batches in 1138.41 sec.    training loss:\t\t4.00521037456\n",
      "Done 6610 batches in 1140.28 sec.    training loss:\t\t4.00528162399\n",
      "Done 6620 batches in 1141.72 sec.    training loss:\t\t4.00511452277\n",
      "Done 6630 batches in 1143.16 sec.    training loss:\t\t4.0050771961\n",
      "Done 6640 batches in 1144.78 sec.    training loss:\t\t4.00509402626\n",
      "Done 6650 batches in 1146.69 sec.    training loss:\t\t4.00520079365\n",
      "Done 6660 batches in 1148.56 sec.    training loss:\t\t4.00518225209\n",
      "Done 6670 batches in 1150.51 sec.    training loss:\t\t4.00512664343\n",
      "Done 6680 batches in 1152.17 sec.    training loss:\t\t4.00484656417\n",
      "Done 6690 batches in 1153.76 sec.    training loss:\t\t4.00471445163\n",
      "Done 6700 batches in 1155.50 sec.    training loss:\t\t4.00468257075\n",
      "Done 6710 batches in 1157.30 sec.    training loss:\t\t4.00462407074\n",
      "Done 6720 batches in 1159.29 sec.    training loss:\t\t4.00449900535\n",
      "Done 6730 batches in 1160.84 sec.    training loss:\t\t4.00454541815\n",
      "Done 6740 batches in 1162.68 sec.    training loss:\t\t4.00455713074\n",
      "Done 6750 batches in 1164.45 sec.    training loss:\t\t4.00457439016\n",
      "Done 6760 batches in 1166.20 sec.    training loss:\t\t4.0045022124\n",
      "Done 6770 batches in 1167.75 sec.    training loss:\t\t4.00444955033\n",
      "Done 6780 batches in 1169.44 sec.    training loss:\t\t4.00457996235\n",
      "Done 6790 batches in 1171.40 sec.    training loss:\t\t4.00468660008\n",
      "Done 6800 batches in 1173.35 sec.    training loss:\t\t4.00471427202\n",
      "Done 6810 batches in 1175.13 sec.    training loss:\t\t4.00485226177\n",
      "Done 6820 batches in 1176.94 sec.    training loss:\t\t4.00492130602\n",
      "Done 6830 batches in 1178.71 sec.    training loss:\t\t4.00492743091\n",
      "Done 6840 batches in 1180.45 sec.    training loss:\t\t4.00503098024\n",
      "Done 6850 batches in 1182.48 sec.    training loss:\t\t4.00506340545\n",
      "Done 6860 batches in 1184.32 sec.    training loss:\t\t4.00503711867\n",
      "Done 6870 batches in 1185.81 sec.    training loss:\t\t4.00509061945\n",
      "Done 6880 batches in 1188.10 sec.    training loss:\t\t4.0051061895\n",
      "Done 6890 batches in 1190.44 sec.    training loss:\t\t4.0050467414\n",
      "Done 6900 batches in 1192.03 sec.    training loss:\t\t4.00490011395\n",
      "Done 6910 batches in 1193.80 sec.    training loss:\t\t4.00486615608\n",
      "Done 6920 batches in 1195.55 sec.    training loss:\t\t4.00474742296\n",
      "Done 6930 batches in 1197.39 sec.    training loss:\t\t4.00470607226\n",
      "Done 6940 batches in 1199.21 sec.    training loss:\t\t4.00466837643\n",
      "Done 6950 batches in 1200.93 sec.    training loss:\t\t4.00440317552\n",
      "Done 6960 batches in 1202.60 sec.    training loss:\t\t4.0043305868\n",
      "Done 6970 batches in 1204.28 sec.    training loss:\t\t4.00421985827\n",
      "Done 6980 batches in 1206.36 sec.    training loss:\t\t4.00426458073\n",
      "Done 6990 batches in 1207.84 sec.    training loss:\t\t4.00424105688\n",
      "Done 7000 batches in 1209.49 sec.    training loss:\t\t4.00409658756\n",
      "Done 7010 batches in 1211.13 sec.    training loss:\t\t4.0040772786\n",
      "Done 7020 batches in 1213.11 sec.    training loss:\t\t4.00405768788\n",
      "Done 7030 batches in 1215.25 sec.    training loss:\t\t4.00412382177\n",
      "Done 7040 batches in 1217.23 sec.    training loss:\t\t4.00403164927\n",
      "Done 7050 batches in 1218.72 sec.    training loss:\t\t4.00402182515\n",
      "Done 7060 batches in 1220.30 sec.    training loss:\t\t4.00405180927\n",
      "Done 7070 batches in 1221.97 sec.    training loss:\t\t4.00394546628\n",
      "Done 7080 batches in 1223.85 sec.    training loss:\t\t4.00404706799\n",
      "Done 7090 batches in 1225.39 sec.    training loss:\t\t4.00406230269\n",
      "Done 7100 batches in 1227.07 sec.    training loss:\t\t4.00402366259\n",
      "Done 7110 batches in 1228.75 sec.    training loss:\t\t4.00386186863\n",
      "Done 7120 batches in 1230.29 sec.    training loss:\t\t4.00367306779\n",
      "Done 7130 batches in 1232.17 sec.    training loss:\t\t4.00366077416\n",
      "Done 7140 batches in 1233.69 sec.    training loss:\t\t4.00354992852\n",
      "Done 7150 batches in 1235.56 sec.    training loss:\t\t4.00354897546\n",
      "Done 7160 batches in 1237.15 sec.    training loss:\t\t4.00348195583\n",
      "Done 7170 batches in 1239.00 sec.    training loss:\t\t4.00340662059\n",
      "Done 7180 batches in 1240.61 sec.    training loss:\t\t4.00334600908\n",
      "Done 7190 batches in 1242.23 sec.    training loss:\t\t4.00339319368\n",
      "Done 7200 batches in 1243.93 sec.    training loss:\t\t4.00333480467\n",
      "Done 7210 batches in 1245.42 sec.    training loss:\t\t4.00337599875\n",
      "Done 7220 batches in 1247.08 sec.    training loss:\t\t4.00341435192\n",
      "Done 7230 batches in 1248.74 sec.    training loss:\t\t4.00345907778\n",
      "Done 7240 batches in 1250.85 sec.    training loss:\t\t4.00340313131\n",
      "Done 7250 batches in 1252.46 sec.    training loss:\t\t4.00346657681\n",
      "Done 7260 batches in 1254.20 sec.    training loss:\t\t4.00348052906\n",
      "Done 7270 batches in 1255.57 sec.    training loss:\t\t4.00340457901\n",
      "Done 7280 batches in 1257.33 sec.    training loss:\t\t4.00346169413\n",
      "Done 7290 batches in 1259.23 sec.    training loss:\t\t4.00352427462\n",
      "Done 7300 batches in 1261.00 sec.    training loss:\t\t4.00344764337\n",
      "Done 7310 batches in 1262.37 sec.    training loss:\t\t4.00340136989\n",
      "Done 7320 batches in 1264.21 sec.    training loss:\t\t4.00343261872\n",
      "Done 7330 batches in 1266.12 sec.    training loss:\t\t4.00341988696\n",
      "Done 7340 batches in 1268.25 sec.    training loss:\t\t4.00334614912\n",
      "Done 7350 batches in 1270.30 sec.    training loss:\t\t4.00339636452\n",
      "Done 7360 batches in 1271.76 sec.    training loss:\t\t4.00334590916\n",
      "Done 7370 batches in 1273.37 sec.    training loss:\t\t4.00318640657\n",
      "Done 7380 batches in 1275.08 sec.    training loss:\t\t4.00314948107\n",
      "Done 7390 batches in 1277.40 sec.    training loss:\t\t4.00325442106\n",
      "Done 7400 batches in 1278.99 sec.    training loss:\t\t4.00322243864\n",
      "Done 7410 batches in 1280.73 sec.    training loss:\t\t4.00323173215\n",
      "Done 7420 batches in 1282.41 sec.    training loss:\t\t4.00329445588\n",
      "Done 7430 batches in 1283.93 sec.    training loss:\t\t4.00335628255\n",
      "Done 7440 batches in 1285.45 sec.    training loss:\t\t4.00329416292\n",
      "Done 7450 batches in 1287.52 sec.    training loss:\t\t4.00343233227\n",
      "Done 7460 batches in 1289.28 sec.    training loss:\t\t4.00363719083\n",
      "Done 7470 batches in 1290.80 sec.    training loss:\t\t4.00366134966\n",
      "Done 7480 batches in 1292.57 sec.    training loss:\t\t4.00363871347\n",
      "Done 7490 batches in 1294.22 sec.    training loss:\t\t4.0035858867\n",
      "Done 7500 batches in 1295.92 sec.    training loss:\t\t4.00358928006\n",
      "Done 7510 batches in 1298.04 sec.    training loss:\t\t4.00369363568\n",
      "Done 7520 batches in 1299.94 sec.    training loss:\t\t4.00361087401\n",
      "Done 7530 batches in 1301.62 sec.    training loss:\t\t4.0035507654\n",
      "Done 7540 batches in 1303.15 sec.    training loss:\t\t4.00347455108\n",
      "Done 7550 batches in 1304.66 sec.    training loss:\t\t4.00344976125\n",
      "Done 7560 batches in 1306.16 sec.    training loss:\t\t4.00334671064\n",
      "Done 7570 batches in 1307.76 sec.    training loss:\t\t4.00332976666\n",
      "Done 7580 batches in 1309.61 sec.    training loss:\t\t4.00342554625\n",
      "Done 7590 batches in 1311.25 sec.    training loss:\t\t4.00354793436\n",
      "Done 7600 batches in 1312.89 sec.    training loss:\t\t4.00369145738\n",
      "Done 7610 batches in 1314.44 sec.    training loss:\t\t4.00369867389\n",
      "Done 7620 batches in 1316.11 sec.    training loss:\t\t4.00370372618\n",
      "Done 7630 batches in 1317.87 sec.    training loss:\t\t4.00362001868\n",
      "Done 7640 batches in 1319.47 sec.    training loss:\t\t4.00349455538\n",
      "Done 7650 batches in 1321.09 sec.    training loss:\t\t4.00346876144\n",
      "Done 7660 batches in 1322.61 sec.    training loss:\t\t4.0034036834\n",
      "Done 7670 batches in 1324.17 sec.    training loss:\t\t4.00329587096\n",
      "Done 7680 batches in 1325.76 sec.    training loss:\t\t4.00325100223\n",
      "Done 7690 batches in 1327.52 sec.    training loss:\t\t4.00327090333\n",
      "Done 7700 batches in 1329.32 sec.    training loss:\t\t4.00311598508\n",
      "Done 7710 batches in 1330.85 sec.    training loss:\t\t4.00300042954\n",
      "Done 7720 batches in 1332.73 sec.    training loss:\t\t4.00291574162\n",
      "Done 7730 batches in 1334.33 sec.    training loss:\t\t4.00291952414\n",
      "Done 7740 batches in 1336.02 sec.    training loss:\t\t4.00299240455\n",
      "Done 7750 batches in 1338.12 sec.    training loss:\t\t4.00301743643\n",
      "Done 7760 batches in 1339.96 sec.    training loss:\t\t4.00301151322\n",
      "Done 7770 batches in 1341.89 sec.    training loss:\t\t4.00305695917\n",
      "Done 7780 batches in 1343.57 sec.    training loss:\t\t4.00299993263\n",
      "Done 7790 batches in 1345.57 sec.    training loss:\t\t4.00301295931\n",
      "Done 7800 batches in 1347.19 sec.    training loss:\t\t4.00294669867\n",
      "Done 7810 batches in 1348.78 sec.    training loss:\t\t4.00281619519\n",
      "Done 7820 batches in 1350.86 sec.    training loss:\t\t4.00283971632\n",
      "Done 7830 batches in 1352.62 sec.    training loss:\t\t4.00273283542\n",
      "Done 7840 batches in 1354.82 sec.    training loss:\t\t4.00275705767\n",
      "Done 7850 batches in 1356.61 sec.    training loss:\t\t4.00264692868\n",
      "Done 7860 batches in 1358.43 sec.    training loss:\t\t4.00252823378\n",
      "Done 7870 batches in 1360.19 sec.    training loss:\t\t4.00254483096\n",
      "Done 7880 batches in 1361.85 sec.    training loss:\t\t4.00258434602\n",
      "Done 7890 batches in 1363.59 sec.    training loss:\t\t4.0025175492\n",
      "Done 7900 batches in 1365.27 sec.    training loss:\t\t4.00246334912\n",
      "Done 7910 batches in 1367.32 sec.    training loss:\t\t4.00247456467\n",
      "Done 7920 batches in 1368.90 sec.    training loss:\t\t4.00246359546\n",
      "Done 7930 batches in 1370.83 sec.    training loss:\t\t4.00250404371\n",
      "Done 7940 batches in 1372.42 sec.    training loss:\t\t4.00246912166\n",
      "Done 7950 batches in 1374.21 sec.    training loss:\t\t4.00246667229\n",
      "Done 7960 batches in 1375.98 sec.    training loss:\t\t4.00242168182\n",
      "Done 7970 batches in 1377.72 sec.    training loss:\t\t4.00242658523\n",
      "Done 7980 batches in 1379.34 sec.    training loss:\t\t4.00235590989\n",
      "Done 7990 batches in 1381.11 sec.    training loss:\t\t4.00230757417\n",
      "Done 8000 batches in 1382.79 sec.    training loss:\t\t4.00224138939\n",
      "Done 8010 batches in 1384.53 sec.    training loss:\t\t4.00204141539\n",
      "Done 8020 batches in 1386.49 sec.    training loss:\t\t4.00201868589\n",
      "Done 8030 batches in 1388.24 sec.    training loss:\t\t4.00201289553\n",
      "Done 8040 batches in 1390.07 sec.    training loss:\t\t4.00210895212\n",
      "Done 8050 batches in 1391.72 sec.    training loss:\t\t4.00209486843\n",
      "Done 8060 batches in 1393.96 sec.    training loss:\t\t4.00217674213\n",
      "Done 8070 batches in 1395.97 sec.    training loss:\t\t4.00218255747\n",
      "Done 8080 batches in 1397.55 sec.    training loss:\t\t4.00207576513\n",
      "Done 8090 batches in 1399.47 sec.    training loss:\t\t4.00216603866\n",
      "Done 8100 batches in 1401.02 sec.    training loss:\t\t4.00218802523\n",
      "Done 8110 batches in 1402.76 sec.    training loss:\t\t4.00211223409\n",
      "Done 8120 batches in 1404.70 sec.    training loss:\t\t4.00219475169\n",
      "Done 8130 batches in 1406.39 sec.    training loss:\t\t4.00212848181\n",
      "Done 8140 batches in 1408.03 sec.    training loss:\t\t4.00211159155\n",
      "Done 8150 batches in 1409.72 sec.    training loss:\t\t4.00177485925\n",
      "Done 8160 batches in 1411.34 sec.    training loss:\t\t4.00182862892\n",
      "Done 8170 batches in 1413.10 sec.    training loss:\t\t4.00173281377\n",
      "Done 8180 batches in 1414.76 sec.    training loss:\t\t4.00164544113\n",
      "Done 8190 batches in 1416.55 sec.    training loss:\t\t4.00149037893\n",
      "Done 8200 batches in 1418.16 sec.    training loss:\t\t4.00140296951\n",
      "Done 8210 batches in 1419.72 sec.    training loss:\t\t4.00134689131\n",
      "Done 8220 batches in 1421.16 sec.    training loss:\t\t4.00132917064\n",
      "Done 8230 batches in 1422.87 sec.    training loss:\t\t4.00137477147\n",
      "Done 8240 batches in 1424.60 sec.    training loss:\t\t4.00137553727\n",
      "Done 8250 batches in 1426.50 sec.    training loss:\t\t4.00149392351\n",
      "Done 8260 batches in 1428.37 sec.    training loss:\t\t4.00143353809\n",
      "Done 8270 batches in 1430.38 sec.    training loss:\t\t4.00151611383\n",
      "Done 8280 batches in 1431.89 sec.    training loss:\t\t4.00144103665\n",
      "Done 8290 batches in 1433.46 sec.    training loss:\t\t4.00140398377\n",
      "Done 8300 batches in 1435.07 sec.    training loss:\t\t4.00130323545\n",
      "Done 8310 batches in 1436.93 sec.    training loss:\t\t4.00136954254\n",
      "Done 8320 batches in 1438.71 sec.    training loss:\t\t4.00123616411\n",
      "Done 8330 batches in 1440.69 sec.    training loss:\t\t4.00117771039\n",
      "Done 8340 batches in 1442.30 sec.    training loss:\t\t4.00111699553\n",
      "Done 8350 batches in 1443.94 sec.    training loss:\t\t4.00116962556\n",
      "Done 8360 batches in 1445.35 sec.    training loss:\t\t4.00102973626\n",
      "Done 8370 batches in 1447.10 sec.    training loss:\t\t4.00104185564\n",
      "Done 8380 batches in 1448.82 sec.    training loss:\t\t4.00099043926\n",
      "Done 8390 batches in 1450.38 sec.    training loss:\t\t4.00086561693\n",
      "Done 8400 batches in 1453.41 sec.    training loss:\t\t4.00093635122\n",
      "Done 8410 batches in 1455.01 sec.    training loss:\t\t4.00079952131\n",
      "Done 8420 batches in 1456.53 sec.    training loss:\t\t4.0008380709\n",
      "Done 8430 batches in 1458.11 sec.    training loss:\t\t4.00077395518\n",
      "Done 8440 batches in 1459.82 sec.    training loss:\t\t4.00072107394\n",
      "Done 8450 batches in 1461.21 sec.    training loss:\t\t4.00052034948\n",
      "Done 8460 batches in 1462.78 sec.    training loss:\t\t4.00031075261\n",
      "Done 8470 batches in 1464.20 sec.    training loss:\t\t4.00020941168\n",
      "Done 8480 batches in 1466.23 sec.    training loss:\t\t4.00016538284\n",
      "Done 8490 batches in 1468.14 sec.    training loss:\t\t4.00014145035\n",
      "Done 8500 batches in 1470.33 sec.    training loss:\t\t4.00010897998\n",
      "Done 8510 batches in 1472.15 sec.    training loss:\t\t4.00006238515\n",
      "Done 8520 batches in 1474.17 sec.    training loss:\t\t3.99992848341\n",
      "Done 8530 batches in 1475.64 sec.    training loss:\t\t3.99990753638\n",
      "Done 8540 batches in 1477.35 sec.    training loss:\t\t3.99996895142\n",
      "Done 8550 batches in 1479.14 sec.    training loss:\t\t4.00005400384\n",
      "Done 8560 batches in 1480.83 sec.    training loss:\t\t3.99992321646\n",
      "Done 8570 batches in 1482.50 sec.    training loss:\t\t3.9998792519\n",
      "Done 8580 batches in 1484.46 sec.    training loss:\t\t3.99978656997\n",
      "Done 8590 batches in 1486.36 sec.    training loss:\t\t3.99971756902\n",
      "Done 8600 batches in 1488.02 sec.    training loss:\t\t3.99966055036\n",
      "Done 8610 batches in 1489.57 sec.    training loss:\t\t3.99955343943\n",
      "Done 8620 batches in 1491.21 sec.    training loss:\t\t3.99946909626\n",
      "Done 8630 batches in 1492.84 sec.    training loss:\t\t3.99954204581\n",
      "Done 8640 batches in 1494.41 sec.    training loss:\t\t3.99953161081\n",
      "Done 8650 batches in 1496.20 sec.    training loss:\t\t3.99947739042\n",
      "Done 8660 batches in 1497.83 sec.    training loss:\t\t3.99938744293\n",
      "Done 8670 batches in 1499.45 sec.    training loss:\t\t3.99930511052\n",
      "Done 8680 batches in 1501.01 sec.    training loss:\t\t3.99939522949\n",
      "Done 8690 batches in 1502.54 sec.    training loss:\t\t3.99938929454\n",
      "Done 8700 batches in 1504.09 sec.    training loss:\t\t3.99934228875\n",
      "Done 8710 batches in 1505.85 sec.    training loss:\t\t3.99929757083\n",
      "Done 8720 batches in 1507.68 sec.    training loss:\t\t3.9993561597\n",
      "Done 8730 batches in 1509.68 sec.    training loss:\t\t3.99928238108\n",
      "Done 8740 batches in 1511.26 sec.    training loss:\t\t3.99914549682\n",
      "Done 8750 batches in 1513.11 sec.    training loss:\t\t3.99915463908\n",
      "Done 8760 batches in 1515.18 sec.    training loss:\t\t3.99910667084\n",
      "Done 8770 batches in 1516.77 sec.    training loss:\t\t3.99904615387\n",
      "Done 8780 batches in 1518.55 sec.    training loss:\t\t3.99902432915\n",
      "Done 8790 batches in 1520.25 sec.    training loss:\t\t3.99899381666\n",
      "Done 8800 batches in 1521.89 sec.    training loss:\t\t3.99902654082\n",
      "Done 8810 batches in 1523.36 sec.    training loss:\t\t3.99891048135\n",
      "Done 8820 batches in 1524.87 sec.    training loss:\t\t3.99880294935\n",
      "Done 8830 batches in 1526.62 sec.    training loss:\t\t3.99879242504\n",
      "Done 8840 batches in 1528.46 sec.    training loss:\t\t3.99859471515\n",
      "Done 8850 batches in 1530.35 sec.    training loss:\t\t3.9986043808\n",
      "Done 8860 batches in 1532.18 sec.    training loss:\t\t3.99860060931\n",
      "Done 8870 batches in 1533.56 sec.    training loss:\t\t3.99848449386\n",
      "Done 8880 batches in 1534.98 sec.    training loss:\t\t3.99828949436\n",
      "Done 8890 batches in 1536.87 sec.    training loss:\t\t3.99820724159\n",
      "Done 8900 batches in 1538.48 sec.    training loss:\t\t3.9981284133\n",
      "Done 8910 batches in 1540.32 sec.    training loss:\t\t3.99805924852\n",
      "Done 8920 batches in 1542.34 sec.    training loss:\t\t3.99802950674\n",
      "Done 8930 batches in 1543.97 sec.    training loss:\t\t3.9979063316\n",
      "Done 8940 batches in 1545.44 sec.    training loss:\t\t3.99799629711\n",
      "Done 8950 batches in 1546.79 sec.    training loss:\t\t3.99784972393\n",
      "Done 8960 batches in 1548.57 sec.    training loss:\t\t3.99788864324\n",
      "Done 8970 batches in 1549.95 sec.    training loss:\t\t3.99775740183\n",
      "Done 8980 batches in 1551.74 sec.    training loss:\t\t3.99777440074\n",
      "Done 8990 batches in 1553.18 sec.    training loss:\t\t3.99766008281\n",
      "Done 9000 batches in 1555.33 sec.    training loss:\t\t3.99771457121\n",
      "Done 9010 batches in 1557.13 sec.    training loss:\t\t3.99773359923\n",
      "Done 9020 batches in 1558.73 sec.    training loss:\t\t3.99770573417\n",
      "Done 9030 batches in 1560.50 sec.    training loss:\t\t3.99768352213\n",
      "Done 9040 batches in 1562.46 sec.    training loss:\t\t3.99761350693\n",
      "Done 9050 batches in 1563.92 sec.    training loss:\t\t3.99742195048\n",
      "Done 9060 batches in 1565.57 sec.    training loss:\t\t3.99730279938\n",
      "Done 9070 batches in 1567.69 sec.    training loss:\t\t3.99728356801\n",
      "Done 9080 batches in 1569.29 sec.    training loss:\t\t3.99722328499\n",
      "Done 9090 batches in 1571.14 sec.    training loss:\t\t3.99714486276\n",
      "Done 9100 batches in 1572.72 sec.    training loss:\t\t3.99708892102\n",
      "Done 9110 batches in 1574.40 sec.    training loss:\t\t3.99705401011\n",
      "Done 9120 batches in 1576.21 sec.    training loss:\t\t3.99697997405\n",
      "Done 9130 batches in 1577.68 sec.    training loss:\t\t3.99680866036\n",
      "Done 9140 batches in 1579.49 sec.    training loss:\t\t3.99678756406\n",
      "Done 9150 batches in 1580.97 sec.    training loss:\t\t3.99673236133\n",
      "Done 9160 batches in 1582.54 sec.    training loss:\t\t3.99659510697\n",
      "Done 9170 batches in 1584.85 sec.    training loss:\t\t3.99672330747\n",
      "Done 9180 batches in 1587.00 sec.    training loss:\t\t3.99669135508\n",
      "Done 9190 batches in 1588.78 sec.    training loss:\t\t3.99663150609\n",
      "Done 9200 batches in 1590.38 sec.    training loss:\t\t3.99657448279\n",
      "Done 9210 batches in 1591.97 sec.    training loss:\t\t3.99650541649\n",
      "Done 9220 batches in 1593.55 sec.    training loss:\t\t3.99638490705\n",
      "Done 9230 batches in 1595.47 sec.    training loss:\t\t3.9964004499\n",
      "Done 9240 batches in 1597.08 sec.    training loss:\t\t3.9964620094\n",
      "Done 9250 batches in 1598.58 sec.    training loss:\t\t3.99631178346\n",
      "Done 9260 batches in 1600.36 sec.    training loss:\t\t3.9962619272\n",
      "Done 9270 batches in 1601.62 sec.    training loss:\t\t3.99618956176\n",
      "Done 9280 batches in 1603.35 sec.    training loss:\t\t3.99624292383\n",
      "Done 9290 batches in 1605.26 sec.    training loss:\t\t3.99604581816\n",
      "Done 9300 batches in 1607.14 sec.    training loss:\t\t3.99615075775\n",
      "Done 9310 batches in 1608.55 sec.    training loss:\t\t3.99613588914\n",
      "Done 9320 batches in 1610.69 sec.    training loss:\t\t3.99630792821\n",
      "Done 9330 batches in 1612.42 sec.    training loss:\t\t3.9961246413\n",
      "Done 9340 batches in 1614.08 sec.    training loss:\t\t3.99602997502\n",
      "Done 9350 batches in 1615.88 sec.    training loss:\t\t3.99598903587\n",
      "Done 9360 batches in 1617.57 sec.    training loss:\t\t3.99597057425\n",
      "Done 9370 batches in 1619.26 sec.    training loss:\t\t3.99582405973\n",
      "Done 9380 batches in 1620.72 sec.    training loss:\t\t3.99576201241\n",
      "Done 9390 batches in 1622.41 sec.    training loss:\t\t3.99584661881\n",
      "Done 9400 batches in 1624.21 sec.    training loss:\t\t3.99585269903\n",
      "Done 9410 batches in 1626.34 sec.    training loss:\t\t3.99573772721\n",
      "Done 9420 batches in 1627.93 sec.    training loss:\t\t3.99563994405\n",
      "Done 9430 batches in 1629.71 sec.    training loss:\t\t3.99556319013\n",
      "Done 9440 batches in 1631.36 sec.    training loss:\t\t3.99543775807\n",
      "Done 9450 batches in 1633.49 sec.    training loss:\t\t3.99538553175\n",
      "Done 9460 batches in 1635.09 sec.    training loss:\t\t3.9953704434\n",
      "Done 9470 batches in 1636.78 sec.    training loss:\t\t3.99534476179\n",
      "Done 9480 batches in 1638.79 sec.    training loss:\t\t3.99544570215\n",
      "Done 9490 batches in 1640.16 sec.    training loss:\t\t3.99531760736\n",
      "Done 9500 batches in 1642.08 sec.    training loss:\t\t3.99517491524\n",
      "Done 9510 batches in 1643.67 sec.    training loss:\t\t3.99511563658\n",
      "Done 9520 batches in 1645.54 sec.    training loss:\t\t3.99498634601\n",
      "Done 9530 batches in 1647.39 sec.    training loss:\t\t3.99497718886\n",
      "Done 9540 batches in 1649.44 sec.    training loss:\t\t3.99500278127\n",
      "Done 9550 batches in 1651.15 sec.    training loss:\t\t3.99505054399\n",
      "Done 9560 batches in 1652.95 sec.    training loss:\t\t3.99507795495\n",
      "Done 9570 batches in 1654.55 sec.    training loss:\t\t3.99506095485\n",
      "Done 9580 batches in 1656.74 sec.    training loss:\t\t3.99503833314\n",
      "Done 9590 batches in 1658.58 sec.    training loss:\t\t3.99500557551\n",
      "Done 9600 batches in 1660.31 sec.    training loss:\t\t3.99501154654\n",
      "Done 9610 batches in 1661.96 sec.    training loss:\t\t3.99502026571\n",
      "Done 9620 batches in 1663.68 sec.    training loss:\t\t3.99498744551\n",
      "Done 9630 batches in 1665.38 sec.    training loss:\t\t3.99486149813\n",
      "Done 9640 batches in 1667.33 sec.    training loss:\t\t3.99490931093\n",
      "Done 9650 batches in 1668.88 sec.    training loss:\t\t3.99489577746\n",
      "Done 9660 batches in 1670.49 sec.    training loss:\t\t3.99487597333\n",
      "Done 9670 batches in 1672.09 sec.    training loss:\t\t3.99484468853\n",
      "Done 9680 batches in 1673.59 sec.    training loss:\t\t3.99483516827\n",
      "Done 9690 batches in 1675.61 sec.    training loss:\t\t3.99483021821\n",
      "Done 9700 batches in 1677.24 sec.    training loss:\t\t3.99464443411\n",
      "Done 9710 batches in 1678.88 sec.    training loss:\t\t3.99462180209\n",
      "Done 9720 batches in 1680.51 sec.    training loss:\t\t3.99454913007\n",
      "Done 9730 batches in 1681.99 sec.    training loss:\t\t3.99445378589\n",
      "Done 9740 batches in 1683.24 sec.    training loss:\t\t3.99445867955\n",
      "Done 9750 batches in 1684.75 sec.    training loss:\t\t3.99445167495\n",
      "Done 9760 batches in 1686.29 sec.    training loss:\t\t3.99441106424\n",
      "Done 9770 batches in 1688.11 sec.    training loss:\t\t3.99440817206\n",
      "Done 9780 batches in 1689.73 sec.    training loss:\t\t3.99433261361\n",
      "Done 9790 batches in 1692.01 sec.    training loss:\t\t3.99438671749\n",
      "Done 9800 batches in 1693.89 sec.    training loss:\t\t3.99437917863\n",
      "Done 9810 batches in 1695.72 sec.    training loss:\t\t3.99440376875\n",
      "Done 9820 batches in 1697.80 sec.    training loss:\t\t3.99431793845\n",
      "Done 9830 batches in 1699.38 sec.    training loss:\t\t3.99427439119\n",
      "Done 9840 batches in 1700.92 sec.    training loss:\t\t3.99422378782\n",
      "Done 9850 batches in 1702.42 sec.    training loss:\t\t3.99421174149\n",
      "Done 9860 batches in 1703.95 sec.    training loss:\t\t3.99415405928\n",
      "Done 9870 batches in 1705.70 sec.    training loss:\t\t3.99409565827\n",
      "Done 9880 batches in 1707.48 sec.    training loss:\t\t3.99407188366\n",
      "Done 9890 batches in 1709.56 sec.    training loss:\t\t3.99406767504\n",
      "Done 9900 batches in 1711.45 sec.    training loss:\t\t3.99399471098\n",
      "Done 9910 batches in 1713.38 sec.    training loss:\t\t3.99392963341\n",
      "Done 9920 batches in 1714.99 sec.    training loss:\t\t3.9937685077\n",
      "Done 9930 batches in 1716.54 sec.    training loss:\t\t3.99366401178\n",
      "Done 9940 batches in 1718.27 sec.    training loss:\t\t3.99372607436\n",
      "Done 9950 batches in 1720.03 sec.    training loss:\t\t3.99370116246\n",
      "Done 9960 batches in 1722.22 sec.    training loss:\t\t3.99374374893\n",
      "Done 9970 batches in 1723.72 sec.    training loss:\t\t3.99372333629\n",
      "Done 9980 batches in 1725.46 sec.    training loss:\t\t3.99363878043\n",
      "Done 9990 batches in 1727.07 sec.    training loss:\t\t3.99357525231\n",
      "Done 10000 batches in 1728.82 sec.    training loss:\t\t3.99346674118\n",
      "Done 10010 batches in 1730.51 sec.    training loss:\t\t3.99332000483\n",
      "Done 10020 batches in 1732.16 sec.    training loss:\t\t3.99331606082\n",
      "Done 10030 batches in 1733.66 sec.    training loss:\t\t3.99317312226\n",
      "Done 10040 batches in 1735.24 sec.    training loss:\t\t3.99318449305\n",
      "Done 10050 batches in 1737.22 sec.    training loss:\t\t3.99309909296\n",
      "Done 10060 batches in 1738.65 sec.    training loss:\t\t3.99308700535\n",
      "Done 10070 batches in 1740.70 sec.    training loss:\t\t3.9932121341\n",
      "Done 10080 batches in 1742.41 sec.    training loss:\t\t3.99313802546\n",
      "Done 10090 batches in 1744.24 sec.    training loss:\t\t3.99311181979\n",
      "Done 10100 batches in 1745.94 sec.    training loss:\t\t3.99310710555\n",
      "Done 10110 batches in 1747.74 sec.    training loss:\t\t3.99313323587\n",
      "Done 10120 batches in 1749.34 sec.    training loss:\t\t3.99303313048\n",
      "Done 10130 batches in 1751.23 sec.    training loss:\t\t3.99304265854\n",
      "Done 10140 batches in 1753.00 sec.    training loss:\t\t3.99310618754\n",
      "Done 10150 batches in 1754.79 sec.    training loss:\t\t3.99308572659\n",
      "Done 10160 batches in 1756.68 sec.    training loss:\t\t3.99316974813\n",
      "Done 10170 batches in 1758.20 sec.    training loss:\t\t3.99325770366\n",
      "Done 10180 batches in 1759.97 sec.    training loss:\t\t3.99316626827\n",
      "Done 10190 batches in 1761.67 sec.    training loss:\t\t3.99318265484\n",
      "Done 10200 batches in 1763.19 sec.    training loss:\t\t3.99312398541\n",
      "Done 10210 batches in 1765.07 sec.    training loss:\t\t3.99300031828\n",
      "Done 10220 batches in 1766.75 sec.    training loss:\t\t3.99297208518\n",
      "Done 10230 batches in 1768.09 sec.    training loss:\t\t3.99293659528\n",
      "Done 10240 batches in 1769.60 sec.    training loss:\t\t3.99288215083\n",
      "Done 10250 batches in 1772.23 sec.    training loss:\t\t3.99303022345\n",
      "Done 10260 batches in 1773.83 sec.    training loss:\t\t3.99298291257\n",
      "Done 10270 batches in 1775.86 sec.    training loss:\t\t3.99296649343\n",
      "Done 10280 batches in 1777.14 sec.    training loss:\t\t3.99283509029\n",
      "Done 10290 batches in 1778.84 sec.    training loss:\t\t3.99279493588\n",
      "Done 10300 batches in 1780.16 sec.    training loss:\t\t3.99260094548\n",
      "Done 10310 batches in 1781.58 sec.    training loss:\t\t3.99248419326\n",
      "Done 10320 batches in 1783.15 sec.    training loss:\t\t3.99244879907\n",
      "Done 10330 batches in 1784.61 sec.    training loss:\t\t3.99241418721\n",
      "Done 10340 batches in 1786.44 sec.    training loss:\t\t3.99235688058\n",
      "Done 10350 batches in 1788.12 sec.    training loss:\t\t3.99227195625\n",
      "Done 10360 batches in 1790.03 sec.    training loss:\t\t3.99230180873\n",
      "Done 10370 batches in 1791.81 sec.    training loss:\t\t3.9923751952\n",
      "Done 10380 batches in 1793.62 sec.    training loss:\t\t3.99229506279\n",
      "Done 10390 batches in 1795.65 sec.    training loss:\t\t3.99222371046\n",
      "Done 10400 batches in 1797.30 sec.    training loss:\t\t3.99216620828\n",
      "Done 10410 batches in 1798.84 sec.    training loss:\t\t3.99204637997\n",
      "Done 10420 batches in 1800.71 sec.    training loss:\t\t3.99210025912\n",
      "Done 10430 batches in 1802.36 sec.    training loss:\t\t3.99208059599\n",
      "Done 10440 batches in 1804.18 sec.    training loss:\t\t3.99199404347\n",
      "Done 10450 batches in 1806.30 sec.    training loss:\t\t3.99206317386\n",
      "Done 10460 batches in 1807.94 sec.    training loss:\t\t3.99201902876\n",
      "Done 10470 batches in 1810.06 sec.    training loss:\t\t3.99197249369\n",
      "Done 10480 batches in 1811.90 sec.    training loss:\t\t3.99190606024\n",
      "Done 10490 batches in 1813.67 sec.    training loss:\t\t3.9920119081\n",
      "Done 10500 batches in 1815.57 sec.    training loss:\t\t3.99194804562\n",
      "Done 10510 batches in 1817.38 sec.    training loss:\t\t3.99188773566\n",
      "Done 10520 batches in 1819.21 sec.    training loss:\t\t3.99190264977\n",
      "Done 10530 batches in 1820.89 sec.    training loss:\t\t3.9918456227\n",
      "Done 10540 batches in 1823.11 sec.    training loss:\t\t3.9918932404\n",
      "Done 10550 batches in 1825.12 sec.    training loss:\t\t3.99203338562\n",
      "Done 10560 batches in 1826.86 sec.    training loss:\t\t3.99205610217\n",
      "Done 10570 batches in 1828.73 sec.    training loss:\t\t3.99194754575\n",
      "Done 10580 batches in 1830.42 sec.    training loss:\t\t3.99201484896\n",
      "Done 10590 batches in 1831.97 sec.    training loss:\t\t3.99191853858\n",
      "Done 10600 batches in 1833.72 sec.    training loss:\t\t3.99185053218\n",
      "Done 10610 batches in 1835.44 sec.    training loss:\t\t3.99187768546\n",
      "Done 10620 batches in 1837.02 sec.    training loss:\t\t3.99181829505\n",
      "Done 10630 batches in 1838.73 sec.    training loss:\t\t3.99182320031\n",
      "Done 10640 batches in 1840.63 sec.    training loss:\t\t3.99185963549\n",
      "Done 10650 batches in 1842.16 sec.    training loss:\t\t3.99190956765\n",
      "Done 10660 batches in 1843.90 sec.    training loss:\t\t3.99190381499\n",
      "Done 10670 batches in 1845.58 sec.    training loss:\t\t3.99191599256\n",
      "Done 10680 batches in 1847.28 sec.    training loss:\t\t3.99198513806\n",
      "Done 10690 batches in 1849.21 sec.    training loss:\t\t3.99195301539\n",
      "Done 10700 batches in 1850.94 sec.    training loss:\t\t3.99190579512\n",
      "Done 10710 batches in 1852.71 sec.    training loss:\t\t3.99185737186\n",
      "Done 10720 batches in 1854.49 sec.    training loss:\t\t3.99188273211\n",
      "Done 10730 batches in 1856.29 sec.    training loss:\t\t3.99183456442\n",
      "Done 10740 batches in 1857.79 sec.    training loss:\t\t3.99177811299\n",
      "Done 10750 batches in 1859.27 sec.    training loss:\t\t3.99167927221\n",
      "Done 10760 batches in 1860.88 sec.    training loss:\t\t3.99165973787\n",
      "Done 10770 batches in 1862.69 sec.    training loss:\t\t3.99160536403\n",
      "Done 10780 batches in 1864.51 sec.    training loss:\t\t3.99157843389\n",
      "Done 10790 batches in 1866.03 sec.    training loss:\t\t3.9915303084\n",
      "Done 10800 batches in 1867.80 sec.    training loss:\t\t3.99147280232\n",
      "Done 10810 batches in 1869.66 sec.    training loss:\t\t3.991561285\n",
      "Done 10820 batches in 1871.43 sec.    training loss:\t\t3.99141502541\n",
      "Done 10830 batches in 1873.21 sec.    training loss:\t\t3.99135952744\n",
      "Done 10840 batches in 1874.80 sec.    training loss:\t\t3.99132905409\n",
      "Done 10850 batches in 1876.63 sec.    training loss:\t\t3.99141609664\n",
      "Done 10860 batches in 1878.40 sec.    training loss:\t\t3.99138985211\n",
      "Done 10870 batches in 1879.91 sec.    training loss:\t\t3.99139498309\n",
      "Done 10880 batches in 1882.06 sec.    training loss:\t\t3.9912633588\n",
      "Done 10890 batches in 1883.54 sec.    training loss:\t\t3.9912704207\n",
      "Done 10900 batches in 1884.80 sec.    training loss:\t\t3.99120245627\n",
      "Done 10910 batches in 1886.47 sec.    training loss:\t\t3.99124595354\n",
      "Done 10920 batches in 1888.29 sec.    training loss:\t\t3.99129871071\n",
      "Done 10930 batches in 1889.97 sec.    training loss:\t\t3.9913085354\n",
      "Done 10940 batches in 1891.91 sec.    training loss:\t\t3.99129667949\n",
      "Done 10950 batches in 1894.07 sec.    training loss:\t\t3.99126723725\n",
      "Done 10960 batches in 1896.36 sec.    training loss:\t\t3.99126068693\n",
      "Done 10970 batches in 1898.46 sec.    training loss:\t\t3.99126801452\n",
      "Done 10980 batches in 1900.11 sec.    training loss:\t\t3.99120377489\n",
      "Done 10990 batches in 1901.80 sec.    training loss:\t\t3.9911757347\n",
      "Done 11000 batches in 1903.31 sec.    training loss:\t\t3.99106587178\n",
      "Done 11010 batches in 1905.07 sec.    training loss:\t\t3.99108276943\n",
      "Done 11020 batches in 1906.82 sec.    training loss:\t\t3.99099460431\n",
      "Done 11030 batches in 1908.76 sec.    training loss:\t\t3.99098948842\n",
      "Done 11040 batches in 1910.86 sec.    training loss:\t\t3.99100550273\n",
      "Done 11050 batches in 1912.65 sec.    training loss:\t\t3.99098004281\n",
      "Done 11060 batches in 1914.30 sec.    training loss:\t\t3.99091495761\n",
      "Done 11070 batches in 1915.96 sec.    training loss:\t\t3.99085462145\n",
      "Done 11080 batches in 1917.50 sec.    training loss:\t\t3.99082468785\n",
      "Done 11090 batches in 1919.27 sec.    training loss:\t\t3.99071989962\n",
      "Done 11100 batches in 1921.65 sec.    training loss:\t\t3.99082027143\n",
      "Done 11110 batches in 1923.13 sec.    training loss:\t\t3.99082006157\n",
      "Done 11120 batches in 1924.54 sec.    training loss:\t\t3.99080124019\n",
      "Done 11130 batches in 1926.62 sec.    training loss:\t\t3.99076150237\n",
      "Done 11140 batches in 1928.20 sec.    training loss:\t\t3.99064505588\n",
      "Done 11150 batches in 1929.89 sec.    training loss:\t\t3.99063529205\n",
      "Done 11160 batches in 1931.63 sec.    training loss:\t\t3.99056861409\n",
      "Done 11170 batches in 1933.52 sec.    training loss:\t\t3.99063246649\n",
      "Done 11180 batches in 1935.52 sec.    training loss:\t\t3.99063348843\n",
      "Done 11190 batches in 1937.24 sec.    training loss:\t\t3.99055610664\n",
      "Done 11200 batches in 1938.96 sec.    training loss:\t\t3.9904655306\n",
      "Done 11210 batches in 1940.42 sec.    training loss:\t\t3.9904180709\n",
      "Done 11220 batches in 1942.11 sec.    training loss:\t\t3.99041868684\n",
      "Done 11230 batches in 1943.78 sec.    training loss:\t\t3.99036130733\n",
      "Done 11240 batches in 1945.48 sec.    training loss:\t\t3.99024770474\n",
      "Done 11250 batches in 1947.49 sec.    training loss:\t\t3.9902415681\n",
      "Done 11260 batches in 1948.95 sec.    training loss:\t\t3.99016297473\n",
      "Done 11270 batches in 1950.92 sec.    training loss:\t\t3.99012115162\n",
      "Done 11280 batches in 1952.53 sec.    training loss:\t\t3.99005577511\n",
      "Done 11290 batches in 1953.94 sec.    training loss:\t\t3.98993451954\n",
      "Done 11300 batches in 1955.71 sec.    training loss:\t\t3.9899474428\n",
      "Done 11310 batches in 1957.61 sec.    training loss:\t\t3.98988877595\n",
      "Done 11320 batches in 1959.52 sec.    training loss:\t\t3.98988891937\n",
      "Done 11330 batches in 1961.24 sec.    training loss:\t\t3.98984005152\n",
      "Done 11340 batches in 1962.73 sec.    training loss:\t\t3.9897043977\n",
      "Done 11350 batches in 1964.27 sec.    training loss:\t\t3.98968768071\n",
      "Done 11360 batches in 1966.04 sec.    training loss:\t\t3.98957327109\n",
      "Done 11370 batches in 1967.87 sec.    training loss:\t\t3.98965508904\n",
      "Done 11380 batches in 1969.53 sec.    training loss:\t\t3.98961034668\n",
      "Done 11390 batches in 1971.41 sec.    training loss:\t\t3.98973004215\n",
      "Done 11400 batches in 1972.98 sec.    training loss:\t\t3.98971159931\n",
      "Done 11410 batches in 1974.90 sec.    training loss:\t\t3.98970874018\n",
      "Done 11420 batches in 1976.62 sec.    training loss:\t\t3.98968344885\n",
      "Done 11430 batches in 1978.41 sec.    training loss:\t\t3.98972767373\n",
      "Done 11440 batches in 1980.07 sec.    training loss:\t\t3.98967629294\n",
      "Done 11450 batches in 1981.61 sec.    training loss:\t\t3.98966549228\n",
      "Done 11460 batches in 1983.15 sec.    training loss:\t\t3.98955907514\n",
      "Done 11470 batches in 1984.62 sec.    training loss:\t\t3.98942373815\n",
      "Done 11480 batches in 1986.38 sec.    training loss:\t\t3.98934594636\n",
      "Done 11490 batches in 1988.16 sec.    training loss:\t\t3.98926995753\n",
      "Done 11500 batches in 1989.79 sec.    training loss:\t\t3.98928511056\n",
      "Done 11510 batches in 1991.57 sec.    training loss:\t\t3.9892958826\n",
      "Done 11520 batches in 1993.22 sec.    training loss:\t\t3.98928298058\n",
      "Done 11530 batches in 1995.13 sec.    training loss:\t\t3.98917432241\n",
      "Done 11540 batches in 1996.67 sec.    training loss:\t\t3.98916108608\n",
      "Done 11550 batches in 1998.40 sec.    training loss:\t\t3.9890864012\n",
      "Done 11560 batches in 1999.87 sec.    training loss:\t\t3.98902858766\n",
      "Done 11570 batches in 2001.46 sec.    training loss:\t\t3.98904318692\n",
      "Done 11580 batches in 2002.84 sec.    training loss:\t\t3.98897692169\n",
      "Done 11590 batches in 2004.75 sec.    training loss:\t\t3.98904754015\n",
      "Done 11600 batches in 2006.62 sec.    training loss:\t\t3.98902952046\n",
      "Done 11610 batches in 2008.93 sec.    training loss:\t\t3.98888195617\n",
      "Done 11620 batches in 2010.51 sec.    training loss:\t\t3.98883437277\n",
      "Done 11630 batches in 2012.67 sec.    training loss:\t\t3.98877587306\n",
      "Done 11640 batches in 2014.74 sec.    training loss:\t\t3.98874659141\n",
      "Done 11650 batches in 2016.33 sec.    training loss:\t\t3.98863467196\n",
      "Done 11660 batches in 2017.87 sec.    training loss:\t\t3.98863368345\n",
      "Done 11670 batches in 2019.31 sec.    training loss:\t\t3.98855037983\n",
      "Done 11680 batches in 2020.97 sec.    training loss:\t\t3.98853134401\n",
      "Done 11690 batches in 2022.55 sec.    training loss:\t\t3.98845291556\n",
      "Done 11700 batches in 2024.06 sec.    training loss:\t\t3.98838481465\n",
      "Done 11710 batches in 2026.34 sec.    training loss:\t\t3.98840232679\n",
      "Done 11720 batches in 2027.92 sec.    training loss:\t\t3.98842284824\n",
      "Done 11730 batches in 2029.58 sec.    training loss:\t\t3.98832039463\n",
      "Done 11740 batches in 2031.54 sec.    training loss:\t\t3.98834997592\n",
      "Done 11750 batches in 2033.64 sec.    training loss:\t\t3.98841757086\n",
      "Done 11760 batches in 2035.43 sec.    training loss:\t\t3.98838941218\n",
      "Done 11770 batches in 2037.18 sec.    training loss:\t\t3.98834544144\n",
      "Done 11780 batches in 2038.79 sec.    training loss:\t\t3.98833981552\n",
      "Done 11790 batches in 2040.23 sec.    training loss:\t\t3.98831243529\n",
      "Done 11800 batches in 2042.19 sec.    training loss:\t\t3.98829221572\n",
      "Done 11810 batches in 2043.81 sec.    training loss:\t\t3.98825641516\n",
      "Done 11820 batches in 2045.62 sec.    training loss:\t\t3.98829118376\n",
      "Done 11830 batches in 2047.44 sec.    training loss:\t\t3.98831928228\n",
      "Done 11840 batches in 2049.09 sec.    training loss:\t\t3.98831253314\n",
      "Done 11850 batches in 2050.85 sec.    training loss:\t\t3.98826240043\n",
      "Done 11860 batches in 2052.44 sec.    training loss:\t\t3.98820406586\n",
      "Done 11870 batches in 2053.85 sec.    training loss:\t\t3.98810612368\n",
      "Done 11880 batches in 2055.42 sec.    training loss:\t\t3.98807745267\n",
      "Done 11890 batches in 2057.05 sec.    training loss:\t\t3.98809028071\n",
      "Done 11900 batches in 2058.59 sec.    training loss:\t\t3.98808732996\n",
      "Done 11910 batches in 2060.26 sec.    training loss:\t\t3.98804357692\n",
      "Done 11920 batches in 2062.15 sec.    training loss:\t\t3.98798345369\n",
      "Done 11930 batches in 2063.75 sec.    training loss:\t\t3.98798015347\n",
      "Done 11940 batches in 2065.35 sec.    training loss:\t\t3.98791933737\n",
      "Done 11950 batches in 2067.24 sec.    training loss:\t\t3.98787161109\n",
      "Done 11960 batches in 2068.89 sec.    training loss:\t\t3.98793256448\n",
      "Done 11970 batches in 2070.92 sec.    training loss:\t\t3.98790739037\n",
      "Done 11980 batches in 2072.56 sec.    training loss:\t\t3.98781027199\n",
      "Done 11990 batches in 2074.40 sec.    training loss:\t\t3.98775235732\n",
      "Done 12000 batches in 2075.94 sec.    training loss:\t\t3.98769984831\n",
      "Done 12010 batches in 2077.64 sec.    training loss:\t\t3.98765822616\n",
      "Done 12020 batches in 2079.36 sec.    training loss:\t\t3.9876619311\n",
      "Done 12030 batches in 2081.00 sec.    training loss:\t\t3.98759373437\n",
      "Done 12040 batches in 2082.94 sec.    training loss:\t\t3.98757303632\n",
      "Done 12050 batches in 2084.63 sec.    training loss:\t\t3.98756268351\n",
      "Done 12060 batches in 2086.29 sec.    training loss:\t\t3.98761436564\n",
      "Done 12070 batches in 2088.27 sec.    training loss:\t\t3.98760292682\n",
      "Done 12080 batches in 2089.80 sec.    training loss:\t\t3.98759378955\n",
      "Done 12090 batches in 2091.79 sec.    training loss:\t\t3.98759626049\n",
      "Done 12100 batches in 2093.62 sec.    training loss:\t\t3.98755126862\n",
      "Done 12110 batches in 2095.44 sec.    training loss:\t\t3.98748262421\n",
      "Done 12120 batches in 2097.14 sec.    training loss:\t\t3.98744629704\n",
      "Done 12130 batches in 2098.75 sec.    training loss:\t\t3.98736168344\n",
      "Done 12140 batches in 2100.57 sec.    training loss:\t\t3.98726025066\n",
      "Done 12150 batches in 2102.62 sec.    training loss:\t\t3.98719630124\n",
      "Done 12160 batches in 2104.14 sec.    training loss:\t\t3.98715562464\n",
      "Done 12170 batches in 2105.99 sec.    training loss:\t\t3.98710145921\n",
      "Done 12180 batches in 2107.62 sec.    training loss:\t\t3.9871890163\n",
      "Done 12190 batches in 2109.71 sec.    training loss:\t\t3.9870795444\n",
      "Done 12200 batches in 2111.56 sec.    training loss:\t\t3.98712138627\n",
      "Done 12210 batches in 2113.14 sec.    training loss:\t\t3.98705812569\n",
      "Done 12220 batches in 2114.86 sec.    training loss:\t\t3.98712427969\n",
      "Done 12230 batches in 2116.41 sec.    training loss:\t\t3.98704262643\n",
      "Done 12240 batches in 2117.97 sec.    training loss:\t\t3.98697961668\n",
      "Done 12250 batches in 2119.83 sec.    training loss:\t\t3.98698761121\n",
      "Done 12260 batches in 2121.75 sec.    training loss:\t\t3.98694281965\n",
      "Done 12270 batches in 2123.14 sec.    training loss:\t\t3.9868883635\n",
      "Done 12280 batches in 2124.77 sec.    training loss:\t\t3.98686785712\n",
      "Done 12290 batches in 2126.80 sec.    training loss:\t\t3.98685440978\n",
      "Done 12300 batches in 2128.49 sec.    training loss:\t\t3.98675183052\n",
      "Done 12310 batches in 2130.57 sec.    training loss:\t\t3.98667945323\n",
      "Done 12320 batches in 2132.45 sec.    training loss:\t\t3.98662924604\n",
      "Done 12330 batches in 2133.93 sec.    training loss:\t\t3.98649305011\n",
      "Done 12340 batches in 2135.82 sec.    training loss:\t\t3.986412412\n",
      "Done 12350 batches in 2137.87 sec.    training loss:\t\t3.9864684609\n",
      "Done 12360 batches in 2139.53 sec.    training loss:\t\t3.98639590648\n",
      "Done 12370 batches in 2141.20 sec.    training loss:\t\t3.98641620141\n",
      "Done 12380 batches in 2142.92 sec.    training loss:\t\t3.98636526904\n",
      "Done 12390 batches in 2144.88 sec.    training loss:\t\t3.98639855448\n",
      "Done 12400 batches in 2146.26 sec.    training loss:\t\t3.98634942185\n",
      "Done 12410 batches in 2148.10 sec.    training loss:\t\t3.98645162048\n",
      "Done 12420 batches in 2150.03 sec.    training loss:\t\t3.9864081605\n",
      "Done 12430 batches in 2152.04 sec.    training loss:\t\t3.98639588636\n",
      "Done 12440 batches in 2153.97 sec.    training loss:\t\t3.98646201327\n",
      "Done 12450 batches in 2155.81 sec.    training loss:\t\t3.98643522322\n",
      "Done 12460 batches in 2157.15 sec.    training loss:\t\t3.98633357022\n",
      "Done 12470 batches in 2158.95 sec.    training loss:\t\t3.98635530826\n",
      "Done 12480 batches in 2160.83 sec.    training loss:\t\t3.98626544451\n",
      "Done 12490 batches in 2162.55 sec.    training loss:\t\t3.98619887556\n",
      "Done 12500 batches in 2163.95 sec.    training loss:\t\t3.98615743704\n",
      "Done 12510 batches in 2165.50 sec.    training loss:\t\t3.98610251276\n",
      "Done 12520 batches in 2167.22 sec.    training loss:\t\t3.98605788929\n",
      "Done 12530 batches in 2168.96 sec.    training loss:\t\t3.98599589385\n",
      "Done 12540 batches in 2170.82 sec.    training loss:\t\t3.98594363669\n",
      "Done 12550 batches in 2172.36 sec.    training loss:\t\t3.98584737512\n",
      "Done 12560 batches in 2174.13 sec.    training loss:\t\t3.98583978356\n",
      "Done 12570 batches in 2175.82 sec.    training loss:\t\t3.98591506549\n",
      "Done 12580 batches in 2177.36 sec.    training loss:\t\t3.98584347543\n",
      "Done 12590 batches in 2179.01 sec.    training loss:\t\t3.98575088788\n",
      "Done 12600 batches in 2180.74 sec.    training loss:\t\t3.98569015915\n",
      "Done 12610 batches in 2182.41 sec.    training loss:\t\t3.9856228141\n",
      "Done 12620 batches in 2184.08 sec.    training loss:\t\t3.98551815282\n",
      "Done 12630 batches in 2185.56 sec.    training loss:\t\t3.9854146782\n",
      "Done 12640 batches in 2187.23 sec.    training loss:\t\t3.98537968014\n",
      "Done 12650 batches in 2189.03 sec.    training loss:\t\t3.98527515566\n",
      "Done 12660 batches in 2191.05 sec.    training loss:\t\t3.9852874222\n",
      "Done 12670 batches in 2192.96 sec.    training loss:\t\t3.98523569073\n",
      "Done 12680 batches in 2194.54 sec.    training loss:\t\t3.98525352266\n",
      "Done 12690 batches in 2196.33 sec.    training loss:\t\t3.98522846674\n",
      "Done 12700 batches in 2198.21 sec.    training loss:\t\t3.985282622\n",
      "Done 12710 batches in 2199.84 sec.    training loss:\t\t3.98531053183\n",
      "Done 12720 batches in 2201.60 sec.    training loss:\t\t3.98527090544\n",
      "Done 12730 batches in 2203.20 sec.    training loss:\t\t3.98529389309\n",
      "Done 12740 batches in 2204.99 sec.    training loss:\t\t3.98532097044\n",
      "Done 12750 batches in 2206.63 sec.    training loss:\t\t3.98523495386\n",
      "Done 12760 batches in 2208.24 sec.    training loss:\t\t3.98519686898\n",
      "Done 12770 batches in 2210.22 sec.    training loss:\t\t3.98510069328\n",
      "Done 12780 batches in 2212.24 sec.    training loss:\t\t3.98504162894\n",
      "Done 12790 batches in 2213.91 sec.    training loss:\t\t3.98506581002\n",
      "Done 12800 batches in 2215.80 sec.    training loss:\t\t3.98505523313\n",
      "Done 12810 batches in 2217.47 sec.    training loss:\t\t3.98499225051\n",
      "Done 12820 batches in 2219.60 sec.    training loss:\t\t3.98489004188\n",
      "Done 12830 batches in 2221.51 sec.    training loss:\t\t3.98491656758\n",
      "Done 12840 batches in 2223.26 sec.    training loss:\t\t3.98489839073\n",
      "Done 12850 batches in 2224.92 sec.    training loss:\t\t3.98482552896\n",
      "Done 12860 batches in 2226.71 sec.    training loss:\t\t3.98473148924\n",
      "Done 12870 batches in 2228.61 sec.    training loss:\t\t3.98465441622\n",
      "Done 12880 batches in 2230.22 sec.    training loss:\t\t3.98452467361\n",
      "Done 12890 batches in 2232.25 sec.    training loss:\t\t3.98448633234\n",
      "Done 12900 batches in 2233.85 sec.    training loss:\t\t3.9844540384\n",
      "Done 12910 batches in 2235.58 sec.    training loss:\t\t3.98435953999\n",
      "Done 12920 batches in 2236.97 sec.    training loss:\t\t3.98431086653\n",
      "Done 12930 batches in 2238.63 sec.    training loss:\t\t3.98420668526\n",
      "Done 12940 batches in 2240.46 sec.    training loss:\t\t3.98412464494\n",
      "Done 12950 batches in 2242.12 sec.    training loss:\t\t3.98411642012\n",
      "Done 12960 batches in 2243.69 sec.    training loss:\t\t3.98396028479\n",
      "Done 12970 batches in 2245.21 sec.    training loss:\t\t3.98392903035\n",
      "Done 12980 batches in 2246.61 sec.    training loss:\t\t3.98393345671\n",
      "Done 12990 batches in 2248.09 sec.    training loss:\t\t3.98391859205\n",
      "Done 13000 batches in 2249.86 sec.    training loss:\t\t3.98397196364\n",
      "Done 13010 batches in 2251.50 sec.    training loss:\t\t3.98394125613\n",
      "Done 13020 batches in 2253.52 sec.    training loss:\t\t3.98391224237\n",
      "Done 13030 batches in 2255.22 sec.    training loss:\t\t3.98388112653\n",
      "Done 13040 batches in 2256.80 sec.    training loss:\t\t3.98384674308\n",
      "Done 13050 batches in 2258.64 sec.    training loss:\t\t3.98377646362\n",
      "Done 13060 batches in 2261.03 sec.    training loss:\t\t3.98384796799\n",
      "Done 13070 batches in 2262.87 sec.    training loss:\t\t3.98388550311\n",
      "Done 13080 batches in 2264.65 sec.    training loss:\t\t3.98384848924\n",
      "Done 13090 batches in 2266.58 sec.    training loss:\t\t3.98381026837\n",
      "Done 13100 batches in 2268.43 sec.    training loss:\t\t3.98370119877\n",
      "Done 13110 batches in 2269.96 sec.    training loss:\t\t3.98361816801\n",
      "Done 13120 batches in 2271.80 sec.    training loss:\t\t3.98368806565\n",
      "Done 13130 batches in 2273.57 sec.    training loss:\t\t3.98363595741\n",
      "Done 13140 batches in 2275.21 sec.    training loss:\t\t3.98363287503\n",
      "Done 13150 batches in 2277.29 sec.    training loss:\t\t3.98359742083\n",
      "Done 13160 batches in 2278.87 sec.    training loss:\t\t3.9834853023\n",
      "Done 13170 batches in 2280.33 sec.    training loss:\t\t3.98337236546\n",
      "Done 13180 batches in 2281.99 sec.    training loss:\t\t3.98333829401\n",
      "Done 13190 batches in 2283.56 sec.    training loss:\t\t3.98326320068\n",
      "Done 13200 batches in 2285.17 sec.    training loss:\t\t3.98328752727\n",
      "Done 13210 batches in 2286.89 sec.    training loss:\t\t3.9832146071\n",
      "Done 13220 batches in 2288.36 sec.    training loss:\t\t3.98310456848\n",
      "Done 13230 batches in 2290.19 sec.    training loss:\t\t3.98309183784\n",
      "Done 13240 batches in 2292.59 sec.    training loss:\t\t3.98312947768\n",
      "Done 13250 batches in 2294.86 sec.    training loss:\t\t3.98317063816\n",
      "Done 13260 batches in 2296.81 sec.    training loss:\t\t3.98314100966\n",
      "Done 13270 batches in 2298.62 sec.    training loss:\t\t3.9831719182\n",
      "Done 13280 batches in 2300.20 sec.    training loss:\t\t3.9830399582\n",
      "Done 13290 batches in 2302.03 sec.    training loss:\t\t3.98300481713\n",
      "Done 13300 batches in 2304.09 sec.    training loss:\t\t3.98302072627\n",
      "Done 13310 batches in 2305.71 sec.    training loss:\t\t3.98299477168\n",
      "Done 13320 batches in 2307.32 sec.    training loss:\t\t3.98292876341\n",
      "Done 13330 batches in 2309.03 sec.    training loss:\t\t3.98294213537\n",
      "Done 13340 batches in 2310.87 sec.    training loss:\t\t3.98290957427\n",
      "Done 13350 batches in 2312.57 sec.    training loss:\t\t3.98298515057\n",
      "Done 13360 batches in 2314.47 sec.    training loss:\t\t3.98294338413\n",
      "Done 13370 batches in 2316.25 sec.    training loss:\t\t3.982920757\n",
      "Done 13380 batches in 2317.99 sec.    training loss:\t\t3.98288055247\n",
      "Done 13390 batches in 2319.81 sec.    training loss:\t\t3.98278808612\n",
      "Done 13400 batches in 2321.63 sec.    training loss:\t\t3.98278411545\n",
      "Done 13410 batches in 2323.24 sec.    training loss:\t\t3.98273821398\n",
      "Done 13420 batches in 2325.06 sec.    training loss:\t\t3.98272556777\n",
      "Done 13430 batches in 2326.86 sec.    training loss:\t\t3.98267966279\n",
      "Done 13440 batches in 2328.52 sec.    training loss:\t\t3.98263907692\n",
      "Done 13450 batches in 2330.33 sec.    training loss:\t\t3.98259234061\n",
      "Done 13460 batches in 2331.93 sec.    training loss:\t\t3.98253227162\n",
      "Done 13470 batches in 2333.74 sec.    training loss:\t\t3.98247188741\n",
      "Done 13480 batches in 2335.28 sec.    training loss:\t\t3.98240200106\n",
      "Done 13490 batches in 2337.45 sec.    training loss:\t\t3.98248798983\n",
      "Done 13500 batches in 2339.04 sec.    training loss:\t\t3.98244433288\n",
      "Done 13510 batches in 2340.75 sec.    training loss:\t\t3.98236761635\n",
      "Done 13520 batches in 2342.45 sec.    training loss:\t\t3.98237328563\n",
      "Done 13530 batches in 2344.22 sec.    training loss:\t\t3.98232490816\n",
      "Done 13540 batches in 2345.79 sec.    training loss:\t\t3.98224708677\n",
      "Done 13550 batches in 2347.46 sec.    training loss:\t\t3.98211419996\n",
      "Done 13560 batches in 2349.26 sec.    training loss:\t\t3.98207445716\n",
      "Done 13570 batches in 2351.37 sec.    training loss:\t\t3.98212518446\n",
      "Done 13580 batches in 2352.99 sec.    training loss:\t\t3.98211210699\n",
      "Done 13590 batches in 2355.05 sec.    training loss:\t\t3.98211661613\n",
      "Done 13600 batches in 2357.19 sec.    training loss:\t\t3.98206333173\n",
      "Done 13610 batches in 2358.96 sec.    training loss:\t\t3.98201106779\n",
      "Done 13620 batches in 2360.64 sec.    training loss:\t\t3.98193629637\n",
      "Done 13630 batches in 2362.72 sec.    training loss:\t\t3.98198263874\n",
      "Done 13640 batches in 2364.50 sec.    training loss:\t\t3.98195576252\n",
      "Done 13650 batches in 2366.51 sec.    training loss:\t\t3.98198137622\n",
      "Done 13660 batches in 2368.25 sec.    training loss:\t\t3.98191259035\n",
      "Done 13670 batches in 2370.23 sec.    training loss:\t\t3.98188226272\n",
      "Done 13680 batches in 2372.53 sec.    training loss:\t\t3.98193551413\n",
      "Done 13690 batches in 2374.33 sec.    training loss:\t\t3.98192122707\n",
      "Done 13700 batches in 2375.92 sec.    training loss:\t\t3.98187911584\n",
      "Done 13710 batches in 2377.82 sec.    training loss:\t\t3.98187351048\n",
      "Done 13720 batches in 2379.68 sec.    training loss:\t\t3.98197121601\n",
      "Done 13730 batches in 2381.31 sec.    training loss:\t\t3.98197088132\n",
      "Done 13740 batches in 2383.30 sec.    training loss:\t\t3.98195857356\n",
      "Done 13750 batches in 2384.96 sec.    training loss:\t\t3.98190558049\n",
      "Done 13760 batches in 2386.69 sec.    training loss:\t\t3.98188102957\n",
      "Done 13770 batches in 2388.24 sec.    training loss:\t\t3.98175173633\n",
      "Done 13780 batches in 2389.99 sec.    training loss:\t\t3.981748744\n",
      "Done 13790 batches in 2391.86 sec.    training loss:\t\t3.98173196141\n",
      "Done 13800 batches in 2393.28 sec.    training loss:\t\t3.98168044161\n",
      "Done 13810 batches in 2394.99 sec.    training loss:\t\t3.98160186596\n",
      "Done 13820 batches in 2396.43 sec.    training loss:\t\t3.98153705447\n",
      "Done 13830 batches in 2398.41 sec.    training loss:\t\t3.98156210576\n",
      "Done 13840 batches in 2400.23 sec.    training loss:\t\t3.98158051134\n",
      "Done 13850 batches in 2402.18 sec.    training loss:\t\t3.98159760709\n",
      "Done 13860 batches in 2404.10 sec.    training loss:\t\t3.98163432977\n",
      "Done 13870 batches in 2405.79 sec.    training loss:\t\t3.98166079856\n",
      "Done 13880 batches in 2407.41 sec.    training loss:\t\t3.98167701125\n",
      "Done 13890 batches in 2408.96 sec.    training loss:\t\t3.98162723384\n",
      "Done 13900 batches in 2410.55 sec.    training loss:\t\t3.9815761556\n",
      "Done 13910 batches in 2412.38 sec.    training loss:\t\t3.98152181996\n",
      "Done 13920 batches in 2414.08 sec.    training loss:\t\t3.98140884936\n",
      "Done 13930 batches in 2415.74 sec.    training loss:\t\t3.98132562114\n",
      "Done 13940 batches in 2417.37 sec.    training loss:\t\t3.98129538195\n",
      "Done 13950 batches in 2419.67 sec.    training loss:\t\t3.98130225947\n",
      "Done 13960 batches in 2422.03 sec.    training loss:\t\t3.98136884457\n",
      "Done 13970 batches in 2423.44 sec.    training loss:\t\t3.98127859888\n",
      "Done 13980 batches in 2425.09 sec.    training loss:\t\t3.98120480607\n",
      "Done 13990 batches in 2426.67 sec.    training loss:\t\t3.98116287064\n",
      "Done 14000 batches in 2428.42 sec.    training loss:\t\t3.98115822729\n",
      "Done 14010 batches in 2430.18 sec.    training loss:\t\t3.98110157257\n",
      "Done 14020 batches in 2431.88 sec.    training loss:\t\t3.98106564868\n",
      "Done 14030 batches in 2433.70 sec.    training loss:\t\t3.98109420083\n",
      "Done 14040 batches in 2435.31 sec.    training loss:\t\t3.98105894477\n",
      "Done 14050 batches in 2437.47 sec.    training loss:\t\t3.98106856029\n",
      "Done 14060 batches in 2439.06 sec.    training loss:\t\t3.98108124163\n",
      "Done 14070 batches in 2440.68 sec.    training loss:\t\t3.98107454575\n",
      "Done 14080 batches in 2442.30 sec.    training loss:\t\t3.98099892454\n",
      "Done 14090 batches in 2443.95 sec.    training loss:\t\t3.98093761711\n",
      "Done 14100 batches in 2445.77 sec.    training loss:\t\t3.98085138167\n",
      "Done 14110 batches in 2447.19 sec.    training loss:\t\t3.98079492644\n",
      "Done 14120 batches in 2448.95 sec.    training loss:\t\t3.98082937375\n",
      "Done 14130 batches in 2450.63 sec.    training loss:\t\t3.98076743236\n",
      "Done 14140 batches in 2452.15 sec.    training loss:\t\t3.98080127076\n",
      "Done 14150 batches in 2453.92 sec.    training loss:\t\t3.9807841844\n",
      "Done 14160 batches in 2455.76 sec.    training loss:\t\t3.98079708187\n",
      "Done 14170 batches in 2457.90 sec.    training loss:\t\t3.98078520126\n",
      "Done 14180 batches in 2459.60 sec.    training loss:\t\t3.98084086957\n",
      "Done 14190 batches in 2461.12 sec.    training loss:\t\t3.98085996728\n",
      "Done 14200 batches in 2462.91 sec.    training loss:\t\t3.9808358683\n",
      "Done 14210 batches in 2464.45 sec.    training loss:\t\t3.98077051416\n",
      "Done 14220 batches in 2466.11 sec.    training loss:\t\t3.98078554964\n",
      "Done 14230 batches in 2467.71 sec.    training loss:\t\t3.98067192594\n",
      "Done 14240 batches in 2469.26 sec.    training loss:\t\t3.98066663161\n",
      "Done 14250 batches in 2470.86 sec.    training loss:\t\t3.98059278267\n",
      "Done 14260 batches in 2472.40 sec.    training loss:\t\t3.98058053594\n",
      "Done 14270 batches in 2474.00 sec.    training loss:\t\t3.98045009728\n",
      "Done 14280 batches in 2475.57 sec.    training loss:\t\t3.98040998433\n",
      "Done 14290 batches in 2477.38 sec.    training loss:\t\t3.98036510864\n",
      "Done 14300 batches in 2479.06 sec.    training loss:\t\t3.98034604211\n",
      "Done 14310 batches in 2480.79 sec.    training loss:\t\t3.98032903396\n",
      "Done 14320 batches in 2482.35 sec.    training loss:\t\t3.980304686\n",
      "Done 14330 batches in 2484.12 sec.    training loss:\t\t3.98028004747\n",
      "Done 14340 batches in 2485.44 sec.    training loss:\t\t3.98024506913\n",
      "Done 14350 batches in 2487.22 sec.    training loss:\t\t3.98018847884\n",
      "Done 14360 batches in 2488.87 sec.    training loss:\t\t3.98013605436\n",
      "Done 14370 batches in 2490.36 sec.    training loss:\t\t3.98011947652\n",
      "Done 14380 batches in 2492.06 sec.    training loss:\t\t3.98010374531\n",
      "Done 14390 batches in 2493.82 sec.    training loss:\t\t3.98001861395\n",
      "Done 14400 batches in 2495.51 sec.    training loss:\t\t3.98003168518\n",
      "Done 14410 batches in 2497.18 sec.    training loss:\t\t3.98000226482\n",
      "Done 14420 batches in 2498.65 sec.    training loss:\t\t3.97988198389\n",
      "Done 14430 batches in 2500.44 sec.    training loss:\t\t3.97991198495\n",
      "Done 14440 batches in 2502.03 sec.    training loss:\t\t3.97983291782\n",
      "Done 14450 batches in 2503.93 sec.    training loss:\t\t3.97983190306\n",
      "Done 14460 batches in 2505.82 sec.    training loss:\t\t3.9797690517\n",
      "Done 14470 batches in 2507.58 sec.    training loss:\t\t3.97983597435\n",
      "Done 14480 batches in 2509.07 sec.    training loss:\t\t3.97980626056\n",
      "Done 14490 batches in 2510.80 sec.    training loss:\t\t3.97973399429\n",
      "Done 14500 batches in 2512.41 sec.    training loss:\t\t3.97972370832\n",
      "Done 14510 batches in 2513.97 sec.    training loss:\t\t3.97957242199\n",
      "Done 14520 batches in 2515.99 sec.    training loss:\t\t3.97948580122\n",
      "Done 14530 batches in 2517.54 sec.    training loss:\t\t3.97944561926\n",
      "Done 14540 batches in 2518.95 sec.    training loss:\t\t3.97940962239\n",
      "Done 14550 batches in 2520.75 sec.    training loss:\t\t3.97938666516\n",
      "Done 14560 batches in 2522.46 sec.    training loss:\t\t3.97938253457\n",
      "Done 14570 batches in 2524.64 sec.    training loss:\t\t3.97940870796\n",
      "Done 14580 batches in 2526.36 sec.    training loss:\t\t3.97935399393\n",
      "Done 14590 batches in 2528.30 sec.    training loss:\t\t3.97930164215\n",
      "Done 14600 batches in 2529.90 sec.    training loss:\t\t3.97924071968\n",
      "Done 14610 batches in 2531.29 sec.    training loss:\t\t3.97916525957\n",
      "Done 14620 batches in 2532.81 sec.    training loss:\t\t3.97909529979\n",
      "Done 14630 batches in 2534.66 sec.    training loss:\t\t3.97902896877\n",
      "Done 14640 batches in 2536.30 sec.    training loss:\t\t3.9789817999\n",
      "Done 14650 batches in 2537.71 sec.    training loss:\t\t3.97894325637\n",
      "Done 14660 batches in 2539.09 sec.    training loss:\t\t3.97889089664\n",
      "Done 14670 batches in 2540.45 sec.    training loss:\t\t3.97884564058\n",
      "Done 14680 batches in 2542.24 sec.    training loss:\t\t3.97875690619\n",
      "Done 14690 batches in 2543.91 sec.    training loss:\t\t3.97874599384\n",
      "Done 14700 batches in 2546.14 sec.    training loss:\t\t3.97878742925\n",
      "Done 14710 batches in 2548.29 sec.    training loss:\t\t3.97874801067\n",
      "Done 14720 batches in 2550.09 sec.    training loss:\t\t3.97870384599\n",
      "Done 14730 batches in 2551.47 sec.    training loss:\t\t3.97865699888\n",
      "Done 14740 batches in 2553.15 sec.    training loss:\t\t3.9785925892\n",
      "Done 14750 batches in 2554.89 sec.    training loss:\t\t3.97854533144\n",
      "Done 14760 batches in 2556.59 sec.    training loss:\t\t3.97852421411\n",
      "Done 14770 batches in 2558.18 sec.    training loss:\t\t3.97838168088\n",
      "Done 14780 batches in 2559.57 sec.    training loss:\t\t3.97835624026\n",
      "Done 14790 batches in 2561.26 sec.    training loss:\t\t3.97835153989\n",
      "Done 14800 batches in 2563.00 sec.    training loss:\t\t3.97839967695\n",
      "Done 14810 batches in 2564.59 sec.    training loss:\t\t3.97841076616\n",
      "Done 14820 batches in 2566.38 sec.    training loss:\t\t3.97838400565\n",
      "Done 14830 batches in 2568.00 sec.    training loss:\t\t3.9782586131\n",
      "Done 14840 batches in 2569.73 sec.    training loss:\t\t3.97816407062\n",
      "Done 14850 batches in 2571.22 sec.    training loss:\t\t3.97812476478\n",
      "Done 14860 batches in 2572.73 sec.    training loss:\t\t3.97806671516\n",
      "Done 14870 batches in 2574.41 sec.    training loss:\t\t3.97812603908\n",
      "Done 14880 batches in 2576.10 sec.    training loss:\t\t3.97807337038\n",
      "Done 14890 batches in 2577.71 sec.    training loss:\t\t3.97800560576\n",
      "Done 14900 batches in 2579.73 sec.    training loss:\t\t3.97801958974\n",
      "Done 14910 batches in 2581.62 sec.    training loss:\t\t3.97804369034\n",
      "Done 14920 batches in 2583.12 sec.    training loss:\t\t3.97803190367\n",
      "Done 14930 batches in 2584.73 sec.    training loss:\t\t3.97797170422\n",
      "Done 14940 batches in 2586.14 sec.    training loss:\t\t3.9779699711\n",
      "Done 14950 batches in 2587.66 sec.    training loss:\t\t3.97794194076\n",
      "Done 14960 batches in 2589.35 sec.    training loss:\t\t3.97792988381\n",
      "Done 14970 batches in 2590.94 sec.    training loss:\t\t3.97788215469\n",
      "Done 14980 batches in 2592.54 sec.    training loss:\t\t3.97785567728\n",
      "Done 14990 batches in 2594.37 sec.    training loss:\t\t3.97779225212\n",
      "Done 15000 batches in 2596.59 sec.    training loss:\t\t3.97776208816\n",
      "Done 15010 batches in 2598.36 sec.    training loss:\t\t3.97773843175\n",
      "Done 15020 batches in 2600.06 sec.    training loss:\t\t3.97779226773\n",
      "Done 15030 batches in 2601.84 sec.    training loss:\t\t3.97778861399\n",
      "Done 15040 batches in 2603.61 sec.    training loss:\t\t3.97777728221\n",
      "Done 15050 batches in 2605.41 sec.    training loss:\t\t3.97771066295\n",
      "Done 15060 batches in 2607.11 sec.    training loss:\t\t3.97768736551\n",
      "Done 15070 batches in 2608.68 sec.    training loss:\t\t3.97761177097\n",
      "Done 15080 batches in 2610.19 sec.    training loss:\t\t3.9775518411\n",
      "Done 15090 batches in 2611.77 sec.    training loss:\t\t3.97741668607\n",
      "Done 15100 batches in 2613.25 sec.    training loss:\t\t3.97738557353\n",
      "Done 15110 batches in 2614.88 sec.    training loss:\t\t3.97738588571\n",
      "Done 15120 batches in 2616.64 sec.    training loss:\t\t3.97739562127\n",
      "Done 15130 batches in 2618.81 sec.    training loss:\t\t3.97743706628\n",
      "Done 15140 batches in 2620.35 sec.    training loss:\t\t3.97744237342\n",
      "Done 15150 batches in 2622.07 sec.    training loss:\t\t3.977394724\n",
      "Done 15160 batches in 2624.61 sec.    training loss:\t\t3.97750509751\n",
      "Done 15170 batches in 2626.30 sec.    training loss:\t\t3.97751355034\n",
      "Done 15180 batches in 2627.85 sec.    training loss:\t\t3.97750015099\n",
      "Done 15190 batches in 2629.96 sec.    training loss:\t\t3.97744305382\n",
      "Done 15200 batches in 2631.54 sec.    training loss:\t\t3.97747749773\n",
      "Done 15210 batches in 2633.31 sec.    training loss:\t\t3.97741410983\n",
      "Done 15220 batches in 2635.34 sec.    training loss:\t\t3.97738516177\n",
      "Done 15230 batches in 2636.84 sec.    training loss:\t\t3.9773826735\n",
      "Done 15240 batches in 2638.28 sec.    training loss:\t\t3.97733081895\n",
      "Done 15250 batches in 2640.22 sec.    training loss:\t\t3.9773618417\n",
      "Done 15260 batches in 2641.70 sec.    training loss:\t\t3.97722406295\n",
      "Done 15270 batches in 2643.69 sec.    training loss:\t\t3.97727207165\n",
      "Done 15280 batches in 2645.41 sec.    training loss:\t\t3.97721298384\n",
      "Done 15290 batches in 2646.99 sec.    training loss:\t\t3.97723593936\n",
      "Done 15300 batches in 2648.59 sec.    training loss:\t\t3.97717256827\n",
      "Done 15310 batches in 2650.94 sec.    training loss:\t\t3.97716755591\n",
      "Done 15320 batches in 2652.44 sec.    training loss:\t\t3.97712901957\n",
      "Done 15330 batches in 2654.24 sec.    training loss:\t\t3.97708570872\n",
      "Done 15340 batches in 2656.07 sec.    training loss:\t\t3.97703834382\n",
      "Done 15350 batches in 2658.03 sec.    training loss:\t\t3.97700807739\n",
      "Done 15360 batches in 2659.68 sec.    training loss:\t\t3.97698278075\n",
      "Done 15370 batches in 2661.35 sec.    training loss:\t\t3.97693605888\n",
      "Done 15380 batches in 2662.88 sec.    training loss:\t\t3.97687074844\n",
      "Done 15390 batches in 2664.57 sec.    training loss:\t\t3.97686924279\n",
      "Done 15400 batches in 2666.21 sec.    training loss:\t\t3.97680916402\n",
      "Done 15410 batches in 2667.73 sec.    training loss:\t\t3.97678861145\n",
      "Done 15420 batches in 2669.34 sec.    training loss:\t\t3.97675666266\n",
      "Done 15430 batches in 2671.27 sec.    training loss:\t\t3.97676204872\n",
      "Done 15440 batches in 2672.90 sec.    training loss:\t\t3.97673058067\n",
      "Done 15450 batches in 2674.51 sec.    training loss:\t\t3.97673776835\n",
      "Done 15460 batches in 2676.05 sec.    training loss:\t\t3.97671345659\n",
      "Done 15470 batches in 2677.49 sec.    training loss:\t\t3.97667405376\n",
      "Done 15480 batches in 2679.06 sec.    training loss:\t\t3.97666867867\n",
      "Done 15490 batches in 2680.94 sec.    training loss:\t\t3.97673906198\n",
      "Done 15500 batches in 2683.00 sec.    training loss:\t\t3.97678340381\n",
      "Done 15510 batches in 2684.99 sec.    training loss:\t\t3.97676201116\n",
      "Done 15520 batches in 2686.75 sec.    training loss:\t\t3.97671330275\n",
      "Done 15530 batches in 2688.31 sec.    training loss:\t\t3.97663867365\n",
      "Done 15540 batches in 2689.69 sec.    training loss:\t\t3.97656720429\n",
      "Done 15550 batches in 2691.03 sec.    training loss:\t\t3.97649033528\n",
      "Done 15560 batches in 2692.94 sec.    training loss:\t\t3.97648906723\n",
      "Done 15570 batches in 2694.61 sec.    training loss:\t\t3.97651961687\n",
      "Done 15580 batches in 2696.96 sec.    training loss:\t\t3.97653258695\n",
      "Done 15590 batches in 2698.42 sec.    training loss:\t\t3.97647514515\n",
      "Done 15600 batches in 2700.24 sec.    training loss:\t\t3.97644232667\n",
      "Done 15610 batches in 2701.85 sec.    training loss:\t\t3.97634867275\n",
      "Done 15620 batches in 2704.22 sec.    training loss:\t\t3.97636838648\n",
      "Done 15630 batches in 2705.76 sec.    training loss:\t\t3.97638191722\n",
      "Done 15640 batches in 2707.52 sec.    training loss:\t\t3.97633577686\n",
      "Done 15650 batches in 2709.22 sec.    training loss:\t\t3.97629311645\n",
      "Done 15660 batches in 2711.02 sec.    training loss:\t\t3.97629170241\n",
      "Done 15670 batches in 2712.48 sec.    training loss:\t\t3.97617676377\n",
      "Done 15680 batches in 2714.91 sec.    training loss:\t\t3.97622599532\n",
      "Done 15690 batches in 2716.49 sec.    training loss:\t\t3.97620427186\n",
      "Done 15700 batches in 2718.28 sec.    training loss:\t\t3.97617106883\n",
      "Done 15710 batches in 2719.83 sec.    training loss:\t\t3.9760957567\n",
      "Done 15720 batches in 2721.48 sec.    training loss:\t\t3.97605853625\n",
      "Done 15730 batches in 2723.27 sec.    training loss:\t\t3.97603192148\n",
      "Done 15740 batches in 2725.12 sec.    training loss:\t\t3.97602843572\n",
      "Done 15750 batches in 2726.71 sec.    training loss:\t\t3.97594316115\n",
      "Done 15760 batches in 2728.35 sec.    training loss:\t\t3.97584613813\n",
      "Done 15770 batches in 2730.11 sec.    training loss:\t\t3.9757903233\n",
      "Done 15780 batches in 2731.85 sec.    training loss:\t\t3.97577683903\n",
      "Done 15790 batches in 2733.89 sec.    training loss:\t\t3.97581205575\n",
      "Done 15800 batches in 2735.88 sec.    training loss:\t\t3.97581722676\n",
      "Done 15810 batches in 2737.59 sec.    training loss:\t\t3.97581572929\n",
      "Done 15820 batches in 2739.36 sec.    training loss:\t\t3.97585802244\n",
      "Done 15830 batches in 2740.74 sec.    training loss:\t\t3.97583616911\n",
      "Done 15840 batches in 2742.44 sec.    training loss:\t\t3.97579050644\n",
      "Done 15850 batches in 2743.80 sec.    training loss:\t\t3.97579746239\n",
      "Done 15860 batches in 2745.97 sec.    training loss:\t\t3.9758240843\n",
      "Done 15870 batches in 2747.59 sec.    training loss:\t\t3.97581786315\n",
      "Done 15880 batches in 2749.41 sec.    training loss:\t\t3.97581948176\n",
      "Done 15890 batches in 2751.12 sec.    training loss:\t\t3.97580024497\n",
      "Done 15900 batches in 2752.64 sec.    training loss:\t\t3.97575305718\n",
      "Done 15910 batches in 2755.04 sec.    training loss:\t\t3.97574938899\n",
      "Done 15920 batches in 2757.16 sec.    training loss:\t\t3.97576096176\n",
      "Done 15930 batches in 2758.70 sec.    training loss:\t\t3.97571451838\n",
      "Done 15940 batches in 2760.67 sec.    training loss:\t\t3.97569415369\n",
      "Done 15950 batches in 2762.54 sec.    training loss:\t\t3.97564503535\n",
      "Done 15960 batches in 2763.96 sec.    training loss:\t\t3.97553892068\n",
      "Done 15970 batches in 2765.44 sec.    training loss:\t\t3.97541148904\n",
      "Done 15980 batches in 2767.52 sec.    training loss:\t\t3.97538062157\n",
      "Done 15990 batches in 2769.25 sec.    training loss:\t\t3.97530075596\n",
      "Done 16000 batches in 2770.92 sec.    training loss:\t\t3.9752633252\n",
      "Done 16010 batches in 2772.99 sec.    training loss:\t\t3.97527966681\n",
      "Done 16020 batches in 2774.58 sec.    training loss:\t\t3.9752070547\n",
      "Done 16030 batches in 2776.51 sec.    training loss:\t\t3.975136034\n",
      "Done 16040 batches in 2778.25 sec.    training loss:\t\t3.975143361\n",
      "Done 16050 batches in 2779.74 sec.    training loss:\t\t3.97507067887\n",
      "Done 16060 batches in 2781.49 sec.    training loss:\t\t3.97507943835\n",
      "Done 16070 batches in 2783.18 sec.    training loss:\t\t3.97509386488\n",
      "Done 16080 batches in 2785.06 sec.    training loss:\t\t3.97506741236\n",
      "Done 16090 batches in 2786.66 sec.    training loss:\t\t3.97510141171\n",
      "Done 16100 batches in 2788.44 sec.    training loss:\t\t3.97513020385\n",
      "Done 16110 batches in 2790.32 sec.    training loss:\t\t3.97513265457\n",
      "Done 16120 batches in 2792.03 sec.    training loss:\t\t3.97502830916\n",
      "Done 16130 batches in 2793.70 sec.    training loss:\t\t3.9749862912\n",
      "Done 16140 batches in 2795.45 sec.    training loss:\t\t3.97489141121\n",
      "Done 16150 batches in 2797.40 sec.    training loss:\t\t3.97483578198\n",
      "Done 16160 batches in 2798.76 sec.    training loss:\t\t3.97475320512\n",
      "Done 16170 batches in 2801.13 sec.    training loss:\t\t3.97473273459\n",
      "Done 16180 batches in 2803.09 sec.    training loss:\t\t3.97469541994\n",
      "Done 16190 batches in 2804.77 sec.    training loss:\t\t3.97469590062\n",
      "Done 16200 batches in 2806.39 sec.    training loss:\t\t3.97466491121\n",
      "Done 16210 batches in 2808.33 sec.    training loss:\t\t3.97469873115\n",
      "Done 16220 batches in 2809.85 sec.    training loss:\t\t3.9746370117\n",
      "Done 16230 batches in 2811.53 sec.    training loss:\t\t3.97462148835\n",
      "Done 16240 batches in 2813.32 sec.    training loss:\t\t3.9746210835\n",
      "Done 16250 batches in 2814.95 sec.    training loss:\t\t3.97463494754\n",
      "Done 16260 batches in 2816.61 sec.    training loss:\t\t3.9746121279\n",
      "Done 16270 batches in 2818.41 sec.    training loss:\t\t3.97459045517\n",
      "Done 16280 batches in 2819.98 sec.    training loss:\t\t3.9745813223\n",
      "Done 16290 batches in 2821.68 sec.    training loss:\t\t3.97458897363\n",
      "Done 16300 batches in 2823.36 sec.    training loss:\t\t3.97457360243\n",
      "Done 16310 batches in 2825.30 sec.    training loss:\t\t3.97463037601\n",
      "Done 16320 batches in 2827.43 sec.    training loss:\t\t3.97465416453\n",
      "Done 16330 batches in 2828.93 sec.    training loss:\t\t3.97456129419\n",
      "Done 16340 batches in 2830.74 sec.    training loss:\t\t3.97449398042\n",
      "Done 16350 batches in 2832.74 sec.    training loss:\t\t3.97450868125\n",
      "Done 16360 batches in 2834.63 sec.    training loss:\t\t3.97451259581\n",
      "Done 16370 batches in 2836.53 sec.    training loss:\t\t3.97445240347\n",
      "Done 16380 batches in 2838.25 sec.    training loss:\t\t3.97443487261\n",
      "Done 16390 batches in 2839.92 sec.    training loss:\t\t3.9744101996\n",
      "Done 16400 batches in 2841.31 sec.    training loss:\t\t3.97428431203\n",
      "Done 16410 batches in 2843.36 sec.    training loss:\t\t3.97428788529\n",
      "Done 16420 batches in 2844.82 sec.    training loss:\t\t3.97414365384\n",
      "Done 16430 batches in 2846.79 sec.    training loss:\t\t3.97416316308\n",
      "Done 16440 batches in 2848.38 sec.    training loss:\t\t3.97418870734\n",
      "Done 16450 batches in 2850.24 sec.    training loss:\t\t3.97415555222\n",
      "Done 16460 batches in 2851.87 sec.    training loss:\t\t3.9741365108\n",
      "Done 16470 batches in 2853.77 sec.    training loss:\t\t3.97411082877\n",
      "Done 16480 batches in 2855.80 sec.    training loss:\t\t3.97403621386\n",
      "Done 16490 batches in 2857.50 sec.    training loss:\t\t3.97403755986\n",
      "Done 16500 batches in 2859.11 sec.    training loss:\t\t3.97396052246\n",
      "Done 16510 batches in 2860.66 sec.    training loss:\t\t3.97393911107\n",
      "Done 16520 batches in 2862.48 sec.    training loss:\t\t3.97392632064\n",
      "Done 16530 batches in 2864.20 sec.    training loss:\t\t3.97384816533\n",
      "Done 16540 batches in 2865.88 sec.    training loss:\t\t3.97385580687\n",
      "Done 16550 batches in 2867.68 sec.    training loss:\t\t3.97379280988\n",
      "Done 16560 batches in 2869.16 sec.    training loss:\t\t3.9737272277\n",
      "Done 16570 batches in 2870.68 sec.    training loss:\t\t3.97369833817\n",
      "Done 16580 batches in 2872.60 sec.    training loss:\t\t3.97368224434\n",
      "Done 16590 batches in 2874.09 sec.    training loss:\t\t3.97369019627\n",
      "Done 16600 batches in 2875.91 sec.    training loss:\t\t3.97366897279\n",
      "Done 16610 batches in 2877.40 sec.    training loss:\t\t3.97358757927\n",
      "Done 16620 batches in 2879.28 sec.    training loss:\t\t3.97360455577\n",
      "Done 16630 batches in 2880.93 sec.    training loss:\t\t3.97360777259\n",
      "Done 16640 batches in 2882.55 sec.    training loss:\t\t3.97358924226\n",
      "Done 16650 batches in 2884.53 sec.    training loss:\t\t3.97363810549\n",
      "Done 16660 batches in 2885.99 sec.    training loss:\t\t3.97356251072\n",
      "Done 16670 batches in 2887.33 sec.    training loss:\t\t3.97344272305\n",
      "Done 16680 batches in 2889.08 sec.    training loss:\t\t3.97333213391\n",
      "Done 16690 batches in 2891.11 sec.    training loss:\t\t3.97323354699\n",
      "Done 16700 batches in 2892.84 sec.    training loss:\t\t3.97325402631\n",
      "Done 16710 batches in 2894.42 sec.    training loss:\t\t3.97321108523\n",
      "Done 16720 batches in 2896.13 sec.    training loss:\t\t3.97320130422\n",
      "Done 16730 batches in 2898.14 sec.    training loss:\t\t3.97325400133\n",
      "Done 16740 batches in 2899.90 sec.    training loss:\t\t3.97327179772\n",
      "Done 16750 batches in 2901.43 sec.    training loss:\t\t3.97323664529\n",
      "Done 16760 batches in 2902.98 sec.    training loss:\t\t3.97324690549\n",
      "Done 16770 batches in 2904.76 sec.    training loss:\t\t3.97323779906\n",
      "Done 16780 batches in 2906.96 sec.    training loss:\t\t3.97325695699\n",
      "Done 16790 batches in 2908.69 sec.    training loss:\t\t3.97325769557\n",
      "Done 16800 batches in 2910.29 sec.    training loss:\t\t3.97318716052\n",
      "Done 16810 batches in 2911.93 sec.    training loss:\t\t3.973163561\n",
      "Done 16820 batches in 2913.59 sec.    training loss:\t\t3.97315298213\n",
      "Done 16830 batches in 2915.36 sec.    training loss:\t\t3.97312597558\n",
      "Done 16840 batches in 2917.50 sec.    training loss:\t\t3.97312743788\n",
      "Done 16850 batches in 2919.37 sec.    training loss:\t\t3.97322317929\n",
      "Done 16860 batches in 2920.98 sec.    training loss:\t\t3.97313582168\n",
      "Done 16870 batches in 2922.53 sec.    training loss:\t\t3.9731328009\n",
      "Done 16880 batches in 2924.04 sec.    training loss:\t\t3.97309092189\n",
      "Done 16890 batches in 2925.56 sec.    training loss:\t\t3.97307118369\n",
      "Done 16900 batches in 2927.42 sec.    training loss:\t\t3.97308571363\n",
      "Done 16910 batches in 2929.00 sec.    training loss:\t\t3.97305328606\n",
      "Done 16920 batches in 2930.78 sec.    training loss:\t\t3.9729907459\n",
      "Done 16930 batches in 2932.54 sec.    training loss:\t\t3.9730184853\n",
      "Done 16940 batches in 2934.32 sec.    training loss:\t\t3.97298798353\n",
      "Done 16950 batches in 2936.15 sec.    training loss:\t\t3.97298896452\n",
      "Done 16960 batches in 2937.48 sec.    training loss:\t\t3.97291337974\n",
      "Done 16970 batches in 2938.97 sec.    training loss:\t\t3.9728836782\n",
      "Done 16980 batches in 2940.89 sec.    training loss:\t\t3.97285198823\n",
      "Done 16990 batches in 2942.54 sec.    training loss:\t\t3.97280619814\n",
      "Done 17000 batches in 2944.53 sec.    training loss:\t\t3.97278907157\n",
      "Done 17010 batches in 2946.49 sec.    training loss:\t\t3.97276321845\n",
      "Done 17020 batches in 2948.47 sec.    training loss:\t\t3.97278932172\n",
      "Done 17030 batches in 2950.32 sec.    training loss:\t\t3.97279019077\n",
      "Done 17040 batches in 2952.04 sec.    training loss:\t\t3.97276076623\n",
      "Done 17050 batches in 2953.79 sec.    training loss:\t\t3.97275973632\n",
      "Done 17060 batches in 2955.47 sec.    training loss:\t\t3.97272243837\n",
      "Done 17070 batches in 2957.34 sec.    training loss:\t\t3.9726537006\n",
      "Done 17080 batches in 2958.96 sec.    training loss:\t\t3.97260014659\n",
      "Done 17090 batches in 2960.50 sec.    training loss:\t\t3.9725773172\n",
      "Done 17100 batches in 2962.02 sec.    training loss:\t\t3.97255466736\n",
      "Done 17110 batches in 2963.73 sec.    training loss:\t\t3.97249864171\n",
      "Done 17120 batches in 2965.20 sec.    training loss:\t\t3.97236444403\n",
      "Done 17130 batches in 2966.85 sec.    training loss:\t\t3.97231273396\n",
      "Done 17140 batches in 2968.29 sec.    training loss:\t\t3.97223138502\n",
      "Done 17150 batches in 2970.41 sec.    training loss:\t\t3.97219664739\n",
      "Done 17160 batches in 2971.86 sec.    training loss:\t\t3.97211635878\n",
      "Done 17170 batches in 2973.48 sec.    training loss:\t\t3.9720236217\n",
      "Done 17180 batches in 2974.98 sec.    training loss:\t\t3.97199301747\n",
      "Done 17190 batches in 2976.67 sec.    training loss:\t\t3.97195937747\n",
      "Done 17200 batches in 2978.36 sec.    training loss:\t\t3.9719123045\n",
      "Done 17210 batches in 2979.86 sec.    training loss:\t\t3.97192204652\n",
      "Done 17220 batches in 2981.54 sec.    training loss:\t\t3.97189917815\n",
      "Done 17230 batches in 2983.38 sec.    training loss:\t\t3.97183448334\n",
      "Done 17240 batches in 2985.18 sec.    training loss:\t\t3.9718367361\n",
      "Done 17250 batches in 2986.94 sec.    training loss:\t\t3.97181332306\n",
      "Done 17260 batches in 2988.77 sec.    training loss:\t\t3.97178321086\n",
      "Done 17270 batches in 2990.43 sec.    training loss:\t\t3.97171984005\n",
      "Done 17280 batches in 2992.44 sec.    training loss:\t\t3.97171527812\n",
      "Done 17290 batches in 2994.27 sec.    training loss:\t\t3.97168655076\n",
      "Done 17300 batches in 2996.08 sec.    training loss:\t\t3.97168389167\n",
      "Done 17310 batches in 2998.05 sec.    training loss:\t\t3.97171835103\n",
      "Done 17320 batches in 2999.91 sec.    training loss:\t\t3.97168645074\n",
      "Done 17330 batches in 3001.47 sec.    training loss:\t\t3.97160047102\n",
      "Done 17340 batches in 3002.93 sec.    training loss:\t\t3.97163914008\n",
      "Done 17350 batches in 3004.82 sec.    training loss:\t\t3.97158455746\n",
      "Done 17360 batches in 3006.41 sec.    training loss:\t\t3.97157681446\n",
      "Done 17370 batches in 3007.86 sec.    training loss:\t\t3.97148493941\n",
      "Done 17380 batches in 3009.53 sec.    training loss:\t\t3.97145963287\n",
      "Done 17390 batches in 3011.09 sec.    training loss:\t\t3.97141181048\n",
      "Done 17400 batches in 3012.92 sec.    training loss:\t\t3.97136327086\n",
      "Done 17410 batches in 3014.50 sec.    training loss:\t\t3.9712468634\n",
      "Done 17420 batches in 3016.02 sec.    training loss:\t\t3.97122644858\n",
      "Done 17430 batches in 3017.66 sec.    training loss:\t\t3.97114530336\n",
      "Done 17440 batches in 3019.47 sec.    training loss:\t\t3.97108195819\n",
      "Done 17450 batches in 3021.53 sec.    training loss:\t\t3.97104787283\n",
      "Done 17460 batches in 3023.39 sec.    training loss:\t\t3.97098692453\n",
      "Done 17470 batches in 3025.15 sec.    training loss:\t\t3.97090486614\n",
      "Done 17480 batches in 3026.70 sec.    training loss:\t\t3.97088003299\n",
      "Done 17490 batches in 3028.10 sec.    training loss:\t\t3.97088021972\n",
      "Done 17500 batches in 3029.87 sec.    training loss:\t\t3.97086262011\n",
      "Done 17510 batches in 3031.75 sec.    training loss:\t\t3.97087187024\n",
      "Done 17520 batches in 3033.53 sec.    training loss:\t\t3.97086345926\n",
      "Done 17530 batches in 3035.29 sec.    training loss:\t\t3.97083225437\n",
      "Done 17540 batches in 3037.15 sec.    training loss:\t\t3.97088609787\n",
      "Done 17550 batches in 3038.93 sec.    training loss:\t\t3.97082877238\n",
      "Done 17560 batches in 3040.97 sec.    training loss:\t\t3.97074794118\n",
      "Done 17570 batches in 3042.88 sec.    training loss:\t\t3.97070457236\n",
      "Done 17580 batches in 3044.59 sec.    training loss:\t\t3.97069758116\n",
      "Done 17590 batches in 3046.18 sec.    training loss:\t\t3.97070398672\n",
      "Done 17600 batches in 3047.80 sec.    training loss:\t\t3.97060702258\n",
      "Done 17610 batches in 3049.48 sec.    training loss:\t\t3.97059300055\n",
      "Done 17620 batches in 3051.78 sec.    training loss:\t\t3.97054719802\n",
      "Done 17630 batches in 3054.02 sec.    training loss:\t\t3.97054675943\n",
      "Done 17640 batches in 3055.58 sec.    training loss:\t\t3.97054390507\n",
      "Done 17650 batches in 3057.14 sec.    training loss:\t\t3.97048196374\n",
      "Done 17660 batches in 3058.67 sec.    training loss:\t\t3.97045411556\n",
      "Done 17670 batches in 3060.61 sec.    training loss:\t\t3.97039780234\n",
      "Done 17680 batches in 3062.55 sec.    training loss:\t\t3.97041287504\n",
      "Done 17690 batches in 3064.16 sec.    training loss:\t\t3.97038790688\n",
      "Done 17700 batches in 3065.63 sec.    training loss:\t\t3.97038299593\n",
      "Done 17710 batches in 3067.32 sec.    training loss:\t\t3.97033969377\n",
      "Done 17720 batches in 3069.18 sec.    training loss:\t\t3.9704038317\n",
      "Done 17730 batches in 3070.70 sec.    training loss:\t\t3.97031379192\n",
      "Done 17740 batches in 3072.25 sec.    training loss:\t\t3.97032083594\n",
      "Done 17750 batches in 3074.01 sec.    training loss:\t\t3.97033647181\n",
      "Done 17760 batches in 3075.65 sec.    training loss:\t\t3.97031479748\n",
      "Done 17770 batches in 3077.24 sec.    training loss:\t\t3.97029173027\n",
      "Done 17780 batches in 3078.92 sec.    training loss:\t\t3.97028851619\n",
      "Done 17790 batches in 3080.69 sec.    training loss:\t\t3.97034342298\n",
      "Done 17800 batches in 3082.57 sec.    training loss:\t\t3.97029585394\n",
      "Done 17810 batches in 3084.07 sec.    training loss:\t\t3.9702131306\n",
      "Done 17820 batches in 3085.76 sec.    training loss:\t\t3.97021482998\n",
      "Done 17830 batches in 3087.42 sec.    training loss:\t\t3.9701578414\n",
      "Done 17840 batches in 3089.32 sec.    training loss:\t\t3.97016850127\n",
      "Done 17850 batches in 3090.98 sec.    training loss:\t\t3.97011418845\n",
      "Done 17860 batches in 3092.78 sec.    training loss:\t\t3.97010722256\n",
      "Done 17870 batches in 3094.53 sec.    training loss:\t\t3.97008267032\n",
      "Done 17880 batches in 3096.21 sec.    training loss:\t\t3.97009414996\n",
      "Done 17890 batches in 3098.02 sec.    training loss:\t\t3.97012985474\n",
      "Done 17900 batches in 3099.63 sec.    training loss:\t\t3.97012111713\n",
      "Done 17910 batches in 3101.16 sec.    training loss:\t\t3.9701098132\n",
      "Done 17920 batches in 3102.91 sec.    training loss:\t\t3.97015449515\n",
      "Done 17930 batches in 3104.35 sec.    training loss:\t\t3.9701186801\n",
      "Done 17940 batches in 3105.87 sec.    training loss:\t\t3.97006362241\n",
      "Done 17950 batches in 3107.80 sec.    training loss:\t\t3.97004245933\n",
      "Done 17960 batches in 3109.87 sec.    training loss:\t\t3.97008151243\n",
      "Done 17970 batches in 3111.54 sec.    training loss:\t\t3.97004699488\n",
      "Done 17980 batches in 3113.06 sec.    training loss:\t\t3.97006881083\n",
      "Done 17990 batches in 3114.89 sec.    training loss:\t\t3.97009522792\n",
      "Done 18000 batches in 3116.37 sec.    training loss:\t\t3.97003093988\n",
      "Done 18010 batches in 3118.28 sec.    training loss:\t\t3.97006412147\n",
      "Done 18020 batches in 3119.98 sec.    training loss:\t\t3.97004922904\n",
      "Done 18030 batches in 3122.10 sec.    training loss:\t\t3.97002016153\n",
      "Done 18040 batches in 3123.51 sec.    training loss:\t\t3.97001155177\n",
      "Done 18050 batches in 3124.98 sec.    training loss:\t\t3.97000099372\n",
      "Done 18060 batches in 3126.88 sec.    training loss:\t\t3.96997619147\n",
      "Done 18070 batches in 3128.97 sec.    training loss:\t\t3.9698961129\n",
      "Done 18080 batches in 3130.68 sec.    training loss:\t\t3.96984959366\n",
      "Done 18090 batches in 3132.35 sec.    training loss:\t\t3.96980743506\n",
      "Done 18100 batches in 3134.39 sec.    training loss:\t\t3.96982926914\n",
      "Done 18110 batches in 3136.06 sec.    training loss:\t\t3.96979576791\n",
      "Done 18120 batches in 3137.70 sec.    training loss:\t\t3.96978037859\n",
      "Done 18130 batches in 3139.48 sec.    training loss:\t\t3.96975138397\n",
      "Done 18140 batches in 3140.94 sec.    training loss:\t\t3.96962950019\n",
      "Done 18150 batches in 3142.85 sec.    training loss:\t\t3.96966928347\n",
      "Done 18160 batches in 3144.23 sec.    training loss:\t\t3.96956378732\n",
      "Done 18170 batches in 3146.06 sec.    training loss:\t\t3.96953121487\n",
      "Done 18180 batches in 3147.98 sec.    training loss:\t\t3.96955104352\n",
      "Done 18190 batches in 3149.59 sec.    training loss:\t\t3.96954957789\n",
      "Done 18200 batches in 3151.17 sec.    training loss:\t\t3.96956943589\n",
      "Done 18210 batches in 3152.80 sec.    training loss:\t\t3.96956306123\n",
      "Done 18220 batches in 3154.45 sec.    training loss:\t\t3.96949555862\n",
      "Done 18230 batches in 3156.49 sec.    training loss:\t\t3.96949875292\n",
      "Done 18240 batches in 3157.84 sec.    training loss:\t\t3.96936886529\n",
      "Done 18250 batches in 3159.47 sec.    training loss:\t\t3.96935544407\n",
      "Done 18260 batches in 3160.98 sec.    training loss:\t\t3.96937347346\n",
      "Done 18270 batches in 3162.78 sec.    training loss:\t\t3.96930500251\n",
      "Done 18280 batches in 3164.42 sec.    training loss:\t\t3.96925103537\n",
      "Done 18290 batches in 3166.15 sec.    training loss:\t\t3.96923859551\n",
      "Done 18300 batches in 3168.47 sec.    training loss:\t\t3.96926751013\n",
      "Done 18310 batches in 3170.75 sec.    training loss:\t\t3.96932574761\n",
      "Done 18320 batches in 3172.50 sec.    training loss:\t\t3.96933032714\n",
      "Done 18330 batches in 3173.97 sec.    training loss:\t\t3.96925959608\n",
      "Done 18340 batches in 3175.80 sec.    training loss:\t\t3.96922931546\n",
      "Done 18350 batches in 3177.45 sec.    training loss:\t\t3.96921899614\n",
      "Done 18360 batches in 3179.16 sec.    training loss:\t\t3.96920250018\n",
      "Done 18370 batches in 3181.11 sec.    training loss:\t\t3.96924923977\n",
      "Done 18380 batches in 3182.87 sec.    training loss:\t\t3.9692133542\n",
      "Done 18390 batches in 3184.60 sec.    training loss:\t\t3.96921524772\n",
      "Done 18400 batches in 3186.50 sec.    training loss:\t\t3.96928160333\n",
      "Done 18410 batches in 3188.47 sec.    training loss:\t\t3.96926130749\n",
      "Done 18420 batches in 3190.41 sec.    training loss:\t\t3.96928049142\n",
      "Done 18430 batches in 3191.78 sec.    training loss:\t\t3.96927742292\n",
      "Done 18440 batches in 3193.49 sec.    training loss:\t\t3.969223522\n",
      "Done 18450 batches in 3195.14 sec.    training loss:\t\t3.9691386489\n",
      "Done 18460 batches in 3196.81 sec.    training loss:\t\t3.96907456621\n",
      "Done 18470 batches in 3198.14 sec.    training loss:\t\t3.96900171113\n",
      "Done 18480 batches in 3199.79 sec.    training loss:\t\t3.9689311367\n",
      "Done 18490 batches in 3201.47 sec.    training loss:\t\t3.96891231102\n",
      "Done 18500 batches in 3203.05 sec.    training loss:\t\t3.96887488651\n",
      "Done 18510 batches in 3205.03 sec.    training loss:\t\t3.96887588263\n",
      "Done 18520 batches in 3206.49 sec.    training loss:\t\t3.96884136726\n",
      "Done 18530 batches in 3208.13 sec.    training loss:\t\t3.9688857349\n",
      "Done 18540 batches in 3209.79 sec.    training loss:\t\t3.96884657588\n",
      "Done 18550 batches in 3211.40 sec.    training loss:\t\t3.96878389513\n",
      "Done 18560 batches in 3213.57 sec.    training loss:\t\t3.96882963475\n",
      "Done 18570 batches in 3215.42 sec.    training loss:\t\t3.96877576709\n",
      "Done 18580 batches in 3217.05 sec.    training loss:\t\t3.96872812062\n",
      "Done 18590 batches in 3218.88 sec.    training loss:\t\t3.96867899556\n",
      "Done 18600 batches in 3220.49 sec.    training loss:\t\t3.96863391578\n",
      "Done 18610 batches in 3222.08 sec.    training loss:\t\t3.96862221359\n",
      "Done 18620 batches in 3223.60 sec.    training loss:\t\t3.96860595072\n",
      "Done 18630 batches in 3225.33 sec.    training loss:\t\t3.96857382097\n",
      "Done 18640 batches in 3227.10 sec.    training loss:\t\t3.9685542284\n",
      "Done 18650 batches in 3228.79 sec.    training loss:\t\t3.96855606264\n",
      "Done 18660 batches in 3230.46 sec.    training loss:\t\t3.96855173798\n",
      "Done 18670 batches in 3232.05 sec.    training loss:\t\t3.96848052584\n",
      "Done 18680 batches in 3234.15 sec.    training loss:\t\t3.96847669804\n",
      "Done 18690 batches in 3235.80 sec.    training loss:\t\t3.9685305067\n",
      "Done 18700 batches in 3237.52 sec.    training loss:\t\t3.96853202493\n",
      "Done 18710 batches in 3239.12 sec.    training loss:\t\t3.96850715906\n",
      "Done 18720 batches in 3240.88 sec.    training loss:\t\t3.96848875594\n",
      "Done 18730 batches in 3243.01 sec.    training loss:\t\t3.96850365768\n",
      "Done 18740 batches in 3244.69 sec.    training loss:\t\t3.96834342774\n",
      "Done 18750 batches in 3246.27 sec.    training loss:\t\t3.9682976767\n",
      "Done 18760 batches in 3247.87 sec.    training loss:\t\t3.96824478746\n",
      "Done 18770 batches in 3249.48 sec.    training loss:\t\t3.96820680964\n",
      "Done 18780 batches in 3250.83 sec.    training loss:\t\t3.9681625915\n",
      "Done 18790 batches in 3252.38 sec.    training loss:\t\t3.9681442474\n",
      "Done 18800 batches in 3253.95 sec.    training loss:\t\t3.96809418002\n",
      "Done 18810 batches in 3255.65 sec.    training loss:\t\t3.96811184684\n",
      "Done 18820 batches in 3257.79 sec.    training loss:\t\t3.96806095228\n",
      "Done 18830 batches in 3259.54 sec.    training loss:\t\t3.96802936108\n",
      "Done 18840 batches in 3261.04 sec.    training loss:\t\t3.96796376405\n",
      "Done 18850 batches in 3262.54 sec.    training loss:\t\t3.96794063753\n",
      "Done 18860 batches in 3264.38 sec.    training loss:\t\t3.96793394677\n",
      "Done 18870 batches in 3266.06 sec.    training loss:\t\t3.96790914939\n",
      "Done 18880 batches in 3267.63 sec.    training loss:\t\t3.96783693887\n",
      "Done 18890 batches in 3269.08 sec.    training loss:\t\t3.96775359302\n",
      "Done 18900 batches in 3270.66 sec.    training loss:\t\t3.96774839429\n",
      "Done 18910 batches in 3272.72 sec.    training loss:\t\t3.96770365562\n",
      "Done 18920 batches in 3274.81 sec.    training loss:\t\t3.9677293988\n",
      "Done 18930 batches in 3276.75 sec.    training loss:\t\t3.9677189388\n",
      "Done 18940 batches in 3278.51 sec.    training loss:\t\t3.96772656062\n",
      "Done 18950 batches in 3280.19 sec.    training loss:\t\t3.96772349554\n",
      "Done 18960 batches in 3282.09 sec.    training loss:\t\t3.9677049556\n",
      "Done 18970 batches in 3283.72 sec.    training loss:\t\t3.96768310889\n",
      "Done 18980 batches in 3285.39 sec.    training loss:\t\t3.96767242333\n",
      "Done 18990 batches in 3287.05 sec.    training loss:\t\t3.96758414825\n",
      "Done 19000 batches in 3288.91 sec.    training loss:\t\t3.96757042296\n",
      "Done 19010 batches in 3290.50 sec.    training loss:\t\t3.96745938463\n",
      "Done 19020 batches in 3292.20 sec.    training loss:\t\t3.96741607503\n",
      "Done 19030 batches in 3294.00 sec.    training loss:\t\t3.96738972967\n",
      "Done 19040 batches in 3295.73 sec.    training loss:\t\t3.96738702386\n",
      "Done 19050 batches in 3297.38 sec.    training loss:\t\t3.96741027791\n",
      "Done 19060 batches in 3298.87 sec.    training loss:\t\t3.96738419595\n",
      "Done 19070 batches in 3300.98 sec.    training loss:\t\t3.96733555136\n",
      "Done 19080 batches in 3302.67 sec.    training loss:\t\t3.96733011081\n",
      "Done 19090 batches in 3304.46 sec.    training loss:\t\t3.96730254305\n",
      "Done 19100 batches in 3306.11 sec.    training loss:\t\t3.9672763813\n",
      "Done 19110 batches in 3307.75 sec.    training loss:\t\t3.96728659024\n",
      "Done 19120 batches in 3309.49 sec.    training loss:\t\t3.96729858723\n",
      "Done 19130 batches in 3310.98 sec.    training loss:\t\t3.96724329374\n",
      "Done 19140 batches in 3312.49 sec.    training loss:\t\t3.96717541624\n",
      "Done 19150 batches in 3314.32 sec.    training loss:\t\t3.9671908673\n",
      "Done 19160 batches in 3316.04 sec.    training loss:\t\t3.96720907273\n",
      "Done 19170 batches in 3317.64 sec.    training loss:\t\t3.96723132561\n",
      "Done 19180 batches in 3319.39 sec.    training loss:\t\t3.96720567637\n",
      "Done 19190 batches in 3321.38 sec.    training loss:\t\t3.96714733108\n",
      "Done 19200 batches in 3323.32 sec.    training loss:\t\t3.96715918505\n",
      "Done 19210 batches in 3325.14 sec.    training loss:\t\t3.9670892949\n",
      "Done 19220 batches in 3326.72 sec.    training loss:\t\t3.96708870888\n",
      "Done 19230 batches in 3328.43 sec.    training loss:\t\t3.96702926592\n",
      "Done 19240 batches in 3329.80 sec.    training loss:\t\t3.96701117212\n",
      "Done 19250 batches in 3331.38 sec.    training loss:\t\t3.96698895754\n",
      "Done 19260 batches in 3332.97 sec.    training loss:\t\t3.96696125637\n",
      "Done 19270 batches in 3334.70 sec.    training loss:\t\t3.96687628492\n",
      "Done 19280 batches in 3336.52 sec.    training loss:\t\t3.96687188189\n",
      "Done 19290 batches in 3338.24 sec.    training loss:\t\t3.96684505123\n",
      "Done 19300 batches in 3339.96 sec.    training loss:\t\t3.96682911849\n",
      "Done 19310 batches in 3341.87 sec.    training loss:\t\t3.9667785636\n",
      "Done 19320 batches in 3343.48 sec.    training loss:\t\t3.96674900578\n",
      "Done 19330 batches in 3345.24 sec.    training loss:\t\t3.96670182018\n",
      "Done 19340 batches in 3347.49 sec.    training loss:\t\t3.96668455161\n",
      "Done 19350 batches in 3349.44 sec.    training loss:\t\t3.96662590321\n",
      "Done 19360 batches in 3350.94 sec.    training loss:\t\t3.96660284872\n",
      "Done 19370 batches in 3352.71 sec.    training loss:\t\t3.96656774178\n",
      "Done 19380 batches in 3354.40 sec.    training loss:\t\t3.96651613675\n",
      "Done 19390 batches in 3356.39 sec.    training loss:\t\t3.96649221652\n",
      "Done 19400 batches in 3358.14 sec.    training loss:\t\t3.96647763884\n",
      "Done 19410 batches in 3359.78 sec.    training loss:\t\t3.9664347345\n",
      "Done 19420 batches in 3361.52 sec.    training loss:\t\t3.96633746085\n",
      "Done 19430 batches in 3363.42 sec.    training loss:\t\t3.96632245409\n",
      "Done 19440 batches in 3365.03 sec.    training loss:\t\t3.96630882799\n",
      "Done 19450 batches in 3366.65 sec.    training loss:\t\t3.96626774468\n",
      "Done 19460 batches in 3368.31 sec.    training loss:\t\t3.96626236224\n",
      "Done 19470 batches in 3369.74 sec.    training loss:\t\t3.96616894284\n",
      "Done 19480 batches in 3371.64 sec.    training loss:\t\t3.96620013361\n",
      "Done 19490 batches in 3373.09 sec.    training loss:\t\t3.96616204459\n",
      "Done 19500 batches in 3375.07 sec.    training loss:\t\t3.96612289875\n",
      "Done 19510 batches in 3376.95 sec.    training loss:\t\t3.96607676341\n",
      "Done 19520 batches in 3378.93 sec.    training loss:\t\t3.96613883706\n",
      "Done 19530 batches in 3380.24 sec.    training loss:\t\t3.96609351882\n",
      "Done 19540 batches in 3381.91 sec.    training loss:\t\t3.96604018599\n",
      "Done 19550 batches in 3383.83 sec.    training loss:\t\t3.96604457928\n",
      "Done 19560 batches in 3385.94 sec.    training loss:\t\t3.96602294185\n",
      "Done 19570 batches in 3387.50 sec.    training loss:\t\t3.96598375386\n",
      "Done 19580 batches in 3389.63 sec.    training loss:\t\t3.96595770268\n",
      "Done 19590 batches in 3391.29 sec.    training loss:\t\t3.96593258501\n",
      "Done 19600 batches in 3392.98 sec.    training loss:\t\t3.96588957917\n",
      "Done 19610 batches in 3395.16 sec.    training loss:\t\t3.96584709837\n",
      "Done 19620 batches in 3396.68 sec.    training loss:\t\t3.9658347897\n",
      "Done 19630 batches in 3398.57 sec.    training loss:\t\t3.96577081254\n",
      "Done 100 batches in 2.96 sec.\n",
      "Done 200 batches in 5.85 sec.\n",
      "Done 300 batches in 8.81 sec.\n",
      "Done 400 batches in 11.62 sec.\n",
      "Done 500 batches in 14.45 sec.\n",
      "Done 600 batches in 17.27 sec.\n",
      "Done 700 batches in 20.10 sec.\n",
      "Done 800 batches in 23.20 sec.\n",
      "Done 900 batches in 25.94 sec.\n",
      "Done 1000 batches in 28.65 sec.\n",
      "Done 1100 batches in 31.33 sec.\n",
      "Done 1200 batches in 34.03 sec.\n",
      "Done 1300 batches in 36.98 sec.\n",
      "Done 1400 batches in 39.91 sec.\n",
      "Done 1500 batches in 42.83 sec.\n",
      "Done 1600 batches in 45.63 sec.\n",
      "Done 1700 batches in 48.37 sec.\n",
      "Done 1800 batches in 50.96 sec.\n",
      "Done 1900 batches in 53.56 sec.\n",
      "Done 2000 batches in 56.10 sec.\n",
      "Done 2100 batches in 58.76 sec.\n",
      "Done 2200 batches in 61.46 sec.\n",
      "Done 2300 batches in 64.25 sec.\n",
      "Done 2400 batches in 67.33 sec.\n",
      "Epoch 2 of 2 took 3467.955s\n",
      "  training loss:\t\t3.965771\n",
      "  validation loss:\t\t3.966845\n"
     ]
    }
   ],
   "source": [
    "# training, taken from mnist.py in lasagne examples\n",
    "\n",
    "num_epochs = 2\n",
    "mt_batch_size = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    time_wasted = 0\n",
    "    training_time = 0\n",
    "    \n",
    "    for batch in iterate_minibatches(mt_train, mt_batch_size):\n",
    "        \n",
    "        inputs, targets, mask, t = batch\n",
    "        \n",
    "#         inp_u = np.unique(inputs)\n",
    "#         if inp_u[0] == -1:\n",
    "#             inp_u = inp_u[1:]\n",
    "#         voc_mask = np.zeros((voc_size,), dtype=np.int32)\n",
    "#         voc_mask[inp_u] = 1\n",
    "        \n",
    "        batch_training_time = time.time()\n",
    "#         batch_err = train_fn(inputs, targets, mask, voc_mask)\n",
    "        batch_err = train_fn(inputs, targets, mask)\n",
    "\n",
    "        train_err += batch_err\n",
    "        training_time += time.time() - batch_training_time\n",
    "        train_batches += 1\n",
    "        \n",
    "        time_wasted += t\n",
    "        if not train_batches % 10:\n",
    "            print \"Done {} batches in {:.2f} sec.    training loss:\\t\\t{}\".format(\n",
    "                train_batches, time.time() - start_time, train_err / train_batches)\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_err = 0\n",
    "    val_batches = 0\n",
    "    start_time2 = time.time()\n",
    "    \n",
    "    for batch in iterate_minibatches(mt_val, mt_batch_size):\n",
    "        inputs, targets, mask, t = batch\n",
    "        \n",
    "#         inp_u = np.unique(inputs)\n",
    "#         if inp_u[0] == -1:\n",
    "#             inp_u = inp_u[1:]\n",
    "#         voc_mask = np.zeros((voc_size,), dtype=np.int32)\n",
    "#         voc_mask[inp_u] = 1\n",
    "        \n",
    "#         err = val_fn(inputs, targets, mask, np.ones((voc_size,), dtype=np.int32))\n",
    "        err = val_fn(inputs, targets, mask)\n",
    "        val_err += err\n",
    "        val_batches += 1\n",
    "        if not val_batches % 100:\n",
    "            print \"Done {} batches in {:.2f} sec.\".format(\n",
    "                val_batches, time.time() - start_time2)\n",
    "\n",
    "    # Then we print the results for this epoch:\n",
    "    print \"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time)\n",
    "    print \"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches)\n",
    "    print \"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches)\n",
    "    #print \"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "    #    val_acc / val_batches * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savez('fsoft_trained_singleLSTM.npz', *L.layers.get_all_param_values(net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_var = T.imatrix('inputs')\n",
    "gen_net = build_hsoft_rnnlm(input_var, None, None, voc_size, emb_size, rec_size)\n",
    "probs = L.layers.get_output(gen_net)[:,-1,:]\n",
    "get_probs = theano.function([input_var], probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_var = T.imatrix('inputs')\n",
    "gen_net = build_sampledsoft_rnnlm(input_var, None, np.ones((voc_size,), dtype=np.int32), voc_size, emb_size, rec_size, target_var=None)\n",
    "probs = L.layers.get_output(gen_net)[:,-1,:]\n",
    "get_probs = theano.function([input_var], probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_var = T.imatrix('inputs')\n",
    "gen_net = build_simple_rnnlm(input_var, None, voc_size, emb_size, rec_size)\n",
    "probs = L.layers.get_output(gen_net)[:,-1,:]\n",
    "get_probs = theano.function([input_var], probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with np.load('fsoft_trained_singleLSTM.npz') as f:\n",
    "    param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "    L.layers.set_all_param_values(gen_net, param_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnd_next_word(probs, size=1):\n",
    "    return np.random.choice(np.arange(probs.shape[0], dtype=np.int32), size=size, p=probs)\n",
    "\n",
    "def beam_search(get_probs_fun, beam=10, init_seq='', mode='rr'):\n",
    "    utt = [1] + map(lambda w: mt_w_to_i.get(w, mt_w_to_i['<unk>']), init_seq.split())\n",
    "    utt = np.asarray(utt, dtype=np.int32)[np.newaxis]\n",
    "    \n",
    "    if mode[0] == 's':\n",
    "        words = get_probs_fun(utt)[0].argpartition(-beam)[-beam:].astype(np.int32)\n",
    "    elif mode[0] == 'r':\n",
    "        words = rnd_next_word(get_probs_fun(utt)[0], beam)\n",
    "    \n",
    "    candidates = utt.repeat(beam, axis=0)\n",
    "    candidates = np.hstack([candidates, words[np.newaxis].T])\n",
    "    scores = np.zeros(beam)\n",
    "    \n",
    "    while 0 not in candidates[:,-1] and candidates.shape[1] < 100:\n",
    "        \n",
    "        if mode[1] == 's':\n",
    "            log_probs = np.log(get_probs_fun(candidates))\n",
    "            tot_scores = log_probs + scores[np.newaxis].T\n",
    "\n",
    "            idx = tot_scores.ravel().argpartition(-beam)[-beam:]\n",
    "            i,j = idx / tot_scores.shape[1], (idx % tot_scores.shape[1]).astype(np.int32)\n",
    "\n",
    "            scores = tot_scores[i,j]\n",
    "\n",
    "            candidates = np.hstack([candidates[i], j[np.newaxis].T])\n",
    "            \n",
    "        elif mode[1] == 'r':\n",
    "            probs = get_probs_fun(candidates)\n",
    "            words = []\n",
    "            for k in xrange(beam):\n",
    "                words.append(rnd_next_word(probs[k], beam)) # this doesn't have to be exactly 'beam'\n",
    "            words = np.array(words)\n",
    "            idx = np.indices((beam, words.shape[1]))[0]\n",
    "            tot_scores = scores[np.newaxis].T + np.log(probs)[idx, words]\n",
    "                \n",
    "            idx = tot_scores.ravel().argpartition(-beam)[-beam:]\n",
    "            i,j = idx / tot_scores.shape[1], (idx % tot_scores.shape[1])\n",
    "\n",
    "            scores = tot_scores[i,j]\n",
    "\n",
    "            candidates = np.hstack([candidates[i], words[i,j][np.newaxis].T])\n",
    "            \n",
    "#         print candidates\n",
    "#         print scores\n",
    "\n",
    "    return candidates[candidates[:,-1] == 0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"it ' s not a good thing . i ' m not going to be here . i ' m not going to be here . i ' m not going to be here .\""
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utt = beam_search(get_probs, init_seq='', beam=20, mode='rr')\n",
    "\n",
    "text = map(lambda i: mt_i_to_w[i], list(utt))\n",
    "' '.join(text[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'and then far , you know ? <unk> , put that one little hot . i mean . then not !'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rnd_next_word(probs):\n",
    "    return np.random.choice(np.arange(len(probs[0])), p=probs[0])\n",
    "\n",
    "init_seq = ''\n",
    "utt = [1] + map(lambda w: mt_w_to_i.get(w, mt_w_to_i['<unk>']), init_seq.split())\n",
    "utt = np.asarray(utt, dtype=np.int32)[np.newaxis]\n",
    "\n",
    "i = 0\n",
    "while mt_i_to_w[utt[0,-1]] != '<utt_end>' and i < 50:\n",
    "    word_probs = get_probs(utt)\n",
    "    next_idx = rnd_next_word(word_probs)\n",
    "    utt = np.append(utt, next_idx)[np.newaxis].astype(np.int32)\n",
    "    i += 1\n",
    "    \n",
    "text = map(lambda i: mt_i_to_w[i], list(utt[0]))\n",
    "' '.join(text[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

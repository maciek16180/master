{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 970 (CNMeM is enabled with initial size: 30.0% of memory, cuDNN 5005)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import time\n",
    "from itertools import chain\n",
    "\n",
    "import lasagne as L\n",
    "\n",
    "from SimpleRNNLM import SimpleRNNLM, iterate_minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remember, now the pad value is the same as the <utt_end> token\n",
    "\n",
    "pad_value = -1 # <utt_end>'s vector is the last one\n",
    "\n",
    "def split_utt(utt):\n",
    "    u1, u2, u3 = [i for i,j in enumerate(utt) if j == 1]\n",
    "    return [utt[:u2], utt[u2:u3], utt[u3:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mt_path = \"/pio/data/data/mtriples/\"\n",
    "mt_path = \"/home/maciek/Desktop/mgr/DATA/MovieTriples_Dataset/\"\n",
    "\n",
    "def load_mt(path=mt_path):\n",
    "    tr = np.load(mt_path + 'Training.triples.pkl')\n",
    "    vl = np.load(mt_path + 'Validation.triples.pkl')\n",
    "    ts = np.load(mt_path + 'Test.triples.pkl')\n",
    "    \n",
    "#     tr = chain(*map(split_utt, tr))\n",
    "#     vl = chain(*map(split_utt, vl))\n",
    "#     ts = chain(*map(split_utt, ts))\n",
    "    \n",
    "    return tr, vl, ts\n",
    "\n",
    "train, valid, test = load_mt()\n",
    "\n",
    "train = [utt for utt in train if len(utt) < 200]\n",
    "valid = [utt for utt in valid if len(utt) < 200]\n",
    "test  = [utt for utt in test  if len(utt) < 200]\n",
    "\n",
    "\n",
    "def get_mt_voc(mt_path=mt_path, train_len=len(train)):\n",
    "    word_list = np.load(mt_path + 'Training.dict.pkl')\n",
    "    word_list.sort(key=lambda x: x[1])\n",
    "    freqs = np.array(map(lambda x: x[2], word_list) + [train_len])\n",
    "    total_count = float(sum(freqs))\n",
    "    \n",
    "    words = map(lambda x: x[:2], word_list)\n",
    "    \n",
    "    w_to_idx = dict(words)\n",
    "    w_to_idx['<utt_end>'] = pad_value\n",
    "    idx_to_w = {v : k for (k,v) in w_to_idx.items()}\n",
    "    \n",
    "    return idx_to_w, w_to_idx, len(w_to_idx), freqs / total_count\n",
    "\n",
    "idx_to_w, w_to_idx, voc_size, freqs = get_mt_voc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec_embs, word2vec_embs_mask = np.load(mt_path + 'Word2Vec_WordEmb.pkl')\n",
    "word2vec_embs = np.vstack([word2vec_embs, L.init.GlorotUniform()((1,300))]).astype(np.float32)\n",
    "word2vec_embs_mask = np.vstack([word2vec_embs_mask, np.ones((1,300))])\n",
    "\n",
    "w2v_train_mask = np.where(word2vec_embs_mask[:,0] == 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model...\n",
      "Compiling theano functions...\n",
      "Building a network for generation...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "def update_fn(loss, params):\n",
    "    return L.updates.adagrad(loss, params, learning_rate=.1)\n",
    "\n",
    "net = SimpleRNNLM(voc_size=voc_size,\n",
    "                  emb_size=300,\n",
    "                  rec_size=300,\n",
    "                  mode='ssoft',\n",
    "                  num_sampled=200,\n",
    "                  emb_init=word2vec_embs,\n",
    "                  ssoft_probs=freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.81697651466769"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(3.46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net.load_params(fname='5ep_w2vInit_300_300_ssoft(uni,200,non-unique)_bs50_cut200.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 10 batches in 2.47s\ttraining loss:\t7.069125\n",
      "Done 20 batches in 4.78s\ttraining loss:\t6.368708\n",
      "Done 30 batches in 6.96s\ttraining loss:\t6.141846\n",
      "Done 40 batches in 9.18s\ttraining loss:\t5.991481\n",
      "Done 50 batches in 11.69s\ttraining loss:\t5.871182\n",
      "Done 60 batches in 13.86s\ttraining loss:\t5.749675\n",
      "Done 70 batches in 16.08s\ttraining loss:\t5.658591\n",
      "Done 80 batches in 18.27s\ttraining loss:\t5.575968\n",
      "Done 90 batches in 20.70s\ttraining loss:\t5.499310\n",
      "Done 100 batches in 22.80s\ttraining loss:\t5.426299\n",
      "Done 110 batches in 25.10s\ttraining loss:\t5.362113\n",
      "Done 120 batches in 27.47s\ttraining loss:\t5.302588\n",
      "Done 130 batches in 29.70s\ttraining loss:\t5.247442\n",
      "Done 140 batches in 32.00s\ttraining loss:\t5.197363\n",
      "Done 150 batches in 34.09s\ttraining loss:\t5.150574\n",
      "Done 160 batches in 36.26s\ttraining loss:\t5.107230\n",
      "Done 170 batches in 38.57s\ttraining loss:\t5.070034\n",
      "Done 180 batches in 40.96s\ttraining loss:\t5.035285\n",
      "Done 190 batches in 43.25s\ttraining loss:\t5.001125\n",
      "Done 200 batches in 45.91s\ttraining loss:\t4.969031\n",
      "Done 210 batches in 48.37s\ttraining loss:\t4.938821\n",
      "Done 220 batches in 50.73s\ttraining loss:\t4.911104\n",
      "Done 230 batches in 52.96s\ttraining loss:\t4.883311\n",
      "Done 240 batches in 55.06s\ttraining loss:\t4.857661\n",
      "Done 250 batches in 57.50s\ttraining loss:\t4.834012\n",
      "Done 260 batches in 59.78s\ttraining loss:\t4.811628\n",
      "Done 270 batches in 61.96s\ttraining loss:\t4.790988\n",
      "Done 280 batches in 64.17s\ttraining loss:\t4.771044\n",
      "Done 290 batches in 66.40s\ttraining loss:\t4.751850\n",
      "Done 300 batches in 68.54s\ttraining loss:\t4.732604\n",
      "Done 310 batches in 70.74s\ttraining loss:\t4.716247\n",
      "Done 320 batches in 73.14s\ttraining loss:\t4.701784\n",
      "Done 330 batches in 75.44s\ttraining loss:\t4.687095\n",
      "Done 340 batches in 77.78s\ttraining loss:\t4.672855\n",
      "Done 350 batches in 80.30s\ttraining loss:\t4.657468\n",
      "Done 360 batches in 82.49s\ttraining loss:\t4.643431\n",
      "Done 370 batches in 84.73s\ttraining loss:\t4.630581\n",
      "Done 380 batches in 87.10s\ttraining loss:\t4.617907\n",
      "Done 390 batches in 89.35s\ttraining loss:\t4.604878\n",
      "Done 400 batches in 91.51s\ttraining loss:\t4.592580\n",
      "Done 410 batches in 93.74s\ttraining loss:\t4.580440\n",
      "Done 420 batches in 96.13s\ttraining loss:\t4.568335\n",
      "Done 430 batches in 98.43s\ttraining loss:\t4.557123\n",
      "Done 440 batches in 100.81s\ttraining loss:\t4.546847\n",
      "Done 450 batches in 103.11s\ttraining loss:\t4.537415\n",
      "Done 460 batches in 105.30s\ttraining loss:\t4.527091\n",
      "Done 470 batches in 107.70s\ttraining loss:\t4.518036\n",
      "Done 480 batches in 109.89s\ttraining loss:\t4.508171\n",
      "Done 490 batches in 111.97s\ttraining loss:\t4.499793\n",
      "Done 500 batches in 114.18s\ttraining loss:\t4.490959\n",
      "Done 510 batches in 116.37s\ttraining loss:\t4.482180\n",
      "Done 520 batches in 118.70s\ttraining loss:\t4.474281\n",
      "Done 530 batches in 120.85s\ttraining loss:\t4.466948\n",
      "Done 540 batches in 123.05s\ttraining loss:\t4.459386\n",
      "Done 550 batches in 125.27s\ttraining loss:\t4.452074\n",
      "Done 560 batches in 127.57s\ttraining loss:\t4.444901\n",
      "Done 570 batches in 129.89s\ttraining loss:\t4.437428\n",
      "Done 580 batches in 131.90s\ttraining loss:\t4.430289\n",
      "Done 590 batches in 134.09s\ttraining loss:\t4.423918\n",
      "Done 600 batches in 136.37s\ttraining loss:\t4.417870\n",
      "Done 610 batches in 138.56s\ttraining loss:\t4.411317\n",
      "Done 620 batches in 141.02s\ttraining loss:\t4.404532\n",
      "Done 630 batches in 143.04s\ttraining loss:\t4.397814\n",
      "Done 640 batches in 145.18s\ttraining loss:\t4.391126\n",
      "Done 650 batches in 147.48s\ttraining loss:\t4.384476\n",
      "Done 660 batches in 149.58s\ttraining loss:\t4.378853\n",
      "Done 670 batches in 152.04s\ttraining loss:\t4.373216\n",
      "Done 680 batches in 154.20s\ttraining loss:\t4.367536\n",
      "Done 690 batches in 156.42s\ttraining loss:\t4.362236\n",
      "Done 700 batches in 158.47s\ttraining loss:\t4.356207\n",
      "Done 710 batches in 160.80s\ttraining loss:\t4.351342\n",
      "Done 720 batches in 163.18s\ttraining loss:\t4.346238\n",
      "Done 730 batches in 165.25s\ttraining loss:\t4.341247\n",
      "Done 740 batches in 167.50s\ttraining loss:\t4.335925\n",
      "Done 750 batches in 169.73s\ttraining loss:\t4.331006\n",
      "Done 760 batches in 171.94s\ttraining loss:\t4.326496\n",
      "Done 770 batches in 174.21s\ttraining loss:\t4.321858\n",
      "Done 780 batches in 176.38s\ttraining loss:\t4.316927\n",
      "Done 790 batches in 178.64s\ttraining loss:\t4.312102\n",
      "Done 800 batches in 180.95s\ttraining loss:\t4.307400\n",
      "Done 810 batches in 183.09s\ttraining loss:\t4.302754\n",
      "Done 820 batches in 185.28s\ttraining loss:\t4.298150\n",
      "Done 830 batches in 187.31s\ttraining loss:\t4.294071\n",
      "Done 840 batches in 189.48s\ttraining loss:\t4.289964\n",
      "Done 850 batches in 191.56s\ttraining loss:\t4.285451\n",
      "Done 860 batches in 193.76s\ttraining loss:\t4.281550\n",
      "Done 870 batches in 195.96s\ttraining loss:\t4.277253\n",
      "Done 880 batches in 198.06s\ttraining loss:\t4.273397\n",
      "Done 890 batches in 200.34s\ttraining loss:\t4.269858\n",
      "Done 900 batches in 202.50s\ttraining loss:\t4.266090\n",
      "Done 910 batches in 204.47s\ttraining loss:\t4.262228\n",
      "Done 920 batches in 206.57s\ttraining loss:\t4.258594\n",
      "Done 930 batches in 208.86s\ttraining loss:\t4.255038\n",
      "Done 940 batches in 211.07s\ttraining loss:\t4.251224\n",
      "Done 950 batches in 213.33s\ttraining loss:\t4.247770\n",
      "Done 960 batches in 215.63s\ttraining loss:\t4.244234\n",
      "Done 970 batches in 217.73s\ttraining loss:\t4.240808\n",
      "Done 980 batches in 219.95s\ttraining loss:\t4.237349\n",
      "Done 990 batches in 222.03s\ttraining loss:\t4.233742\n",
      "Done 1000 batches in 224.23s\ttraining loss:\t4.230215\n",
      "Done 1010 batches in 226.51s\ttraining loss:\t4.226913\n",
      "Done 1020 batches in 228.49s\ttraining loss:\t4.223329\n",
      "Done 1030 batches in 230.84s\ttraining loss:\t4.220646\n",
      "Done 1040 batches in 233.24s\ttraining loss:\t4.217435\n",
      "Done 1050 batches in 235.48s\ttraining loss:\t4.214177\n",
      "Done 1060 batches in 237.70s\ttraining loss:\t4.211237\n",
      "Done 1070 batches in 239.87s\ttraining loss:\t4.208113\n",
      "Done 1080 batches in 242.03s\ttraining loss:\t4.204849\n",
      "Done 1090 batches in 244.45s\ttraining loss:\t4.201966\n",
      "Done 1100 batches in 246.67s\ttraining loss:\t4.198781\n",
      "Done 1110 batches in 248.89s\ttraining loss:\t4.195758\n",
      "Done 1120 batches in 251.23s\ttraining loss:\t4.193237\n",
      "Done 1130 batches in 253.46s\ttraining loss:\t4.190461\n",
      "Done 1140 batches in 255.74s\ttraining loss:\t4.187444\n",
      "Done 1150 batches in 257.86s\ttraining loss:\t4.184936\n",
      "Done 1160 batches in 260.17s\ttraining loss:\t4.182257\n",
      "Done 1170 batches in 262.54s\ttraining loss:\t4.180108\n",
      "Done 1180 batches in 264.78s\ttraining loss:\t4.177418\n",
      "Done 1190 batches in 267.20s\ttraining loss:\t4.174880\n",
      "Done 1200 batches in 269.54s\ttraining loss:\t4.172073\n",
      "Done 1210 batches in 271.89s\ttraining loss:\t4.169551\n",
      "Done 1220 batches in 274.06s\ttraining loss:\t4.167250\n",
      "Done 1230 batches in 276.30s\ttraining loss:\t4.164387\n",
      "Done 1240 batches in 278.61s\ttraining loss:\t4.161840\n",
      "Done 1250 batches in 281.02s\ttraining loss:\t4.159280\n",
      "Done 1260 batches in 283.26s\ttraining loss:\t4.156764\n",
      "Done 1270 batches in 285.45s\ttraining loss:\t4.154409\n",
      "Done 1280 batches in 287.71s\ttraining loss:\t4.152116\n",
      "Done 1290 batches in 289.93s\ttraining loss:\t4.149589\n",
      "Done 1300 batches in 292.20s\ttraining loss:\t4.147082\n",
      "Done 1310 batches in 294.57s\ttraining loss:\t4.145044\n",
      "Done 1320 batches in 296.61s\ttraining loss:\t4.142838\n",
      "Done 1330 batches in 298.81s\ttraining loss:\t4.140786\n",
      "Done 1340 batches in 300.98s\ttraining loss:\t4.138414\n",
      "Done 1350 batches in 303.35s\ttraining loss:\t4.136055\n",
      "Done 1360 batches in 305.74s\ttraining loss:\t4.133858\n",
      "Done 1370 batches in 308.03s\ttraining loss:\t4.131514\n",
      "Done 1380 batches in 310.14s\ttraining loss:\t4.129308\n",
      "Done 1390 batches in 312.52s\ttraining loss:\t4.126983\n",
      "Done 1400 batches in 314.79s\ttraining loss:\t4.124705\n",
      "Done 1410 batches in 317.09s\ttraining loss:\t4.122527\n",
      "Done 1420 batches in 319.12s\ttraining loss:\t4.120610\n",
      "Done 1430 batches in 321.28s\ttraining loss:\t4.118394\n",
      "Done 1440 batches in 323.56s\ttraining loss:\t4.116402\n",
      "Done 1450 batches in 325.95s\ttraining loss:\t4.114321\n",
      "Done 1460 batches in 328.11s\ttraining loss:\t4.112228\n",
      "Done 1470 batches in 330.20s\ttraining loss:\t4.110694\n",
      "Done 1480 batches in 332.62s\ttraining loss:\t4.108775\n",
      "Done 1490 batches in 334.99s\ttraining loss:\t4.106817\n",
      "Done 1500 batches in 337.33s\ttraining loss:\t4.104886\n",
      "Done 1510 batches in 339.63s\ttraining loss:\t4.102926\n",
      "Done 1520 batches in 341.88s\ttraining loss:\t4.100911\n",
      "Done 1530 batches in 344.20s\ttraining loss:\t4.099005\n",
      "Done 1540 batches in 346.46s\ttraining loss:\t4.097026\n",
      "Done 1550 batches in 348.62s\ttraining loss:\t4.095220\n",
      "Done 1560 batches in 350.94s\ttraining loss:\t4.093398\n",
      "Done 1570 batches in 353.20s\ttraining loss:\t4.091776\n",
      "Done 1580 batches in 355.27s\ttraining loss:\t4.089831\n",
      "Done 1590 batches in 357.79s\ttraining loss:\t4.087883\n",
      "Done 1600 batches in 359.92s\ttraining loss:\t4.086050\n",
      "Done 1610 batches in 361.89s\ttraining loss:\t4.084462\n",
      "Done 1620 batches in 364.29s\ttraining loss:\t4.082813\n",
      "Done 1630 batches in 366.51s\ttraining loss:\t4.081020\n",
      "Done 1640 batches in 369.04s\ttraining loss:\t4.079426\n",
      "Done 1650 batches in 371.48s\ttraining loss:\t4.077609\n",
      "Done 1660 batches in 373.72s\ttraining loss:\t4.075935\n",
      "Done 1670 batches in 376.08s\ttraining loss:\t4.074232\n",
      "Done 1680 batches in 378.47s\ttraining loss:\t4.072474\n",
      "Done 1690 batches in 380.88s\ttraining loss:\t4.070784\n",
      "Done 1700 batches in 382.93s\ttraining loss:\t4.069019\n",
      "Done 1710 batches in 385.18s\ttraining loss:\t4.067226\n",
      "Done 1720 batches in 387.35s\ttraining loss:\t4.065605\n",
      "Done 1730 batches in 389.80s\ttraining loss:\t4.064022\n",
      "Done 1740 batches in 391.99s\ttraining loss:\t4.062348\n",
      "Done 1750 batches in 394.28s\ttraining loss:\t4.061159\n",
      "Done 1760 batches in 396.61s\ttraining loss:\t4.059609\n",
      "Done 1770 batches in 398.69s\ttraining loss:\t4.058151\n",
      "Done 1780 batches in 401.10s\ttraining loss:\t4.056578\n",
      "Done 1790 batches in 403.35s\ttraining loss:\t4.055172\n",
      "Done 1800 batches in 405.62s\ttraining loss:\t4.053680\n",
      "Done 1810 batches in 407.93s\ttraining loss:\t4.052161\n",
      "Done 1820 batches in 410.40s\ttraining loss:\t4.050591\n",
      "Done 1830 batches in 412.68s\ttraining loss:\t4.049168\n",
      "Done 1840 batches in 414.87s\ttraining loss:\t4.047673\n",
      "Done 1850 batches in 417.10s\ttraining loss:\t4.046253\n",
      "Done 1860 batches in 419.20s\ttraining loss:\t4.044635\n",
      "Done 1870 batches in 421.49s\ttraining loss:\t4.043374\n",
      "Done 1880 batches in 423.62s\ttraining loss:\t4.041799\n",
      "Done 1890 batches in 426.08s\ttraining loss:\t4.040355\n",
      "Done 1900 batches in 428.31s\ttraining loss:\t4.038913\n",
      "Done 1910 batches in 430.65s\ttraining loss:\t4.037479\n",
      "Done 1920 batches in 433.01s\ttraining loss:\t4.035996\n",
      "Done 1930 batches in 435.42s\ttraining loss:\t4.034585\n",
      "Done 1940 batches in 437.99s\ttraining loss:\t4.033358\n",
      "Done 1950 batches in 440.32s\ttraining loss:\t4.031976\n",
      "Done 1960 batches in 442.76s\ttraining loss:\t4.030534\n",
      "Done 1970 batches in 444.79s\ttraining loss:\t4.029186\n",
      "Done 1980 batches in 446.99s\ttraining loss:\t4.027695\n",
      "Done 1990 batches in 449.30s\ttraining loss:\t4.026389\n",
      "Done 2000 batches in 451.80s\ttraining loss:\t4.025096\n",
      "Done 2010 batches in 454.36s\ttraining loss:\t4.023791\n",
      "Done 2020 batches in 456.70s\ttraining loss:\t4.022368\n",
      "Done 2030 batches in 459.08s\ttraining loss:\t4.020980\n",
      "Done 2040 batches in 461.37s\ttraining loss:\t4.019752\n",
      "Done 2050 batches in 463.53s\ttraining loss:\t4.018435\n",
      "Done 2060 batches in 465.96s\ttraining loss:\t4.017157\n",
      "Done 2070 batches in 468.28s\ttraining loss:\t4.015820\n",
      "Done 2080 batches in 470.65s\ttraining loss:\t4.014690\n",
      "Done 2090 batches in 472.96s\ttraining loss:\t4.013437\n",
      "Done 2100 batches in 475.22s\ttraining loss:\t4.012096\n",
      "Done 2110 batches in 477.36s\ttraining loss:\t4.010746\n",
      "Done 2120 batches in 479.65s\ttraining loss:\t4.009539\n",
      "Done 2130 batches in 481.78s\ttraining loss:\t4.008243\n",
      "Done 2140 batches in 484.02s\ttraining loss:\t4.006951\n",
      "Done 2150 batches in 486.45s\ttraining loss:\t4.005713\n",
      "Done 2160 batches in 488.83s\ttraining loss:\t4.004554\n",
      "Done 2170 batches in 491.31s\ttraining loss:\t4.003345\n",
      "Done 2180 batches in 493.49s\ttraining loss:\t4.002240\n",
      "Done 2190 batches in 495.59s\ttraining loss:\t4.000999\n",
      "Done 2200 batches in 497.95s\ttraining loss:\t3.999688\n",
      "Done 2210 batches in 500.35s\ttraining loss:\t3.998499\n",
      "Done 2220 batches in 502.60s\ttraining loss:\t3.997257\n",
      "Done 2230 batches in 504.73s\ttraining loss:\t3.996096\n",
      "Done 2240 batches in 507.17s\ttraining loss:\t3.994945\n",
      "Done 2250 batches in 509.57s\ttraining loss:\t3.993786\n",
      "Done 2260 batches in 511.79s\ttraining loss:\t3.992510\n",
      "Done 2270 batches in 514.13s\ttraining loss:\t3.991422\n",
      "Done 2280 batches in 516.52s\ttraining loss:\t3.990289\n",
      "Done 2290 batches in 519.07s\ttraining loss:\t3.989259\n",
      "Done 2300 batches in 521.50s\ttraining loss:\t3.988187\n",
      "Done 2310 batches in 523.97s\ttraining loss:\t3.987148\n",
      "Done 2320 batches in 526.32s\ttraining loss:\t3.986108\n",
      "Done 2330 batches in 528.88s\ttraining loss:\t3.985021\n",
      "Done 2340 batches in 531.19s\ttraining loss:\t3.983891\n",
      "Done 2350 batches in 533.56s\ttraining loss:\t3.982854\n",
      "Done 2360 batches in 535.79s\ttraining loss:\t3.981749\n",
      "Done 2370 batches in 538.12s\ttraining loss:\t3.980710\n",
      "Done 2380 batches in 540.36s\ttraining loss:\t3.979572\n",
      "Done 2390 batches in 542.64s\ttraining loss:\t3.978462\n",
      "Done 2400 batches in 544.98s\ttraining loss:\t3.977413\n",
      "Done 2410 batches in 547.41s\ttraining loss:\t3.976435\n",
      "Done 2420 batches in 549.94s\ttraining loss:\t3.975457\n",
      "Done 2430 batches in 552.29s\ttraining loss:\t3.974505\n",
      "Done 2440 batches in 554.54s\ttraining loss:\t3.973479\n",
      "Done 2450 batches in 556.87s\ttraining loss:\t3.972450\n",
      "Done 2460 batches in 559.17s\ttraining loss:\t3.971570\n",
      "Done 2470 batches in 561.55s\ttraining loss:\t3.970656\n",
      "Done 2480 batches in 563.99s\ttraining loss:\t3.969620\n",
      "Done 2490 batches in 566.51s\ttraining loss:\t3.968587\n",
      "Done 2500 batches in 568.89s\ttraining loss:\t3.967586\n",
      "Done 2510 batches in 571.16s\ttraining loss:\t3.966592\n",
      "Done 2520 batches in 573.43s\ttraining loss:\t3.965771\n",
      "Done 2530 batches in 576.05s\ttraining loss:\t3.964767\n",
      "Done 2540 batches in 578.35s\ttraining loss:\t3.963708\n",
      "Done 2550 batches in 580.82s\ttraining loss:\t3.962836\n",
      "Done 2560 batches in 583.21s\ttraining loss:\t3.961966\n",
      "Done 2570 batches in 585.57s\ttraining loss:\t3.960961\n",
      "Done 2580 batches in 588.17s\ttraining loss:\t3.959971\n",
      "Done 2590 batches in 590.43s\ttraining loss:\t3.959007\n",
      "Done 2600 batches in 592.80s\ttraining loss:\t3.958148\n",
      "Done 2610 batches in 595.27s\ttraining loss:\t3.957253\n",
      "Done 2620 batches in 597.78s\ttraining loss:\t3.956217\n",
      "Done 2630 batches in 600.13s\ttraining loss:\t3.955274\n",
      "Done 2640 batches in 602.70s\ttraining loss:\t3.954323\n",
      "Done 2650 batches in 605.06s\ttraining loss:\t3.953379\n",
      "Done 2660 batches in 607.31s\ttraining loss:\t3.952587\n",
      "Done 2670 batches in 609.48s\ttraining loss:\t3.951713\n",
      "Done 2680 batches in 612.02s\ttraining loss:\t3.950839\n",
      "Done 2690 batches in 614.39s\ttraining loss:\t3.950051\n",
      "Done 2700 batches in 616.78s\ttraining loss:\t3.949177\n",
      "Done 2710 batches in 618.92s\ttraining loss:\t3.948240\n",
      "Done 2720 batches in 621.18s\ttraining loss:\t3.947408\n",
      "Done 2730 batches in 623.42s\ttraining loss:\t3.946521\n",
      "Done 2740 batches in 625.67s\ttraining loss:\t3.945598\n",
      "Done 2750 batches in 628.77s\ttraining loss:\t3.944729\n",
      "Done 2760 batches in 631.15s\ttraining loss:\t3.944047\n",
      "Done 2770 batches in 633.72s\ttraining loss:\t3.943169\n",
      "Done 2780 batches in 635.76s\ttraining loss:\t3.942255\n",
      "Done 2790 batches in 638.15s\ttraining loss:\t3.941399\n",
      "Done 2800 batches in 640.38s\ttraining loss:\t3.940572\n",
      "Done 2810 batches in 642.66s\ttraining loss:\t3.939798\n",
      "Done 2820 batches in 644.93s\ttraining loss:\t3.938972\n",
      "Done 2830 batches in 647.62s\ttraining loss:\t3.938143\n",
      "Done 2840 batches in 649.99s\ttraining loss:\t3.937214\n",
      "Done 2850 batches in 652.28s\ttraining loss:\t3.936290\n",
      "Done 2860 batches in 654.90s\ttraining loss:\t3.935433\n",
      "Done 2870 batches in 657.36s\ttraining loss:\t3.934615\n",
      "Done 2880 batches in 659.56s\ttraining loss:\t3.933776\n",
      "Done 2890 batches in 662.01s\ttraining loss:\t3.932944\n",
      "Done 2900 batches in 664.44s\ttraining loss:\t3.932143\n",
      "Done 2910 batches in 666.49s\ttraining loss:\t3.931251\n",
      "Done 2920 batches in 668.83s\ttraining loss:\t3.930373\n",
      "Done 2930 batches in 671.09s\ttraining loss:\t3.929480\n",
      "Done 2940 batches in 673.47s\ttraining loss:\t3.928676\n",
      "Done 2950 batches in 675.91s\ttraining loss:\t3.927835\n",
      "Done 2960 batches in 678.36s\ttraining loss:\t3.927138\n",
      "Done 2970 batches in 680.57s\ttraining loss:\t3.926328\n",
      "Done 2980 batches in 682.86s\ttraining loss:\t3.925629\n",
      "Done 2990 batches in 685.23s\ttraining loss:\t3.924826\n",
      "Done 3000 batches in 687.69s\ttraining loss:\t3.923985\n",
      "Done 3010 batches in 689.94s\ttraining loss:\t3.923259\n",
      "Done 3020 batches in 692.06s\ttraining loss:\t3.922475\n",
      "Done 3030 batches in 694.45s\ttraining loss:\t3.921664\n",
      "Done 3040 batches in 696.91s\ttraining loss:\t3.920936\n",
      "Done 3050 batches in 699.43s\ttraining loss:\t3.920098\n",
      "Done 3060 batches in 701.74s\ttraining loss:\t3.919326\n",
      "Done 3070 batches in 704.12s\ttraining loss:\t3.918521\n",
      "Done 3080 batches in 706.44s\ttraining loss:\t3.917740\n",
      "Done 3090 batches in 708.79s\ttraining loss:\t3.917043\n",
      "Done 3100 batches in 711.01s\ttraining loss:\t3.916324\n",
      "Done 3110 batches in 713.35s\ttraining loss:\t3.915686\n",
      "Done 3120 batches in 715.75s\ttraining loss:\t3.914843\n",
      "Done 3130 batches in 718.25s\ttraining loss:\t3.914071\n",
      "Done 3140 batches in 720.44s\ttraining loss:\t3.913320\n",
      "Done 3150 batches in 722.46s\ttraining loss:\t3.912589\n",
      "Done 3160 batches in 724.86s\ttraining loss:\t3.911966\n",
      "Done 3170 batches in 727.20s\ttraining loss:\t3.911096\n",
      "Done 3180 batches in 729.65s\ttraining loss:\t3.910414\n",
      "Done 3190 batches in 731.97s\ttraining loss:\t3.909733\n",
      "Done 3200 batches in 734.29s\ttraining loss:\t3.909092\n",
      "Done 3210 batches in 736.60s\ttraining loss:\t3.908478\n",
      "Done 3220 batches in 738.99s\ttraining loss:\t3.907769\n",
      "Done 3230 batches in 741.45s\ttraining loss:\t3.906992\n",
      "Done 3240 batches in 743.83s\ttraining loss:\t3.906343\n",
      "Done 3250 batches in 746.18s\ttraining loss:\t3.905668\n",
      "Done 3260 batches in 748.41s\ttraining loss:\t3.904965\n",
      "Done 3270 batches in 750.83s\ttraining loss:\t3.904260\n",
      "Done 3280 batches in 753.36s\ttraining loss:\t3.903614\n",
      "Done 3290 batches in 755.54s\ttraining loss:\t3.902932\n",
      "Done 3300 batches in 757.98s\ttraining loss:\t3.902191\n",
      "Done 3310 batches in 760.20s\ttraining loss:\t3.901536\n",
      "Done 3320 batches in 762.38s\ttraining loss:\t3.900828\n",
      "Done 3330 batches in 764.85s\ttraining loss:\t3.900163\n",
      "Done 3340 batches in 767.18s\ttraining loss:\t3.899509\n",
      "Done 3350 batches in 769.56s\ttraining loss:\t3.898950\n",
      "Done 3360 batches in 771.88s\ttraining loss:\t3.898272\n",
      "Done 3370 batches in 774.24s\ttraining loss:\t3.897532\n",
      "Done 3380 batches in 776.56s\ttraining loss:\t3.896847\n",
      "Done 3390 batches in 778.77s\ttraining loss:\t3.896185\n",
      "Done 3400 batches in 781.14s\ttraining loss:\t3.895491\n",
      "Done 3410 batches in 783.54s\ttraining loss:\t3.894863\n",
      "Done 3420 batches in 785.66s\ttraining loss:\t3.894202\n",
      "Done 3430 batches in 787.86s\ttraining loss:\t3.893536\n",
      "Done 3440 batches in 790.19s\ttraining loss:\t3.892920\n",
      "Done 3450 batches in 792.37s\ttraining loss:\t3.892438\n",
      "Done 3460 batches in 794.83s\ttraining loss:\t3.891723\n",
      "Done 3470 batches in 797.15s\ttraining loss:\t3.891115\n",
      "Done 3480 batches in 799.51s\ttraining loss:\t3.890456\n",
      "Done 3490 batches in 801.92s\ttraining loss:\t3.889980\n",
      "Done 3500 batches in 804.38s\ttraining loss:\t3.889311\n",
      "Done 3510 batches in 806.71s\ttraining loss:\t3.888695\n",
      "Done 3520 batches in 809.03s\ttraining loss:\t3.888082\n",
      "Done 3530 batches in 811.22s\ttraining loss:\t3.887432\n",
      "Done 3540 batches in 813.57s\ttraining loss:\t3.886846\n",
      "Done 3550 batches in 815.74s\ttraining loss:\t3.886129\n",
      "Done 3560 batches in 818.27s\ttraining loss:\t3.885533\n",
      "Done 3570 batches in 820.69s\ttraining loss:\t3.884884\n",
      "Done 3580 batches in 823.08s\ttraining loss:\t3.884234\n",
      "Done 3590 batches in 825.54s\ttraining loss:\t3.883579\n",
      "Done 3600 batches in 828.12s\ttraining loss:\t3.883007\n",
      "Done 3610 batches in 830.57s\ttraining loss:\t3.882443\n",
      "Done 3620 batches in 832.69s\ttraining loss:\t3.881739\n",
      "Done 3630 batches in 834.85s\ttraining loss:\t3.881171\n",
      "Done 3640 batches in 837.26s\ttraining loss:\t3.880555\n",
      "Done 3650 batches in 839.65s\ttraining loss:\t3.879890\n",
      "Done 3660 batches in 841.88s\ttraining loss:\t3.879316\n",
      "Done 3670 batches in 844.07s\ttraining loss:\t3.878699\n",
      "Done 3680 batches in 846.27s\ttraining loss:\t3.878062\n",
      "Done 3690 batches in 848.49s\ttraining loss:\t3.877428\n",
      "Done 3700 batches in 850.90s\ttraining loss:\t3.876898\n",
      "Done 3710 batches in 853.09s\ttraining loss:\t3.876228\n",
      "Done 3720 batches in 855.37s\ttraining loss:\t3.875597\n",
      "Done 3730 batches in 857.61s\ttraining loss:\t3.875010\n",
      "Done 3740 batches in 859.98s\ttraining loss:\t3.874369\n",
      "Done 3750 batches in 862.27s\ttraining loss:\t3.873794\n",
      "Done 3760 batches in 864.72s\ttraining loss:\t3.873181\n",
      "Done 3770 batches in 866.97s\ttraining loss:\t3.872703\n",
      "Done 3780 batches in 869.22s\ttraining loss:\t3.872196\n",
      "Done 3790 batches in 871.60s\ttraining loss:\t3.871621\n",
      "Done 3800 batches in 874.01s\ttraining loss:\t3.871033\n",
      "Done 3810 batches in 876.42s\ttraining loss:\t3.870496\n",
      "Done 3820 batches in 878.89s\ttraining loss:\t3.869979\n",
      "Done 3830 batches in 881.10s\ttraining loss:\t3.869373\n",
      "Done 3840 batches in 883.54s\ttraining loss:\t3.868865\n",
      "Done 3850 batches in 886.18s\ttraining loss:\t3.868310\n",
      "Done 3860 batches in 888.61s\ttraining loss:\t3.867835\n",
      "Done 3870 batches in 891.02s\ttraining loss:\t3.867305\n",
      "Done 3880 batches in 893.38s\ttraining loss:\t3.866735\n",
      "Done 3890 batches in 895.56s\ttraining loss:\t3.866169\n",
      "Done 100 batches in 7.41s\n",
      "Done 200 batches in 14.74s\n",
      "Done 300 batches in 21.96s\n",
      "Done 400 batches in 29.56s\n",
      "Done 500 batches in 37.44s\n",
      "Done 600 batches in 44.84s\n",
      "Done 700 batches in 52.66s\n",
      "Done 800 batches in 60.34s\n",
      "Done 900 batches in 68.31s\n",
      "Epoch 1 of 1 took 972.50s\n",
      "  training loss:\t\t3.865752\n",
      "  validation loss:\t\t3.707451\n"
     ]
    }
   ],
   "source": [
    "net.train_model(num_epochs=1,\n",
    "                path=None,\n",
    "                save_params=False,\n",
    "                train_batch_size=50,\n",
    "                train_data=train,\n",
    "                val_batch_size=25,\n",
    "                val_data=valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnd_next_word(probs, size=1):\n",
    "    return np.random.choice(np.append(np.arange(probs.shape[0]-1), -1).astype(np.int32), \n",
    "                            size=size, p=probs)\n",
    "\n",
    "def beam_search(get_probs_fun, beam=10, init_seq='', mode='rr'):\n",
    "    utt = map(lambda w: w_to_idx.get(w, w_to_idx['<unk>']), init_seq.split())\n",
    "    if len(utt) == 0 or utt[0] != 1:\n",
    "        utt = [1] + utt\n",
    "    utt = np.asarray(utt, dtype=np.int32)[np.newaxis]\n",
    "    \n",
    "    if mode[0] == 's':\n",
    "        words = get_probs_fun(utt)[0].argpartition(-beam)[-beam:].astype(np.int32)\n",
    "        words[words==voc_size-1] = pad_value\n",
    "    elif mode[0] == 'r':\n",
    "        words = rnd_next_word(get_probs_fun(utt)[0], beam)\n",
    "    \n",
    "    candidates = utt.repeat(beam, axis=0)\n",
    "    candidates = np.hstack([candidates, words[np.newaxis].T])\n",
    "    scores = np.zeros(beam)\n",
    "    \n",
    "#     print candidates\n",
    "    \n",
    "    while candidates.shape[1] < 100 and pad_value not in candidates[:,-1]:\n",
    "        \n",
    "        if mode[1] == 's':\n",
    "            log_probs = np.log(get_probs_fun(candidates))\n",
    "            tot_scores = log_probs + scores[np.newaxis].T\n",
    "\n",
    "            idx = tot_scores.ravel().argpartition(-beam)[-beam:]\n",
    "            i,j = divmod(idx, tot_scores.shape[1])\n",
    "            j[j==voc_size-1] = pad_value\n",
    "            \n",
    "            scores = tot_scores[i,j]\n",
    "\n",
    "            candidates = np.hstack([candidates[i], j[np.newaxis].T.astype(np.int32)])\n",
    "            \n",
    "        elif mode[1] == 'r':\n",
    "            probs = get_probs_fun(candidates)\n",
    "            words = []\n",
    "            for k in xrange(beam):\n",
    "                words.append(rnd_next_word(probs[k], beam)) # this doesn't have to be exactly 'beam'\n",
    "            words = np.array(words)\n",
    "            idx = np.indices((beam, words.shape[1]))[0]\n",
    "            tot_scores = scores[np.newaxis].T + np.log(probs)[idx, words]\n",
    "                \n",
    "            idx = tot_scores.ravel().argpartition(-beam)[-beam:]\n",
    "            i,j = divmod(idx, tot_scores.shape[1])\n",
    "\n",
    "            scores = tot_scores[i,j]\n",
    "\n",
    "            candidates = np.hstack([candidates[i], words[i,j][np.newaxis].T])\n",
    "            \n",
    "#     print candidates[:,:10]\n",
    "#     print scores[:10]\n",
    "        \n",
    "    cands = candidates[candidates[:,-1] == pad_value]\n",
    "    if cands.size > 0:\n",
    "        return candidates[candidates[:,-1] == pad_value][0]\n",
    "    return candidates[scores.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s> excuse me ? </s> <s> i ' m not going to be a lot of time . </s> <s> i ' m not going to be a <unk> . </s> <s> i ' m not going to be a long time . </s> <s> i ' m not . </s> <s> i ' m not going to be a lot of time . </s> <s> i ' m not going to be a lot of time . </s> <s> i ' m not going to be a lot of <unk> . </s> <s> i ' m not a <unk> . </s> <s> i ' m not going to be a lot of <unk> . </s> <s> i ' m not going to be a lot of time . </s> <s> i ' m not going to be a lot of <unk> . </s> <s> i ' m not going to be a lot of time . </s> <s> i ' m not going to be a man . </s> <s> i ' m not going to be a lot of time . </s> <s> i ' m not going to be a lot of time . </s> <s> i ' m not going to be a <unk> . </s> <s> i ' m not going to be a lot of time . </s> <s> i ' m not going to be a lot of time . </s> <s> i ' m not going to be a lot of time . </s> <s> i ' m not going to be a lot of <unk> . </s> <s> i ' m not a <unk> . </s> <s> i ' m not going to be a lot of time . </s> <s> i ' m not going to tell you what i ' m going to be . </s> <s> i ' m not going to be a <unk> . </s> <s> i ' m not going to be a lot of time . </s> <s> i ' m not going to be a lot of time . </s> <s> i ' m not . </s> <s> i ' m not a <unk> . </s> <s> i ' m not going to be a <unk> . </s> <s> i ' m not . </s> <s> i ' m not going to be a lot of time . </s> <s> i ' m not going to be a lot of <unk> . </s> <s> i ' m not going to be a lot of <unk> . </s> <s> i ' m not going to be a lot of time . </s> <s> i ' m not going to be a <unk> . </s> <s> i ' m not going to be a <unk> . </s> <s> i ' m not going to be a lot of time . </s> <s> i ' m not going to be a lot of time . </s> <s> i ' m not going to be a lot\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utt = beam_search(net.get_probs_fn, init_seq='', beam=10, mode='rr')\n",
    "\n",
    "text = map(lambda i: idx_to_w[i], list(utt))\n",
    "' '.join([t for t in text])# if t not in ['<s>', '</s>', '<utt_end>']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"so you do big bones . an instant spa news name ? what do you remember she seem good about your production assistant ? right with the bouquet after the street . these that ' s always a gentlemen , as it would not happen . one of the premium sessions . that was they is innocent by somebody ' s making ok , i can bust four hundred examiner hundred up , <number> authority -- it may sign of me too you called the f <unk> chamber , exactly what is that ?\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_seq = ''\n",
    "utt = [1] + map(lambda w: w_to_idx.get(w, w_to_idx['<unk>']), init_seq.split())\n",
    "utt = np.asarray(utt, dtype=np.int32)[np.newaxis]\n",
    "\n",
    "i = 0\n",
    "while utt[0,-1] != -1 and i < 100:\n",
    "    word_probs = net.get_probs_fn(utt)[0]\n",
    "    next_idx = rnd_next_word(word_probs)\n",
    "    utt = np.append(utt, next_idx)[np.newaxis].astype(np.int32)\n",
    "    i += 1\n",
    "    \n",
    "text = map(lambda i: idx_to_w[i], list(utt[0]))\n",
    "' '.join([t for t in text if t not in ['<s>', '</s>', '<utt_end>']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

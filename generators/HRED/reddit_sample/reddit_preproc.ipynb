{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json, nltk, io, pickle\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from collections import defaultdict as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this is just for debug reasons, I use diffrent dictionary\n",
    "\n",
    "words = []\n",
    "with io.open('/pio/data/data/reddit_sample/freqs', 'r', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 20000:\n",
    "            break\n",
    "        words.append(line.split()[1])\n",
    "\n",
    "words = set(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess singles and make a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "single = []\n",
    "with io.open('/pio/data/data/reddit_sample/utterances.single.shuffled', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        single.append(nltk.word_tokenize(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ws = dd(lambda: 0)\n",
    "for s in single:\n",
    "    for w in s:\n",
    "        ws[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ws_sorted = sorted(ws.items(), key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "voc = list(zip(*ws_sorted[:20000])[0])\n",
    "voc.insert(0, '</s>')\n",
    "voc.insert(0, '<s>')\n",
    "voc.insert(0, '<unk>')\n",
    "\n",
    "i_to_w = voc\n",
    "w_to_i = {v:k for (k,v) in enumerate(voc)}\n",
    "voc = set(voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/pio/data/data/reddit_sample/wordlist.pkl', 'w') as f:\n",
    "    pickle.dump(i_to_w, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "single_cut = [[1] + [w_to_i.get(w, 0) for w in s] + [2] for s in single]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('/pio/data/data/reddit_sample/utterances.single.shuffled.pkl', 'w') as f:\n",
    "    pickle.dump(single_cut, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freqs = [0] * len(voc)\n",
    "\n",
    "for s in single_cut:\n",
    "    for w in s:\n",
    "        freqs[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freqs = np.array(freqs, dtype=np.float32)\n",
    "freqs /= sum(1 for s in single_cut for w in s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('/pio/data/data/reddit_sample/freqs.pkl', 'w') as f:\n",
    "    pickle.dump(freqs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pairs = []\n",
    "with io.open('/pio/data/data/reddit_sample/utterances.pairs.shuffled', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        pairs.append(map(nltk.word_tokenize, line.split('\\t')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pairs_cut = [[[1] + [w_to_i.get(w, 0) for w in u] + [2] for u in s] for s in pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('/pio/data/data/reddit_sample/utterances.pairs.shuffled.pkl', 'w') as f:\n",
    "    pickle.dump(pairs_cut, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 342, 2], [1, 28, 40, 1918, 4, 2]]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_cut[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a train - test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_count = len(single_cut) / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = single_cut[:-test_count], single_cut[-test_count:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('/pio/data/data/reddit_sample/utterances.single.shuffled.train.pkl', 'w') as f:\n",
    "    pickle.dump(train, f)\n",
    "    \n",
    "with open('/pio/data/data/reddit_sample/utterances.single.shuffled.test.pkl', 'w') as f:\n",
    "    pickle.dump(test, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Same with pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_count_pairs = len(pairs_cut) / 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pairs, test_pairs = pairs_cut[:-test_count_pairs], pairs_cut[-test_count_pairs:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/pio/data/data/reddit_sample/utterances.pairs.shuffled.train.pkl', 'w') as f:\n",
    "    pickle.dump(train_pairs, f)\n",
    "    \n",
    "with open('/pio/data/data/reddit_sample/utterances.pairs.shuffled.test.pkl', 'w') as f:\n",
    "    pickle.dump(test_pairs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RedditV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v3_all = []\n",
    "v3_words = set()\n",
    "\n",
    "with io.open('/pio/data/data/reddit_sample/v3/pairsv3.uniq.censored', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        s1, s2 = line.split('\\t')\n",
    "        ws1 = nltk.word_tokenize(s1.lower())\n",
    "        ws2 = nltk.word_tokenize(s2.lower())\n",
    "        v3_words |= set(ws1)\n",
    "        v3_words |= set(ws2)\n",
    "        v3_all.append([ws1, ws2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32493"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(v3_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'when',\n",
       "  u'do',\n",
       "  u'you',\n",
       "  u'think',\n",
       "  u'we',\n",
       "  u'will',\n",
       "  u'see',\n",
       "  u'a',\n",
       "  u'fight',\n",
       "  u'of',\n",
       "  u'this',\n",
       "  u'magnitude',\n",
       "  u'again',\n",
       "  u'?'],\n",
       " [u'hopefully', u',', u'never', u'.']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v3_all[241442]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "glove_words = np.load('/pio/data/data/glove_vec/6B/glove/glove.6B.wordlist.pkl')\n",
    "glove_w_to_i = {glove_words[i] : i for i in range(len(glove_words))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'<s>'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_words[400002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_count = len(v3_all) / 10\n",
    "inds = np.random.choice(len(v3_all), size=test_count, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ns_for_s(s):\n",
    "    return [400002] + [glove_w_to_i.get(w, 0) for w in s] + [400003]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v3_num = []\n",
    "\n",
    "for s1, s2 in v3_all:\n",
    "    v3_num.append([get_ns_for_s(s1), get_ns_for_s(s2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inds = set(inds)\n",
    "test_set = []\n",
    "train_set = []\n",
    "\n",
    "for i in xrange(len(v3_num)):\n",
    "    if i in inds:\n",
    "        test_set.append(v3_num[i])\n",
    "    else:\n",
    "        train_set.append(v3_num[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/pio/data/data/reddit_sample/v3/pairsv3.uniq.censored.glove6B.train.pkl', 'w') as f:\n",
    "    pickle.dump(train_set, f)\n",
    "    \n",
    "with open('/pio/data/data/reddit_sample/v3/pairsv3.uniq.censored.glove6B.test.pkl', 'w') as f:\n",
    "    pickle.dump(test_set, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = [n for d in v3_num for s in d for n in s[1:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.array(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = np.unique(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4334973,   53231,   17589,   10620,    3574,    3230,    2162,\n",
       "           1825,    1143,    1467]),\n",
       " array([      0. ,   39972.6,   79945.2,  119917.8,  159890.4,  199863. ,\n",
       "         239835.6,  279808.2,  319780.8,  359753.4,  399726. ]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.histogram(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([19092,  3558,  1302,   603,   382,   262,   192,   173,   143,   149]),\n",
       " array([      0. ,   39972.6,   79945.2,  119917.8,  159890.4,  199863. ,\n",
       "         239835.6,  279808.2,  319780.8,  359753.4,  399726. ]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.histogram(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(s) for d in v3_num for s in d])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json, nltk, io, pickle\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from collections import defaultdict as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this is just for debug reasons, I use diffrent dictionary\n",
    "\n",
    "words = []\n",
    "with io.open('/pio/data/data/reddit_sample/freqs', 'r', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 20000:\n",
    "            break\n",
    "        words.append(line.split()[1])\n",
    "\n",
    "words = set(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess singles and make a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "single = []\n",
    "with io.open('/pio/data/data/reddit_sample/utterances.single.shuffled', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        single.append(nltk.word_tokenize(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ws = dd(lambda: 0)\n",
    "for s in single:\n",
    "    for w in s:\n",
    "        ws[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ws_sorted = sorted(ws.items(), key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "voc = list(zip(*ws_sorted[:20000])[0])\n",
    "voc.insert(0, '</s>')\n",
    "voc.insert(0, '<s>')\n",
    "voc.insert(0, '<unk>')\n",
    "\n",
    "i_to_w = voc\n",
    "w_to_i = {v:k for (k,v) in enumerate(voc)}\n",
    "voc = set(voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/pio/data/data/reddit_sample/wordlist.pkl', 'w') as f:\n",
    "    pickle.dump(i_to_w, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "single_cut = [[1] + [w_to_i.get(w, 0) for w in s] + [2] for s in single]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('/pio/data/data/reddit_sample/utterances.single.shuffled.pkl', 'w') as f:\n",
    "    pickle.dump(single_cut, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freqs = [0] * len(voc)\n",
    "\n",
    "for s in single_cut:\n",
    "    for w in s:\n",
    "        freqs[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freqs = np.array(freqs, dtype=np.float32)\n",
    "freqs /= sum(1 for s in single_cut for w in s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('/pio/data/data/reddit_sample/freqs.pkl', 'w') as f:\n",
    "    pickle.dump(freqs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pairs = []\n",
    "with io.open('/pio/data/data/reddit_sample/utterances.pairs.shuffled', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        pairs.append(map(nltk.word_tokenize, line.split('\\t')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pairs_cut = [[[1] + [w_to_i.get(w, 0) for w in u] + [2] for u in s] for s in pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('/pio/data/data/reddit_sample/utterances.pairs.shuffled.pkl', 'w') as f:\n",
    "    pickle.dump(pairs_cut, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 342, 2], [1, 28, 40, 1918, 4, 2]]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_cut[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a train - test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_count = len(single_cut) / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = single_cut[:-test_count], single_cut[-test_count:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('/pio/data/data/reddit_sample/utterances.single.shuffled.train.pkl', 'w') as f:\n",
    "    pickle.dump(train, f)\n",
    "    \n",
    "with open('/pio/data/data/reddit_sample/utterances.single.shuffled.test.pkl', 'w') as f:\n",
    "    pickle.dump(test, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Same with pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_count_pairs = len(pairs_cut) / 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pairs, test_pairs = pairs_cut[:-test_count_pairs], pairs_cut[-test_count_pairs:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/pio/data/data/reddit_sample/utterances.pairs.shuffled.train.pkl', 'w') as f:\n",
    "    pickle.dump(train_pairs, f)\n",
    "    \n",
    "with open('/pio/data/data/reddit_sample/utterances.pairs.shuffled.test.pkl', 'w') as f:\n",
    "    pickle.dump(test_pairs, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: GeForce GTX 780 (CNMeM is enabled with initial size: 30.0% of memory, cuDNN 5105)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import time\n",
    "\n",
    "import lasagne as L\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from reddit_load import load_singles, load_pairs, get_reddit_voc\n",
    "\n",
    "%aimport HRED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.62 s, sys: 109 ms, total: 9.72 s\n",
      "Wall time: 9.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "reddit_path = \"/pio/data/data/reddit_sample/\"\n",
    "\n",
    "train_singles, test_singles = load_singles(path=reddit_path)\n",
    "data_pairs = load_pairs(path=reddit_path)\n",
    "idx_to_w, w_to_idx, voc_size, freqs = get_reddit_voc(path=reddit_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model...\n",
      "Compiling theano functions...\n",
      "Building a network for generating...\n",
      "Done\n",
      "CPU times: user 36.5 s, sys: 965 ms, total: 37.5 s\n",
      "Wall time: 37.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "hred_net = HRED.HRED(voc_size=voc_size,\n",
    "                     emb_size=300,\n",
    "                     lv1_rec_size=300, \n",
    "                     lv2_rec_size=300, \n",
    "                     out_emb_size=300, \n",
    "                     num_sampled=200,\n",
    "                     ssoft_probs=freqs,\n",
    "                     n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hred_net.load_params('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 100 batches in 5.00s\ttraining loss:\t3.428550\n",
      "Done 200 batches in 9.97s\ttraining loss:\t3.421801\n",
      "Done 300 batches in 14.99s\ttraining loss:\t3.422451\n",
      "Done 400 batches in 19.95s\ttraining loss:\t3.414273\n",
      "Done 500 batches in 24.35s\ttraining loss:\t3.395562\n",
      "Done 600 batches in 28.43s\ttraining loss:\t3.354878\n",
      "Done 700 batches in 32.85s\ttraining loss:\t3.345874\n",
      "Done 800 batches in 37.86s\ttraining loss:\t3.360305\n",
      "Done 900 batches in 42.60s\ttraining loss:\t3.361262\n",
      "Done 1000 batches in 47.48s\ttraining loss:\t3.366504\n",
      "Done 1100 batches in 52.54s\ttraining loss:\t3.374706\n",
      "Done 1200 batches in 57.65s\ttraining loss:\t3.379332\n",
      "Done 1300 batches in 62.66s\ttraining loss:\t3.383389\n",
      "Done 1400 batches in 67.66s\ttraining loss:\t3.385690\n",
      "Done 1500 batches in 72.73s\ttraining loss:\t3.387937\n",
      "Done 1600 batches in 77.82s\ttraining loss:\t3.391383\n",
      "Done 1700 batches in 82.68s\ttraining loss:\t3.392958\n",
      "Done 1800 batches in 85.33s\ttraining loss:\t3.362766\n",
      "Done 1900 batches in 89.99s\ttraining loss:\t3.361663\n",
      "Done 2000 batches in 94.85s\ttraining loss:\t3.363971\n",
      "Done 2100 batches in 96.99s\ttraining loss:\t3.326400\n",
      "Done 2200 batches in 99.94s\ttraining loss:\t3.303825\n",
      "Done 2300 batches in 104.91s\ttraining loss:\t3.308987\n",
      "Done 2400 batches in 109.64s\ttraining loss:\t3.310637\n",
      "Done 2500 batches in 114.44s\ttraining loss:\t3.312929\n",
      "Done 2600 batches in 119.37s\ttraining loss:\t3.317765\n",
      "Done 2700 batches in 124.19s\ttraining loss:\t3.320777\n",
      "Done 2800 batches in 129.21s\ttraining loss:\t3.323297\n",
      "Done 2900 batches in 133.98s\ttraining loss:\t3.325065\n",
      "Done 3000 batches in 139.09s\ttraining loss:\t3.328608\n",
      "Done 3100 batches in 143.74s\ttraining loss:\t3.328521\n",
      "Done 3200 batches in 148.76s\ttraining loss:\t3.331962\n",
      "Done 3300 batches in 153.55s\ttraining loss:\t3.332994\n",
      "Done 3400 batches in 158.47s\ttraining loss:\t3.334687\n",
      "Done 3500 batches in 163.52s\ttraining loss:\t3.337504\n",
      "Done 3600 batches in 168.38s\ttraining loss:\t3.338439\n",
      "Done 3700 batches in 173.27s\ttraining loss:\t3.339498\n",
      "Done 3800 batches in 178.33s\ttraining loss:\t3.341534\n",
      "Done 3900 batches in 183.23s\ttraining loss:\t3.341984\n",
      "Done 4000 batches in 187.99s\ttraining loss:\t3.343080\n",
      "Done 4100 batches in 193.07s\ttraining loss:\t3.345124\n",
      "Done 4200 batches in 197.91s\ttraining loss:\t3.346413\n",
      "Done 4300 batches in 202.86s\ttraining loss:\t3.347371\n",
      "Done 4400 batches in 207.84s\ttraining loss:\t3.348810\n",
      "Done 4500 batches in 212.66s\ttraining loss:\t3.348936\n",
      "Done 4600 batches in 217.47s\ttraining loss:\t3.349333\n",
      "Done 4700 batches in 222.21s\ttraining loss:\t3.349221\n",
      "Done 4800 batches in 227.18s\ttraining loss:\t3.350744\n",
      "Done 4900 batches in 231.93s\ttraining loss:\t3.351112\n",
      "Done 5000 batches in 236.99s\ttraining loss:\t3.352300\n",
      "Done 5100 batches in 241.89s\ttraining loss:\t3.352861\n",
      "Done 5200 batches in 246.67s\ttraining loss:\t3.352331\n",
      "Done 5300 batches in 251.43s\ttraining loss:\t3.352643\n",
      "Done 5400 batches in 256.38s\ttraining loss:\t3.353570\n",
      "Done 5500 batches in 261.35s\ttraining loss:\t3.354185\n",
      "Done 5600 batches in 266.44s\ttraining loss:\t3.355435\n",
      "Done 5700 batches in 271.41s\ttraining loss:\t3.356232\n",
      "Done 5800 batches in 276.24s\ttraining loss:\t3.355837\n",
      "Done 5900 batches in 280.46s\ttraining loss:\t3.352065\n",
      "Done 6000 batches in 285.43s\ttraining loss:\t3.352880\n",
      "Done 6100 batches in 290.29s\ttraining loss:\t3.353812\n",
      "Done 6200 batches in 295.10s\ttraining loss:\t3.354064\n",
      "Done 6300 batches in 299.89s\ttraining loss:\t3.353981\n",
      "Done 6400 batches in 304.75s\ttraining loss:\t3.354698\n",
      "Done 6500 batches in 309.84s\ttraining loss:\t3.355447\n",
      "Done 6600 batches in 312.64s\ttraining loss:\t3.344616\n",
      "Done 6700 batches in 317.32s\ttraining loss:\t3.345339\n",
      "Done 6800 batches in 322.35s\ttraining loss:\t3.346360\n",
      "Done 6900 batches in 326.55s\ttraining loss:\t3.340021\n",
      "Done 7000 batches in 331.14s\ttraining loss:\t3.340170\n",
      "Done 7100 batches in 336.26s\ttraining loss:\t3.341681\n",
      "Done 7200 batches in 339.22s\ttraining loss:\t3.334692\n",
      "Done 7300 batches in 342.89s\ttraining loss:\t3.331145\n",
      "Done 7400 batches in 347.72s\ttraining loss:\t3.331568\n",
      "Done 7500 batches in 352.31s\ttraining loss:\t3.331861\n",
      "Done 7600 batches in 357.23s\ttraining loss:\t3.333004\n",
      "Done 7700 batches in 362.21s\ttraining loss:\t3.333788\n",
      "Done 7800 batches in 367.23s\ttraining loss:\t3.334768\n",
      "Done 7900 batches in 372.30s\ttraining loss:\t3.335551\n",
      "Done 8000 batches in 377.19s\ttraining loss:\t3.335925\n",
      "Done 8100 batches in 382.24s\ttraining loss:\t3.336791\n",
      "Done 8200 batches in 387.14s\ttraining loss:\t3.337220\n",
      "Done 8300 batches in 392.17s\ttraining loss:\t3.337931\n",
      "Done 8400 batches in 396.75s\ttraining loss:\t3.336967\n",
      "Done 8500 batches in 401.77s\ttraining loss:\t3.337559\n",
      "Done 8600 batches in 406.68s\ttraining loss:\t3.338129\n",
      "Done 8700 batches in 411.63s\ttraining loss:\t3.338805\n",
      "Done 8800 batches in 416.30s\ttraining loss:\t3.338236\n",
      "Done 8900 batches in 421.30s\ttraining loss:\t3.338801\n",
      "Done 9000 batches in 426.31s\ttraining loss:\t3.339661\n",
      "Done 9100 batches in 431.30s\ttraining loss:\t3.340363\n",
      "Done 9200 batches in 436.32s\ttraining loss:\t3.341013\n",
      "Done 9300 batches in 441.29s\ttraining loss:\t3.341597\n",
      "Done 9400 batches in 446.11s\ttraining loss:\t3.341663\n",
      "Done 9500 batches in 450.75s\ttraining loss:\t3.340806\n",
      "Done 9600 batches in 455.73s\ttraining loss:\t3.341199\n",
      "Done 9700 batches in 460.72s\ttraining loss:\t3.341527\n",
      "Done 9800 batches in 465.70s\ttraining loss:\t3.342097\n",
      "Done 9900 batches in 470.11s\ttraining loss:\t3.340926\n",
      "Done 10000 batches in 472.26s\ttraining loss:\t3.333082\n",
      "Done 10100 batches in 475.11s\ttraining loss:\t3.327698\n",
      "Done 10200 batches in 480.09s\ttraining loss:\t3.328707\n",
      "Done 10300 batches in 485.13s\ttraining loss:\t3.329389\n",
      "Done 10400 batches in 490.03s\ttraining loss:\t3.329261\n",
      "Done 10500 batches in 495.04s\ttraining loss:\t3.329918\n",
      "Done 10600 batches in 499.87s\ttraining loss:\t3.329940\n",
      "Done 10700 batches in 504.60s\ttraining loss:\t3.329851\n",
      "Done 10800 batches in 509.68s\ttraining loss:\t3.330947\n",
      "Done 10900 batches in 514.75s\ttraining loss:\t3.331647\n",
      "Done 11000 batches in 519.73s\ttraining loss:\t3.331958\n",
      "Done 11100 batches in 524.56s\ttraining loss:\t3.332008\n",
      "Done 11200 batches in 529.46s\ttraining loss:\t3.332297\n",
      "Done 11300 batches in 533.86s\ttraining loss:\t3.329958\n",
      "Done 11400 batches in 538.69s\ttraining loss:\t3.330312\n",
      "Done 11500 batches in 543.63s\ttraining loss:\t3.330588\n",
      "Done 11600 batches in 548.64s\ttraining loss:\t3.331057\n",
      "Done 11700 batches in 553.64s\ttraining loss:\t3.331453\n",
      "Done 11800 batches in 558.62s\ttraining loss:\t3.331927\n",
      "Done 11900 batches in 563.68s\ttraining loss:\t3.332581\n",
      "Done 12000 batches in 568.65s\ttraining loss:\t3.332860\n",
      "Done 12100 batches in 573.68s\ttraining loss:\t3.333435\n",
      "Done 12200 batches in 578.17s\ttraining loss:\t3.332561\n",
      "Done 12300 batches in 583.13s\ttraining loss:\t3.333272\n",
      "Done 12400 batches in 588.15s\ttraining loss:\t3.333813\n",
      "Done 12500 batches in 592.81s\ttraining loss:\t3.333055\n",
      "Done 12600 batches in 595.92s\ttraining loss:\t3.328162\n",
      "Done 12700 batches in 600.67s\ttraining loss:\t3.328408\n",
      "Done 12800 batches in 605.55s\ttraining loss:\t3.328652\n",
      "Done 12900 batches in 610.59s\ttraining loss:\t3.329297\n",
      "Done 13000 batches in 615.60s\ttraining loss:\t3.329774\n",
      "Done 13100 batches in 620.57s\ttraining loss:\t3.330050\n",
      "Done 13200 batches in 625.07s\ttraining loss:\t3.329675\n",
      "Done 13300 batches in 629.49s\ttraining loss:\t3.329219\n",
      "Done 13400 batches in 634.03s\ttraining loss:\t3.329054\n",
      "Done 13500 batches in 638.89s\ttraining loss:\t3.329420\n",
      "Done 13600 batches in 643.82s\ttraining loss:\t3.329859\n",
      "Done 13700 batches in 648.63s\ttraining loss:\t3.329812\n",
      "Done 13800 batches in 653.34s\ttraining loss:\t3.329309\n",
      "Done 13900 batches in 658.17s\ttraining loss:\t3.329473\n",
      "Done 14000 batches in 663.20s\ttraining loss:\t3.329979\n",
      "Done 14100 batches in 668.01s\ttraining loss:\t3.330005\n",
      "Done 14200 batches in 672.58s\ttraining loss:\t3.329859\n",
      "Done 14300 batches in 677.48s\ttraining loss:\t3.329879\n",
      "Done 14400 batches in 680.93s\ttraining loss:\t3.325560\n",
      "Done 14500 batches in 685.90s\ttraining loss:\t3.326033\n",
      "Done 14600 batches in 690.94s\ttraining loss:\t3.326532\n",
      "Done 14700 batches in 695.98s\ttraining loss:\t3.326988\n",
      "Done 14800 batches in 700.98s\ttraining loss:\t3.327079\n",
      "Done 14900 batches in 705.24s\ttraining loss:\t3.326119\n",
      "Done 15000 batches in 708.69s\ttraining loss:\t3.323729\n",
      "Done 15100 batches in 713.67s\ttraining loss:\t3.324178\n",
      "Done 15200 batches in 718.68s\ttraining loss:\t3.324622\n",
      "Done 15300 batches in 723.43s\ttraining loss:\t3.324472\n",
      "Done 15400 batches in 728.30s\ttraining loss:\t3.324736\n",
      "Done 15500 batches in 733.19s\ttraining loss:\t3.325081\n",
      "Done 15600 batches in 738.21s\ttraining loss:\t3.325348\n",
      "Done 15700 batches in 742.98s\ttraining loss:\t3.325366\n",
      "Done 15800 batches in 747.88s\ttraining loss:\t3.325408\n",
      "Done 15900 batches in 752.28s\ttraining loss:\t3.324579\n",
      "Done 16000 batches in 757.07s\ttraining loss:\t3.324494\n",
      "Done 16100 batches in 762.10s\ttraining loss:\t3.324864\n",
      "Done 16200 batches in 767.13s\ttraining loss:\t3.325366\n",
      "Done 16300 batches in 771.52s\ttraining loss:\t3.324946\n",
      "Done 16400 batches in 776.50s\ttraining loss:\t3.325376\n",
      "Done 16500 batches in 781.21s\ttraining loss:\t3.325471\n",
      "Done 16600 batches in 786.18s\ttraining loss:\t3.325722\n",
      "Done 16700 batches in 790.91s\ttraining loss:\t3.325644\n",
      "Done 16800 batches in 795.42s\ttraining loss:\t3.325366\n",
      "Done 16900 batches in 800.24s\ttraining loss:\t3.325689\n",
      "Done 17000 batches in 805.18s\ttraining loss:\t3.326023\n",
      "Done 17100 batches in 810.25s\ttraining loss:\t3.326415\n",
      "Done 17200 batches in 815.20s\ttraining loss:\t3.326544\n",
      "Done 17300 batches in 820.25s\ttraining loss:\t3.326807\n",
      "Done 17400 batches in 824.97s\ttraining loss:\t3.326099\n",
      "Done 17500 batches in 830.07s\ttraining loss:\t3.326552\n",
      "Done 17600 batches in 834.50s\ttraining loss:\t3.326173\n",
      "Done 17700 batches in 839.34s\ttraining loss:\t3.326290\n",
      "Done 17800 batches in 843.80s\ttraining loss:\t3.325348\n",
      "Done 17900 batches in 848.85s\ttraining loss:\t3.325716\n",
      "Done 18000 batches in 853.23s\ttraining loss:\t3.325222\n",
      "Done 18100 batches in 858.14s\ttraining loss:\t3.325485\n",
      "Done 18200 batches in 863.17s\ttraining loss:\t3.325744\n",
      "Done 18300 batches in 868.25s\ttraining loss:\t3.326111\n",
      "Done 18400 batches in 872.98s\ttraining loss:\t3.326179\n",
      "Done 18500 batches in 877.04s\ttraining loss:\t3.325336\n",
      "Done 18600 batches in 882.06s\ttraining loss:\t3.325804\n",
      "Done 18700 batches in 886.83s\ttraining loss:\t3.325582\n",
      "Done 18800 batches in 891.83s\ttraining loss:\t3.325894\n",
      "Done 18900 batches in 896.74s\ttraining loss:\t3.325916\n",
      "Done 19000 batches in 901.77s\ttraining loss:\t3.326071\n",
      "Done 19100 batches in 906.72s\ttraining loss:\t3.326343\n",
      "Done 19200 batches in 911.64s\ttraining loss:\t3.326510\n",
      "Done 19300 batches in 916.65s\ttraining loss:\t3.326690\n",
      "Done 19400 batches in 921.21s\ttraining loss:\t3.326446\n",
      "Done 19500 batches in 925.11s\ttraining loss:\t3.325255\n",
      "Done 19600 batches in 930.02s\ttraining loss:\t3.325485\n",
      "Done 19700 batches in 935.05s\ttraining loss:\t3.325646\n",
      "Done 19800 batches in 939.64s\ttraining loss:\t3.325417\n",
      "Done 19900 batches in 943.98s\ttraining loss:\t3.324903\n",
      "Done 20000 batches in 948.84s\ttraining loss:\t3.325148\n",
      "Done 20100 batches in 953.50s\ttraining loss:\t3.325056\n",
      "Done 20200 batches in 958.31s\ttraining loss:\t3.324960\n",
      "Done 20300 batches in 963.45s\ttraining loss:\t3.325267\n",
      "Done 20400 batches in 968.48s\ttraining loss:\t3.325593\n",
      "Done 20500 batches in 972.26s\ttraining loss:\t3.324069\n",
      "Done 20600 batches in 977.16s\ttraining loss:\t3.324470\n",
      "Done 20700 batches in 981.88s\ttraining loss:\t3.324387\n",
      "Done 20800 batches in 986.86s\ttraining loss:\t3.324582\n",
      "Done 20900 batches in 991.93s\ttraining loss:\t3.324800\n",
      "Done 21000 batches in 996.94s\ttraining loss:\t3.325052\n",
      "Done 21100 batches in 1001.65s\ttraining loss:\t3.324821\n",
      "Done 21200 batches in 1006.72s\ttraining loss:\t3.325147\n",
      "Done 21300 batches in 1011.75s\ttraining loss:\t3.325309\n",
      "Done 21400 batches in 1016.79s\ttraining loss:\t3.325337\n",
      "Done 21500 batches in 1021.72s\ttraining loss:\t3.325572\n",
      "Done 21600 batches in 1026.73s\ttraining loss:\t3.325679\n",
      "Done 21700 batches in 1031.35s\ttraining loss:\t3.325646\n",
      "Done 21800 batches in 1036.31s\ttraining loss:\t3.325697\n",
      "Done 21900 batches in 1041.34s\ttraining loss:\t3.325957\n",
      "Done 22000 batches in 1046.31s\ttraining loss:\t3.326135\n",
      "Done 22100 batches in 1050.99s\ttraining loss:\t3.325867\n",
      "Done 22200 batches in 1055.98s\ttraining loss:\t3.326012\n",
      "Done 22300 batches in 1061.03s\ttraining loss:\t3.326172\n",
      "Done 22400 batches in 1066.08s\ttraining loss:\t3.326399\n",
      "Done 22500 batches in 1071.08s\ttraining loss:\t3.326538\n",
      "Done 22600 batches in 1076.01s\ttraining loss:\t3.326555\n",
      "Done 22700 batches in 1081.01s\ttraining loss:\t3.326665\n",
      "Done 22800 batches in 1085.77s\ttraining loss:\t3.326649\n",
      "Done 22900 batches in 1090.77s\ttraining loss:\t3.326861\n",
      "Done 23000 batches in 1095.73s\ttraining loss:\t3.326972\n",
      "Done 23100 batches in 1100.80s\ttraining loss:\t3.327179\n",
      "Done 23200 batches in 1105.88s\ttraining loss:\t3.327121\n",
      "Done 23300 batches in 1110.68s\ttraining loss:\t3.327186\n",
      "Done 23400 batches in 1115.66s\ttraining loss:\t3.327272\n",
      "Done 23500 batches in 1120.24s\ttraining loss:\t3.326663\n",
      "Done 23600 batches in 1124.80s\ttraining loss:\t3.326606\n",
      "Done 23700 batches in 1129.69s\ttraining loss:\t3.326539\n",
      "Done 23800 batches in 1134.50s\ttraining loss:\t3.326445\n",
      "Done 23900 batches in 1139.06s\ttraining loss:\t3.326406\n",
      "Done 24000 batches in 1143.94s\ttraining loss:\t3.326515\n",
      "Done 24100 batches in 1148.97s\ttraining loss:\t3.326662\n",
      "Done 24200 batches in 1153.84s\ttraining loss:\t3.326742\n",
      "Done 24300 batches in 1158.66s\ttraining loss:\t3.326632\n",
      "Done 24400 batches in 1163.64s\ttraining loss:\t3.326845\n",
      "Done 24500 batches in 1168.51s\ttraining loss:\t3.326745\n",
      "Done 24600 batches in 1173.22s\ttraining loss:\t3.326516\n",
      "Done 24700 batches in 1178.19s\ttraining loss:\t3.326578\n",
      "Done 24800 batches in 1182.55s\ttraining loss:\t3.325784\n",
      "Done 24900 batches in 1187.18s\ttraining loss:\t3.325500\n",
      "Done 25000 batches in 1191.61s\ttraining loss:\t3.325338\n",
      "Done 25100 batches in 1196.11s\ttraining loss:\t3.325016\n",
      "Done 25200 batches in 1201.01s\ttraining loss:\t3.325070\n",
      "Done 25300 batches in 1205.85s\ttraining loss:\t3.325145\n",
      "Done 25400 batches in 1210.73s\ttraining loss:\t3.325114\n",
      "Done 25500 batches in 1215.27s\ttraining loss:\t3.324840\n",
      "Done 25600 batches in 1220.32s\ttraining loss:\t3.325056\n",
      "Done 25700 batches in 1225.33s\ttraining loss:\t3.325218\n",
      "Done 25800 batches in 1230.12s\ttraining loss:\t3.325281\n",
      "Done 25900 batches in 1235.08s\ttraining loss:\t3.325460\n",
      "Done 26000 batches in 1239.89s\ttraining loss:\t3.325565\n",
      "Done 26100 batches in 1244.74s\ttraining loss:\t3.325752\n",
      "Done 26200 batches in 1249.72s\ttraining loss:\t3.325842\n",
      "Done 26300 batches in 1254.76s\ttraining loss:\t3.325962\n",
      "Done 26400 batches in 1259.65s\ttraining loss:\t3.325664\n",
      "Done 26500 batches in 1263.73s\ttraining loss:\t3.324755\n",
      "Done 26600 batches in 1268.60s\ttraining loss:\t3.324928\n",
      "Done 26700 batches in 1273.37s\ttraining loss:\t3.325080\n",
      "Done 26800 batches in 1278.38s\ttraining loss:\t3.325223\n",
      "Done 26900 batches in 1283.31s\ttraining loss:\t3.325327\n",
      "Done 27000 batches in 1288.34s\ttraining loss:\t3.325504\n",
      "Done 27100 batches in 1293.11s\ttraining loss:\t3.325501\n",
      "Done 27200 batches in 1298.12s\ttraining loss:\t3.325670\n",
      "Done 27300 batches in 1303.18s\ttraining loss:\t3.325858\n",
      "Done 27400 batches in 1308.04s\ttraining loss:\t3.325860\n",
      "Done 27500 batches in 1312.97s\ttraining loss:\t3.325924\n",
      "Done 27600 batches in 1317.90s\ttraining loss:\t3.325995\n",
      "Done 27700 batches in 1322.95s\ttraining loss:\t3.326144\n",
      "Done 27800 batches in 1328.01s\ttraining loss:\t3.326177\n",
      "Done 27900 batches in 1332.91s\ttraining loss:\t3.326159\n",
      "Done 28000 batches in 1337.93s\ttraining loss:\t3.326284\n",
      "Done 28100 batches in 1342.87s\ttraining loss:\t3.326165\n",
      "Done 28200 batches in 1347.86s\ttraining loss:\t3.326165\n",
      "Done 28300 batches in 1351.71s\ttraining loss:\t3.325242\n",
      "Done 28400 batches in 1356.59s\ttraining loss:\t3.325370\n",
      "Done 28500 batches in 1360.85s\ttraining loss:\t3.324929\n",
      "Done 28600 batches in 1365.89s\ttraining loss:\t3.325054\n",
      "Done 28700 batches in 1370.93s\ttraining loss:\t3.325121\n",
      "Done 28800 batches in 1375.96s\ttraining loss:\t3.325219\n",
      "Done 28900 batches in 1381.00s\ttraining loss:\t3.325235\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.3252608175728415"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2ep\n",
    "\n",
    "hred_net.train_one_epoch(data_singles, 60, log_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hred_net.save_params('')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

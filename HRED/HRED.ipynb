{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 970 (CNMeM is enabled with initial size: 30.0% of memory, cuDNN 5005)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import time\n",
    "\n",
    "import lasagne as L\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../rnn_ex/')\n",
    "\n",
    "from HRED import HRED\n",
    "from mt_load import load_mt, get_mt_voc, get_w2v_embs\n",
    "from load_subtle import load_subtle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remember, now the pad value is the same as the <utt_end> token\n",
    "\n",
    "pad_value = -1 # <utt_end>'s vector is the last one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "Failed to interpret file '/pio/data/data/mtriples/Subtle_Dataset.triples.pkl' as a pickle",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-a96cd8c94280>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msubtle_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"/pio/data/data/mtriples/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtrain_subtle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_subtle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubtle_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/i258346/Desktop/masters_thesis/HRED/load_subtle.pyc\u001b[0m in \u001b[0;36mload_subtle\u001b[1;34m(path, split, trim)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_subtle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'Subtle_Dataset.triples.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/i258346/.local/lib/python2.7/site-packages/numpy/lib/npyio.pyc\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    427\u001b[0m             \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m                 raise IOError(\n\u001b[1;32m--> 429\u001b[1;33m                     \"Failed to interpret file %s as a pickle\" % repr(file))\n\u001b[0m\u001b[0;32m    430\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mown_fid\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: Failed to interpret file '/pio/data/data/mtriples/Subtle_Dataset.triples.pkl' as a pickle"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "subtle_path = \"/pio/data/data/mtriples/\"\n",
    "\n",
    "train_subtle = load_subtle(subtle_path, split=True, trim=200)\n",
    "print time.time() - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# mt_path = \"/pio/data/data/mtriples/\"\n",
    "mt_path = \"../DATA/MovieTriples_Dataset/\"\n",
    "\n",
    "train, valid, test = load_mt(path=mt_path, split=True, trim=200)\n",
    "idx_to_w, w_to_idx, voc_size, freqs = get_mt_voc(path=mt_path, train_len=len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word2vec_embs, word2vec_embs_mask = get_w2v_embs(path=mt_path)\n",
    "\n",
    "w2v_train_mask = np.where(word2vec_embs_mask[:,0] == 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model...\n",
      "Compiling theano functions...\n",
      "Building a network for generating...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "hred_net = HRED(voc_size=voc_size,\n",
    "                emb_size=300,\n",
    "                lv1_rec_size=300, \n",
    "                lv2_rec_size=300, \n",
    "                out_emb_size=300, \n",
    "                num_sampled=200,\n",
    "                ssoft_probs=freqs,\n",
    "                emb_init=word2vec_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hred_net.load_params('trained_models/subtleFixed_300_300_300_300_ssoft200unigr_bs30_cut200_early5.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_utt(utt):\n",
    "    return ' '.join([idx_to_w[x] if x != voc_size-1 else '<utt_end>' for x in utt])\n",
    "\n",
    "def rnd_next_word(probs, size=1):\n",
    "    return np.random.choice(np.append(np.arange(probs.shape[0]-1), -1).astype(np.int32), \n",
    "                            size=size, p=probs)\n",
    "\n",
    "def utt_to_array(utt):\n",
    "    arr = np.array([w_to_idx[w] for w in utt])[np.newaxis].astype(np.int32)\n",
    "    arr[arr == -voc_size] = -1\n",
    "    return arr\n",
    "\n",
    "def context_summary(context, lookup=True):\n",
    "    con_init = np.zeros((1, hred_net.lv2_rec_size), dtype=np.float32)\n",
    "    for utt in context:\n",
    "        con_init = hred_net.get_new_con_init_fn(utt_to_array(utt) if lookup else utt, con_init)\n",
    "    return con_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def beam_search(beam, dec_init, gamma=0, init_seq=np.array([[1]]), keep_penalty=True):\n",
    "    seq = np.repeat(init_seq.astype(np.int32), beam, axis=0)\n",
    "    probs, dec_init = hred_net.get_probs_and_new_dec_init_fn(seq, dec_init)\n",
    "    \n",
    "    words = probs[0].argpartition(-beam)[-beam:].astype(np.int32)\n",
    "    words[words==voc_size-1] = pad_value\n",
    "    scores = np.log(probs[0][words])\n",
    "    seq = np.hstack([seq, words[:, np.newaxis]])\n",
    "    \n",
    "    finished = []\n",
    "    \n",
    "    while seq.shape[1] < 50:# and len(finished) < beam:\n",
    "        probs, dec_init = hred_net.get_probs_and_new_dec_init_fn(seq[:,-1:], dec_init)\n",
    "        words = probs.argpartition(-beam, axis=1)[:, -beam:].astype(np.int32)\n",
    "        next_word_scores = np.log(probs[np.indices((beam, beam))[0], words])\n",
    "\n",
    "        new_scores = next_word_scores + scores[:, np.newaxis]\n",
    "        \n",
    "        # this line is for implementing https://arxiv.org/abs/1611.08562\n",
    "        new_scores_penalized = (new_scores + (new_scores.argsort(axis=1) + 1) * gamma).ravel()\n",
    "        \n",
    "        order = (-new_scores_penalized).argsort().astype(np.int32)\n",
    "        new_seq = []\n",
    "        new_dec_inits = []\n",
    "        scores = []\n",
    "        \n",
    "        new_scores = new_scores_penalized if keep_penalty else new_scores.ravel()\n",
    "        \n",
    "        for idx in order:\n",
    "            if len(new_seq) == beam:\n",
    "                break\n",
    "            \n",
    "            i,j = divmod(idx, beam)\n",
    "            \n",
    "            extended_seq = np.concatenate([seq[i], np.array([words[i,j]])])\n",
    "            if extended_seq[-1] == w_to_idx['</s>']:\n",
    "                finished.append((extended_seq, new_scores[idx]))\n",
    "            else:\n",
    "                new_seq.append(extended_seq)\n",
    "                new_dec_inits.append(dec_init[i])\n",
    "                scores.append(new_scores[idx])\n",
    "        \n",
    "        if not new_seq:\n",
    "            print 'Ending...'\n",
    "            break\n",
    "                \n",
    "        seq = np.array(new_seq)\n",
    "        scores = np.array(scores)\n",
    "        dec_init = np.array(new_dec_inits)\n",
    "    \n",
    "#     for utt, s in zip(seq, scores):\n",
    "#         print '{:.4f} {}'.format(s, print_utt(utt))\n",
    "#         print ''\n",
    "#     print '#########################################\\n'\n",
    "    \n",
    "    return finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: DBS (https://arxiv.org/pdf/1610.02424.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> i couldn ' t say . </s>\n",
      "<s> you were a prosecutor . </s>\n"
     ]
    }
   ],
   "source": [
    "context = map(lambda x: np.array(x, dtype=np.int32)[np.newaxis], train[99:101])\n",
    "lookup = False\n",
    "for u in context:\n",
    "    print print_utt(u[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "context = ['<s> hi . </s>'.split(), '<s> hello , what \\' s up ? </s>'.split()]\n",
    "lookup = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "context = ['<s> yeah , okay . </s>'.split(), '<s> well , i guess i \\' ll be going now . </s>'.split()]\n",
    "lookup = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "context= ['<s> what would the table think about if it could think ? </s>'.split()]\n",
    "lookup = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "context= ['<s> i saw a pretty good movie yesterday . </s>'.split()]\n",
    "lookup = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "context= ['<s> hi . </s>'.split()]\n",
    "lookup = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-20.876   <person> said something to go\n",
      "\n",
      "-23.479   you just went on in --\n",
      "\n",
      "-27.928   i didn ' t go to school .\n",
      "\n",
      "-30.696   i didn ' t go there all .\n",
      "\n",
      "-36.759   i didn ' t go to school to work .\n",
      "\n",
      "-41.050   i didn ' t go to school to leave you ?\n",
      "\n",
      "-49.771   i didn ' t go to school to leave you at that moment !\n",
      "\n",
      "-52.475   i didn ' t go to school to leave you to meet this party .\n",
      "\n",
      "-52.762   i didn ' t want any to hear of what it came after .\n",
      "\n",
      "-56.936   i didn ' t want any to hear of what it came after you .\n",
      "\n",
      "-63.469   i didn ' t go to school to leave you to meet this party with your boyfriend .\n",
      "\n",
      "-72.360   i didn ' t want any to hear of what he did , do , do the wrong thing\n",
      "\n",
      "-74.995   i didn ' t want any to hear of what he did , do , do the wrong act -\n",
      "\n",
      "-80.293   i didn ' t want any to hear of what he did , do , we should get in with .\n",
      "\n",
      "-85.641   i didn ' t go to school to leave you to meet this party with your boyfriend . she is so sorry !\n",
      "\n",
      "-88.973   i didn ' t go to school to leave you to meet this party with your boyfriend . she is in her bed\n",
      "\n",
      "-90.678   i didn ' t go to school to leave you to meet this party with your boyfriend . she is in her bed .\n",
      "\n",
      "-90.703   i didn ' t want any to hear of what he did , do , we should do some peace on his mother .\n",
      "\n",
      "-91.939   i didn ' t go to school to leave you to meet this party with your boyfriend . she is in her bed ?\n",
      "\n",
      "-92.099   i didn ' t want any to hear of what he did , do , we should do the way the next month .\n",
      "\n",
      "-98.457   i didn ' t want any to hear of what he did , do , we should do the way the next month , <person> -\n",
      "\n",
      "-100.945   i didn ' t want any to hear of what he did , do , we should do the way the next month , <unk> !\n",
      "\n",
      "-101.338   i didn ' t want any to hear of what he did , do , we should do the way the next month , <unk> .\n",
      "\n",
      "-105.515   i didn ' t want any to hear of what he did , do , we should do the way the next month , <unk> . <person>\n",
      "\n",
      "-108.445   i didn ' t want any to hear of what he did , do , we should do the way the next month , we could see --\n",
      "\n",
      "-121.496   i didn ' t want any to hear of what he did , do , we should do some peace on his mother . what would be the problem to ?\n",
      "\n",
      "-122.980   i didn ' t want any to hear of what he did , do , we should do the way the next month , we could see where <person> and he --\n",
      "\n",
      "-134.018   i didn ' t want any to hear of what he did , do , we should do the way the next month , we could see where they would say we met .\n",
      "\n",
      "-134.272   i didn ' t want any to hear of what he did , do , we should do the way the next month , we could see where they will have come over ?\n",
      "\n",
      "-138.328   i didn ' t want any to hear of what he did , do , we should do the way the next month , we could see where they would say that the whole night\n",
      "\n",
      "-142.273   i didn ' t want any to hear of what he did , do , we should do the way the next month , we could see where they would say that the whole family needs\n",
      "\n",
      "-146.190   i didn ' t want any to hear of what he did , do , we should do the way the next month , we could see where they would say that the time i wanted !\n",
      "\n",
      "-146.658   i didn ' t want any to hear of what he did , do , we should do the way the next month , we could see where they would say that i would ' ve lost\n",
      "\n",
      "-152.115   i didn ' t want any to hear of what he did , do , we should do the way the next month , we could see where they would say that the time i wanted to know ?\n",
      "\n",
      "-153.729   i didn ' t want any to hear of what he did , do , we should do the way the next month , we could see where they would say that the time i wanted to meet in\n",
      "\n",
      "-154.244   i didn ' t want any to hear of what he did , do , we should do the way the next month , we could see where they would say that the time i wanted to meet you\n",
      "\n",
      "-154.776   i didn ' t want any to hear of what he did , do , we should do the way the next month , we could see where they would say that the time i wanted to know about\n",
      "\n",
      "-155.371   i didn ' t want any to hear of what he did , do , we should do the way the next month , we could see where they would say that i would ' ve lost her head\n",
      "\n",
      "-161.910   i didn ' t want any to hear of what he did , do , we should do the way the next month , we could see where they would say that the time i wanted to meet you and we\n",
      "\n",
      "-163.532   i didn ' t want any to hear of what he did , do , we should do the way the next month , we could see where they would say that the time i wanted to meet <person> was there\n",
      "\n",
      "-169.747   i didn ' t want any to hear of what he did , do , we should do the way the next month , we could see where they would say that the time i wanted to meet you and we can .\n",
      "\n",
      "-171.350   i didn ' t want any to hear of what he did , do , we should do the way the next month , we could see where they would say that the time i wanted to meet you and we can do\n",
      "\n",
      "-173.903   i didn ' t want any to hear of what he did , do , we should do the way the next month , we could see where they would say that the time i wanted to meet you and we left here .\n",
      "\n",
      "-178.699   i didn ' t want any to hear of what he did , do , we should do the way the next month , we could see where they would say that the time i wanted to meet you and we can do whatever else\n",
      "\n",
      "-183.500   i didn ' t want any to hear of what he did , do , we should do the way the next month , we could see where they would say that the time i wanted to meet you and we can do whatever they know\n",
      "\n"
     ]
    }
   ],
   "source": [
    "beam_size = 10\n",
    "con_init = context_summary(context, lookup)\n",
    "W = L.layers.get_all_param_values(hred_net.train_net)[31]\n",
    "b = L.layers.get_all_param_values(hred_net.train_net)[32]\n",
    "dec_init = np.repeat(con_init.dot(W) + b, beam_size, axis=0)\n",
    "\n",
    "mean = False\n",
    "keep_penalty = not mean\n",
    "\n",
    "beamsearch = beam_search(beam_size, dec_init, gamma=-1, init_seq=utt_to_array('<s> '.split()), \n",
    "                         keep_penalty=keep_penalty)\n",
    "\n",
    "# print print_utt(beamsearch)\n",
    "\n",
    "\n",
    "len_bonus = lambda size: np.log(size)**3\n",
    "\n",
    "def fn_score(x, y, mean=mean, len_bonus=len_bonus):\n",
    "    denom = (x.size - 2) if mean else 1\n",
    "    return (y + len_bonus(x.size)) / denom\n",
    "\n",
    "for utt, scr in sorted(beamsearch, key=lambda (x,y): fn_score(x, y), reverse=True):\n",
    "    print '{:.3f}  '.format(fn_score(utt, scr)), print_utt(utt[1:-1])\n",
    "    print ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> you lied to me so many times -- </s>\n",
      "<s> reggie -- trust me once more -- please . </s>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "beam_search() got an unexpected keyword argument 'mean'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-175-81d01a05c34d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     beamsearch = beam_search(beam_size, dec_init, gamma=-.5, init_seq=utt_to_array('<s> '.split()), \n\u001b[0;32m---> 15\u001b[0;31m                              keep_penalty=True, mean=False)\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mprint_utt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeamsearch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: beam_search() got an unexpected keyword argument 'mean'"
     ]
    }
   ],
   "source": [
    "beam_size = 10\n",
    "\n",
    "for i in xrange(0, 100, 3):\n",
    "    context = map(lambda x: np.array(x, dtype=np.int32)[np.newaxis], train[i:i+2])\n",
    "    lookup = False\n",
    "    for u in context:\n",
    "        print print_utt(u[0])\n",
    "        \n",
    "    con_init = context_summary(context, lookup)\n",
    "    W = L.layers.get_all_param_values(hred_net.train_net)[31]\n",
    "    b = L.layers.get_all_param_values(hred_net.train_net)[32]\n",
    "    dec_init = np.repeat(con_init.dot(W) + b, beam_size, axis=0)\n",
    "\n",
    "    beamsearch = beam_search(beam_size, dec_init, gamma=-.5, init_seq=utt_to_array('<s> '.split()), \n",
    "                             keep_penalty=True, mean=False)\n",
    "\n",
    "    print print_utt(beamsearch)\n",
    "\n",
    "    print '\\n'\n",
    "\n",
    "# for utt in beamsearch:\n",
    "#     print_utt(utt)\n",
    "#     print ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 10 batches in 0.86s\ttraining loss:\t7.176166\n",
      "Done 20 batches in 1.69s\ttraining loss:\t6.175796\n",
      "Done 30 batches in 2.44s\ttraining loss:\t5.697121\n",
      "Done 40 batches in 3.47s\ttraining loss:\t5.374195\n",
      "Done 50 batches in 4.27s\ttraining loss:\t5.207509\n",
      "Done 60 batches in 5.11s\ttraining loss:\t5.063584\n",
      "Done 70 batches in 5.92s\ttraining loss:\t4.934647\n",
      "Done 80 batches in 6.69s\ttraining loss:\t4.840843\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-e0bc5d556518>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhred_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_one_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_subtle2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/i258346/Desktop/masters_thesis/rnn_ex/SimpleRNNLM.pyc\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[1;34m(self, train_data, batch_size, log_interval)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m             \u001b[0mnum_batch_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m             \u001b[0mtrain_err\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnum_batch_words\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m             \u001b[0mtrain_batches\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m             \u001b[0mnum_training_words\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnum_batch_words\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/i258346/.local/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    882\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 884\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/i258346/.local/lib/python2.7/site-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n)\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNoParams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    859\u001b[0m             \u001b[1;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 860\u001b[1;33m             \u001b[1;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    861\u001b[0m                 \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    862\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hred_net.train_one_epoch(train_subtle2, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 100 batches in 4.07s\n",
      "Done 200 batches in 8.39s\n",
      "Done 300 batches in 12.95s\n",
      "Done 400 batches in 17.68s\n",
      "Done 500 batches in 21.70s\n",
      "Done 600 batches in 26.34s\n",
      "Done 700 batches in 31.03s\n",
      "Done 800 batches in 35.56s\n",
      "Done 900 batches in 40.01s\n",
      "Done 1000 batches in 44.49s\n",
      "Done 1100 batches in 48.81s\n",
      "Done 1200 batches in 53.21s\n",
      "Done 1300 batches in 57.49s\n",
      "Done 1400 batches in 61.62s\n",
      "Done 1500 batches in 66.13s\n",
      "Done 1600 batches in 70.65s\n",
      "Done 1700 batches in 74.90s\n",
      "Done 1800 batches in 79.24s\n",
      "Done 1900 batches in 83.80s\n",
      "Done 2000 batches in 88.35s\n",
      "Done 2100 batches in 92.88s\n",
      "Done 2200 batches in 97.45s\n",
      "Done 2300 batches in 101.66s\n",
      "Done 2400 batches in 106.09s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.2819798801888118"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hred_net.validate(test, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''full softmax, bs=30'''\n",
    "# train, 1 dir, 1 epoch: 3.485554076321884\n",
    "# val: 3.455356876018342\n",
    "\n",
    "# train, 2 dir, concat, 1 epoch: 3.4864403798772239\n",
    "# val: 3.4579001751897063\n",
    "\n",
    "# train, 2 dir, L2 + concat, 1 epoch: 3.4881669768474675\n",
    "# val: 3.4584704095551695\n",
    "# training time: ~4700s\n",
    "\n",
    "'''sampled softmax'''\n",
    "# bs=30\n",
    "# train, 2 dir, L2 + concat, 1 epoch: 3.486180601246621\n",
    "# val: 3.4811877499289308\n",
    "# training time: ~2300s\n",
    "\n",
    "# bs=60\n",
    "# train, 2 dir, L2 + concat, 1 epoch: 3.5235153449672456\n",
    "# val: 3.5063306987542759\n",
    "# training time: ~1900s"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
